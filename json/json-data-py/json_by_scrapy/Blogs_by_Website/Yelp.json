[{"website": "Yelp", "title": "Whose Code is it Anyway?", "author": ["\n        \n  Luis Pérez, Software Engineer; Mitali Parthasarathy, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/01/whose-code-is-it-anyway.html", "abstract": "Improving Code Ownership at Yelp In this prior blog post , Kent talked about how the Engineering Effectiveness (EE) organization was created at Yelp to reduce communication complexity between core teams and product teams. Core teams need to communicate infrastructure changes, manage the deprecation of libraries and tools, and evangelize new tooling to other teams at a regular cadence. EE has been investing in building tools that can communicate these changes and provide insights into what might make product teams more effective in shipping code quickly and safely. In order to measure the engineering effectiveness of Yelp, we need to measure the effectiveness of its organizations and the teams that make up those organizations. But how do we know what a team is responsible for? We needed a way to assign an owner to something (let’s call this an entity ) that we want to measure. Once an entity has an owner, we can collect metrics on that entity and derive the health score (i.e., effectiveness) for that owner. These metrics can then be aggregated by team, organization, or even the entire Engineering division, so that we can identify areas that we can collectively improve. And this is how the Ownership microservice was born. Who is an Owner? Until now we have had different ways of defining an owner at Yelp. It could be a single engineer, engineering team, or engineering org. However, these definitions of ownership can be vulnerable to reorganizations (engineers moving teams or companies) or creative naming resulting in fictional teams owning code (the owner named “Super Friends plus Julia” was hard to track down to a real team!). Eventually, we settled on requiring that owners are canonical engineering teams (more on how we keep track of canonical teams later). When teams split, merge or get renamed, we expect the Ownership information to be updated to reflect the new teams that were formed. Why does Yelp value Ownership? Apart from being able to measure the effectiveness of a team, we generally think it is a good idea to know who owns certain code or infrastructure. Here are some reasons behind our thinking: Better for Planning Decision-making at Yelp happens in a distributed and decentralized manner. Teams are responsible for their own short and long term planning. Knowing what services a team is accountable for allows that team to better plan their roadmap for maintaining those services. They can choose which bugs to fix, what features need to be built and what services can be deprecated. Teams that are dependent on these services also get a good understanding of what they can expect and when. Better for Triaging Issues Alerting the right team to triage an incident as quickly as possible is critical to maintaining our uptime SLAs. Our monitoring and alerting systems can hook into the Ownership service to automatically alert the right team when an incident is reported. We also have Slack integrations for ad-hoc issue triage. Better for Collaboration As Yelp gets larger, it’s no longer feasible for everyone to memorize the core competencies of other teams. An engineer might need to get quickly ramped up on another team’s area of expertise to contribute a bug fix or feature that is blocking them. Having clear ownership helps to easily identify experts on teams and collaborate with them which exemplifies one of our core values of ‘playing well with others’. Avoid Duplication of Effort Prior to the development of the Ownership service, several teams across Yelp independently were developing their own specific ways to track ownership for org-wide migrations or infrastructure updates that they were responsible for. The Ownership service was implemented to combine all of these small independent efforts and reduce the duplication of work by individual teams. Keep things from falling through the cracks We have many systems running in production that don’t need constant maintenance because they run smoothly on their own, but without clear ownership, historical context and key information can get lost along the way. To avoid this, Ownership allows us to make sure that these types of systems don’t get dropped when, for example, the original developers move teams or leave the company. What can be owned? Pretty much anything - especially anything where knowing whom to contact is important! We currently track the ownership of a wide range of entities: from API endpoints to Debian packages to automated tests. The most common entity that can be owned is a Git repository. Code lives in repositories and we can assign the entire repository to an owner by creating a YAML file at the root of the repository named OWNERS . Here’s an example of an OWNERS file for the Ownership service (very meta!): teams : - Developer Insights & Automation Ownership can also be assigned on a directory level by creating the OWNERS file for each directory. This is useful in the case of monorepos (for example: our Android or iOS apps). We originally had more information in this file like team members, Slack channels, Jira projects, etc., but we now query our HR software for that in order to centralize where information comes from and maintain its accuracy. We also created Git pre-receive and pre-commit hooks that will check for the presence of (and validate!) an OWNERS file to ensure that we are always assigning canonical owners to repositories. This validation will also try to suggest potential canonical teams based on a team lookup of the developer pushing any invalid ownership change. We can also assign ownership to Slack channels, Jira projects, and PagerDuty schedules and escalation policies which helps with having the right process in place to reach out to an expert or alert the right team during an incident. Ownership Ecosystem Overview of how Ownership works The Ownership service provides APIs to create, update and delete owners and entities. Entities can also be assigned one or more owners. Indexers are batch jobs that run periodically to detect entities and update their ownership in the service. These jobs collect information from all the sources of ownership information and store them in the database. For example, we have a job that reads OWNERS files and populates the ‘Git repository’ entity along with its owner into the Ownership database. Another example is the indexer that pulls data from our HR software and stores team information in the database. We also have a UI to view and edit general information about entities and owners, and automate the process of executing ownership changes related to team changes. How do we use Ownership? Now that we are able to attribute entities to a team, we can track the health of the team by auditing these entities. We use Ownership to power the EE Metrics tool that can surface a number of audits in the areas of developer velocity, reliability and code quality pertaining to each team. More about how EE Metrics works will be in a future blog post! As more tools start adopting the Ownership service, we discover more use cases for entities that would be valuable to own. One example is tracking a team’s on-call burden by associating a PagerDuty escalation policy with an engineering team. Another use case is to track the outstanding action items after an incident in the form of JIRA tickets. The Ownership service can be easily extended to create a new indexer to ingest new entities that we want to audit. Anyone can use our web frontend to look up ownership information: Example of an ownership information page Alternatively, a developer can also use our API to quickly look up an owner’s information for use-cases that aren’t conducive to a web UI: e.g., automatically adding reviewers to a code review, automated Slack messaging, etc. Keeping Ownership Updated One of the challenges with Ownership is keeping it updated. Entity ownership is not static and there are times when the team responsible for an entity can change, for example, due to an engineering-wide reorganization or split of a single team across functional lines. This entropy has historically made it difficult to maintain an accurate source of truth for ownership. That’s why we’ve created a web frontend to the Ownership service that allows managers and leads to not only visualize what their team currently owns, but also to easily transfer entities to other teams. Example usage of migration UI Additionally, we periodically validate that certain metadata about a team are still valid (e.g., Slack channels haven’t been archived, Jira projects aren’t read-only, etc.) and automatically ticket managers to resolve any issues found. Towards the future! We’re heartened by all the internal usages of Ownership (ranging from Slack bots that query Ownership to automated Pull Requests using Ownership to assign code reviewers) and are excited to see any and all future applications of Ownership to come. Teams are happy to have a single source of truth and the ability to easily update ownership information by themselves. Acknowledgements We’d also like to thank the following people for helping us make Ownership a reality: Anja Berens ( pharaun ), Software Engineer James Flinn, Software Engineer Jonathan Chu, Software Engineer Kyle Deal, Software Engineer Tweet Become an Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2021-01-13"}, {"website": "Yelp", "title": "Passwordless Login: Reengaging Business Owners with Less Friction", "author": ["\n        \n  Krista Poon, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/03/passwordless-login.html", "abstract": "As various teams at Yelp were focused on developing features to help businesses adapt to COVID-19, some teams were looking ahead and developing features that would help businesses in the later stages or after the pandemic. The Challenge Early on in the pandemic, we saw some businesses pause advertising on Yelp as government regulations required many businesses temporarily close or limit their operations. However, businesses quickly adjusted to the local regulations, while implementing health and safety precautions to keep their staff and customers safe. Through this adjustment we wanted to ensure it was easy to restart advertising right where they left off. Our data revealed that as of April 2017, most business owners used password-based logins to sign into their business owner account. However, if they forgot their password, it could be a frustrating experience for them to continue into the app. A typical Reset Password flow looked like: After receiving their Reset Password link, they were presented with Depending on what the users entered into the two input boxes, they could receive the following error messages: Please enter a password Oops, the passwords you entered don’t match! Please choose a password of at least 6 characters This password is insecure, please try a different one. This password has been used in the past year. Please enter a different password. … and more… Our data showed that we sent one of these errors about 7,500 times a day. Our Approach To resolve this, our solution was to remove the need to authenticate a business owner with a password by creating a passwordless login. Yelp will send a unique link (called Magic Links) which are short-lived (ie. 1-hour to up to 3 days) links that provide automatic login functionality. A Magic Link will automatically open the Yelp for Business app, verify a business owner’s credentials, log them in, and then optionally redirect them to anywhere in the app of our choosing. Magic Links are one-time use and time sensitive, so the links will eventually expire if they aren’t used. To unlock the full capabilities of this feature, we also appended each Magic Link with a redirect link that takes them to a specific page after automatic successful login. The redirect link can consist of any deeplink that we already currently support. Particularly for this initiative, we redirected our users to our One Click Restart screen, which allowed business owners to restart their ads with Yelp. We have implemented this logic on Android, iOS, and the web. Even if business owners did not have the Yelp for Businesses app installed on their device, they can still take advantage of this feature. Cool! But how is this magical? With this solution, we are able to provide a seamless user journey for business owners to the end goal, securely and with only one click. Technically, it was a creative and innovative solution. Figure 1 shows the original status quo before we implemented Magic Links and Figure 2 shows the sequence of steps after implementing Magic Links. At Yelp, UrlCatcherActivities are Activities that are responsible for handling deeplinks. We have deeplinks that are preceded with http://biz.yelp.com or yelp-biz://. Given two deeplinks that are exactly the same with the exception of the host, the app could reroute them differently. Since Magic Links could be sent via device notifications or emails, we needed to support both URI hosts. The MagicLinkUrlCatcherActivity was responsible for intercepting all Magic Links via the Android Manifest and acting on it. It validated the Magic Link and provided feedback for both successful and unsuccessful validations. Our Magic Link schemas looked like this:\nhttps://biz.yelp.com/login/passwordless/ ?return_url=https://biz.yelp.com/ads/i2kK8NtpmtuKf84NYm0d3A/ An invalid Magic Link could consist of an expired, malformed, or missing MAGICLINKTOKEN. On successful validation, we logged the user into the app. The return_url is an optional parameter. If it is present and also a valid deeplink that we supported, we then forwarded the redirect url embedded in the Magic Link to downstream UrlCatchersActivities. From that point on, the app behaved as status quo. If the return_url was not specified, we redirected to the home screen. On unsuccessful validation, the activity is responsible for redirecting the user to the Log In screen so that users may try to enter in their credentials manually. If successful, we redirected the user to the embedded link within the Magic Link. We only wrote Magic Link related logic once When the project first began, we had (naively) thought that this project would be simple (refer to Figure 1). Our initial strategy was to write the Magic Link logic in both the UrlCatcherActivities, but like most projects, the more we worked on it the more we realized that there were a lot of edge cases we had to handle. Accounting for each edge case for each UrlCatcherActivity would duplicate code and double our blast radius. On top of that, each requirement change, no matter the size, would have to be duplicated. We quickly realized that we should refactor all the code into one place sooner rather than later. The Magic Link high level logic (illustrated by diagram below) was refactored into the MagicLinkUrlCatcherActivity. We did not change any existing downstream logic Each UrlCatcherActivity already contained business logic that required some time to understand. By moving all Magic Link related logic into the MagicLinkUrlCatcherActivity, we only passed the optional redirect url into the downstream logic. We isolated our testing to the MagicLinkUrlCatcherActivity alone We didn’t need to test this end-to-end. We only had to concentrate on 4 main areas: Input validation We validated that all deeplinks into the app were successfully triaged by the AndroidManifest to either go to one of the downstream UrlCatcherActivities or the MagicLinkUrlCatcherActivity Magic Link validation We tested that the MagicLinkUrlCatcherActivity was able to handle successful and unsuccessful validation of the Magic Link. Redirect links are passed to the correct Catcher Lastly, we wrote tests to ensure that the embedded redirect links within the Magic Link would get passed to the correct downstream UrlCatcherActivities. Also verify when there were no redirect links passed. Analytics We verified that the correct analytics were fired at specific points in the code so that Yelp can track usage and other metrics of interest. Using Magic Links together with deeplinks reduced friction for our business owners to log into their accounts and resume advertising.. By making passwords obsolete on login we reduced user churn resulting from abandoned password resets. We hope this feature will help our business owners get the word out about their business and better communicate and engage with their customers. Acknowledgements Shoutout goes to Karlo Pagtakhan and Khushboo Puneet for working on this with me! Also, thank you to Blake Larkin, Eric Hernandez, Rajan Roy, Joshua Walstrom, Patrick Fitzgerald, and Mark Brady for technical review and editing. Tweet Become an Android Engineer at Yelp! We're working on cool interesting problems everyday! Come join our Android team! View Job Back to blog", "date": "2021-03-01"}, {"website": "Yelp", "title": "Boosting user conversion with UX performance wins", "author": ["\n        \n  Shubham Gupta, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/01/boosting-user-conversion-with-ux-performance-wins.html", "abstract": "Everyone loves graphs going up and to the right, unless they reflect your page load timings. This blog post is about curtailing higher page load times. Yelp for Business Owners allows business owners to manage their listing, respond to reviews, edit their business information, upload business photos, and more. Business owners can also purchase Yelp Ads and profile products to target local audiences and enhance their business’s presence on Yelp. In this blog post, you’ll learn about the ways we improved the UX performance of our ads purchase flow by dramatically reducing the load times. You’ll be able to apply the same tactics to your own flow and hopefully achieve results similar to ours: Background Our core ads purchase flow is a single-page React application powered by Python-based backend services and GraphQL. Over the past couple of years, it has grown from a four step process to a seven step process with new features to provide better ad campaign controls. However, as we added more features, performance suffered. Our page-load P75 timings increased from 3 seconds to 6 seconds for desktop users. This slowdown was even more pronounced for our mobile users due to increased constraints in network speeds and reliability. It's a known fact that faster-loading pages directly benefit user conversion. We wanted to measure how much faster performance affected the bottom line and ran a lightweight experiment to measure the relationship between performance and user conversion. We made some backend optimizations to reduce page load timings by one second, and immediately observed a 12% relative increase in conversion rate. This early win gave us confidence in our future investments along with full buy-in and support from our product team. Performance Metrics The first step in our performance effort was to set up a framework that would standardize the metrics and logging across all our flows. We decided to target two specific metrics: First Contentful Paint (FCP) FCP is the browser’s time spent rendering any image or text after sending the page load request. It is widely accepted as a key metric in the industry to measure your web page’s performance. Targeting FCPs was critical because it hints to the user that their page is starting to load. During our experimentation, we found that a user was much more likely to leave our site during a page load than after they saw any content, even if they only saw a loading spinner. Since a page load event depends on multiple systems (such as web browser, routing layers, authentication proxies, etc.), we further broke down our FCP into the following units to help categorize our efforts: Redirect time: How long the browser spent following redirects (HTTP 303s). Request time: How long the request-response cycle took for the main request inside Yelp servers. Rendering time: How long it took for the browser to render the first contentful paint after receiving the initial response. The image below shows the breakdown of our timings in the units discussed above. Time to Interactive (TTI) This metric measures the time spent by the browser to fully load a page. It captures any client-side rendering logic and async data fetching required to render the complete user experience. At Yelp, we  call this metric Yelp Page Complete (YPC). It is critical to capture TTI since many of our applications render a shimmer or a page shell after the initial page load, and then the respective components fetch their data. TTI helps capture the entire user experience timings. We have several other similar flows on the biz site with their own data fetching strategy. To make the integration convenient across all of them, we created a shared JavaScript package to consolidate all of the logic related to logging, polyfills, batching/throttling of logging related AJAX calls, etc. In the end, the integration only required adding a couple of lines to start logging all the performance metrics. Performance Tooling We relied on many tools that were critical to our effort that are worth mentioning here: Zipkin OpenZipkin is an open-source distributed tracing system set up at Yelp. It helped identify bottlenecks during the request lifecycle inside Yelp servers. Our request travels through multiple services, and this tool was indispensable in identifying potential optimizations on the backend. Here is a sample Zipkin trace: Source: https://zipkin.io/ Webpack Bundle Analyzer Webpack Bundle Analyzer helped us visualize our JavaScript bundles’ content with an interactive zoomable treemap. This tool was crucial to identify optimizations in our frontend assets that we discuss later. Below is a sample treemap interaction from the plugin’s Github repository: Source: https://github.com/webpack-contrib/webpack-bundle-analyzer Splunk We ingested all of our performance metrics in Redshift database tables and visualized them as Splunk dashboards. These helped us track our progress in real-time while deploying changes. Below is an example dashboard: Chrome DevTools Chrome’s tooling provided terrific insights into our frontend performance issues. We specifically relied on the flame charts under the Performance tab to identify where the browser’s main thread was blocking and how much time was being spent in our assets loading, parsing, and evaluation. Google Lighthouse also provided actionable opportunities and diagnostic information. Deep Dive After learning from the gathered metrics, we planned on tackling the performance issue on all fronts: backend, frontend, and infrastructure. Below are a few things that are worth sharing: Frontend Optimizations Our JavaScript bundles had been growing slowly due to continuous feature additions over the past couple of years. Yelp’s in-house tooling already enforced general best practices such as gzip compression, bundle minification, dead code elimination, etc. So most of the issues were part of our application setup. After analyzing our bundle using the tooling above, we employed various techniques listed below to reduce our gzipped bundle size from 576 KB to 312 KB, almost 50% reduction! Code Splitting : Serving code for all the seven pages of our purchase flow during the initial page load was undoubtedly wasteful. We opted to use loadable components to create separate chunks for different steps that would load on demand. This chunking helped reduce our bundle size by 15%. However, loading these assets on demand added a small delay on every page load, so we wrote a helper function to preload all the chunks using the useful requestIdleCallback function to avoid any UX behavior changes. Tree Shaking : Yelp’s recent default Webpack settings enable dead code elimination. Looking into our bundle treemaps, we realized that tree shaking wasn’t working for some of our older packages because they were still using older build settings. So, a hunt began to figure out all such packages, and we ended up further reducing our bundle size by 30% by just upgrading their build. Replacing Packages with Heavy Footprint : We identified a few packages being used infrequently in our code that occupied an unreasonable portion of our bundle. The primary example was moment.js that was used only twice but occupied 5% of the bundle. We were able to replace it with date-fns , which is tree-shakeable. Fun fact: the project status of momentjs now itself recommends using alternatives. Deduplicating Packages : We use Yarn for our dependency management, and (before Yarn V2) it didn’t deduplicate the packages with overlapping ranges. For our large apps, deduplication had a noticeable impact on our bundle sizes. Yarn V2 helped solve this problem for us. Reducing Component Re-rendering: React profiler identified that specific core page components such as the navigation bar were re-rendering wastefully during the page load. This re-rendering blocked the main thread and delayed FCP. We resolved this by adding memoization on top of these components. Server-side Optimizations Yelp’s growing service architecture presented some interesting roadblocks. As the request traveled through multiple services (including a monolith), its lifecycle was complicated. For example, the page-load request went through 3 services and depended upon up to 5 downstream services for fetching data. The efforts listed below helped us bring down our request timings: Removing Proxy Layers : All biz site requests were proxied through Yelp’s monolith because it handled authentication and authorization for logged-in business owners. This proxy was expensive. Earlier this year, we packaged up the authentication and authorization business logic into a reusable Python package. This optimization entailed integrating with that package, setting our service up to accept traffic directly from our routing layer, and rolling it out carefully via dark-launching . It helped us save 250ms from our request time along by getting rid of legacy code. Parallelizing Network Calls: We rely on several downstream services for fetching data during page load. Zipkin helped us uncover that we had laid out some of our network calls in a blocking manner that slowed down the entire request. At Yelp, we use Futures built with Bravado , which allows us to send network requests concurrently. We rewrote the request code to fire off all the network requests at the top of business logic and avoided starting any new network request later in the code. It helped us shave 300ms from our request timings. While this issue can regress, we documented best practices for this behavior to help prevent them in the future. Eliminating Redirects : Legacy pages, old flows, third-party blog posts, etc., contributed to redirects before the user landed on the final URL/page. These redirects were a few seconds in some cases for our mobile traffic. We documented all of the redirects using the HTTP Referer header and tackled them accordingly. Server-side Rendering : Before this effort, our flow was rendered entirely client-side, i.e., we didn’t send any HTML in the request’s response. We only sent the JavaScript bundle and relied entirely on the browser and React app to generate HTML for serving the page’s content. We identified that this adversely affected our FCP, especially on mobile clients with limited CPU and memory. We already had a (React) component rendering service based on Hypernova set up at Yelp. We integrated with that service and started rendering the first page’s markup from the server. We immediately saw significant benefits for all the clients. We transferred the rendering load to the server, as evident in the graphs below, but the rendering time took a steep drop and the net impact was lower FCP time. Also, long gone was our loading shimmer! Pre-warming Cache: We have a few computationally expensive tasks in our requests, such as building a category tree object created by reading configurations from disk. We cached these objects in memory, but we identified that our higher latency P90 requests still suffered because they would always get a cache miss. We created an internal endpoint whose sole responsibility was to warm up all the caches and create expensive cacheable objects. We used a uWSGI hook that would be called every time a worker was created to make a call to this internal endpoint. It helped bring our P95s down by almost 2 seconds across all clients. Vertical Scaling: Last but not least, we also tried deploying our service and its dependent services on highly performant z1d.6xlarge EC2 instances . We saw marginal improvements (up to 100msec) on page load timings, but some of the other computationally expensive AJAX APIs saw more significant gains. For example, our POST endpoint responsible for purchasing the products got 20% faster, leading to lower timeouts. Final results After four months of focused effort with a dedicated engineering team, we achieved results that made this investment worthwhile. It was not just a win for our conversion metrics, but also for our customers, who now experienced substantially faster loading pages. The keys results that we achieved for our ads purchase flow: We reduced our P75 FCPs from 3.25s to 1.80s, a 45% improvement. We reduced our p75 YPCs from 4.31s to 3.21s, a 25% improvement. We saw up to 15% lift in our conversion rate. Below are a couple of graphs that show our progress over time: Acknowledgements Shoutout to my teammates on this project: Thibault Ravera, Bobby Roeder, Frank She, Austin Tai, Yang Wang and Matt Wen. Shoutout to Dennis Coldwell, Aaron Gurin, Blake Larkin and Alex Levy for technical review and editing. Tweet Become an Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2021-01-27"}, {"website": "Yelp", "title": "Powering Messaging Enabledness with Yelp's Data Infrastructure", "author": ["\n        \n  Billy Barbaro, Tech Lead; John Stimac, Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/04/powering-messaging-enabledness-with-yelps-data-infrastructure.html", "abstract": "In addition to helping people find great places to eat, Yelp connects people with great local professionals to help them accomplish tasks like making their next big move, fixing that leaky faucet, or repairing a broken phone screen. Instead of spending time calling several businesses, users can utilize Yelp’s Request a Quote feature to reach out to several businesses at once, receive cost estimates from those businesses, and ultimately hire the right local professional for the job. This post focuses on how Yelp’s Data Pipeline is used to efficiently compute which businesses are eligible for the feature, and also introduces Yelp’s Data Lake, which we use to track historical values of the feature for offline metrics and analytics. Background While most businesses can be reached via the phone number listed on their Yelp business page,  only a subset of businesses are eligible for Yelp’s messaging feature (at least for now!). We refer to the ability for a business to receive messages from users as “messaging enabledness” (or sometimes, just enabledness). It is determined by checking several different conditions about the business. For instance, the business owner must opt-in to the feature and they must have a valid email address so they can be notified about new messages, among other things. Computing messaging enabledness is tricky since checking all the criteria requires joining and fetching values from several different SQL tables. For some features, like deciding whether or not to display the “Request a Quote” button on a business’s page, it’s essential to correctly identify a business’s enabledness, even if it takes extra time to perform all those joins. For other applications of the data, such as analysis or indexing, we can tolerate the risk of a stale value in order to speed things up, so a cached mapping of an identifier for the business (business_id) to its messaging enabledness is stored in its own SQL table. This is kept up to date by a batch which runs periodically to recompute the value for all businesses. Storing Historical Changes In addition to storing the current state of enabledness, Yelp is also interested in persisting a historical record of messaging enabledness for businesses. This allows the company to measure the health of the Request a Quote feature in addition to being an invaluable source of information when investigating any pesky bugs that might pop up. There are millions of businesses listed on Yelp, so storing this history in a SQL table is not efficient in terms of storage cost or query time. Another option was to store a nightly snapshot of the table, but that would have resulted in duplicated information day to day, would have been more difficult to query, and wouldn’t have captured multiple changes to the same business in a single day. What we really want to store is a change log of the table. Remember that this data is stored in a SQL table. If you’ve been following along with the Data Pipeline posts on the Yelp engineering blog, you’ll know that Yelp has developed a tool called the Replication Handler which publishes a message to our Kafka infrastructure for every update to a SQL database. By connecting this tool to the table caching businesses’ messaging enabledness, a full history of changes can be written to a Kafka stream. Now if only we had a way to store this stream… Yelp’s Data Lake Yelp’s Data Lake is our solution for storing schematized data at scale. Our Data Lake is built on top of the Apache Parquet format, Amazon S3, and Amazon Glue. This S3 based architecture allows us to cheaply store data, making it possible for us to keep records over a long period of time. Our Data Lake implementation also integrates with our in-house schema management system, Schematizer . Data from Kafka can easily flow into our Data Lake through our Data Lake Sink Connector. The connector provides a fully-managed way for moving data to the Data Lake, without engineers having to worry about any underlying infrastructure. All engineers need to do is specify which data they want in the Data Lake, either though our datapipe CLI tool or through our Pipeline Studio web UI. $ datapipe datalake add-connection --namespace main --source message_enabledness\nData connection created successfully\nConnection #9876\n  Namespace:         main\n  Source:            message_enabledness\n  Destination:       datalake Once in the Data Lake, data can power a wide variety of analytic systems. Data can be read with Amazon Athena or from Spark jobs. Using Redshift Spectrum, we also allow analytics from Redshift, where Data Lake data can be joined with data we put in Redshift using our Redshift Sink Connector . Redshift Spectrum can also be used to power Tableau dashboards based on Data Lake data. Getting More Granular We previously mentioned that the messaging enabledness table was updated periodically. Even though changes to the table are being persisted to the Data Lake, with this approach we’re not able to identify the time the change happened and have no way to tell why the value changed (i.e. which of the criteria triggered this change). In order to catch these changes in real time, each time an update happens that might affect a business’s enabledness, an asynchronous task can be submitted to recompute the value and store it in the table along with the reason for the change. The code looks something like this: def update_value(business_id, new_value):\n    update_value_for_business(business_id, new_value)\n    update_enabledness_async(business_id, reason)\n\ndef update_enabledness_async(business_id, reason):\n    current_enabledness = get_enabledness_from_cache(business_id)\n    updated_enabledness = compute_enabledness(business_id)\n    if not current_enabledness == updated_enabledness:\n        set_enabledness(business_id, updated_enabledness, reason) While this works, you might be able to spot a shortcoming: anytime an engineer adds a new way to update a value which might change a business’s enabledness, they also need to remember to call the update_enabledness_async method. Even though this approach might capture all the changes when it is first written, as the code evolves over time a single mistake can cause the data stored in the table to be inaccurate. def update_value_v2(business_id, new_value)\n    update_value_for_business(business_id, new_value)\n    # Something is missing here... Reacting to the Changes Looking more closely at the system above, there is something peculiar about the update call. Recomputing enabledness isn’t really an operation that should happen after one of the criteria was updated. Instead it happens as a result of the value being changed. Rather than depending on engineers and code reviewers to remember that the update_enabledness_async function must be triggered manually, what if we could build a system that triggered this update as a result of the change? Each of the criteria for enabledness is stored in a SQL table, and as we discussed earlier in the post, the Replication Handler can be used to publish changes to those tables to Yelp’s Data Pipeline! Consumers of those topics (specifically, Paastorm spolts ) can be set up to call the update_enabledness_async task on any relevant change! Conclusion This post introduces the Yelp Data Lake and demonstrates how the Data Lake Sink Connector makes it easy to track the historical value of a business’s messaging enabledness. It also shows how the Streaming Infrastructure you’ve read about in previous blog posts (or at least the ones you’re about to read right after you finish this one!) is used to solve real engineering problems at Yelp, allowing systems to react to data changes without the need to write custom components or complex logic. Acknowledgements Thanks to Mohammad Mohtasham, Vipul Singh, Francesco Di Chiara, and Stuart Elston who assisted at various stages of design and implementation of this project. Thanks to Blake Larkin, William O’Connor, and Ryan Irwin for technical review and editing. Tweet Become an Engineer at Yelp Are you interested in using streaming infrastructure to help solve tough engineering problems? View Job Back to blog", "date": "2021-04-05"}, {"website": "Yelp", "title": "Engineering Career Series: Building a happy, diverse, and inclusive engineering team", "author": ["\n        \n  Sam Eaton, Chief Technology Officer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/04/engineering-career-series-building-a-happy-diverse-and-inclusive-engineering-team.html", "abstract": "I considered writing this as a clickbaity listicle: “7 secrets of engineering team management - you won’t believe number three!” Unfortunately that’s impossible, because it’s a much harder topic, and anyway, number three is: “many years of ongoing investment in building the right team culture, making a lot of mistakes, and learning from them.” Less catchy, but much more what this series is going to try and cover… I’ve been at Yelp for eight years now, and I’ve been leading engineering teams for almost 25 years in both the UK and the US, at a wide variety of companies, at different scales and stages of their development, and in very different parts of the technology industry. Over that time, with the assistance of many of my colleagues and mentors, I’ve developed a set of principles that guide my approach to management and building engineering team cultures. When I joined Yelp, I found a company with values that aligned very well with these principles, at all levels of the company, so I’ve been lucky to be able to try and apply them thoroughly in practice here. A loose set of principles Teamwork matters more than individual brilliance People are indeed individually brilliant, and everyone has unique life experience and talents to contribute to their work. However, it takes teams to build something at the scale of Yelp. Building a culture that values empathy and teamwork pays dividends. A corollary of this is that a strong “no assholes” rule is vital. Diversity leads to success, but only if there’s equity, inclusion, and belonging There’s plenty of evidence that diversity makes teams more effective , but that doesn’t mean that just hiring a diverse team automatically leads to success. To really succeed, you have to build a company culture where you genuinely deliver an inclusive and equitable experience for everyone. Building and cultivating a culture where everyone can thrive and feel like they belong requires you to constantly examine what you’re doing as a company and what the real impact of it is on your teams. That includes listening to people’s lived experiences and constantly trying to improve. Distributed teams help diversity It’s a lot easier to build truly diverse teams if you’re not limited to having to hire people near the places you have offices. We’d already been hiring in multiple countries for some years at Yelp. Re-examining remote work and distributed teams during the pandemic has highlighted both the scale of the opportunity to really build teams that “meet people where they live,” but also  the challenges in building successful distributed teams, abandoning the idea of a “head office” and creating a culture where everyone has an equal opportunity to succeed. “No process” is another name for “bias” It’s really easy to have no process in small organizations - which is where you start by default, and the flexibility of not having a process offers lots of advantages at first. The thing is, you never really have no process, you just have a process that you’ve never written down and examined critically. And processes that you’ve never examined critically generally hide a world of unexamined unfairness, even with the best of intentions. You need to articulate and examine what these implicit processes are and make them more explicit, to eliminate that unfairness and the biased outcomes it produces. That means looking in depth at how you hire, how career advancement works, how you compensate people, how you think about technical leadership, and many other more innocuous seeming things where you encode unintentional biases into the system and culture of your company, influencing the likelihood that different people thrive or fail. You have to walk the walk It’s no use just saying you want to be better at building diverse, inclusive, happy teams. You need to actually change things, measure the results of your changes, look at that data, and then try and improve things again. This continuous iteration driven by data is vital, you must be really transparent and accountable about what you’re doing, and its successes and failures. This directly relates to the previous principle about process, but fundamentally underpins every effort to improve here. And yes, it’s hard. And you will fall on your face sometimes, publicly. And it will hurt. And you need to get up again and keep trying, because that’s the only way things will improve. The series Rather than just hearing from me on how we’ve approached trying to live up to some of these principles at Yelp, we have a series of blog posts over the coming months to further explain. These blog posts will go into detail on the how as well as the why, and share some of what we’ve tried, what worked and what didn’t, in an attempt to give back to the many people whose ideas and learning we’ve built on over the years. Over the next few months we’ll cover: Hiring a diverse team: reducing bias in engineering interviews How Yelp has approached hiring over the years, and the major lessons we learned. Once we started to standardise our approach to interviewing, we were able to analyse the data to find out if we were actually living up to our good intentions. How we onboard engineers across the world at Yelp Once you’ve hired someone amazing, you need to set them up for success on day one. The initial onboarding is vital, but is only part of the process. We’ve found that it’s critical to have a strong mentorship program for new hires, and that means choosing the right people to mentor and train them well. Mentorship doesn’t just stop at onboarding either, so we run an ongoing training and career development program to make sure people from diverse backgrounds can all succeed at Yelp. Career paths for engineers at Yelp Yelp previously had a completely flat “no levels” individual contributor career framework for Engineering. We’ll cover how we designed and redesigned our framework for career growth and levelling to move away from that, and discuss how that shift increased fairness and equity. Technical leadership at Yelp Why we approach technical leadership as a role you can choose to take on at Yelp, rather than just a level within our career levelling framework, and how we’ve tried to build a collaborative, cross-pollinating community of technical leaders who work together regularly to solve “big picture” problems, rather than just being experts in their own fields. How Yelp approaches engineering management What “success” looks like for managers at Yelp, what we ask managers to do and to value, how we’ve built this into the career path for managers, and how we hire and onboard them. Ensuring pay equity & career progression in Yelp Engineering “Walking the walk” meant actually examining in detail how we compensated people and how they progressed in their career, and whether that was actually fair and equitable across all demographics at Yelp. And then publishing the outcomes to the whole Engineering team and committing to do so annually, whatever the results were. Fostering inclusion & belonging within Yelp Engineering Improving inclusion and belonging requires you to provide for teams and groups in many different ways, like supporting Employee Resource Groups to encourage communities to socialise, collaborate, and empower themselves, providing flexible working practices to suit people with different needs, abilities, and lifestyles, as well as designing systems and processes that give people the support they need in the time and place and manner they need it. I hope you’ll find this series informative and helpful. I welcome the opportunity to share our triumphs and setbacks with you, and look forward to the feedback on what we’re doing well, and what we still need to learn to do better. And last but not least, if this sounds like the kind of company culture that you’d like to be a part of, and you’d like to help make it better… we’re hiring ! Tweet Back to blog", "date": "2021-04-08"}, {"website": "Yelp", "title": "Engineering Career Series: Hiring a diverse team by reducing bias", "author": ["\n        \n  Elias Alberts, Director of Technical Talent\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/04/engineering-career-series-hiring-a-diverse-team-by-reducing-bias.html", "abstract": "Compared to where we started, Yelp’s technical organization has made a lot of headway over the years when it comes to diverse hiring. While our approach to this work continues to evolve, we’ve made significant progress in improving the diversity of our organizations by, among other things, reducing gender and ethnicity bias in our interview process. We’re here to share some of what we’ve learned to help others in their own efforts. If you’ve come looking for the secret formula to emulate our success, I can’t help you there, unfortunately. Anyone offering otherwise is probably selling you something. And, to be sure, you’re going to need to buy some things along the way. But, if that newest iteration of Bias Blaster 9000 sounds too good to be true, that’s because it is. There are no easy fixes here. In the 9 years I’ve been with Yelp, we have taken several major strides to evolve our hiring processes and strategy that got us to where we are today. What I’m covering in this blog is arguably the most critical of the changes we’ve made: tracking every bit of data possible and running regular analyses to better diagnose the bias in our engineering interviews. A Data Oriented Approach Today, we’re monitoring every stage of every candidate’s interview process, as well as several data points about the candidates themselves. We track how many candidates apply organically versus how many are sourced and how each group performs on the first round interview. We monitor the offer rates by gender identity and how each group is converting to offer acceptance. We’re able to determine down to the level of individual questions in our interview process whether they are being passed at equal rates by all people being interviewed. To know these things, we’ve become deliberate about tracking data and analyzing it. We automatically publish daily updates to a host of dashboards that monitor the health of our pipelines. We report weekly on the state of our hiring pipelines so that we can make adjustments as needed. We don’t make changes to our interview process without first knowing that we can measure the effects. With these procedures in place, we are truly able to systematically identify and address problems. We’ve come a long way from where we started. It sounds somewhat absurd in hindsight, but early on during my tenure at Yelp, we didn’t even know how many people we needed to hire. We just knew that we needed more engineers, and that we needed them last month. We monitored how many people we were hiring per month alongside our offer-to-accept conversion rate. Sort of. As long as we remembered to track them, but it wasn’t a big deal if we forgot either. There was a lot of room for improvement. An Opportunity To Start Fresh Just prior to the onset of the pandemic, our recruiting team was presented with a new opportunity as Yelp Engineering decided to expand its footprint to Toronto. As the pandemic unfolded, our plans pivoted from focusing on Toronto to remote hiring in Canada at large. This was our first opportunity to enter a new talent market properly with the knowledge we’d gained over the previous years. And it seems to be working: In Q1 2021, 19% of our engineering hires in Canada identified as  Black or Latinx (together, underrepresented minorities or URM), and we saw even more impressive gains in leadership positions, too. Some Advice Start Now I’ve often regretted our inability to make quicker decisions for lack of data. It takes time to build up a sizable enough data set to understand your processes and detect the bias in them. In Yelp’s case, depending on the current rate of hiring, we’re typically able to understand the state of affairs with statistical significance after a month or two of data collection. There are of course variables that impact this. For example, top of funnel stages, such as first round interviews, produce more data. Nine years ago, it would have taken us significantly longer to produce useful data. This is especially true at the later stages of the pipeline, when the number of candidates are reduced, and for demographics that are typically underrepresented in tech, because there weren’t enough in the pipeline to make statistically significant conclusions about. If you’re just getting started, the sooner you’re tracking recruiting data, the sooner you’ll be making meaningful changes to your processes. Essential Data Points If we were starting fresh today, there are three data points I’d want to make sure we started collecting immediately. Proceed/Did not proceed rates at every stage of the interview process - This one might go without saying, but it’s the foundation everything else is built on. Start from the point of contact all the way through to offer acceptance. Everything else is useless without an understanding of the proceed/did not proceed rates at each interview stage. Candidate source - Knowing where your candidates are coming from generates a number of insights. Do applicants from career fairs or job boards get more offers? Most people jump to wanting to find the most successful sources, but it’s equally valuable to know your least successful sources. Candidates from certain sources falling out of your pipeline at a disproportionate rate can be very telling. We’ve seen this manifest as non-traditional CS educations, such as bootcamps, being rejected at disproportionate rates. This indicated that we needed to be more explicit in our evaluation criteria for interviews that we’re unconcerned with an applicant’s educational background, and changing these criteria has been highly effective at making sure candidates with a wide range of education backgrounds proceed equally through the pipeline. Candidate demographics - Being able to analyze your pipeline by the demographics of the candidates is extremely helpful. For instance, it’s well known that there is a gender disparity in the tech space. Tying gender or ethnicity back to the previous two data points allows for powerful insights into which interview stages are problematic. As an example, we were able to identify early on in our data that women were less likely than men to attempt the code test, which is the first step to our interview process. A surprisingly effective intervention here was to ask all candidates a second time to participate, which is a good reminder that you don’t always need to reinvent the wheel to make change. Point 3 comes with two very important caveats. Collecting this data is subject to different legal requirements depending on your location. Consult legal experts before moving forward. No one responsible for making hiring decisions can have access to this data. The trackers that hold this data are managed by our operations people and access is granted only to the sourcers and recruiters tracking the data. Assess Your Systems Don’t let perfect be the enemy of good when tracking your data. Teams can be overwhelmed with the possibilities of what to track and how to go about it. A good place to start is by getting a grasp on what your existing systems can provide. You likely have some sort of applicant tracking system (ATS) that can provide some types of pipeline metrics. Learn what your system can do for you and how it does it. It’s likely you’ll have to supplement your ATS with custom-made solutions, as there’s going to be data that your ATS is unable to provide. Don’t be too good for spreadsheets. I know, I know, there has to be a better way. There’s always a better way. Getting the data matters more than how you’re getting it. If spreadsheets allow you to track your data while you find a more permanent off-the-shelf solution or your teammates in engineering build you something, do it. We’ve relied on spreadsheets for years. Even though we’ve incorporated tools such as Tableau, spreadsheets have remained an important part of our system. Proceduralize Good, reliable data depends on maintaining consistent data collection practices. Depending on your systems, some of this will be automatic. At Yelp, we track a sizable amount of data manually and use our ATS for everything it can accurately, automatically track. For everything else, we rely on our recruiters and sourcers to manually track data in spreadsheets. Each recruiter and sourcer has a centrally managed tracker that they use to track their candidates from start to finish. There is no room for interpretation about what data to collect and how to collect it. Every tracker is exactly the same and every team member tracks the same data. Maintaining and analysing your data should also be the explicit responsibility of someone on your team. For us, things really took off when we created a full-time operations role within the recruiting organization. Taking this work seriously requires constant maintenance that can’t be done in “spare” time. Pitfalls This approach is not foolproof. There are mistakes to be made, and we’ve made our fair share of them. Chief among them is drawing conclusions from data that is not statistically significant. We’re often dealing with fairly small data sets, and it can be very tempting to make changes based on perceived patterns. In these cases, patience is key. If something looks off, definitely pay attention, but try not to jump to conclusions. Rolling a change back hurts and also messes up your painstakingly collected data. It ends up being more damaging in the long run to make changes based on data that hasn’t reached significance. Structured Interviews Part 2 of this post will go into detail on how we’ve built a structured interview process and acted on the data that we’ve collected. Layering structured interviews on top of our data collection and analysis practices has allowed us to make fine-grained tweaks to the interview process that would be otherwise impossible. Our insights have led us to a points-based system in our latest iteration of structured interviews that will further our goal of more equitably scoring interview performance. Lastly, if you’re finding these posts interesting and Yelp sounds like the kind of company culture that you’d like to be a part of… we’re hiring! Tweet Back to blog", "date": "2021-04-22"}, {"website": "Yelp", "title": "One year later: building Trust Levels during COVID", "author": ["\n        \n  Jeffrey Butterfield and Ivy Chen, Software Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/04/one-year-later-building-trust-levels-during-covid.html", "abstract": "From its devastating toll on local economies to its impact on the little things like handshakes and hugs, the COVID-19 pandemic seemed to leave nothing unchanged. Local businesses were especially impacted and forced to make big changes, many overhauling their operations overnight in order to adapt to the new normal. Businesses turned to Yelp to communicate operational changes brought on by the pandemic. They kept their communities in the know by updating the COVID-19 section on their business pages, which was launched at the beginning of the pandemic. They indicated new health and safety precautions, such as wearing masks and enforcing social distancing. They updated their hours, scaled back sit-down dining, pivoted to support takeout and delivery, and even introduced virtual service offerings to remain accessible to their communities. Given the surge in businesses updating their Yelp pages with COVID-19 related changes, we knew it would be important to measure how confident we are that a given piece of business information is still accurate. To address this, Yelp’s Semantic Business Information team built a new internal system called Trust Levels. In this blog post, we will define Trust Levels, take a look at each part of the new system, and end with an example that ties all the pieces together. Defining Trust Levels At Yelp, we call our business information “business properties.” Business properties include anything that we can describe about a business, such as business address, if the business is women-owned, if the business can repair appliances, etc. The numerous business properties found on each Yelp page can share special insights about businesses from retail and restaurants to home and local services. Usually business owners can indicate the business properties on their business page. Consumers are also able to contribute information about a business, which can also be collected through our survey questions answered by people who have checked in or visited that business. Our User Operations team reviews changes to ensure quality and accuracy, and can modify the information as well. However, determining how confident we can be about any given piece of information became especially important as businesses repeatedly had to update how they operate due to changing local government policies. In order to define our confidence levels, we first created a unified vocabulary that could be used across engineering and Product teams, to avoid each team creating its own definition of trust. We created Trust Level labels from Level 1 (L1), which means we are highly confident that the data is both accurate and current, to Level 4 (L4), which means we do not have strong or recent signals to determine accuracy. These levels, which we designed to be simple and easy to refer to, can then be used by various teams without needing to do their own calculations. For example, if a front-end team wants to only display information of the highest confidence level, they can do so by fetching the information and Trust Level from the backend and only displaying it if the Trust Level is L1. Calculation Once we defined this shared vocabulary, we set out to calculate a Trust Level for each of the tens of millions of business property values on our platform. To start, we utilized one of our existing systems that tracks historical business data. The system logs all business changes to a dedicated Kafka stream for offline use cases. Each record contains a source type (business owner, external partner, etc.), source ID (which particular source provided the data), source flow (which feature or callsite the update came through), and timestamp. All of these fields are essential indicators when it comes to determining how confident we are that a given business property is correct. We also realized our property ingestion APIs could be improved to capture another important signal around data freshness. A lot of incoming updates we receive are “non-updating updates” — those with values that match what we already have on file. Previously, most of our ingestion flows discarded these as redundant, so we modified them to instead emit logs to a new, dedicated stream for verification events. Not all verifications are equivalent, so we made sure to include the same source-related fields described above in the Kafka stream with each event, preserving context about the verification that might be useful to us later. Equipped with historical updates and verifications, we wrote a Spark ETL job to periodically pull these logs from S3, join them on business_id and business property, and then execute a series of rules to decide which Trust Level to assign to that pair. While we won’t detail the actual algorithm here, signals of recency and source type ended up being the biggest determiners of a given business property’s Trust Level. Storage After calculating each Trust Level value, we needed a place to store them. Trust Levels are data describing properties, so it made sense to store these values alongside other business property metadata. A metadata table was considered multiple times in the past as we constantly fielded questions about when a property value was created, what time it was last updated, what source type updated the value, or from what flow the value was updated. Instead of running ad hoc queries and pulling together information from multiple datastores, we centralized the metadata in a new table to make it easier to access and eventually expose Trust Levels. We called this table business_property_metadata and gave it the following schema: Here’s an example row of the business_property_metadata table: We chose Cassandra over MySQL as the underlying datastore for this new table, and while our rationale for this could be a blog post of its own, here are the main reasons. We knew the table would hold tens of millions of rows, and we could safely assume clients of the data would be accessing it using the primary key (business_id,  business_property_name). Cassandra provides good read and excellent write performance for data at this scale when rows are always queried on this key, which Cassandra uses in part as a partition key to distribute a table’s data to different nodes. MySQL, which is used extensively throughout Yelp, offers different benefits that were less important for this particular use case. We don’t anticipate needing efficient joins of this metadata with other data entities, nor do we foresee the need for strict transaction mechanisms or strong consistency guarantees around these fields. Cassandra’s eventual consistency semantics are enough for this type of business information. As a final note on storage, our metadata table is easily extendable. We have already included a column, provenance, that captures different fields around the data’s source in case our downstream consumers need access to that information. In the future, we will be able to add more types of metadata to the table as use cases arise. Serving Trust Levels (Online and Offline) The final step in enabling our Trust Levels was ensuring that the data was accessible for various teams to use. To do this, we created new dedicated REST API endpoints for querying and writing to our metadata table. We also backfilled our metadata table with historical data that we already had and calculated Trust Levels for those properties. We then migrated existing calls around business properties to our new API endpoints in order to write live updates to our metadata table. Now, with our metadata table filled with values, internal clients can access Trust Levels and other metadata through our online APIs. For offline access, we already had existing data streams of our business property data published to Yelp’s Data Pipeline and used by teams such as Search and Ads. We needed to make sure that the new metadata information was included alongside the property data in our data pipeline, while also ensuring that the data was easy to consume for our downstream clients. In order to accomplish this, we first aggregated our data from the new metadata table along with other currently consumed tables, using a Yelp stream processing service called Flink Aggregator. The aggregator transforms the data stream to be similarly keyed by business_id, since metadata uses a different primary key (business_id, business_property_name). We then combine these streams using Joinery to produce one data stream that shows the entire current value of that business including metadata. This allows our downstream clients to utilize the same data stream with only slight modifications on their side to read the metadata — including Trust Levels — as well. Example: Business Hours To conclude, let’s connect all the steps described above by walking through an example for a business property that was updated a lot during COVID: Business Hours. Assume there is a business where the business owner last updated their hours two weeks ago, followed by a verification event from a data partner submitting the same hours one week ago. The following diagram illustrates the entire Trust Levels flow for this particular business property. Anyone at Yelp can now use this authoritative confidence label however they need it. A front-end team could use it to power new UI components indicating recently updated hours. A Search engineer could experiment with incorporating it as a feature in a ranking model. A data scientist could analyze if accurate business hours data is correlated with higher user engagement. Whatever it is, the Trust Levels data is ready for them, and becomes another tool we use to build helpful features for consumers and business owners during these unprecedented times. Acknowledgements We would like to thank Devaj Mitra, Surashree Kulkarni, Abhishek Agarwal, Pravinth Vethanayagam, Jeffrey Butterfield (author), Maria Christoforaki, Parthasarathy Gopavarapu, our Semantic Business Information team, our Database Reliability Engineering Team, and our Data Streaming teams who all helped make Trust Levels a reality. Thanks to Venkatesan Padmanabhan and Joshua Flank for technical reviewing and editing of this post. Tweet Back to blog", "date": "2021-04-29"}, {"website": "Yelp", "title": "Engineering Career Series: Using structured interviews to improve equity", "author": ["\n        \n  Grace Jiras Yuan, Technical Recruiting Manager; Kent Wills, Director of Engineering Effectiveness\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2021/05/engineering-career-series-using-structured-interviews-to-improve-equity.html", "abstract": "For years, Yelp continued to use an interview process that was created when we were a 50-200 person Engineering organization, with only a handful of interviewers: Each interviewer wrote their own interview questions A few senior leaders gave overall hire/no hire decisions for every panel Interviewers received ad hoc feedback from senior leaders when it seemed like they were too tough or too easy in their interviews A few things went well: there was a strong sense of personal responsibility for both leaders and interviewers turnaround time for offer approvals was quick and Yelp values could be preserved by senior leaders. As the Engineering organization grew to more than 500 employees, the interviewer pool also grew, from tens to hundreds. This created a few challenges. It became harder to enforce similar standards across interviewers. It became increasingly difficult to tell whether a candidate was strong or if an interview question was correctly calibrated. This lack of structure made it difficult to confidently and consistently identify strong candidates. It also made it difficult to identify whether there were patterns of bias in our interview process. Faced with these challenges, we asked, “How do we continue to hire diverse, amazing talent as we scale our Engineering organization?” Creating Structured Interviews A group of folks across Technical Talent and Engineering banded together to answer this question. Since others had gone down this path before us, we began with a review of prior work by Medium , Quora , and Sensu . Those references, along with our own internal review led to the creation of a structured interview process that reflected what we felt it took to succeed at Yelp. As a first step, we focused on standardizing questions across all of our open roles to four key question types: Problem Solving System Design Ownership, Tenacity, and Curiosity Playing Well with Others The first two interview types focus on the candidate’s technical skills, and the latter two focus on non-technical skills and how aligned the candidate is with Yelp’s values. For the technical portions, we wanted to evaluate the candidate’s skill with technical tasks that would be common in the role they’re applying for, rather than their ability to memorize algorithms or easy-to-search-for trivia. To create these questions, we asked engineers across the organization to take a problem their team recently solved and create an example on a smaller scale. We strongly believe that using real life problems to evaluate skills captures what is needed to actually succeed at Yelp and helps us give more opportunities to people with different backgrounds to be successful in our hiring pipeline. To evaluate questions, we standardized criteria that related to dimensions (Technical Skill, Ownership, Business Insight, Continuous Improvement, and Leadership) that we use internally for leveling engineers. This further aligned internal and external expectations of candidates and employees. Making Data-Informed Decisions Moving to structured interviews allowed us to take the first step to both collect and analyze interview data in a meaningful way. We went from having no comparable feedback to thousands of technical and behavioral data points in a consistent format. This not only gives us the opportunity to monitor the health and size of our pipelines, but it also enables us to identify potential problems or biases at every stage of the interview process. When observing a gap or difference in dropoff rates, we are better able to drill down our focus to specific question sets or interviewers and determine what solutions to implement to directly mitigate bias. First try: what we learned After introducing structured interviews, we soon identified a difference in pass rates across genders in the initial round of technical interviews. Upon closer inspection, we found instances where candidate performance was identical when measuring how many components of a coding question were completed. However, men were progressing to the next stage of the interview process at a higher rate than women. We were able to quickly reduce this gap by replacing individual interviewers’ judgment on a candidate’s performance with standardized pass/fail criteria, which ensured that all qualified candidates moved forward. This was the first of several successful modifications, which have collectively reduced the pass rate gap between genders. Making corrections to the early steps of the interview process has made a huge impact on gender diversity at every subsequent stage. This ultimately increased the likelihood of having more women make it to the final offer stage. With better pipeline observability, we’ve been able to more effectively hire diverse talent by mitigating these biases and reducing false negatives. Second try: defining evaluation criteria While we were now able to both pinpoint and remedy where drop offs were occurring in our interview process, our approach to reducing bias was still reactive. Interpretation of candidate performance varied amongst interviewers, even with the measures we had in place. We recognized that having structured interview questions wasn’t enough, and we needed explicit evaluation criteria for all of our interviews. To address this, we introduced a points-based evaluation criteria to our structured interviews. In this initiative, we further clarified what signals we wanted interviewers to look for and capture. Points are awarded for expected candidate behaviors based on a rubric. Interviewers are required to provide an explanation for when and why points are deducted. This scoring framework can then be aggregated and converted to hiring and initial leveling decisions to maintain consistency across the larger organization. A key benefit of this framework is that interviewers can systematically measure candidate performance during the interview, but the onus of deciding final interview outcome, and, therefore, the possibility of unconscious (or even conscious) bias by the interviewer, is reduced. How we’re evolving If there’s anything we’ve learned from this journey, it’s that improving interviews is an ongoing process of review and adaptation. At Yelp, we’ve made this a shared priority between Technical Talent and Engineering. Our teams work closely with one another and have a dedicated task force with several subgroups composed of folks from both groups that meet on weekly cadences to put this commitment into action. While we still have a lot on our roadmap, here are some key lessons that we have learned so far: Making interview improvements requires a real partnership. It may seem obvious to say this, but, if you’re going to improve engineering interviews, you’re going to need subject matter experts from both engineering and recruiting to capture all the nuances that are often overlooked. Interviewer bias still exists in your hiring process even with a standardized process and structure. A good best practice to combat this is to make sure that the group working on interview processes is reflective of the demographics of your organization, or what you’d like your organization to be. Make sure women and underrepresented minorities are involved. A distributed workforce means different geographies with different cultural considerations and different employment norms, so include engineers representative of all your geographies when standardizing. Our initial task force failed to include folks from our European teams and, thus, some of our interview questions were geared towards Bay Area tech culture. Collecting feedback is imperative towards making progress, so make sure you create feedback loops from all stakeholders: recruiters, recruiting coordinators, interviewers, and hiring managers. Candidates are stakeholders, too, so make sure to have a process to get feedback on their interview experience. Standardization allows for easier review and change, whether that is the pipeline, the interview questions, interview evaluation, training - the list goes on. We’re still in the midst of rolling out our points-based evaluation criteria for structured interviews, and we’re able to move a lot faster instead of needing to reinvent the wheel! Up next: How we onboard engineers across the world at Yelp Equally important to bringing in diverse talent is everything that happens onwards from the moment a candidate becomes an official Yelper. In the next post in this series, we’ll take a closer look at the thought and logistics that we go through to set folks up for success, how we’ve streamlined our onboarding process for distributed teams in the virtual world, and the opportunities for continuous learning we provide to our employees through training and mentorship programs. If you’re finding these posts interesting and Yelp sounds like the kind of company culture that you’d like to be a part of… we’re hiring ! Tweet Back to blog", "date": "2021-05-06"}, {"website": "Yelp", "title": "Improving the performance of the Prometheus JMX Exporter", "author": ["\n        \n  Flavien Raynaud, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/10/improving-the-performance-of-the-prometheus-jmx-exporter.html", "abstract": "At Yelp, usage of Prometheus , the open-source\nmonitoring system and time series database, is blossoming. Yelp is initially\nfocusing on onboarding infrastructure services to be monitored via Prometheus,\none such service being Apache Kafka . This blogpost\ndiscusses some of the performance issues we initially encountered while\nmonitoring Kafka with Prometheus, and how we solved them by contributing back\nto the Prometheus community. Kafka at Yelp primer Kafka is an integral part of Yelp’s infrastructure, clusters are varied in size\nand often contain several thousand topics. By default, Kafka exposes a lot of\nmetrics that can be collected, most of which are crucial to understand the\nstate of a cluster/broker during incidents, or gauge the overall health of a\ncluster/broker. By default, Kafka reports metrics as JMX\n( Java Management Extensions )\nMBeans. Prometheus metrics primer One of the ways to export metrics in Prometheus is via exporters .\nExporters expose metrics from services in a format that\nPrometheus understands. Prometheus shards are then able to collect metrics\nexposed by these exporters. The Prometheus community officially maintains the JMX Exporter , an exporter that\ncan be configured to expose JMX MBeans from virtually any JVM-based process as\nPrometheus metrics. As mentioned above, Kafka is one such process. In order to make Kafka metrics available in Prometheus, we decided to deploy\nthe JMX Exporter alongside Kafka. Figure: Architecture of Prometheus metric collection for a 3-broker Kafka cluster When we initially deployed the JMX Exporter to some of the clusters, we noticed\ncollection time could be as high as 70 seconds (from a broker’s perspective).\nWe tried running the exporter as a Java agent and tweaking the configuration to\ncollect only metrics that were interesting to us, but this did not improve the\nspeed. Figure: Collection time (in seconds) of a single Kafka broker with no prior code change. This meant that metrics usable by automated alerting or engineers would have,\nat best, one datapoint per time series every 70 seconds. This would have made\nmonitoring an infrastructure supporting real-time use cases difficult, e.g:\nnoticing spikes in incoming traffic, garbage collection pauses, etc. would be\nmore difficult to spot. We dug into the JMX Exporter codebase and realised some operations were\nrepeated at every collection. Sometimes hundreds of thousands of times per\ncollection. For Kafka, some metrics are available with a topic-partition\ngranularity; if a Kafka cluster contains thousands of topic-partitions,\nthousands of metrics are exposed. One of the operations that seemed the most\ncostly was matching MBean names against a configured set of regular expressions ,\nwhich then computes Prometheus sample name and labels . The set of regular expressions is immutable over the lifespan of the exporter\nand between configuration reloads. This means that if an MBean name matches one\nof the regular expressions (or does not match any) during the first metric\ncollection, it will match it for all collections until the configuration is\nchanged or reloaded. The result of matching MBean names against the set of\nregular expressions can hence be cached and the time-consuming task of matching\nregular expressions (and computing sample name and labels) skipped during\nfurther collections. After introducing this cache, heavy computations are made only once throughout\nthe lifespan of the exporter. The initial collection does the heavy work of\ncaching and takes a significant amount of time to complete, however subsequent\ncollections take very little time. Collections that used to take 70 seconds,\nnow take around  3 seconds. This allows us to have more fine-grained dashboards\nand alerting. Figure: Collection time (in seconds) before and after enable rules caching. Red line shows the number of MBeans in the cache. This change is now available in the upstream jmx_exporter ,\nand can be toggled on/off depending on the use case. Looking Further As mentioned in the introduction, the usage of Prometheus at Yelp is growing and\nmany systems and teams rely on it for monitoring, dashboards and automated\nalerting. The changes to the JMX exporter are only a small part of a large\ninitiative driven by our Production Engineering team ,\nwatch this space for more insights into this journey! Acknowledgements Brian Brazil for code reviews and best practices Tweet Site Reliability Engineering at Yelp Want to build and manage scaleable, self-healing, globally-distributed systems? View Job Back to blog", "date": "2020-10-02"}, {"website": "Yelp", "title": "The Dream Query: How we scope projects with GraphQL", "author": ["\n        \n  Mark Larah, Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/10/dream-query.html", "abstract": "At Yelp, new web pages and app screens are powered by GraphQL for fetching data. This blog post describes the Dream Query – a pattern our feature teams use\nwhen refactoring or creating new pages. ( Check out our previous blog post to see how we dynamically codegen DataLoaders to implement the server layer!) Scoping a new feature with GraphQL Let’s jump in with an example! Imagine your team is tasked with creating the new version of the “Header\ncomponent” for the website (we’ll use the Yelp.com website in our example). You\nmay receive a design mock that looks like this: Mock Header Component Your mission (should you choose to accept, of course): turn this into code. Alongside the usual planning activities such as OKR docs and estimated\ntimelines, we’ve found it particularly helpful to write out a theoretical\nGraphQL query that could power the page or component - aka the “Dream Query”. Writing a Dream Query The first step is to identify what dynamic data we need to display. In our case\nof the Header component above, we can see the UI showing: Number of unread inbox messages User’s profile photo Therefore we might write something like this: 👇 query { loggedInUser { profilePhoto ( size : \"small\" ) { src } inbox { unreadMessageCount } } } The idea of the dream query is to let developers “just write” the query they\nwished they could write to power the page - with as low barriers to entry as\npossible. In other words, imagine you’re writing the UI code, and you magically\nhave everything already available to you. What query would you write? Here’s a few points to keep in mind: Try to use real types that already exist in the schema. (Use GraphiQL’s\ndocs tab to search.) It’s ok if you don’t get this perfect! Large schemas can contain hundreds\nof types, and it’s easy to miss stuff. (This will ideally be caught in review.) It’s ok to query for types that don’t exist yet. (That’s kind of the\npoint here!) It’s ok if your team is totally new to GraphQL. Don’t worry if you aren’t\nsuper confident in the syntax yet - this will at least provide a great\nstarting point for reviewers. Write the Dream Query before the real application code is written -\nideally as part of the scoping or planning phase. This cuts down on the\noverall iteration cycle, since we aren’t writing any real code to implement\nresolver methods yet. Review Once written, share the Dream Query widely. We do this in a Google Doc, so folks\ncan comment line by line. The goals here are to: Refine the query such that it meets our schema design guidelines. (At\nYelp, we have a community-driven schema review group specifically set up for\nthis.) Find other teams who may be stakeholders in the types being created. Understand the time investment needed for the backend portion of the\nproject (i.e. for creating new GraphQL resolvers). graphql-faker During review, it’s important not to block feature development. We want to be\nable to parallelize the backend and frontend work. We’ve found graphql-faker to be\nreally helpful. It’s a super nifty tool for mocking up a schema, such that you\ncan make “real” queries and iterate on the Dream Query in a live GraphQL\nplayground. This also lets client developers hook up the graphql-faker endpoint inside their\napplication - meaning we can use a Dream Query in development to build the view\nlayer while the schema is still in review. Incrementally using the Dream Query When writing new large pages, or incrementally refactoring a non-GraphQL page to\nuse GraphQL, we may want to roll things out incrementally. Perhaps not all the\nresolvers can be implemented straight away. We’ve found it helpful to paste in the whole Dream Query into the app: const GET_HEADER_DATA = gql `\n  query GetHeaderData {\n    loggedInUser {\n      city\n      displayName\n\n      # TODO: Uncomment and use when each field is supported\n      # profilePhoto(size: \"small\") {\n      #   src\n      # }\n      # inbox {\n      #   unreadMessageCount\n      # }\n      # yearsElite\n    }\n  }\n` ; function MyPage () { const { data , loading , error } = useQuery ( GET_HEADER_DATA ); if ( error ) throw error ; if ( loading ) return null ; const { displayName , city } = data ; return < Header displayName = { displayName } city = { city } />; } Tickets can be created to uncomment specific fields. This provides a way to\nbreak up, parallelize and track how much work there is left to complete the\nmigration. When a type becomes available in the schema, we can uncomment the\nlines and use it in production. Why a dream query and not a dream schema? Schema proposals are great to see too! We recommend starting with the query\nfirst, since this maps directly the interface the product will be using. It also\nallows those unfamiliar with the schema to quickly get an understanding of the\nshape of data the client is requesting, without having to go through all the\noptions that the existing/proposed schema allows for. Takeaways The Dream Query is used as a way to communicate what data the component or page needs, and\nwhat new backend work needs to be done forces us as developers to think critically about what types we’re adding and\nencourages reuse of existing schema provides an opportunity for schema reviewers to catch issues early, before\niteration cycles are spent committing to suboptimal schema design provides a way to chunk up migrations to GraphQL Written By Mark Larah, Software Engineer ( @mark_larah ) Tweet Become an Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2020-10-07"}, {"website": "Yelp", "title": "Flink on PaaSTA: Yelp’s new stream processing platform runs on Kubernetes", "author": ["\n        \n  Antonio Verardi, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/10/flink-on-paasta.html", "abstract": "At Yelp we process terabytes of streaming data a day using Apache Flink to power a wide range of applications: ETL pipelines, push notifications, bot filtering, sessionization and more. We run hundreds and hundreds of Flink jobs, so routine operations like deployments, restarts, and savepoints don’t take thousands of hours of developers’ time, which would be the case without the right degree of automation. The latest addition to our toolshed is a new stream processing platform built on top of PaaSTA , Yelp’s Platform As A Service. Sitting at its core, a Kubernetes operator automatically watches over the deployment and the lifecycle of our fleet of Flink clusters. Flink on PaaSTA on Kubernetes Life before Kubernetes Before the introduction of Kubernetes at Yelp, Flink workloads at Yelp were running on dedicated AWS ElasticMapReduce clusters which come with both Flink and YARN pre-installed. In order to make EMR instances work well with the rest of the Yelp ecosystem, our previous stream processing platform Cascade used to run a chunk of Yelp’s Puppet monolith in a Docker container to apply configurations and to start the common set of daemons running on almost all Yelp’s hosts. Architecture of Cascade Cascade also introduced a per-cluster controller component in charge of the Flink jobs life cycle (starting, stopping, savepointing) and monitoring which we call Flink Supervisor. While this system served us well for years, our developers were experiencing a handful of limitations: It previously took around 30 minutes to spin up a new Flink cluster We needed trained human operators to manually deploy new versions or scale up resources for each cluster We could not upgrade to newer versions of Flink until AWS supported them The complexity of running Puppet in Docker and of maintaining a very different infrastructure from the rest of Yelp was time consuming When Kubernetes started to gain more and more momentum both outside and inside the company, we decided that it was time for a change. Flink loves PaaSTA PaaSTA is Yelp’s Platform As A Service and runs all Yelp’s web services and a few other stateless workloads like batch jobs. Originally developed on top of Apache Mesos , we are now migrating it to Kubernetes. This opened up the opportunity to support more complex workloads thanks to Kubernetes’ powerful primitives. Flink was the first in line and Cassandra is coming up in the very near future (be on the lookout for a new blog post!), both of them developed in tight collaboration with our Compute Infrastructure team. Instead of “just” running Flink on top of Kubernetes using something off-the-shelf, we went down the road of developing a full-fledged platform that would make the experience of running Flink workloads as similar as possible to running any other service at Yelp. We did so to greatly reduce the knowledge necessary for a user to operate Flink clusters and to make our infrastructure very homogeneous with the rest of Yelp’s ecosystem. With Flink on PaaSTA, provisioning a cluster is as easy as writing a YAML configuration file. New code deployments all happen automatically as soon as they are committed to git via Jenkins . The commands provided by PaaSTA for starting, stopping, reading logs or monitoring a web service work exactly the same for any Flink cluster. paasta status command output In addition to UX improvements, we managed to reduce the average time to spin up a Flink cluster from 30 minutes to under 2 minutes and we are now free to hop on the latest version of Flink on our own schedule. Peeking inside the hood At the core of Flink on PaaSTA sits our custom Kubernetes operator , watching over the state of Flink clusters running on Kubernetes and making sure that they always match what is described in the configuration defined by the users. Our PaaSTA glue translates this configuration into Kubernetes Custom Resources , which the operator reads and updates with information taken from the Flink clusters, like the jobs list and status. These resources are also used by the PaaSTA commands to fetch what to show to the users and to interact with the operator for operations like start and stop. The operator knows how to map the high-level definition of a Flink cluster resource into the right Kubernetes primitives like Deployment for scheduling the TaskManagers , Service to make the JobManager discoverable by the other components in the cluster or Ingress to make the Flink web dashboard accessible by our users. The operator together with Jenkins schedules these components in Docker containers which allow us to customize the Flink installation and to select our Flink version of choice for each application. You may find it surprising to see in the diagram below that our legacy Supervisor component still has a place in our new platform. At Yelp we like to approach all our projects with a practical spirit, infrastructure migrations included. While everything the Supervisor is doing could be worked into the operator, we decided to keep it around to reduce the development time by re-using existing features. Even more importantly, minimizing the scope of changes also helped to make the migration from Cascade to PaaSTA as easy as possible for our existing users. For example, we deploy the Supervisor as a Kubernetes Job to leverage its logic for triggering savepoints of all the Flink jobs running on a cluster just before the operator shuts it down. Components of a Flink PaaSTA cluster If you’d love to hear more about the details, we encourage you to check out our talk at Flink Forward . What now? Freeing us from the need to manage hundreds of Flink clusters, Flink on PaaSTA unlocked a new world of possibilities for our users and our Stream Processing team. On the infrastructure side, we are now close to adding Apache Beam support to Flink on PaaSTA in order to make Python stream processing a first-class citizen at Yelp. We are also working on implementing auto scaling and per-job cost reporting for Flink clusters. On the UX side, we are developing tools to allow our users to define complex pipelines of streaming components with a single configuration file. We are also busy building features to shape our on-line machine learning platform. Stay tuned if you want to hear about all the above and more! Tweet Data Streams Platform Engineer at Yelp Want to build next-generation streaming data infrastructure? View Job Back to blog", "date": "2020-10-14"}, {"website": "Yelp", "title": "Introducing Folium: Enabling Reproducible Notebooks at Yelp", "author": ["\n        \n  Lydian Lee, ML Platform Tech Lead; Ryan Irwin, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/10/introducing-folium-enabling-reproducible-notebooks-at-yelp.html", "abstract": "Jupyter notebooks are a key tool that powers Yelp data. It allows us to do ad hoc development interactively and analyze data with visualization support. As a result,  we rely on Jupyter to build models, create features, run Spark jobs for big data analysis, etc. Since notebooks play a crucial role in our business processes, it is really important for us to ensure the notebook output is reproducible. In this blog post, we’ll introduce our notebook archive and sharing service called Folium and its key integrations with our Jupyterhub that enable notebook reproducibility and improve ML engineering developer velocity. Folium for Notebook Archiving & Sharing There are a few ways to archive and share notebooks (i.e., exporting to html, saving .ipnb files in Github, shared network drives). There are also some other higher-level frameworks for notebook archiving, but these frameworks lacked integration with Jupyterhub, searchability, and additional customizations presented in this post. Figure 1. Folium and Jupyterhub Folium is a basic front-end service that also has APIs that interact with our Jupyterhub. These APIs enable uploading after developing a notebook. While uploading a notebook, the user is prompted for tags (i.e., project name, ticket) and a potential description fetched from the notebook automatically. The front-end service part provides the ability to search for notebooks by user, tag, or documentation of the notebooks. It also renders the notebooks in the webpage including the different notebook versions (more on this later!) and extracts a table of contents by extracting markdown in the notebook. The functionality described above laid the basic foundation of notebook archiving and sharing, but we built several additional features that we want to share on helping with reproducibility of notebooks: The notebook running environment is logged so that we can easily reproduce the output. Versions of the same notebooks are grouped together to easily compare their differences. The shared notebooks can be directly imported into Jupyter server so that people can easily reproduce or improve on the existing notebooks. Adjust variables and rerun existing notebooks directly from Folium without going to Jupyterhub. Tags system allows searching and grouping related notebooks. We will talk about each function in more detail in the following sections. Logged Notebook Running Environment We have a Jupyterlab extension installed on our Jupyterhub that takes care of import/export functionality to Folium. When exporting to Folium, the extension gathers the running environment from the current notebook servers, so that the key information is also logged into the notebook’s metadata. Currently we log which docker image and kernel are being used so that when re-running this notebook, we will be able to choose the correct working environment. We also log the memory and CPU/GPUs used so that users can pick the correct amount of resources in order to re-run the notebook. For different tasks, some might need more computation powers, versus some of the tasks may need to have higher memory. Without knowing how the resources are being used by current notebooks, we would likely get out-of-memory issues when rerunning the notebooks. Figure 2. Basic Notebook Information Import Notebook from Folium to Jupyterhub The same Jupyterlab extension mentioned above also allows us to import notebooks directly from Folium via its APIs. People can search and preview all the available Folium notebooks, and directly import them into Jupyterhub. We regularly use this function for collaboration and for improving on old models. Figure 3. Search Folium’s notebook archive and import within Jupyterhub Grouping of Different Versions of Notebooks Often an analysis is valuable enough that it needs to be repeated. This means a user will upload multiple similar notebooks.  When a user does this, we group these similar notebooks together on the same page. Therefore we can directly compare the result of different versions of notebooks. We also use this feature to provide tutorials as well, where you can put the question and answer on the same page for people to learn by themselves. In addition to that, we also link the related code review on this notebook in the Folium, so that people can easily refer to the feedback for the notebooks. Figure 4. Different versions of notebooks linked together and related links are also highlighted. Parametrized Notebooks Besides importing a notebook to Jupyterhub, we also have the feature that allows users to directly rerun the notebook with different parameters in Folium. This helps us reuse the notebooks and quickly get us the result for similar analyses. Figure 5. Substitute variables and rerun notebooks from Folium Tagging and Search System Searching is also a key thing in reusing the notebook.  Without search integration, we will end up having lots of similar notebooks being recreated. This is exactly the issue we’re seeing before improving the tagging and search system on Folium. People have to constantly recreate the same notebooks, because they are not aware there are similar notebooks that can be easily imported and reused. As a result, we fixed the issue by automatically fetching the markdown from notebooks to generate required descriptions that helps users to search for specific notebooks. Free form tagging is also supported and being used widely for teams to tag the notebooks they owned or grouping the notebooks related to specific projects. The Folium web service has a simple search results page (SERP) with filtering by tag and user.  Also, the search API supporting the SERP is also leveraged for searching in the sidebar from Jupyter as shown in Figure 2. Future Work Folium is a tool that not only helps us share the code, but also helps us reuse the built notebooks to accelerate our daily work! On the roadmap, we are looking to continuously improve it by providing the ability to review notebooks, including a view of diffs and commenting. We are also adding more ways to get re-run notebooks delivered, including the option of emailed reports. Acknowledgements Thanks to the Core ML team for building and continuously improving our Jupyter and Folium infrastructure, and thanks to Blake Larkin, Ayush Sharma, Shuting Xi, Jason Sleight for editing the blog post. Tweet Become an ML Platform Engineer at Yelp Interested in designing, building, and deploying ML infrastructure systems? Apply to become an ML Platform Engineer today. View Job Back to blog", "date": "2020-10-21"}, {"website": "Yelp", "title": "Minimizing read-write MySQL downtime", "author": ["\n        \n  Nick Del Nano, Database Reliability Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/11/minimizing-read-write-mysql-downtime.html", "abstract": "The relational database of choice at Yelp is MySQL and it powers much of the Yelp app and yelp.com. MySQL does not include a native high-availability solution for the replacement of a primary server, which is a single point of failure. This is a tradeoff of its dedication to ensuring consistency. Replacing a primary server is sometimes necessary due to planned or unplanned events, like an operating system upgrade, a database crash or hardware failure. This requires pausing data modifications to the database while the server is restarted or replaced and can mean minutes of downtime. Pausing data modifications means that our users can’t perform actions like writing reviews or messaging a home service professional, and this amount of downtime must be minimized to the shortest amount possible. This post details how Yelp has integrated open-source tools to provide advanced MySQL failure detection and execute automated recoveries to minimize the downtime of our read-write MySQL traffic. Characteristics of MySQL infrastructure at Yelp Our MySQL infrastructure is made up of: Hundreds of thousands of queries per second from HTTP services and batch workloads (lots of low latency, user facing web traffic!) Applications connect to MySQL servers through a layer 7 proxy, open-source ProxySQL MySQL clusters have a single primary and use asynchronous replication. Most deployments span geographically sparse data centers (we love scaling with MySQL replicas!) ZooKeeper based service discovery system, used for applications to discover proxies and proxies to discover MySQL databases Open-source Orchestrator deployed to multiple datacenters in raft consensus mode for high availability and failure detection of MySQL servers MySQL primary replacements are performed due to MySQL crashes, hardware failure and maintenance (hardware, operating system, MySQL upgrades). For unplanned failures, Orchestrator detects the failure and initiates the recovery procedure. For planned server upgrades, an on-call engineer can invoke Orchestrator’s primary replacement procedure. We are able to minimize MySQL downtime when replacing a MySQL primary because: MySQL clients (applications) remain connected to a proxy tier Orchestrator detects failure within seconds, then initiates MySQL specific recoveries and elects a new primary server the new primary server indicates to the service discovery system that it is the primary for a set of databases the proxy tier watches for the update to the service discovery system and adds the identity of the new primary server to its configuration When the proxy tier has discovered the new primary server, the replacement is complete and applications are again able to write data to the database. This procedure is completed in seconds! A closer look at how everything fits together: Individual components store and consume data in ZooKeeper, storing their own identities (IP addresses) and reading the identities of other components Applications establish connections to ProxySQL and issue queries ProxySQL maintains a connection pool to each MySQL server, and proxies client connections to connections in its pool Orchestrator maintains a connection pool to each MySQL server, constantly performing health checks and is ready to initiate a failure recovery when necessary ProxySQL as a highly available proxy layer ProxySQL is a high performance, high availability, protocol aware proxy for MySQL. We love ProxySQL because it limits the number of MySQL connections to our MySQL servers and it permits us to replace MySQL servers without requiring applications to re-establish their database connections. Deployment We deploy ProxySQL using AWS Auto-scaling groups and AWS EC2. We configure these servers to run ProxySQL after powering on, using Puppet, and since they are relatively stateless we are able to add or replace ProxySQL capacity very quickly, in less than 10 minutes. Configuring ProxySQL to route to MySQL backends We use ProxySQL’s hostgroup functionality to group MySQL servers into tuples of (MySQL schema, MySQL role), where MySQL schema is one of our vertical shards to isolate workloads and MySQL role is one of {primary, replica, reporting replica} to isolate read/write, read only, and non-user facing read traffic respectively. A single MySQL user maps uniquely to a hostgroup, which means that an application only needs to present a username and password to ProxySQL to be routed and load balanced to the proper database and database role. Each ProxySQL server must be configured with the set of available MySQL servers and continue to stay up to date as MySQL capacity is added, replaced, or when hosts transition between MySQL roles and therefore hostgroups. On a several minute interval, a script runs on each ProxySQL server to read the available MySQL servers and their roles from our ZooKeeper based service discovery system and load them into ProxySQL’s configuration as hostgroups. This script is idempotent and also contains important verification functionality, such as preventing a mass-removal of MySQL servers if an outage of the service discovery system is detected or ensuring that only one server exists in the “primary” hostgroup for each cluster. The latter verification method is a key component of ensuring that our primary failover system is safe in the face of network partitions. Applications connecting to ProxySQL Just as MySQL servers register into service discovery so that they can be discovered by ProxySQL servers, ProxySQL servers register into the same system so that applications are able to discover and connect to them. Applications read the identity of ProxySQL servers from service discovery and supply a username and password deployed with the application to initiate their MySQL connections. Service Discovery At Yelp, the data plane of our service discovery system consists of a daemon on each server that performs HTTP or TCP healthchecks on a service, and if the service is healthy, stores information including the IP address and port of the service in ZooKeeper. If a service fails to respond successfully to its healthcheck, this daemon will remove the state of the failing service instance. A separate daemon is responsible for reading the state in ZooKeeper and proxying requests through the service mesh. MySQL registration and healthcheck MySQL servers are grouped by (MySQL schema, MySQL role) where MySQL role is a value in {primary, replica, reporting replica}. Both the MySQL schema and MySQL role values are represented as files on disk of each MySQL server. These files are understood by the process that performs health checks and are used to represent the (MySQL schema, MySQL role) groupings in ZooKeeper. Our health check for the MySQL replica services is more thorough than only verifying that the MySQL port is open since these servers are running stateful workloads that require significant configuration. Before a MySQL replica is deemed to be healthy, it must pass all of the monitoring checks defined using our monitoring framework. To accommodate this, an HTTP service is deployed on each MySQL server to provide an HTTP health check endpoint which verifies that the server has passed all of its monitoring checks before the MySQL process is considered healthy. Some examples of these monitoring checks are: The server restored from backup successfully The server is replicating and is caught up to real time The server is “warmed” by streaming a MySQL buffer pool from another server in the cluster and loading it into its own buffer pool ProxySQL healthcheck Because ProxySQL servers are lightweight and almost completely stateless, a ProxySQL server is considered healthy as long as it is listening for TCP connections on the defined ProxySQL port. After the ProxySQL process is launched and begins listening for TCP connections, it passes its health check and is discoverable by applications. Orchestrator driven Failure Recovery Orchestrator is an open source MySQL high availability and replication management tool that provides failure detection and automated recovery of MySQL servers. We deploy Orchestrator using its distributed raft mode in order to have the service be highly available and to provide improved failure detection of MySQL servers. Orchestrator’s failure recovery features solve the single point of failure presented with a single primary MySQL configuration mentioned earlier in this post. Upon detecting a failure of a MySQL server, the multiple orchestrator instances running in raft mode will seek consensus of the identified failure, and if a quorum of instances agree, a recovery will proceed. If the failed server is a replica and is a replication source for other replicas, Orchestrator will ensure that these replicas are re-configured to replicate from a healthy replication source. If the failed server is a primary, Orchestrator will proceed to set the failed primary to read-only mode (MySQL variable @@read_only=1), identify a candidate to be promoted to primary, re-configure replicas of the former primary to replicate from the candidate primary, and set the candidate primary to read-write mode (@@read_only=0). Orchestrator handles the MySQL specific changes for replacing a primary server and allows definitions of “failover hooks” to run custom defined commands during different phases of the recovery process. Primary Failover Hooks Orchestrator performs the MySQL specific part of the failover but there are still other changes required, such as modifying the file on disk representing a server’s MySQL role to the service discovery system. An HTTP service exists on each MySQL server in order to support this, and failover hooks are configured to send an HTTP request to both the former and newly promoted primaries to update their MySQL role. After this hook executes, the service discovery daemon will notice that the MySQL role of the promoted primary has changed and will update the identity of the primary server in ZooKeeper. Configuring ProxySQL with the updated Primary As mentioned earlier, each ProxySQL server runs a script on a several minute interval which reads the MySQL service discovery state in ZooKeeper and ingests this data to ProxySQL’s configuration. In order to reduce the recovery time after a primary failover, a separate process runs on ProxySQL servers to watch the identities of MySQL primaries in ZooKeeper and to initiate the previous process immediately when a change is noticed. Perspective of a MySQL client during a primary failover After Orchestrator issues set @@read_only=1 on the former primary, clients will see INSERT/UPDATE/DELETE queries fail. These failures will remain until ProxySQL has updated its hostgroup configuration to replace the failed primary with the promoted one. Neither applications or ProxySQL need to create new TCP connections – clients remain connected to the same ProxySQL server and each ProxySQL server already has an existing pool of connections to the promoted primary because it was previously existing as a replica. After modifying its hostgroup configuration, a ProxySQL server is able to route MySQL traffic to the new primary. Special cases:  network partitioning and avoiding split-brain This failure recovery system is carefully designed to make the right decision in failure scenarios caused by a network partition. A partial or incorrect failure recovery due to a network partition has the potential to leave the system with multiple primary hosts, each believing they are the primary, resulting in a divergence of the dataset known as “split-brain”. It is very difficult to repair a split-brain scenario, so we have several components in this system to help prevent this. One mechanism to prevent the possibility of split-brain is validation in the logic which transforms the service discovery data in ZooKeeper into ProxySQL’s hostgroup configurations. If there is more than 1 primary registered in ZooKeeper, the script will refuse to make changes to the hostgroup configurations and emit an alert to page an on-call responder who can inspect and appropriately remediate this situation. We also set Orchestrator’s PreventCrossDataCenterMasterFailover value to true so that Orchestrator would not ever elect a new MySQL primary in a separate datacenter. We use this setting because we would not want to change the datacenter of a MySQL cluster’s primary without considerable planning and because it reduces the surface area of potential network partition scenarios that could result in split-brain. Conclusions Thanks to these systems, we are able to quickly recover from MySQL failures and maximize the availability of Yelp for our users, ensuring a smooth user experience. Tweet Become a Database Reliability Engineer at Yelp Want to help make our databases even more reliable? View Job Back to blog", "date": "2020-11-09"}, {"website": "Yelp", "title": "Tales of a Mobile Developer on Consumer Growth", "author": ["\n        \n  Arash Nase, Android Developer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/11/tales-of-a-mobile-developer-on-consumer-growth.html", "abstract": "Engineers on Yelp’s Consumer Growth team work closely with product managers, data scientists, and designers to increase user acquisition, engagement and retention to fuel the rest of the business. The team is central to growing the number of active users on the Yelp platform. In-App Update , introduced in Android Pie, is a feature that allows Android users to update the app by showing a prompt while they’re using the app and keeps users in the app as the update is happening in the background. We thought it would be a valuable feature for consumers on Yelp and began researching and using data to determine how we would allocate our time and effort into supporting this new project. In this blog post, I’ll explain the process behind how the Consumer Growth team goes from idea to implementation to feature rollout. Proposals: PEP and CEP At Yelp, many product and technical projects are initiated by engineers, and engineers have two main mediums to share their ideas: Product Enhancement Proposal (PEP) and Code Enhancement Proposal (CEP) documents.\nA PEP is a way of sharing critical info about your feature with others on the product team such as designers, UX researchers, data scientists, and product managers. It should help guide the conversation about the impact of the feature, design, and measurement strategy instead of inefficiently putting that context in emails and issue tracking systems.\nA CEP is a collaboration vehicle for significant changes made to the Yelp code stack. It is an approach for building consensus around technical direction - a way of communicating how you addressed a problem with stakeholders who will not be involved in the details of the implementation (for example, senior devs or tech leads of other teams). It is left to the discretion of each engineering team to determine if something is “CEP-worthy,” depending on the technical complexity of the project. PEP for In-App Update Going back to the example of In-App Update mentioned earlier, our PEP covered the user/business problem, hypothesis, description, and how to measure success. The following are the core topics discussed in the In-App Update PEP. User/business problem Being taken away from the app for updating (e.g., going to Play Store) could lead to user churn, and In-App Update allows us to prompt a user to update their app without being taken away from the app itself. Also, based on research in the past, we’ve seen that updating the Yelp app improved the Lifetime Value of the user. Considering these two points, we presumed In-App Update would increase the monthly active users (MAU) of the app, which is a metric important to the Consumer Growth team. Description Based on the developer website , there are two types of updates apps can adopt: flexible and immediate. We want to use a flexible update mostly but add support for an immediate update in case of an emergency. Only show a flexible update prompt to the user if the user app version is at least four versions behind the latest released app version. Given our weekly release cycle, add the functionality to show the update prompt whenever we want by triggering the update from the backend. If the user discards the prompt, we’re not going to ask them to update again until four weeks later. Hypothesis The PEP hypothesis was that implementing In-App Update will increase MAU and provide a platform to trigger app updates at any time. The feature ensures that more users will take advantage of the latest features and bug fixes of the Yelp Android app. Measuring Success In a PEP, we must determine how we measure success. At Yelp, we use an internal tool called Bunsen to run experiments, log events, and measure success for a feature. We decided to run an A/B experiment with 50% of the users in the status quo cohort and 50% in the treatment cohort - meaning only 50% of the users will experience In-App Update during the experiment run. We usually don’t release features to 100% until we’re sure the users in treatment cohorts are not being affected negatively. Moreover, we wanted to monitor update vs. dismiss rate (participation) for the prompt to make the right trade-offs on to whom and how often we want to show the prompt. To better understand how user activity and MAU are affected, we decided to monitor the average sessions of users per day metric using Bunsen. Choosing the primary metric and deciding on the success criteria is determined by discussions among engineers, PMs, and data scientists. We said, “if In-App Update increases average sessions of users per day and does not have any negative impact on other vital metrics during the experiment run, we ship it to 100%.” CEP for In-App Update As CEPs tend to be more technical and intended to be shared among engineers, for the In-App Update CEP, we explained technical requirements, milestones and issues regarding performance, monitoring, cost, security and privacy, if need be. Requirements One of the main product requirements that would drive the proposed solution is for In-App Update to be backend-driven. Since we want to prompt the user for flexible or immediate updates whenever we want, we need help from backend developers to create an API. This API receives the latest released version code and the user app version code, then tells the client if a flexible or immediate update is needed in a single response. Additionally, we need to figure out a way to calculate if a user is four (or X) versions behind by reading our app version code. Milestones Eventually we need to go from having a list of things to do, to having a list of cost & time estimates for each thing to do, and then to having some idea of what the schedule might look like for delivering. We settled on the following project milestones: Milestone Description Owner ETA M0 Write PEP & CEP EngineerA 1/1 M1 Make it easier to calculate how many app versions behind a user is from the latest update by following semantic versioning EngineerB 1/7 M2 Implement the backend API. This endpoint gets the user app version and latest released app version and determines if a flexible or immediate update is needed EngineerC 1/10 M3 On client, implement InAppUpdateManager class in the Support module to handle everything related to updates in Android codebase EngineerA 1/10 M4 Set up experiment and event logging EngineerA 1/12 M5 Measure success EngineerA, PM 1/20 Implementation The client-side implementation details are outlined in the developer website and are straightforward. The In-App Update flexible prompt for Yelp is shown below. On the home screen, we make a call to our backend API with the user’s app version and latest app version available on the Play Store. The backend figures out if the update prompt should be shown or not, and lets the client know in a single response. It’s also the backend that decides whether immediate or flexible prompts should be shown, making it a backend-driven feature which gives us the flexibility to change to whom we want to show the prompts and when without an app release. Result We monitored user interaction with the feature over 30 days using Bunsen by running an A/B experiment. We created two cohorts, with 50% of our users assigned in the status quo cohort and the other 50% in the treatment cohort. For users that were in the treatment cohort and interacted with the In-App Update feature, we saw a lift in our primary metric (average sessions of users per day) in 30 days. By manual testing the feature, we realized In-App Update would provide a better user experience for users when updating the app by presenting a reliable flow in which they don’t have to leave the app. The experimentation process gave us greater confidence in shipping the feature to 100 percent. It also informed our decision to limit showing the prompt to more select users so as not to inconvenience their app experience. For example, we now only show the prompt to users who are at least eight app versions behind. Doing so was super easy, and was achieved by changing server config files in the backend. Acknowledgments In this blog post we talked about how engineers at Yelp Consumer Growth team use CEPs and PEPs to present their ideas to the team and go from ideation to implementation, and gave you some insight on how we use data-driven development to measure success for a project. Shoutouts to the following Yelpers for their support during the project:\nAditya Vaidyanathan, David Kuo, Matthew Page, Sam Faber-Manning, Rishi Goel, and Rajeev Sarvaria Written by Arash Nase ( @_arashism ) Tweet Become an Android Developer at Yelp “Passionate about mobile space and Android development? We’re hiring! Check out our open positions.” View Job Back to blog", "date": "2020-11-13"}, {"website": "Yelp", "title": "Orchestrating Cassandra on Kubernetes with Operators", "author": ["\n        \n  Raghavendra D Prabhu, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/11/orchestrating-cassandra-on-kubernetes-with-operators.html", "abstract": "This post is about how Yelp is transitioning from the management of Cassandra clusters in EC2 to orchestrating the same clusters in production on Kubernetes. We will start by discussing the EC2-based deployment we have used for the past few years, followed by an introduction to the Cassandra operator, its responsibilities, the core reconciliation workflow of the operator, and finally, the etcd locking we employ for cross-region coordination. Cassandra is a distributed wide-column NoSQL datastore and is used at Yelp for both primary and derived data. Yelp’s infrastructure for Cassandra has been deployed on AWS EC2 and ASG (Autoscaling Group) for a while now. Each Cassandra cluster in production spans multiple AWS regions. A synapse -based seed provider is used during Cassandra cluster bootstrap. AWS EBS is used for storing data and EBS snapshots are used for backups. Intra-AZ mobility of EBS between EC2 instances provides better separation between stateless compute and stateful storage. This separation makes the job of an orchestrator simpler too, as we will see later. We have been using Taskerman , our home-grown Zookeeper and AWS SQS based distributed task manager for managing deployments and lifecycles of multi-region Cassandra clusters. This has supported us well over the years and eliminated our operational toil significantly. Meanwhile, Yelp’s platform as a service - PaaSTA - has been transitioning from Mesos to Kubernetes (k8s). PaaSTA provides good abstractions for fleet management and a consistent interface for operations. With good support for state management in k8s and more importantly, being quite extensible, this provided an opportunity to explore the orchestration of Cassandra on k8s. The Kubernetes operator for Cassandra helps tie all of these together, capture database and infrastructure specific requirements, and operationalize any learnings. Operator Intro The aforementioned Cassandra operator runs on Kubernetes (k8s) and Yelp PaaSTA, with one operator per production region. The operator is written in Go and uses operator-sdk . It manages Cassandra clusters through the abstractions of Custom Resources and Statefulsets . Since it runs on PaaSTA, this also allows us to pause (“Big Red Button”) the operator itself, whenever human intervention is required. The chief responsibilities of the Cassandra operator are: Reconciling the runtime state of the cluster with cluster specification. Scaling the cluster up and down safely one Pod at a time. Lifecycle management: handling dying nodes and pods and replacing them with healthy replacements. Coordinating with operators managing the same cluster in other regions. Safely allowing for new changes to be deployed and the cluster to be restarted. Garbage collection of crashing and pending pods upon reconciliation. Balancing resource utilization and availability with affinities. Managing the TLS credentials required Cassandra internode and client-server authentication. Before we delve deeper into operator workflow, let’s have a brief look at what goes into a typical Cassandra pod . Every Cassandra pod contains one Cassandra container and a few sidecars . The sidecar containers are: HAcheck container for health checking and reporting availability to our service discovery mechanism. Cron container where we run periodic tasks such as those for snapshotting. Sensu container where we deploy our Sensu checks. Prometheus exporter for publishing metrics to Grafana dashboards. Change Data Capture (CDC) publisher to publish table changes to the Yelp Data Pipeline . For the readiness check, we ensure every pod in a cluster in each region has Cassandra in the UN state . For liveness checking, we ensure that Cassandra is up (U) and in one of the leaving (UL) / joining (UJ) / normal (UN) states. Operator Workflow On a very high level, this is what the operator workflow looks like. The Operator watches the k8s Custom Resource (CR) (which is continually refreshed by PaaSTA based on on-disk config updates) and also listens to per-cluster events in a loop. In case the runtime state of the cluster differs from what is desired in the configuration, it updates the state to “reconcile” it with what is desired. Since these changes happen without any human intervention, safety is the number one priority. Let’s take a look at what the cluster specification looks like before jumping into the intricacies of the reconciliation loop. Cluster Specification # yelpsoa-configs/cassandracluster-pnw-dev.yaml feeder_instance : clusterName : feeder replicas : 3 deploy_group : dev.everything cpus : 4 mem : 8Gi disk : 400Gi env : JVM_EXTRA_OPTS : \" -Dcassandra.allow_unsafe_aggressive_sstable_expiration=true\" cassandraConfig : file_cache_size_in_mb : 1024 row_cache_size_in_mb : 512 cdcEnabled : true cdcPublisherConfig : enable_flush : true enable_filler_writes : true publish_every_m : 5 This is a specification of the cluster in yelpsoa-configs , PaaSTA’s config store. Here we are creating a new “feeder” cluster of 3 replicas with 4 AWS vCPUs, 8 GiB memory, and 400 GiB EBS volume attached to each. We are also passing some Cassandra configuration overrides and additional environment variables to the cluster processes. Finally, we can see that we have enabled the CDC for this cluster with its associated configuration.  Note that, in general, all of these options are not required since we have followed a “batteries included” approach wherein a new cluster with a good config and TLS  can be created out-of-the-box. This specification is wrapped into a k8s Custom Resource by PaaSTA when consumed by the operator. Reconciliation Loop Now that we know what the spec looks like, let’s delve deeper into the cluster update reconciliation flow. For the ‘feeder’ cluster whenever there is a new trigger, the operator does the following: Creation : Check if the statefulset (STS) corresponding to the cluster exists. If not, then this is pretty much straightforward in that we create a new STS and wait. Hash-based reconciliation : If the STS exists but the specification has been updated (say, CPU increased from 2 to 4), we need to update the STS. To detect if the specification has been updated, we hash the STS object excluding the number of replicas (since we handle scale up/down differently below). If the hash has changed, we need to update the STS. We explored and tested multiple in-place STS update strategies but they turned out to be flaky, breaking the operator sporadically. Instead, the operator deletes the STS without deleting its pods (“non-cascade deletion”) followed by recreation of the STS from the new spec. The new STS gets attached to the existing pods. This has been stable for us and, in general, much more clean and maintainable. After the recreation of STS, we also garbage collect pre-existing Pending and Failed pods so that STS recreates them. This has helped us in getting the STS unstuck from bad config pushes. Horizontal Scaling : If only the number of replicas has changed, we handle this separately because as a database there are a few actions required to manage Cassandra cluster membership before we scale up/down. First and foremost, since the scale up/down of a cluster is not critical, we check if the cluster is healthy before we proceed. If the cluster is unhealthy, we wait. If the cluster is fine for scale up, we increase STS size by one and replace it with the strategy mentioned above (delete + create). If the cluster is scaling down, we need to decommission the Cassandra replica first before scaling down. Hence, we first decommission the pod with the highest ordinal number (due to the nature of STS, this is the latest pod to join the cluster). Next, we decrement the STS by one and continue with STS replacement as before. A key thing to note is that we scale up/down by one unit at a time. This is from a safety perspective. End of the loop : Finally, if there is nothing to do, the operator requeues a reconcile event to check the cluster after a pre-configured ‘X’ seconds, thus completing that iteration of the controller loop. To avoid head-of-line blocking between clusters, we usually set the maximum number of concurrent reconciliations (“ MaxConcurrentReconciles ” in controller-runtime) to a high value. Locking : Keen observers of the flow diagram above would have noticed some locking and unlocking in it. This is the etcd locking which we use for coordination between multiple regions, and for custom resource status updates. More on this later. The key takeaway, for now, is that the operator removes etcd lock only when the cluster is ready. Cassandra Storage All of this orchestration has been made simpler owing to the intra-AZ mobility of AWS EBS . EBS for each pod of the cluster is provisioned dynamically from the spec with a Persistent Volume Claim (PVC) which accordingly creates the Persistent Volume (EBS in our case). A storage class defines the nature and semantics of this EBS creation. We create a storage class per cluster and it looks like something like: # feeder is the name of the cluster allowVolumeExpansion : true apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : cassandra-ebs-feeder parameters : fsType : xfs type : gp2 provisioner : kubernetes.io/aws-ebs reclaimPolicy : Delete volumeBindingMode : Immediate In short, this ensures that every new pod gets a gp2 EBS volume with XFS filesystem and dynamic volume expansion. The latter allows us to resize EBS volumes on the fly with zero disruption (a topic for another exciting blog post). Finally, with volumeBindingMode of “Immediate”, we get the “compute-follows-data” model which is what we want given that data streaming can be very expensive. With this mode, the storage class ensures volumes are round-robined across multiple Availability Zones first before any of the corresponding pods are created for those volumes.  This also ensures that pods are not created in a different AZ from the volume itself. This mobility of data within the AZ also allows us to run non-production and internal workloads on AWS Spot ASGs while keeping production ones on on-demand / reserved instances, and contribute to cost savings. We use Clusterman to manage the spot and on-demand/reserved instance fleets. Cross-region Coordination In PaaSTA, we have deployed one k8s cluster per AWS region rather than a federated deployment . This means that there is one operator per region for Cassandra. Hence, for Cassandra clusters that span multiple regions, we need to ensure that these operators coordinate with each other. This is where etcd locking comes into play.\nSince we did not find any construct in operator-sdk or container-runtime for coordination between operators, we added our own locking mechanism in the operator. Since etcd is native to Kubernetes and is used for other builtin coordination, we employed it for this use case as well. Broadly speaking, we are employing locking in the following use cases: During cluster creation to ensure clusters don’t bootstrap independently at the same time in different regions. During scale up/down of the cluster, to ensure we do it one region at a time. Ensuring that any breaking change to one region does not propagate to another region. To avoid deadlocks and staleness, we couple the lock with a lease with a preset expiry. This lease is renewed by the operator as part of its reconciliation activity. The lock is released when the cluster is ready for that region. There is so much more to write and describe here - zero-downtime migration strategies we employed, use of anti-affinity for availability, CI/CD pipeline for Cassandra deployments, use of Clusterman for fleet management, Secret management and TLS, IAMs -  and much much more. However, those are reserved for sequels to this blog post. As of this post, we have migrated most of our production clusters to k8s, and migration of the remaining clusters is in progress. As always, there are more interesting challenges and opportunities in this space. If you are interested to know more about these,  what better way is there than to come and work for us. We are hiring ! Acknowledgments Raghavendra Prabhu (author), Sheng Ran, and Matthew Mead-Briggs for the work on the Cassandra Operator.  The author would also like to thank Database Reliability Engineering (DRE) and Compute Infrastructure teams for various contributions to the project. Logo/Icon credits: AWS Architecture Icons . Tweet Become a Software Engineer in the DRE team at Yelp! Interested to know more about Cassandra and Kubernetes? Join our DRE team! View Job Back to blog", "date": "2020-11-16"}, {"website": "Yelp", "title": "Now You See Me: How NICE and PDQ plots Uncover Model Behaviors Hidden by Partial Dependence Plots", "author": ["\n        \n  Shichao Ma, Applied Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/12/now-you-see-me-how-nice-and-pdq-plots-uncover-model-behaviors-hidden-by-partial-dependence-plots.html", "abstract": "Many machine learning (ML) practitioners use partial dependence plots (PDP) to gain insights into model behaviors. But have you run into situations where PDPs average two groups with different behaviors and produce curves applicable to none? Are you longing for tools that help you understand detailed model behavior in a visually manageable way? Look no further! We are thrilled to share with you our newest model interpretation tools: the Nearby Individual Conditional Expectation plot and its companion, the Partial Dependence at Quantiles plot. They highlight local behaviors and hint at how much we may trust such readings. A not NICE world At Yelp, we have ML models for personalized user and business owner recommendations , the retention of advertisers, wait time prediction in Waitlist , ads targeting , business matching , etc. Although the prediction quality is always one of the key priorities for any ML model, we also care deeply about the interpretability of the model. As ML practitioners, we often use model interpretation tools to do sanity checks on how a model is generalizing from the features. More importantly, exposing the “why” behind a model’s behavior to its consumers, who often are not ML practitioners, can give them confidence in its accuracy and generalizability, or lead them to deeper applications and better business decisions. Since most of our models are complex in order to achieve better prediction quality, they can also be harder to decipher. One common question in model understanding is, “How do changes in a feature’s values relate to changes in the prediction?” Previously, we used the popular PDP and the sensitivity plot to take snapshots from the model that are easy for a human to understand. A PDP shows how predictions change, on average, when varying a single feature 1 over its values (e.g., min to max) and holding the other features constant. PDPs can answer questions like, “What would users’ wait time be respectively if the local temperature were 30°F, 50°F, and 70°F?” In contrast, a sensitivity plot varies a single feature relatively (e.g., -15% to +15%) while holding the other features constant. Sensitivity plots can answer questions like, “If the weather had been 10% warmer for these days (temperatures on these days in general are different), how would wait time estimates have changed?” However, these tools are not without some limitations. First of all, both PDP and sensitivity plots operate at an aggregated level, meaning that we average all the data points to achieve one single curve. This aggregation may hide differences in various subpopulations. For example, when creating either a sensitivity plot or a PDP, we could imagine the prediction goes up in half of the population and goes down in the other half when we increase a feature. When we average the two halves together, we may falsely conclude that the feature has no marginal contribution to the prediction. Secondly, when drawing PDPs or sensitivity plots over sparse data regions, both plots become untrustworthy. For example, if we only have a few restaurants open at 0°F, then it’s usually unwise to generalize from a PDP for wait times around such low temperatures. To address these concerns, we came up with two new tools: the Nearby Individual Conditional Expectation (NICE) plot and its companion the Partial Dependence at Quantiles (PDQ) plot. Instead of the aggregate effect, the NICE plot individually draws changes in predictions due to local perturbations on top of the scatter plot between feature values and corresponding predictions. The PDQ plot helps to summarize the heterogeneity in the NICE plot by aggregating partial dependence at different quantiles of predictions. In practice, we often need to review the PDQ plot when we have difficulties in figuring out the general patterns in a NICE plot. What is the NICE plot? NICE plots examine the Individual Conditional Expectation in the neighborhood of the original feature values. Below is an example plot from one feature in one of our retention models. Note: This graph contains 1000 data points and each blue line consists of 7 points. We made this NICE plot using the following algorithm: Select a random sample of data points (if your dataset is large). Make a scatter plot of feature values and model predictions (the black dots). Make nearby perturbations about each feature value (e.g., lower_bound = 0.9 * feature_value and upper_bound = 1.1 * feature_value ) and evenly sample N points within the bounds (we recommend N to be odd so the original feature value is included). Record their corresponding perturbed predictions. Draw lines between the N points and corresponding predictions on the scatter plot (the blue lines). A NICE plot foremost shows the bivariate distribution between feature values and their corresponding predictions. Therefore, it is straightforward to observe the sparsity between the two. In the above example, the model rarely gives a low prediction when the feature is smaller than 4 (exhibited by the white space on the bottom left corner) and it rarely gives a high prediction when the feature value is roughly smaller than 1 (illustrated by the white triangle-like shape on the top left corner). More importantly, this plot only examines marginal effects at the neighborhood of each observed data point, which helps to show heterogeneous effects and may hint at any interaction effects. In the above graph, the marginal effect goes up and then goes down when the feature value is in the range of 0 to 1. Starting from 1, the effect is positive and large in magnitude until the feature value reaches to 2. In the range 2 to 4, we observe some heterogeneous effects: some lines are downward sloping while others are flat, and the flat ones are observed more often when the prediction gets larger. When the feature value is greater than 6, all the NICE lines are flat throughout the region. On the other hand, the information in the PDP and the sensitivity plot of the same feature lacks many details. Note: the y-axes in these figures have narrower ranges than the previous NICE plot because of aggregation. The PDP (left) correctly captures the most significant inverted V-shape structure when the feature is smaller than 4 and the flat shape afterwards, but it loses some subtleties contained in the V-shape. The sensitivity plot (right) is misleading. From it, you may conclude that tweaking the feature would yield a single-peaked relationship with the peak at -20% of the feature value, which is true only in aggregate. From the NICE plot, we can see this relationship fails to hold for most, if not all, individual data points: the marginal effects are flat when the feature values are greater than 4, and have the “wrong” shape for samples in the valley near one. When trying to apply the above algorithm to binary or categorical features, we cannot make nearby perturbations and have to examine the change from one value to another. Below is one example from our system. Note: we apply jitter to the feature values to make the density easier to see. As one can see, the NICE plot is still useful to demonstrate heterogeneous effects. In the above figure, the lines at the top of the figure (corresponding to high prediction values) are flatter. But when the predictions get smaller, the lines get steeper. For comparison, below is the PDP of the same feature: Note: the y-axis in this figure has a narrower range than the previous NICE plot because of aggregation. Clearly, the PDP manages to capture the aggregated trend, but misses the differences in marginal effects when the predicted values are different. Using this plot one cannot see the heterogeneity in these marginal effects. The structures contained in a NICE plot, however, may be both a blessing and a curse. When the model has complex interaction effects, it is hard for a human to decipher all the subtleties from the numerous dots and lines in a NICE plot. To mitigate this issue, we developed a companion tool: the PDQ plot. What is the PDQ plot? A PDQ plot is a variation of the conventional PDP. It stands on the middle ground between the fully local NICE plot and fully global PDP. It plots the partial dependence conditional on some pre-specified quantiles of the predicted values, which helps to simplify the heterogeneity and emphasizes the major structures in a NICE plot. Here is the PDQ of the first NICE plot in this article: We made this PDQ plot using the following algorithm: Select the quantile values to be drawn. Our default values are 0.05, 0.25, 0.5, 0.75, and 0.95. For each quantile, find data points that can produce predictions that are close to the desired quantile (e.g., the desired quantile +/− 0.001). 2 Again for each quantile, generate and plot partial dependencies using only those samples. 3 From the plot, we can easily identify a sharp inverted V-shape structure when the predicted value is small, but this non-monotonic effect gradually flattens out as the prediction increases. When the prediction is sufficiently large (starting from the 0.75 quantile), we do not see a significant drop after the initial rise. In practice, one can use the corresponding PDQ plot to help make sense of the NICE plot. For example, it may be unclear to some readers that the non-monotonic effect gradually flattens as the prediction increases by just inspecting the NICE plot. Indeed, a lot is going on in a small region. But after observing the PDQ plot, one can go back and re-examine the NICE plot. If PDQ plots can represent information confined in NICE plots in a concise fashion, why don’t we solely rely on them? Firstly, PDQ plots still need to aggregate some data. Therefore, it is difficult, if not impossible, to differentiate a mix shift from an inherent behavior change of the model by just examining a PDQ plot. For example, you may possibly think that the gradually flattened V-shape structure is because the negative marginal effects are less steep when the predictions are higher, which can be ruled out with the help of the corresponding NICE plot. Moreover, PDQ plots have a data sparsity issue. We cannot observe the bivariate distribution in PDQ plots. Therefore, in some regions we do not have many, if any, data points. The following two figures constitute a good example. It is very tempting to conclude that the effect gradually flattens out after the feature value is greater than 50 when q=0.95 from the PDQ plot. However, we can see there are almost no data points with feature value greater than 50 and the prediction very high. Therefore, it is probably unjustified to assume such a relationship exists in that region. Finally, why do PDQ plots work in practice? We have repeatedly observed that the patterns in NICE plots can be roughly grouped by the predicted values. This probably is because samples that produce similar predictions are similar for the purpose of a specific prediction task. Therefore, these samples are more likely to share a common marginal effect. Conclusion A NICE plot is an individual conditional expectation plot restricted to feature values near the observed ones. It shows how the model would behave if we perturb a feature near its observed values while keeping all other features fixed. Reading a NICE plot can also tell us how much we can trust such behaviors because the plot contains information about data sparsity. The PDQ plot helps to summarize the heterogeneity in the NICE plot by grouping partial dependence at different quantiles. We typically consult the corresponding PDQ plot when we have difficulties in figuring out the general patterns in a NICE plot. PDQ works because data points with similar predictions behave more similarly than the ones without in a given prediction task. Acknowledgements The original idea of the NICE plot belongs to Jeffrey Seifried. Blake Larkin, Nelson Lee, Eric Liu, Jeffrey Seifried, Vishnu Purushothaman Sreenivasan, and Ning Xu (ordered alphabetically) help read through the earlier versions and make helpful comments. Notes 1 : We can vary multiple features and draw a multivariate PDP, but the interpretation gets very difficult past two features! 2 : To reduce noise, we do not just select a handful of data points exactly at the pre-defined quantile. In general, samples with different predictions may behave differently in terms of their marginal effects, and you don’t want to be fooled by a tiny sample. 3 : You can check scikit-learn’s implementation if you need help computing partial dependencies. Tweet Become an Applied Scientist at Yelp! Are you intrigued by data? Uncover insights and carry out ideas through statistical and predictive models. View Job Back to blog", "date": "2020-12-17"}, {"website": "Yelp", "title": "Yelp Takes on Grace Hopper 2019!", "author": ["\n        \n  Surashree Kulkarni, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/02/grace-hopper-yelp-2019.html", "abstract": "Last October we sent a group of Yelpers to the 2019 Grace Hopper Celebration! Here are a few takeaways and reflections from some of our attendees. Who attended? Surashree K., software engineer on Semantic Business Information Clara M., product design lead on Content Anna F., machine learning engineer on Semantic Business Information Nikunja G., software engineer on Infrastructure Security Catlyn K., software engineer on Stream Processing What was your favorite session? Surashree : Honestly, it’s hard to choose, but the one that stuck with me was the talk by Jackie Tsay and Matthew Dierker on Google’s Smart Compose, the Gmail feature that helps people write emails faster by auto-completing sentences. It was interesting to learn about inherent biases the earlier versions of the model had, and the engineering decisions that went into combatting those. The speakers also talked about some of the feedback they received; one that was especially moving was from a non-native English speaker who was happy to have a feature that would make writing emails in English easier. Clara : Definitely the talk on AI Meets Creativity by Dr. Pinar Yanardag. It was fascinating how she analyzed the way AI algorithms can actually inspire the creative process. She shared an example of an algorithm that analyzed dress patterns and generated a pattern that a fashion designer went on to create. She also shared examples of how this could work for graffiti art, pizza recipes, and even making perfume. The talk was not only visually compelling but also made me think a lot more about how AI can actually help boost creativity rather than stifle it. Anna : One of my favorite sessions was FarmBeats, Microsoft’s AI and IoT system for agriculture by Zerina Kapetanovic. She first described the challenges in setting up “smart” data-driven agriculture, including low rural internet connectivity, electricity access, and the high cost of sensors. She then walked us through creative solutions for each problem, ranging from clever uses of solar panels to dangling smartphones from balloons to approximate drone footage. It was inspiring to see how this collection of workarounds and approximations came together into a coherent and precise solution. A recurring theme throughout many of the sessions I attended was how AI can enhance human capabilities by putting better tools in more people’s hands. AI enables us to get great results even in uncontrolled situations or where precision hardware isn’t available. The FarmBeats talk, described above, demonstrates this in the field of agriculture. I’m excited to see what specialist tools AI will make commonplace in the future. Nikunja : I work in Security, and to see a good representation of women in this field was a welcome change. One of my favorite sessions was the interactive security game put together by three engineers working at OneMedical. The goal was to secure a fictional organization that challenged you to prioritize security projects within an ever-changing threat landscape. The session was highly interactive, informational, and most of all, extremely fun! I never anticipated that such a session could be presented at a conference like Grace Hopper, and I brought back some major takeaways to share with my team at Yelp. What was the best career advice you received? Surashree : My one key takeaway from the conference was the importance of standing up for yourself and others. One of the talks by the CEO of AnitaB.org, Brenda Wilkersan, and COO Jacqueline Copeland, highlighted the still pertinent issue of the gender pay gap in tech and how it isn’t enough for companies to simply hire more diverse people, they also need to create an environment where all groups feel supported. One of our mottos here at Yelp is “Play well with others,” and this talk reminded me that the confidence we have in our daily lives comes from a certain level of privilege that we have to recognize. Catlyn : Don’t be ashamed to ask for more. According to one of the execs from Uber, women ask for less than their worth during the hiring process, resulting in a skewed sense of  self-evaluation. We also tend to refrain from responsibility unless we’re certain we’ll be the perfect candidate for the role. But no one’s perfect and it’s okay to figure things out along the way. You’ll never be 100% ready, so why not just seize the opportunity and enjoy the challenge! What advice would you give a first-time attendee? Surashree : Be prepared to be totally overwhelmed! There are some things you can do to make your life easy during those three days—download the app, check your schedule every day, carry a bottle of water, talk to as many people as possible, and attend as many talks as you can. But really, the sheer size of the conference and the activity around you will be hard to take in at first. Our recruiting team does a wonderful job of organizing, so our job as attendees is really to just make the most of the GHC experience. Nikunja : I feel that however much you prepare, in the end you’ll still feel overwhelmed and unprepared, so my number one suggestion is to go with the flow once you’re there. Having said that, it’s absolutely essential to do some prep before going, like organizing your schedule (pre-registering for sessions and having a good balance of booth duty and conference talks). Also, try attending a mix of sessions! GHC is unique in that it has so many different tracks in one place, so take advantage of it. Anna : Take travel time into account when signing up for sessions! The conference center is half a mile long, and you don’t want to miss a session or show up out of breath due to poor planning. What was your most memorable moment? Surashree : My most memorable moment came right at the beginning of the conference: the keynote by Aicha Evans, the CEO of Zoox. An immigrant from Senegal, her life went from one of domestication to now being the CEO of a company that’s building the next generation of autonomous cars. Her story was soft, gritty, and inspiring—all at once. Her question, “Whose genius are you going to ignite?” highlighted the importance of mentorship and giving back, something I believe we do very well here at Yelp. Clara : The closing keynotes with the DJ playing! It was such a fun atmosphere and a great way to close out the conference by hearing from so many amazing women doing innovative things in their industry. Nikunja : To be honest, this is a tough one, as the whole experience was very memorable in itself. However, there’s one that takes the cake. The conference has several award categories; one of them is the Student of Vision award, which was given to Jhilika Kumar, an undergrad student at Georgia Tech. At such a young age, Jhilika is the founder of AxisAbility, an organization she started to help the lives of differently abled people. Her passion for this cause stems from personal experience. Her brother faced so many challenges in his youth, and she wants to help him and others like him lead a better life. Her video, speech, and determination left so many people inspired, and showed us that no matter how young you are, you can create change. Why should one go to GHC? Clara : Honestly, at first I was skeptical about going to GHC as a product designer since I always thought it was a conference for software engineers. However, once I was there I realized how impactful it is for any woman working in the tech industry. I not only learned about new technologies, but also got the chance to be inspired by and network with other women in my industry. I was also surprised by how many people came by the booth looking to talk specifically with me since they knew Yelp had sent a product designer, which apparently not many other companies had. In general, it’s a great opportunity for everyone—product designers included! Catlyn : It was a great experience to be surrounded by so many brilliant women engineers who either are or once were facing the same career challenges that I am right now. I felt enlightened and empowered attending the talks and chatting with others from companies all over the world. I would strongly encourage everyone, especially those early on in their career, to attend some of the workshops to help you find out what kind of path you want to pave going forward and how you can get there. Surashree : Knowing what I know now, the biggest reason to go to GHC for me is to hear the stories from the lives of other female engineers. Working at Yelp, in the harmonious and safe environment that we have, it can be easy to overlook that not everyone has had the same advantages as myself, and not everyone’s experiences in tech have been the same. There are women who’ve had to deal with difficult situations- perhaps a toxic work culture, or misogyny in some form–and have come out stronger and brave enough to talk about it at conferences like this. So I’d say, go to GHC to learn about other people’s experiences and gain new perspectives! Tweet Become a Web Developer at Yelp Toronto! Join our Engineering team and help millions of people connect with local businesses on Yelp. View Job Back to blog", "date": "2020-02-12"}, {"website": "Yelp", "title": "An Ever Evolving Company Requires an Ever Evolving Communication Plan", "author": ["\n        \n  Kent Wills, Director of Engineering Effectiveness\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/03/an-ever-evolving-company-requires-an-ever-evolving-communication-plan.html", "abstract": "An Ever-Evolving Company Requires an Ever-Evolving Communication Plan It’s 2014 and your teams are divided by platform, something like: Web, Mobile Web, Android, and iOS. In order to launch features, product managers jump from platform to platform and teams move fast. Really fast. Lines of code in each repository increase to the point where you now name them “monoliths.” A few engineers maintain these monoliths when they need to, but no one is solely dedicated to the task. Engineers are distributed by platform; so communication on when to maintain the monoliths is easy, but presents another problem. Can you continue to ship code efficiently if you depend entirely on these monoliths? It turns out that as you increase developers and the size of the code base, the number of rollbacks and unscheduled mobile point releases also increases. At first you notice only a few rollbacks, but as your team grows, you start to estimate when all pushes result in a rollback. This is not the typical “up and to the right graph” that companies look for. Since rollbacks sound like a blocker, we’ve come up with an alternative: microservices. Then another: product teams. Now the company can scale both infrastructure and team organization. Product teams have a common set of infrastructure; the button used on the Growth team is the same button used on the Contributions team. Function-based (core) teams spin up. They work on the parts that individual maintainers worked on in the days of the monolith. They’re dedicated to making sure that, in the long term, we’re coding sustainably. Communication becomes harder. In fact, communication complexity continues to increase. Core teams used to do all the changes needed for maintenance/infrastructure upgrades, but the organization has gotten so large they need to rely on product teams to do the bulk of the work. Core teams generate a list of maintenance items that product teams need to work on, but product teams have to concentrate on adding new products. How do we prioritize work? Before we prioritize work, we need to identify who’s responsible for what. To tackle this problem, Core teams create tooling. Ownership becomes more defined with added metadata to “entities,” an abstract term used to describe things like code and alerting. All this ownership becomes shareable via the ownership service, and, we can now track migrations across the engineering organization with a tool called “migration-status.” We start by defining migrations from a “core team” perspective, but also have migrations from other infrastructure teams. Now that product teams are multi-disciplinary, we start to bombard them with an increasing number of messages to upgrade/migrate their infrastructure. Communication complexity increases and efficiency decreases. We start thinking of a way to tie together priorities from multiple teams. We need a solution that has a global view and seeks to control communication complexity. Just like how a notification platform for your users needs to figure out the right messages to send, we need a tool to surface the right reminders to the right teams. So, which messages are sent to which users? Over the next few blog posts, we’ll walk you through what the Engineering Effectiveness Metrics (EE Metrics) Platform is and how we use it to reduce communication complexity. The first blog post will dive into our “Ownership” service. We’ll be talking about what it is, how we use it, and the value that it brings to our engineering organization. The second post will cover how we use the EE Metrics tool to increase awareness of developer velocity and code quality and to improve prioritization of critical migrations for product teams. We do all of these things while maintaining a safe space for teams and individuals. Tweet Back to blog", "date": "2020-03-06"}, {"website": "Yelp", "title": "Accelerating Retention Experiments with Partially Observed Data", "author": ["\n        \n  Conner DiPaolo, Applied Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/02/accelerating-retention-experiments-with-partially-observed-data.html", "abstract": "Summary Here at Yelp, we generate business wins and a better platform by running A/B tests to measure the revenue impact of different user and business experience interventions. Accurately estimating key revenue indicators, such as the probability a customer retains at least \\(n\\)-days (\\(n\\)-day retention) or the expected dollar amount a customer spends over their first \\(n\\) days (\\(n\\)-day spend) is core to this experimentation process. Historically at Yelp, \\(n\\)-day customer or user retention was typically estimated as the proportion of customers/users we observed for more than \\(n\\) days who retained more than \\(n\\) days. Similarly, \\(n\\)-day spend was estimated as the average amount spent over the first \\(n\\) days since experiment cohorting by businesses we have observed for at least \\(n\\) days. Recently, we transitioned to using two alternative statistical estimators for these metrics: the Kaplan-Meier estimator and the mean cumulative function estimator. These new approaches consider censored data, i.e. partially observed data, like how long a currently subscribed advertiser will retain as a customer. Accordingly, they offer several benefits over the previous approaches, including higher statistical power, lower estimate error, and more robustness against within-experiment seasonality. By performing Monte-Carlo simulations [ 1 , Chapter 24], we determined that using these estimators allowed us to read A/B experiment metrics a fixed number of days earlier after cohorting ends without any drop in statistical power. This amounted to a 12% to 16% reduction in overall required cohorting and observation time, via a 25% to 50% reduction in the time used to observe how people respond to the A/B experiences. Altogether, this improved our ability to iterate on our product. Background: Revenue Experimentation at Yelp The value of a Yelp customer can be quantified in two primary and informative directions: how long a user / business remains active / subscribed in our system (known as retention), as well as the total dollar amount they generate over their lifetime (known as cumulative spend). When we experiment on different user / business experiences, we make a point of estimating the effect of these changes on retention and spend metrics before we make a final ship decision. As a proxy, sometimes dollars might be replaced with less noisy units like ad clicks, page views, etc., but for the purposes of this blog post we will focus on altering a business experience and analyzing \\(n\\)-day retention and spend. The conclusions carry over equally well to experimentation settings that either deal with users or with similarly defined proxy metrics. The diagram below illustrates the typical lifecycle of an A/B test focused on retention and spend. Depending on the type of experience change and the time window used for measurement, the cohorting and observing phases can in many instances take the most time of the whole experimentation pipeline. As such, the acceleration of the observing phase detailed here can provide improvements in our ability to iterate on our product. Uncensored Approaches To Computing Retention & Spend One possible  retention measure is “what percentage of those in cohort \\(C\\) who subscribed to product \\(P\\) at any point during the experiment went on to retain for more than \\(n\\) days,” where \\(C\\), \\(P\\), and \\(n\\) are parameters the experimenter can adjust. Yelp previously computed this \\(n\\)-day retention measure within each experiment cohort as follows: of the customers who started purchasing product \\(P\\) during the experiment who we have observed for at least \\(n\\) days since their initial purchase, report the proportion who are still subscribed at day \\(n\\). Our measure for dollars spent is analogous: we measure \\(n\\)-day spend by considering “on average, how many dollars do those in cohort \\(C\\) spend on products \\(P_1, P_2, \\ldots, P_k\\) over their first \\(n\\) days after being cohorted.” Here, \\(C\\), the product basket \\(P_1, P_2, \\ldots, P_k\\), and \\(n\\) are freely adjustable as above. Standard error estimates for both of these metric estimators which rely on the Central Limit Theorem are always reported as well [ 1 , Theorem 6.16]. These estimators are unbiased and consistent (as the number of people observed for at least \\(n\\) days grows), assuming that the retention of customers is independent of the relative time they enter the experiment. There are a number of potential concerns that can arise from using the estimators mentioned above. Variance: One of the greater concerns is that estimators could have large variance if only a small number of customers had been observed in a cohort during the experiment period. Let’s suppose we want to cohort individuals into an experiment for 70 days and are interested in estimating the 60-day retention of each cohort in the experiment. At day 75, the uncensored estimators above only access the users who arrived during the first 15 days of experiment cohorting, even though we have been cohorting users for 5 times as long. Therefore, unless the sample size is very large or the underlying retention/spend distribution is very concentrated, our estimator will have large variance at this point. This forces us to wait more days after the experiment ends to get a genuine metric read or detect a bona-fide difference in retention or spend of the cohorts. Seasonality: In addition to the variance issue, the retention or spend characteristics of individuals cohorted into an experiment may vary with the experiment runtime. In this situation, the uncensored estimators will in general be biased away from the true population mean retention and spend over the experiment window until after the observation phase is fully completed. For example, if we run a revenue experiment that starts right before Christmas, the 60-day retention estimate 75 days after the hypothetical experiment above was started would be heavily biased towards HVAC contractors and retail stores instead of the large fraction of restaurants that were closed over the holidays. It is feasible that we could declare a difference between the two populations at day 75 and make a conclusion about the experiment results, even though the underlying estimates are biased and reflect only a subset of the population. The Kaplan-Meier Estimator for Retention Our solution to a more effective retention estimate, which can mitigate the problems mentioned above, is based on the so-called Kaplan-Meier estimator of the “survival curve.” The survival curve \\(S(t)\\) is a function of time \\(t\\) which returns the probability that someone would retain for at least \\(t\\) days after subscribing. Accordingly, if we had access to the population survival curve, \\(S(n)\\) would return the proportion of businesses in our population who would retain at least \\(n\\) days, precisely our retention metric. The Kaplan-Meier estimator is a nonparametric estimator of the whole survival curve. Evaluating the estimated curve at time \\(t = n\\) days gives an estimate of the desired retention metric: For a coarser discretization of time than typically used, this Kaplan-Meier estimate of \\(n\\)-day retention first writes the \\(n\\)-day churn as\n\\[S(n) = \\prod_{t=1}^n \\mathrm{Pr}(\\text{remains subscribed through day }t |\\text{ subscribed for first }t - 1\\text{ days}).\\]\nAt this point, each multiplicand is estimated as \\(h_t\\) , the fraction of people we have observed for at least \\(t\\) days and were subscribed at the end of day \\(t - 1\\), who then stayed subscribed through the end of day \\(t\\). The full estimate is then\n\\[S(n) = \\prod_{t=1}^n h_t.\\]\nThis equals the status quo version that does not incorporate censored data if we instead used the individuals observed for at least \\(n\\) days to compute each \\(h_t\\) instead of the larger sample size afforded by using those observed for at least \\(t < n\\) days. Better utilization of available information can increase the precision of our estimates, increase statistical power to detect differences in cohort retentions, and mitigate sensitivity to time-dependent retention characteristics over the course of the experiment. This estimator is a consistent estimator of the whole survival curve (computed by varying \\(n\\)) as both the number of individuals and the length of time we observe each of them increase [ 2 ]. It is not, in general, unbiased [ 3 ], and is also affected by seasonality in the same way that the status quo estimator is affected, although in simulations seasonality had less of an effect on the estimates than with the status quo approach. The Mean Cumulative Function for Spend In the cumulative spend setting, we employed the mean cumulative function estimator detailed in [ 4 ]. This mean cumulative function estimator writes the total spend of a business through day \\(n\\) after cohorting as the sum of the spend on the first day after cohorting, the spend on the second day after cohorting, etc., all the way through the spend on the \\(n\\)-th day after cohorting. Each day-\\(t\\) spend is then estimated as the average day-\\(t\\) spend of people we have seen for at least \\(t\\) days. Since the sample size used to estimate day-1 spend is usually much greater than that used to estimate day-60 spend, this estimator can achieve greater power than a status quo estimator that restricts the sample in each day-\\(t\\) spend estimate to only the people observed for all \\(n\\) days. This estimator is unbiased, consistent as the number of businesses seen through day \\(t\\) increases, and has mean squared error no greater than that of the status quo estimator. In the presence of seasonality, this estimator will be biased in the same way the status quo estimator will be, but we observe in practice that it typically has lower mean squared error despite this fact. The variance of our estimate of expected spend over the \\(n\\) days following cohorting can be written as follows. Mathematically, if \\(s_t\\) is the random variable giving the distribution over dollars spent by a business throughout their \\(t\\)-th day after being cohorted, then the variance of this \\(n\\)-day spend estimate is\n \\[\\sum_{t=1}^{n}\\frac{\\mathrm{Var}(s_t)}{m_t} + \\sum_{t\\neq t’} \\frac{\\mathrm{Cov}(s_t, s_{t’})}{max(m_t, m_{t’})},\\]\nwhere \\(m_t=|\\{i:\\text{individual }i\\text{ observed through day }t\\}|\\).\nThis can be estimated in practice by plugging in empirical unbiased estimates of \\(\\mathrm{Var}(s_t)\\) and \\(\\mathrm{Cov}(s_t,s_{t’})\\). The covariance terms are summed for all \\(t\\neq t’\\) which are both no more than \\(n\\). This result is similar to the one presented in [ 4 ] but differs in our level of discretization. Estimator Simulations In order to realize any acceleration in experimentation under the new estimators, we had to create a policy where experimenters would compute their A/B test metrics using these new estimators earlier than they would under the old, uncensored approaches, all while maintaining a comparable statistical power. Because of a relatively limited number of historical A/B tests with which to evaluate this speed-up empirically, we decided to rely on Monte-Carlo simulation to determine the speed-up to prescribe in practice. Although we ended up going with a simpler policy of reading metrics a fixed number of days earlier, such Monte-Carlo simulation of the speedup could be computed in a bespoke way for each proposed A/B test. This would, in some situations, achieve a much greater speed-up than available under the uniform policy we ended up using, at the expense of complexity. The Simulation Framework All of our simulation data are generated according to the following probabilistic model: An experiment is defined as a collection of initial subscription times \\(t \\sim \\mathrm{Uniform}(0,T \\text{ days})\\) which arrive uniformly between 0 days and \\(T\\) days. \\(T\\) was a pre-set constant that was set to be \\(K\\) days for spend simulations and \\(K+10\\) days for retention simulations; these are similar enough (and well within the range of typical experiment fluctuation) that the results should be interpreted identically. Also note that this is the continuous uniform distribution: people can arrive half-way or three-quarters of the way through any given day. Every individual has some underlying mean retention time \\(\\mu(t)\\) which is typically a constant in every scenario except Simulation 3 where \\(\\mu(t) = T’ + b (2t/T - 1)\\) to simulate within-experiment seasonality of revenue characteristics. Given the mean retention time \\(\\mu(t)\\), the retention time \\(R\\) of a subscriber is exponentially distributed with mean \\(μ(t)\\), which results in a subscription from time \\(t\\) to time \\(t + R \\sim t + \\mathrm{Exp}(\\text{mean}=\\mu(t))\\). Moreover, in all but the last spend simulation, the amount that someone spends in a day is precisely a constant times the fraction of a day they were an active subscriber. We don’t include non-subscribers in the spend simulation here; non-subscribers are emulated in the stress test later. For a target sample size \\(m\\) to collect during the simulated experiment, we independently sample \\(m\\) such subscriptions to create the experimental data. For every simulated experiment, we then wish to estimate the \\(n\\)-day retention/spend at time \\(K + T_r\\) where the read time \\(T_r = 0 \\text{ days}, 1 \\text{ day}, \\ldots,\\) etc. since the experiment finished. When measuring retention and spend at time \\(T_r\\) since the experiment finished, we do not have access to any events (e.g. a subscriber churning) at time later than \\(T_r\\). All experiment scenarios and results are averaged over 1000 independent trials in the retention simulations and 1500 independent trials in the spend simulations. Determining The Speed-up: A Statistical Power Simulation In this simulation, we generated experimental cohorts according to the above data model under various amounts of cohorted subscribing customers and mean retention times. These retention and sample size characteristics were chosen to run the gamut of experimental data we would expect to see in practice. Then, we matched the cohorts with the same sample size pairwise in order to compute the probability that we could detect (with a \\(z\\)-test) the bona-fide difference in retention / spend between the two hypothetical A/B experiences the different cohorts would receive. We estimated the \\(n\\)-day retention probability and \\(n\\)-day spend at day \\(0, 1, \\ldots , n\\) after the experiment cohorting ended using both the status quo estimator and the Kaplan-Meier / mean cumulative function approaches. We stopped estimating spend and retention at day \\(n\\) after the end of the experiment because all the data are guaranteed to be uncensored at this point, and accordingly the status quo and proposed estimators coincide exactly. In all scenarios of interest, the test based on the uncensored approaches have lower statistical power (lower probability of detecting the bona-fide retention difference) than the Kaplan-Meier / mean cumulative function based one where statistically comparable. This is particularly noticeable for moderate sample sizes and moderate differences: in one simulated scenario representative of reality, the status quo based test detects the difference less than half of the time on the day the experiment ends, while the Kaplan-Meier approach succeeds over 80% of the time. In two-thirds of the scenarios tested, the Kaplan-Meier approach succeeds at least 5 percentage points of the time more than the status quo approach the day the experiment ends, and in the majority of those cases the difference is over 10 percentage points. Looking at the simulations results differently, this can be quantified in terms of accelerating the number of days we need to achieve the same statistical power (within a 1% or similarly small relative tolerance) we would achieve if we computed the status quo estimators at day \\(n\\) after cohorting ends (the typical time we historically have read retention / spend experiment metrics.) The speed-up we observed for the mean cumulative function (relative to the total time used for cohorting and waiting to read retention and spend) for the various scenarios considered are presented below. The results for retention with the Kaplan-Meier estimator are similar and are not shown here. Note that the intervals of relative speed-ups are not confidence intervals — they are point estimates — but reflect the fact that the total time used to cohort and wait for retention historically has not been fixed and instead varies within a range of \\(L\\) to \\(U\\) days. If \\(k\\) is the number of days earlier we read our metrics, the reported interval is simply \\(k / U\\) to \\(k / L\\). Relative Speed-up 0.1% Power Tolerance 1% Power Tolerance 2% Power Tolerance Mean 20-27% 25-33% 28-38% 25th Percentile 8-11% 13-18% 17-22% 50th Percentile 14-19% 18-24% 23-30% 75th Percentile 32-42% 43-58% 46-61% To incorporate these estimators across all of Yelp’s experiment analysis, we dictated that individuals should read their experiment metrics with speed-up corresponding to the 50th percentile speed-up we observed in these simulations, under a 0.1% power tolerance as compared to the previous status quo approach. In doing so, under the assumption that our simulations were as representative as we believe, about half of experiment settings would see power no less than 0.1% lower than the status quo approach, but almost all would see power no less than 2% lower than the status quo approach. In light of the marked increase in our ability to iterate on Yelp’s products, this felt like a more-than-fair trade to make. Since in many circumstances the speed-up can be much greater than 12-16% over status quo, bespoke recommendations can and will be made in situations when rapid experimentation is extremely important to Yelp’s bottom line. Stress Test 1: Robustness against Seasonality In order to check that our simulations don’t break down in real world scenarios, we ran a number of stress tests that injected more extreme versions of reality into our data generating model, checking that the results largely mirrored what we see with the original data model. We only considered retention in this simulation, and not cumulative spend. In the first of these stress tests, we consider the case where the average subscriber retention time in a cohort is fixed at some number of days, but where the average retention of an individual varies with respect to when they initially make a purchase during the experiment. Fixing some day-zero bias \\(b\\), the average retention of an individual is linearly interpolated between \\(C + b\\) and \\(C - b\\) over the duration of the experiment in a way such that the population average stays the same. For biases chosen from a predefined set of candidates, we track the bias of the \\(n\\)-day retention probability estimates made by the status quo and Kaplan-Meier approaches as we re-calculate the metrics after the experiment ends. The Kaplan-Meier retention estimator has uniformly lower bias than the status quo approach where they are statistically comparable. Indeed, in situations with positive day-zero bias, the bias of the Kaplan-Meier estimator is on the order of 50% of the bias of the status quo approach. Moreover, the bias of the Kaplan-Meier estimator decreases super-linearly with respect to how long we wait to make the measurement, while the bias of the status quo estimator decreases linearly. Here, linearity means that the error is a line with a negative slope. This is different from the typical use of “linear decrease,” which is commonly used to denote a geometric decay in error. This result increases our confidence that the new estimators won’t return worse results in situations that have within-experiment seasonality. Stress Test 2: Non-Constant, Heavy Tailed Sign-up Budgets In the second stress test, we modified the data generating model so that the amount a person spends each day is not a uniform constant multiple of whether or not they are subscribed, but instead a constant multiple of whether or not they are subscribed that varies across individuals according to some heavy-tailed and bi-modal distribution reflective of actual spend distributions in Yelp products. Bi-modality emulates the inclusion of non-spenders and those who subscribe to much cheaper products in the experiment, while the heavy tail simply reflects the distribution over purchase amounts for people who do subscribe to a variable-cost product like advertisements. In short, the distribution over speed-ups seen in the table above is largely the same with this new noise added, although the speed-ups are slightly reduced. Since the reduction is quite small, as seen in the following table, we can be more confident that our simplified data generating process used in the initial power simulations does reflect reality. Nevertheless, it seems prudent to revise our expectations stated earlier about the properties of our “read metrics \\(n\\) days earlier” policy: about half of experiment settings would see power no less than 1% lower than the status quo approach, but almost all would see power no less than 2% lower than the status quo approach. Relative Speed-up 0.1% Power Tolerance 1% Power Tolerance 2% Power Tolerance Mean 19-26% 22-30% 25-34% 25th Percentile 0-0% 13-17% 15-21% 50th Percentile 14-18% 16-22% 19-25% 75th Percentile 38-51% 38-51% 38-51% Conclusion The Kaplan-Meier and mean cumulative function estimators are simple-to-use tools which can return reduced-variance estimates of \\(n\\)-day retention and cumulative spend. In simulations, these estimators afford a speed-up in non-engineering experiment runtime of 12-16% over uncensored approaches. Combining this computational evidence with real-world experimentation has increased Yelp’s ability to iterate on our product and operations more efficiently. Acknowledgements I would like to thank Anish Balaji, Yinghong Lan, and Jenny Yu for crucial advice and discussion needed to implement the changes to experimentation described here across Yelp. In addition, I genuinely appreciate all the comments from Blake Larkin, Yinghong Lan, Jenny Yu, Woojin Kim, Daniel Yao, Vishnu Purushothaman Sreenivasan, and Jeffrey Seifried that helped refine this blog post from its initial draft into its current form. References Wasserman, L.. “All of statistics: a concise course in statistical inference.” Springer-Verlag New York, 2004. Bitouzé, D., B. Laurent, and P. Massart. “A Dvoretzky–Kiefer–Wolfowitz type inequality for the Kaplan–Meier estimator.” In Annales de l’Institut Henri Poincare (B) Probability and Statistics, vol. 35, no. 6, pp. 735-763. 1999. Luo, D., and S. Saunders. “Bias and mean-square error for the Kaplan-Meier and Nelson-Aalen estimators.” In Journal of Nonparametric Statistics, vol. 3, no. 1, pp. 37-51, 1993. Nelson, W.. “Confidence Limits for Recurrence Data – Applied to Cost or Number of Product Repairs.” In Technometrics, vol. 37, no. 2, pp. 147-157, 1995. Tweet Become an Applied Scientist at Yelp Want to impact our product with statistical modeling and experimentation improvements? View Job Back to blog", "date": "2020-02-20"}, {"website": "Yelp", "title": "Supporting Spark as a First-Class Citizen in Yelp’s Computing Platform", "author": ["\n        \n  Jason Sleight, ML Platform Group Tech Lead; Huadong Liu, Software Engineer; and Stuart Elston, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/03/spark-on-paasta.html", "abstract": "Yelp extensively utilizes distributed batch processing for a diverse set of problems and workflows. Some examples include: Computation over Yelp’s review corpus to identify restaurants that have great views Training ML models to predict personalized business collections for individual users Analytics to extract the most in-demand service offerings for Request a Quote projects On-demand workloads to investigate surges in bot traffic so we can quickly react to keep Yelp safe Over the past two years, Yelp engineering has undertaken a series of projects to consolidate our batch processing technologies and standardize on Apache Spark. These projects aimed to simultaneously accelerate individual developer workflows by providing easier access to powerful APIs for distributed computing, while also making our systems more robust, performant, and cost efficient on a macro scale. Background Throughout Yelp’s history, our batch processing framework of choice was MapReduce, executed via Amazon Elastic MapReduce (AWS EMR). We even constructed our own open source framework, mrjob , which abstracts the details of the underlying MapReduce execution infrastructure away from developers. This way they could focus on the application-specific portions of their workflow instead, like defining their Map and Reduce steps. This framework has served us well over the years, and every day our production environment executes hundreds of mrjobs. Over time though, Yelp developers were increasingly drawn towards Apache Spark.  The foremost advantage of Spark is in-memory computing, but additional advantages include a more expressive API and large library of open source extensions for specialized workloads.  This API flexibility makes it easier for developers to write their distributed processing workloads and results in higher-quality code that is both more performant and easier to maintain. However, without a well-supported backend, provisioning Spark resources was an intensive process that made deploying Spark jobs to production a challenge and all but eliminated Spark from contention for ad hoc workflows en masse. Better support for Spark seemed like a promising direction, so our first step was to add Spark support into Yelp’s mrjob package (seen in mrjob v0.5.7). This enabled developers to write Spark code using the familiar mrjob framework and execute their Spark jobs on AWS EMR. Results from early adopters were encouraging, with one Yelp engineering team going so far as to convert over 30 of their legacy MapReduce mrjob batches into Spark mrjob batches, resulting in an aggregated 80% runtime speedup and 50% cost savings! Clearly, Spark was a direction that could add substantial value to Yelp’s distributed computing platform. Running Spark mrjob batches on AWS EMR was viable for many production batch use cases, but also demonstrated a few problems. Firstly, it was painful to connect to the rest of Yelp’s infrastructure, and consequently workloads had to operate in isolation (e.g., they couldn’t make requests to other Yelp services). Secondly, it was painful to use for ad hoc workloads since it required launching an AWS EMR cluster on demand, which could take up to 30 minutes between provisioning and bootstrapping. Integrating Spark as a first-class citizen in Yelp’s computing platform as a service, PaaSTA , enabled us to ease these pain points while also inheriting all of PaaSTA’s capabilities. Spark on PaaSTA PaaSTA is Yelp’s defacto platform for running services and containerized batches. At its core it’s (currently) built on Apache Mesos .  Spark has native support for Mesos, but was a new framework for PaaSTA, which had previously only executed Marathon for long-running services, and Yelp’s in-house batch scheduling system, Tron , for containerized batches. To set up Spark as a framework in PaaSTA, we needed to select several configuration settings and design the interfaces Spark on PaaSTA would expose to Yelp developers. We elected to run the Spark driver as a Mesos framework in a Docker container using the Spark client deploy mode. Since losing the driver is catastrophic for Spark clusters, we constrained Spark drivers to only run on a dedicated Auto Scaling Group of on-demand EC2 instances. On the other hand, Spark’s resilient data model provides automatic recovery from executor loss, allowing us to run Spark executors in Docker containers on a cluster of EC2 spot instances. For simplicity, we configured Spark on PaaSTA such that the Spark driver and executors used the same Docker image pulled from our internal Docker registries. Yelp has two primary use cases for Spark: offline batches and ad hoc interactive computing. To serve these needs, we created two APIs for Spark on PaaSTA: A command line interface that developers use to schedule Spark batches. Behind the scenes, this interface injects the necessary Spark configuration constraints to connect to PaaSTA’s Mesos masters, provision executors on PaaSTA’s Mesos Agents, pull images from the appropriate Docker registry, and create a SparkSession object. A Python package that developers invoke from arbitrary Python code (e.g., Jupyter notebooks). Much like our command line interface, this package injects the necessary Spark configuration constraints and then returns the resulting SparkSession object. Both of these APIs allow developers to have full control over how to configure their Spark cluster and can provide overrides for any of Spark’s configuration settings (e.g., executor memory, max cores, driver results size, etc.). PaaSTA will then use those values to provision and configure a Spark cluster as requested. Beyond Spark configuration settings, Yelp developers also have the ability to specify the Docker image that Spark uses. This enables developers to easily include custom code (e.g., from their production service) in their Spark workflows. To reduce developer overhead, we’ve constructed an internal debian package that developers can install into their Docker image to automatically include many Spark extensions that are valuable for Yelp workflows, like hadoop-aws, spark-avro, etc. Isolating Spark Jobs with a Dedicated Mesos Pool Initially, we provisioned Spark frameworks on the same Mesos pools as Yelp’s other Mesos frameworks. However, we quickly recognized that Spark workloads have drastically different characteristics than the long-running Marathon services our Mesos pools were configured to support. Two differences in particular convinced us to create a dedicated Mesos pool for Spark jobs. Firstly, Spark workflows are stateful. Yelp heavily uses AWS EC2 spot instances to drive cost savings for our computing platforms, which means an instance can be reclaimed by AWS at any time. Moreover, the PaaSTA cluster autoscaler dynamically scales the Mesos cluster to maintain a desired utilization, and can kill service instances on underutilized Mesos agents without warning. Since Yelp’s Marathon services are stateless and frequently have multiple concurrent instances, these abrupt disruptions are mostly inconsequential. While Spark can recover from losing an executor, losses can result in cached RDDs to be recomputed, thereby increasing load on upstream datastores and degrading developer experience. With a dedicated pool, we can use AWS instance types with lower reclamation rates, and a specially tailored Spark autoscaler (discussed in the next section) can minimize the probability of executor loss caused by PaaSTA. Secondly, Spark workflows are more memory-intensive than service workloads. While one of Spark’s primary advantages over MapReduce is in-memory computing, that advantage is only realized if the Spark cluster has sufficient memory to hold the necessary data. At Yelp, it’s common for our developers to request terabytes of memory in aggregate for a single Spark cluster alongside several hundred CPUs. That memory-to-CPU ratio is substantially different from stateless Marathon services, which are typically CPU-bound with low memory footprints. With a dedicated pool, we’re able to populate Spark frameworks’ Mesos agents with AWS instances that have higher memory capacity and SSD drives in order to deliver a more cost effective system with higher resource utilization. Autoscaling Spark Yelp has been autoscaling our PaaSTA clusters for several years, reducing infrastructure costs by only running as many servers as necessary. We generally use a fairly standard reactive autoscaling algorithm which attempts to keep the most utilized resource (e.g., CPUs or memory) at a desired level (around 80%). For example, if 90 out of 100 CPUs in the cluster were in use, it would add another ~12 CPUs to the cluster to bring CPU utilization back to 80%. If, later on, only 80 of the now 112 CPUs are utilized, it will downscale the cluster back to 100 CPUs. This approach works well for most workloads we run at Yelp, the majority of which are long-running services whose load varies gradually throughout the day in proportion to web traffic. Spark workloads, however, do not have gradually varying needs. Instead, the typical workload makes a large, sudden request for hundreds or thousands of CPUs, abruptly returning these resources a few hours (or even minutes) later when the workload completes. This causes sudden load spikes on the cluster, which is problematic for a reactive autoscaling approach for two reasons. Firstly, a reactive autoscaling approach can only trigger scaling actions when the cluster is already over or under utilized. This is not problematic for gradually shifting workloads since the extra load usually fits into the cluster headroom (i.e., the 20% of CPUs the autoscaler keeps unallocated) while additional capacity is added. However, large Spark jobs can easily exceed the cluster headroom, preventing the workload and anything else on the cluster from obtaining additional resources until more machines are provisioned. Secondly, relying on resource utilization obscures the true quantity of resources needed. If a cluster with 100 CPUs is at 100% utilization and our desired utilization is 80%, then the aforementioned reactive autoscaling strategy will provision 25 additional CPUs regardless of how many CPUs are needed. While it’s possible that 25 CPUs will be sufficient, if the Spark workload requested a thousand CPUs then it will take 11 autoscaling cycles (!) to reach the desired capacity, impeding workloads and causing developer frustration. By relying only on current utilization, we have no way to distinguish between these two cases. See below for an example in which it took our reactive algorithm four cycles and almost one and a half hours to scale the cluster from 100 to 500 CPUs for a Spark job. To solve these problems, we turned to Clusterman, our modular cluster autoscaler, which makes it simple to write custom autoscaling code for specific pools of machines. (You can check out our blogpost to learn more about it!) First, we extended the APIs that developers use to start Spark on PaaSTA jobs to send the Spark workflow’s resource needs to Clusterman. We then created a custom Clusterman signal for Spark that looks at these reported resource needs and compares them to the list of Spark frameworks currently registered with our Mesos clusters. If the framework associated with a given resource request is still running or we’re within a several minute grace period, that resource request is included in Clusterman’s allocation target. Because Clusterman knows the full resource requirements of each job as soon as it starts, we can make sure that Spark on PaaSTA jobs wait as little as possible for resources, regardless of quantity. The graph below shows our new approach performing the same task as the previous one in 15 minutes instead of one and a half hours! Spark on PaaSTA Results Over the past two years, we’ve seen accelerating adoption of Spark on PaaSTA among Yelp developers. Roughly 80% (and climbing) of all scheduled batches are now running Spark on PaaSTA instead of legacy mrjob on AWS EMR! In addition, Yelp developers create hundreds of Spark clusters every day for their ad hoc workloads. Aside from improving job performance and developer experience, moving to Spark on PaaSTA has also resulted in meaningful cost savings. As mentioned earlier, our legacy mrjob package runs on AWS EMR. Since EMR is a managed platform on top of EC2, AWS bills for EMR by taking the underlying EC2 cost and adding a premium. In essence, you can think of EMR as having a usage tax, with the EMR tax rate equal to the EMR premium divided by the EC2 cost. Figure 2 shows the EMR tax rate for different configurations of M5 and R5 instances.  In many cases, the EMR tax is a substantial portion of overall EMR costs, and since Yelp uses spot instances heavily, our aggregate savings by moving Spark jobs from EMR to PaaSTA is over 30%. Given the success of early Spark adoption, an organizational goal for Yelp in 2019 was to migrate all batch processing workloads to Spark on PaaSTA. As you might expect, migrating hundreds of legacy batches (many of which have been running without intervention for years) was a daunting endeavor. Rather than going through batches one by one, we instead migrated legacy MapReduce mrjobs en masse via a mrjob extension that wraps MapReduce code and executes it via Spark on PaaSTA.  While rewriting MapReduce jobs to fully utilize Spark capabilities results in peak performance, we’ve observed significant wins just from running the existing Map and Reduce steps in Spark instead of MapReduce. Conclusions Looking back on our journey, adding Spark support to our computing platform has gone fairly smoothly. Nevertheless, there are still a few things we want to improve. The first is system efficiency. By default, Mesos uses round robin task placement, which spreads the Spark executors to many Mesos agents and results in most agents containing executors for many Spark frameworks. This causes problems for cluster downsizing and can yield low cluster utilization. Instead, we would prefer to pack executors onto fewer Mesos Agents. We are currently experimenting with a patch to Spark’s Mesos scheduler that instead greedily packs executors onto hosts.  We also plan to investigate Spark’s Dynamic Resource Allocation mode as a further improvement to cluster efficiency. The second is stability. We’ve made a deliberate decision to run Spark on spot EC2 instances to benefit from their lower cost, but this choice also means that executor loss is possible. While Spark can recover from this, these events can lead to substantial recomputations that disrupt developer workflows—in worst cases getting stuck in a perpetual crash-recomputation loop that has to be manually terminated. These issues are magnified as the size of our Spark clusters continue to grow; and clusters with thousands of CPUs and TBs of memory are likely to experience at least one executor loss. Some solutions we plan to explore include aggressive checkpointing and/or selectively utilizing on-demand EC2 instances. Finally, we’re in the process of converting our Spark deployment from Mesos to Kubernetes, with the primary advantage being that Kubernetes provides additional control layers for us to tune cluster stability, responsiveness, and efficiency.  These changes are being made as part of PaaSTA itself, meaning that we can change the backend infrastructure without developers needing to alter their Spark usage! We’re continuing to invest in Spark as a premier computing engine at Yelp, so stay tuned for further updates! Acknowledgments Special thanks to everyone on the Core ML and Compute Infrastructure teams for their tireless contributions to bring Spark to all of Yelp! Tweet Become a Distributed Systems Engineer at Yelp Interested in designing, building, and deploying core infrastructure systems? Apply to become a Distributed Systems Engineer today. View Job Back to blog", "date": "2020-03-02"}, {"website": "Yelp", "title": "Open-Sourcing Varanus and Rusty Jetpack", "author": ["\n        \n  Sanae Rosen and Kurt Bonatz, Software Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/02/open-sourcing-varanus-and-rusty-jetpack.html", "abstract": "Varanus The monitor lizards are large lizards in the genus Varanus . Some time ago, our Android app got into a loop of sending data, due to some unlikely interactions between several different systems, which briefly overwhelmed our servers before we were able to turn it off. Fortunately, key code was behind an experiment. Otherwise, apps could have continued misbehaving for days, as there is no guarantee users would immediately update the app. It took an unusual combination of circumstances for this to happen, but this kind of problem seems to be a pervasive concern across the industry, and there are few tools to prevent it. Furthermore, even at the best of times, mobile data can be hard to manage. Every now and then an article comes up about how a widely used app has eaten up users’ data. Unfortunately, there aren’t very many good tools for tracking how much data is sent, or what exactly is responsible for sending too much data in the first place. Also, because updates are optional, all the code you’ve ever written is out there, somewhere (and we’ve had an Android app for almost as long as Android has existed!). If something goes wrong, you may not be able to push a fix to enough people, and with millions of users, all sorts of strange things can happen. While it’s unlikely that something goes catastrophically wrong and you can’t get enough people to update, it’s not impossible. What Does Varanus Do? In building out Varanus, we had two main goals: Always be able to turn off unwanted data on the client, no matter what. Observe how much traffic is generally being sent  so we can spot if something weird happens. With three constraints: It should be exceptionally simple and hard to break. It should work without anyone having to do anything. It can be dropped into our different apps with minimal effort. Also, since it seemed that a lot of people were concerned with this problem but lacked the resources to spend time fixing it, we saw this as an opportunity to contribute something useful to the community. How Does It Work? All traffic on the app automatically passes through Varanus Basically, since all network traffic passes through Varanus,  Android developers don’t even have to think about it for it to work. It counts the number of bytes and requests, and bins them by arbitrary categories of traffic that can be specified programmatically. An error message from the server (or CDN ) then tells the app to hold off on sending more traffic for a bit—one message says to stop sending all traffic, and the other says to stop a specific category of traffic. The code is entirely client-side, and no new backend infrastructure is needed (as long as you have a way of sending custom HTTP error codes from your server if necessary).  Also, no coordination between devices is required, and turning off traffic is simple: all you need is a runbook. Varanus is built around OkHttp interceptors, but with a bit of extra work, there’s no reason other clients couldn’t be supported. It’s also written entirely in Kotlin (like all new code at Yelp). Where Can I Find More Details? Take a look at the README , or the code itself. We have a sample app that explains how it should be used. Rusty Jetpack In preparation for targeting Android 10, Yelp’s Android apps were migrated to use Android X libraries. Unfortunately, with the size of our apps’ codebases, the provided migration tool in Android Studio didn’t work for us. Rusty Jetpack was then born as a Hackathon project to help ensure seamless adoption across many developers with little downtime. What Does Rusty Jetpack Do? The tool migrates all files in a git repository to use the new Android X package name spaces. This includes imports, fully qualified references, pro-guard declarations, and warnings about gradle packages that need to be changed. While this does mean the code won’t compile immediately after using the tool, most of the mundane work is taken care of. And best of all, it achieves all of this in under one second for our largest repository!. Rusty Jetpack is critical to preventing downtime during migrations with rapidly changing codebases. Migrations can easily be kept up to date with the latest changes by re-running the tool, and then being distributed to developers (once the migration has been pushed) for quick adoption without major disruption. To learn more, check out the repository here ! Tweet Become an Android Software Engineer at Yelp Want to help us make even better tools for our Android engineers? View Job Back to blog", "date": "2020-02-06"}, {"website": "Yelp", "title": "How businesses have reacted to COVID-19 using Yelp features", "author": ["\n        \n  Kevin Knaust, Sebastien Couvidat, Carl Bialik, Grace Jiras\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/06/how-businesses-have-reacted-to-covid-19-using-yelp-features.html", "abstract": "Yelp periodically releases an open, all-purpose dataset for learning. The dataset is a subset of our businesses, reviews, and user data to inform government policy, academic research, and business strategy, among other uses. It has provided opportunities including teaching students about databases, helping others study natural language processing, sampling production data while learning to create mobile apps, and discovering compelling research findings . Our most recent dataset was published in March 2020. Businesses everywhere are adapting to the effects of the Coronavirus and have been using Yelp features to stay connected with their customers. To this end, we’re releasing an addendum dataset including the following components, as of June 10, 2020: COVID-19-related business highlights Restaurants with delivery/takeout enabled Restaurants partnered with Grubhub Businesses with Call to Action buttons enabled Does the business still have Request A Quote enabled? Has the business created a custom page banner during COVID-19? Temporary closures Virtual Services offered We hope researchers, academics, and any interested parties will utilize this new data, along with our most recent economic impact report , to investigate and further understand the broad-ranging effects of the coronavirus pandemic. Download the new Yelp dataset here . Tweet Back to blog", "date": "2020-06-15"}, {"website": "Yelp", "title": "Introducing Yelp's Machine Learning Platform", "author": ["\n        \n  Jason Sleight, ML Platform Group Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/07/ML-platform-overview.html", "abstract": "Understanding data is a vital part of Yelp’s success. To connect our consumers with great local businesses, we make millions of recommendations every day for a variety of tasks like: Finding you immediate quotes for a plumber to fix your leaky sink Helping you discover which restaurants are open for delivery right now Identifying the most popular dishes for you to try at those restaurants Inferring possible service offerings so business owners can confidently and accurately represent their business on Yelp In the early days of Yelp circa 2004, engineers painstakingly designed heuristic rules to power recommendations like these, but turned to machine learning (ML) techniques as the product matured and our consumer base grew. Today there are hundreds of ML models powering Yelp in various forms, and ML adoption continues to accelerate. As our ML adoption has grown, our ML infrastructure has grown with it.  Today, we’re announcing our ML Platform, a robust, full feature collection of systems for training and serving ML models built upon open source software.  In this initial blog post, we will be focusing on the motivations and high level design. We have a series of blog posts lined up to discuss the technical details of each component in greater depth, so check back regularly! Yelp’s ML Journey Yelp’s first ML models were concentrated within a few teams, each of whom created custom training and serving infrastructure. These systems were tailored towards the challenges of their own domains, and cross pollination of ideas was infrequent. Owning an ML model was a heavy investment both in terms of modeling, as well as infrastructure maintenance. Over several years, each system was gradually extended by its team’s engineers to address increasingly complex scope and tighter service level objectives (SLOs). The operational burden of maintaining these systems took a heavy toll, and drew ML engineers’ focus away from modeling iterations or product applications. A few years ago, Yelp created a Core ML team to consolidate our ML infrastructure under centrally supported tooling and best practices.  The benefits being: Centrally managed systems for ML workflows would enable ML developers to focus on the product and ML aspects of their project without getting bogged down by infrastructure. By staffing our Core ML team with infrastructure engineers, we could provide new cutting edge capabilities that ML engineers might lack expertise to create or maintain. By consolidating systems we could increase system efficiency to provide a more robust platform, with tighter SLOs and lower costs. Consolidating systems for a topic as broad as ML is daunting, so we began by deconstructing ML systems into three main themes and developed solutions within each: interactive computing, data ETL, and model training/serving. The approach has worked well, and allowed teams to migrate portions of their workflows on to Core ML tooling while leaving other specialized aspects of their domain on legacy systems as needed. In this blogpost, I’ll discuss how we architected our model training and serving systems into a single, unified model platform. Yelp’s ML Platform Goals At a high level, we have a few primary goals for our ML Platform: Opinionated APIs with pre-built implementations for the common cases. Correctness and robustness by default. Leverage open source software. Opinionated APIs Many of Yelp’s ML challenges fall into a limited set of common cases, and for these we want our ML Platform to enforce Yelp’s collective best practices.  Considerations like meta data logging, model versioning, reproducibility, etc. are easy to overlook but invaluable for long term model maintenance. Instead of requiring developers to slog through all of these details, we want our ML Platform to abstract and apply best practices by default. Beyond canonizing our ML workflows, opinionated APIs also enable us to streamline model deployment systems. By focusing developers into narrower approaches, we can support automated model serving systems that allow developers to productionize their model via a couple clicks on a web UI. Correctness and robustness by default One of the most common pain points of Yelp’s historical ML workflows was system verification. Ideally, the same exact code used to train a model should be used to make predictions with the model.  Unfortunately, this is often easier said than done – especially in a diverse, large-scale, distributed production environment like Yelp’s.  We usually train our models in Python but might deploy the models via Java, Scala, Python, inside databases, etc. Even the tiniest inconsistencies can make huge differences for production models. E.g., we encountered an issue where 64-bit floats were unintentionally used by a XGBoost booster for predictions (XGBoost only uses 32-bit floats). The slight floating point differences when numerically encoding an important categorical variable resulted in the model giving approximately random predictions for 35% of instances! Tolerating sparse vector representations, missing values, nulls, and NaNs also requires special consideration.  Especially when different libraries and languages have differing expectations for client side pre-processing on these issues. E.g., some libraries treat zero as missing whereas others have a special designation. It is extremely complicated for developers to think through these implementation details let alone even recognize if a mistake has occurred. When designing our ML Platform, we’ve adopted a test-driven development mindset. All of our code has a full suite of end-to-end integration tests, and we run actual Yelp production models and datasets through our tests to ensure the models give exactly the same results across our entire ecosystem. Beyond ensuring correctness, this also ensures our ML Platform is robust enough to handle messy production data. Leverage Open Source Solutions ML is currently experiencing a renaissance of open source technology. Libraries like Scikit-learn, XGBboost, Tensorflow, and Spark have existed for years and continue to provide the foundational ML capabilities. But newer additions like Kubeflow, MLeap, MLflow, TensorFlow Extended, etc. have reinvented what an ML system should entail and provide ML systems with much needed software engineering best practices. For Yelp’s ML Platform, we recognized that any in-house solution we might construct would be quickly surpassed by the ever-increasing capabilities of these open source projects. Instead we selected the open source libraries best aligned with our needs and constructed thin wrappers around them to allow easier integrations with our legacy code. In cases where open source tools lack capabilities we need, we’re contributing solutions back upstream. ML Platform Technological Overview In future blog posts, we’ll be discussing these systems in greater detail, so check back soon.  For now, I’ll just give a brief overview of the key tech choices and a model’s life cycle within these systems. MLflow and MLeap After evaluating a variety of options, we decided on MLflow and MLeap as the skeleton of our platform. MLflow’s goal is to make managing ML lifecycles simpler, and contains various subcomponents each aimed at different aspects of ML workflows. For our ML Platform, we especially focused on the MLflow Tracking capabilities. We automatically log parameters and metrics to our tracking server, and then developers use MLflow’s web UI to inspect their models’ performance, compare different model versions, etc. MLeap is a serialization format and execution engine, and provides two advantages for our ML Platform. Firstly, MLeap comes out of the box with support for Yelp’s most commonly used ML libraries: Spark, XGBoost, Scikit-learn, and Tensorflow – and additionally can be extended for custom transformers to support edge cases.  Secondly, MLeap is fully portable, and can run inside any JVM-based system including Spark, Flink, ElasticSearch, or microservices.  Taken together, MLeap provides a single solution for our model serving needs like robustness/correctness guarantees and push-button deployment. Typical Code Flow in our ML Platform Offline Code Flow for Training a Model in our ML Platform Developers begin by constructing a training dataset, and then define a pipeline for encoding and modeling their data.  Since Yelp models typically utilize large datasets, Spark is our preferred computational engine.  Developers specify a Spark ML Pipeline for preprocessing, encoding, modeling, and postprocessing their data. Developers then use our provided APIs to fit and serialize their pipeline. Behind the scenes, these functions automatically interact with the appropriate MLflow and MLeap APIs to log and bundle the pipeline and its metadata. Online Code Flow for Serving a Model in our ML Platform To serve models, we constructed a thin wrapper around MLeap that is responsible for fetching bundles from MLflow, loading the bundle into MLeap, and mapping requests into MLeap’s APIs. We created several deployment options for this wrapper, which allows developers to execute their model as a REST microservice, Flink stream processing application, or hosted directly inside Elasticsearch for ranking applications.  In each deployment option, developers simply configure the MLflow id for the models they want to host, and then can start sending requests! What’s Next? We’ve been rolling out our ML Platform incrementally, and observing enthusiastic adoption by our ML practitioners. The ML Platform is full featured, but there are some improvements we have on our roadmap. First up is expanding the set of pre-built models and transformers.  Both MLflow and MLeap are general purpose and allow full customization, but doing so is sometimes an involved process. Rather than requiring developers to learn the internals of MLflow and MLeap, we’re planning to extend our pre-built implementations to cover more of Yelp’s specialized use cases. We’d also like to integrate our model serving systems with Yelp’s A/B experimentation tools. Hosting multiple model versions on a single server is available now, but currently relies on clients to specify which version they want to use in each request. However, we could further abstract this detail and have the serving infrastructure connect directly to the experimentation cohorting logic. Building on the above, we would like to have the actual observed events feed back into the system via Yelp’s real-time streaming infrastructure. By joining the observed events with the predicted events, we can monitor ML performance (for different experiment cohorts) in real-time. This enables several exciting properties like automated alerts for model degradation, real-time model selection via reinforcement learning techniques, etc. Tweet Back to blog", "date": "2020-07-01"}, {"website": "Yelp", "title": "Remember Clusterman? Now It's Open-Source, and Supports Kubernetes Too!", "author": ["\n        \n  David R. Morrison, Compute Infra Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/11/open-source-clusterman.html", "abstract": "Earlier this year, I wrote a blog post showing off some cool features of our in-house compute cluster autoscaler, Clusterman (our Cluster Manager). This time,\nI’m back with two announcements that I’m really excited about! Firstly, in the last few months, we’ve added another\nsupported backend to Clusterman; so not only can it scale Mesos clusters, it can also scale Kubernetes clusters. Second,\nClusterman is now open-source on GitHub so that you, too, can benefit from\nadvanced autoscaling techniques for your compute clusters. If you prefer to just read the code, you can head there now\nto find some examples and documentation on how to use it; and if you’d like to know a bit more about the new features\nand why we’ve built them, read on! Going from Mesos to Kubernetes Over the last five years, we’ve talked (and written ) a lot about\nour compute stack at Yelp; we’ve gone from our monolithic yelp_main repo to a fully-distributed, service-oriented\narchitecture running in the cloud on top of Apache Mesos and our in-house platform-as-a-service, PaaSTA . And, truthfully, without that move, we wouldn’t have been able to grow to the\nscale that we are now. We’ve been hard at work this year preparing our infrastructure for an even more\ngrowth, and realized that the best way to achieve this is to move away from Mesos and onto Kubernetes. Kubernetes allows us to run workloads (Flink, Cassandra, Spark, and Kafka, among others) that were once difficult to\nmanage under Mesos (due to local state requirements). We strongly believe that managing these workloads under a common\nplatform (PaaSTA) will boost our infrastructure engineers’ output by an order of magnitude (can you imagine spinning up\na new Cassandra cluster with just a few lines of YAML? We can!). In addition, we’re migrating all of our existing microservices and batch workloads onto Kubernetes. This was a point of\ndiscussion at Yelp, but we eventually settled on this approach as both a way to reduce the overhead of maintaining two\ncompeting schedulers (Mesos and Kubernetes), and to take advantage of the fast-moving Kubernetes ecosystem. Thanks to\nthe abstractions that PaaSTA provides, we’ve been able to do this migration seamlessly! Our feature developers don’t\nknow their service is running on top of an entirely different compute platform. Of course, to make this migration possible, we need to build support for Kubernetes into all our tooling around our\ncompute clusters, including our very important autoscaler, Clusterman.  Due to Clusterman’s modular design, this was\neasy! We simply defined a new connector class that conforms to the interface the autoscaler expects. This connector\nknows how to talk to the Kubernetes API server to retrieve metrics and statistics about the state of the Kubernetes\ncluster it’s scaling. These metrics are then saved in our metrics data store, which is sent to the signals and\nautoscaling engine to determine how to add or remove compute resources. Why Clusterman?  Why Now? We’re big proponents of open-source software at Yelp; we benefit from the efforts of many other open-source projects and\nrelease what we can back into the community.  Ever since Clusterman’s inception, we’ve had the dream of open-sourcing\nit, and now that it has support for Kubernetes, there’s no better time to do so! Whenever a project like this is released, the first question people ask is, “Why should I use your product instead of\nthis other, established one?” Two such products are the AWS Auto Scaling for Spot\nFleet and the Kubernetes\nCluster Autoscaler . So let’s compare and\ncontrast Clusterman with them: Clusterman Auto Scaling for Spot Fleet Kubernetes Cluster Autoscaler Supports any type of cloud resource (ASGs, spot fleets, etc) Only for Spot Fleets Only supports homogeneous cloud resources in a single nodegroup (all compute resources must be identical); can have multiple nodegroups in a cluster. Pluggable signal architecture Three different scaling choices: target tracking, step functions, or time-based Scales the cluster when pods are waiting to be scheduled Can proactively autoscale to account for delays in node bootstrapping time No proactive scaling Waits for node creation calls (to AWS, MIG, VMSS, etc) to complete before continuing Basic Kubernetes support No knowledge of Kubernetes Supports advanced features like node and pod affinity Can simulate autoscaling decisions on production data No simulator No simulator Extensible (open-source) Closed-source API Extensible (open-source) A few highlights we’d like to call out: firstly, note that Clusterman is the only autoscaler that can support a mixture\nof cloud resources (Spot Fleets, Auto-Scaling Groups, etc.) - it can even handle this in the same cluster! This allows\nfor a very flexible infrastructure design. Moreover, Clusterman’s pluggable signal architecture lets you write any type of scaling signal you can imagine (and\nwrite in code). At Yelp, we generally believe that the Kubernetes Cluster Autoscaler approach (scale up when pods are\nwaiting) is right for “most use cases,” but having the flexibility to create more complex autoscaling behavior is really\nimportant to us. One example of how we’ve benefitted from this capability is Jolt, an internal tool for running unit and\nintegration tests. The Jolt cluster runs millions of tests every day, and has a very predictable workload; thus, we\nwrote a custom signal that allows us to scale up and down before pods get queued up in the “waiting” state, which saves\nour developers a ton of time running tests! To put it another way, the Kubernetes Cluster Autoscaler is reactive, but\nClusterman has enough flexibility to be proactive and scale up before resources are required. To be fair, not everyone needs the ability to make complex autoscaling decisions; many users will be just fine using\nsomething like the AWS Spot Fleet Autoscaler or Kubernetes Cluster Autoscaler. Fortunately for these users, Clusterman\ncan be easily swapped in as needed. For example, it can be configured to read all of the same node labels that the\nKubernetes Cluster Autoscaler does, and behave appropriately. Also note that the Kubernetes Cluster Autoscaler does\nsupport some Kubernetes features that Clusterman doesn’t (yet) know about, like pod affinity and anti-affinity. But\nwe’re constantly adding new features to Clusterman, and of course, pull requests are always welcome! Want to Know More? If you’re as excited as we are about this release, we encourage you to head over to our GitHub and check it out! Give it a star if you like it, and if you have any\nquestions about getting Clusterman set up in your environment, feel free to open an issue or send us an email! Also,\nwe’d love to hear any success stories you have about autoscaling with Clusterman, or Kubernetes in general; you can\nreach us on Twitter ( @YelpEngineering ) or on Facebook\n( @yelpengineers ). David is going to be at KubeCon 2019 and will happily talk your ear off about Clusterman and Kubernetes; ping him on Twitter or find him in the hallway track. A previous version of this post included some factual errors around the Kubernetes Cluster Autoscaler capabilities. Tweet Become an Infrastructure Engineer at Yelp Want to work on exciting projects like Clusterman?  Apply here! View Job Back to blog", "date": "2019-11-11"}, {"website": "Yelp", "title": "Organizing and Securing Third-Party CDN Assets at Yelp", "author": ["\n        \n  Rishabh Rao, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/11/organizing-and-securing-third-party-cdn-assets-at-yelp.html", "abstract": "At Yelp, we use a service-oriented architecture to serve our web pages. This consists of a lot of frontend services, each of which is responsible for serving different pages (e.g., the search page or a business listing page). In these frontend services, we use a couple of third-party JavaScript/CSS assets ( React , Babel polyfill , etc.) to render our web pages. We chose to serve such assets using a third-party Content Delivery Network (CDN) for better performance. In the past, if a frontend service needed to use a third-party JavaScript/CSS asset, engineers had to hard-code its CDN URL. For example: <script src= \"https://cdnjs.cloudflare.com/ajax/libs/jquery/1.8.3/jquery.min.js\" ></script> With hundreds of engineers working at Yelp, it was difficult to ensure the following (for each third-party asset): <script> or <link> tags had a subresource integrity checksum via the integrity attribute (see the section on Subresource integrity checksums below) URLs used the HTTPS protocol Only public CDN providers (approved by our security team) were used Engineers could update to the latest versions easily Organizing our Third-Party Assets Here at Yelp, we’ve built our frontend services using a Python service stack, with Pyramid as our web framework and uWSGI as our web server. We created a shared Python package, cdn_assets , for storing the URLs and subresource integrity checksums of our third-party JavaScript/CSS assets. For each asset, we simply used a Python dictionary with the asset’s semantic version as the key. For example: # React (facebook.github.io/react) CDN_SCRIPT_REACT = { '16.8.6' : CDNAsset . construct_asset ( cdn = CDNDomain . CDNJS , library = 'react' , version = '16.8.6' , filename = 'umd/react.production.min' , filename_unminified = 'umd/react.development' , extension = 'js' , integrity = 'sha384-qn+ML/QkkJxqn4LLs1zjaKxlTg2Bl/6yU/xBTJAgxkmNGc6kMZyeskAG0a7eJBR1' , integrity_unminified = 'sha384-u6DTDagyAFm2JKvgGBO8jWd9YzrDzg6FuBPKWkKIg0/GVA6HM9UkSxH2rzxEJ5GF' , ), '16.8.5' : CDNAsset . construct_asset ( # … similar properties for this version ), # … more versions… } # Babel Polyfill (babeljs.io/docs/usage/polyfill) CDN_SCRIPT_BABEL_POLYFILL = { '6.23.0' : CDNAsset . construct_asset ( cdn = CDNDomain . CDNJS , library = 'babel-polyfill' , version = '6.23.0' , filename = 'polyfill.min' , filename_unminified = 'polyfill' , extension = 'js' , integrity = 'sha384-FbHUaR69a828hqWjPw4PFllFj1bvveKOTWORGkyosCw720HXy/56+2hSuQDaogMb' , integrity_unminified = 'sha384-4L0QKU4TUZXBNNRtCIbt9G73L2fXYHnzgCjL65qwFxsXPvuAf1aB6D3X+LIflqu3' , ), # … more versions… } # … more assets… Usage Here’s a Python code snippet which shows how the asset is included in our Yelp-Cheetah templates: CDN_SCRIPT_REACT [ '16.8.6' ]. generate_script_tag ( minified = True ) # returns <script src=\"https://cdnjs.cloudflare.com/ajax/libs/react/16.8.6/umd/react.production.min.js\" integrity=\"sha384-qn+ML/QkkJxqn4LLs1zjaKxlTg2Bl/6yU/xBTJAgxkmNGc6kMZyeskAG0a7eJBR1\" crossorigin=\"anonymous\"></script> Scaffolding Infrastructure To facilitate ease of use and maintenance, we developed some scaffolding infrastructure to: Define public CDN providers (e.g., Cloudflare CDNJS , Google CDN , etc.) Render minified scripts & styles in the production environment and unminified scripts & styles in the development environment Create a helpful generate_script_tag method, which allows consumers of this package to easily generate an HTML <script> tag with the correct subresource integrity SHA (see the section on Comparing cryptographic hash functions below) We made it easy for engineers to add a new version by creating a make target to calculate the integrity checksum, like so: # Usage: make sri-hash --urls=\"URL1[ URL2 ... URLn] $ make sri-hash --urls = \"https://cdnjs.cloudflare.com/ajax/libs/react/16.8.6/umd/react.production.min.js\" sha384-qn+ML/QkkJxqn4LLs1zjaKxlTg2Bl/6yU/xBTJAgxkmNGc6kMZyeskAG0a7eJBR1 Testing We wrote tests which iterate all versions of all assets to ensure that: URLs point to a valid asset on the CDN Integrity SHA checksums are correct URLs begin with https:// and end with .js or .css Here’s a snippet from one of our test files: # `all_cdn_scripts` is a Pytest fixture; it’s not shown in this snippet. @ pytest . mark . parametrize ( 'script' , all_cdn_scripts ) def test_integrity_hashes_match ( script ): # Test that the unminified URL doesn’t error and has the right integrity hash. resp = requests . get ( script . url_unminified ) resp . raise_for_status () assert ( 'sha384-{}' . format ( base64 . b64encode ( hashlib . sha384 ( resp . content ). digest ()). decode ( 'utf8' )) == script . integrity_unminified ) # Test that the minified URL doesn’t error and has the right integrity hash. resp = requests . get ( script . url ) resp . raise_for_status () assert ( 'sha384-{}' . format ( base64 . b64encode ( hashlib . sha384 ( resp . content ). digest ()). decode ( 'utf8' )) == script . integrity ) def test_sha384_for_all_checksums ( all_cdn_scripts ): SHA384_CHECKSUM_LENGTH = 64 for cdn_script in all_cdn_scripts : assert cdn_script . integrity . startswith ( 'sha384-' ) assert cdn_script . integrity_unminified . startswith ( 'sha384-' ) checksum = cdn_script . integrity . replace ( 'sha384-' , '' ) assert len ( checksum ) == SHA384_CHECKSUM_LENGTH checksum = cdn_script . integrity_unminified . replace ( 'sha384-' , '' ) assert len ( checksum ) == SHA384_CHECKSUM_LENGTH def test_valid_https_urls ( all_cdn_scripts ): https_url_validator = URLValidator ( schemes = [ 'https' ], message = 'HTTPS URL validation failed' ) for cdn_script in all_cdn_scripts : https_url_validator ( cdn_script . url ) def test_valid_script_files ( all_cdn_scripts ): for cdn_script in all_cdn_scripts : assert cdn_script . url . endswith ( '.js' ) def test_minified_and_unminified_urls ( all_cdn_scripts ): for cdn_script in all_cdn_scripts : assert cdn_script . url . endswith ( '.min.js' ) assert not cdn_script . url_unminified . endswith ( '.min.js' ) Securing the Assets That We Use Yelp serves tens of millions of users every month. Ensuring that these users are protected should an attacker gain control of the CDN we’re using is of prime importance. That’s where subresource integrity checksums come into the picture. Subresource Integrity Checksums The web docs on Mozilla Developer Network define Subresource Integrity as: A security feature that enables browsers to verify that resources they fetch (for example, from a CDN) are delivered without unexpected manipulation. It works by allowing you to provide a cryptographic hash that a fetched resource must match. Support for subresource integrity checksum verification is achieved by adding an integrity attribute on the <script> or <link> tags. For example: <script src= \"https://cdnjs.cloudflare.com/ajax/libs/react/16.8.6/umd/react.production.min.js\" integrity= \"sha384-qn+ML/QkkJxqn4LLs1zjaKxlTg2Bl/6yU/xBTJAgxkmNGc6kMZyeskAG0a7eJBR1\" ></script> The web browser will calculate a hash from the contents of the <script> or <link> tag. It will then compare this hash with the integrity attribute’s value. If they don’t match, the browser will stop the <script> or <link> tag from executing. Comparing Cryptographic Hash Functions As per the Subresource Integrity (SRI) specification : Conformant user agents must support the SHA-256, SHA-384 and SHA-512 cryptographic hash functions for use as part of a request’s integrity metadata and may support additional hash functions. Although both SHA-256 and SHA-512 are supported, we recommend using the SHA-384 cryptographic hash function for the integrity attribute. This is largely because SHA-384 is less susceptible to length extension attacks . (See github.com/w3c/webappsec — SRI: upgrade examples to sha384? and github.com/mozilla/srihash.org — Why SHA384? for further information.) Always Using HTTPS for Loading CDN Assets At Yelp, we’ve migrated web traffic to be served exclusively using HTTPS and HSTS . If you’re interested in learning more, check out these excellent blog posts by my colleagues: The Great HTTPS Migration and The Road To HSTS . Protocol Relative URLs It’s recommended to use HTTPS while serving CDN assets instead of protocol-relative URLs. Quoting the article “The Protocol-relative URL” by Paul Irish : Now that SSL is encouraged for everyone and doesn’t have performance concerns , this technique is now an anti-pattern. If the asset you need is available on SSL, then always use the https:// asset.\nAllowing the snippet to request over HTTP opens the door for attacks like the recent Github Man-on-the-side attack . It’s always safe to request HTTPS assets even if your site is on HTTP, however the reverse is not true.\nMore guidance and details in Eric Mills’ guide to CDNs & HTTPS and digitalgov.gov’s writeup on secure analytics hosting . Acknowledgements The work described in this blog post has been carried out and supported by numerous members of the Engineering Team here at Yelp. Particular credit goes to engineers on our Core Web Infrastructure (Webcore) team. Tweet Become a Software Engineer at Yelp Want to help us make even better tools for our full stack engineers? View Job Back to blog", "date": "2019-11-20"}, {"website": "Yelp", "title": "dataloader-codegen: Autogenerate DataLoaders for your GraphQL Server!", "author": ["\n        \n  Mark Larah, Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/04/open-sourcing-dataloader-codegen.html", "abstract": "We’re open sourcing dataloader-codegen , an opinionated JavaScript library for automatically generating DataLoaders over a set of resources (e.g. HTTP endpoints). Go check it out on GitHub! This blog post discusses the motivation and some the lessons we learned along the way. Managing GraphQL DataLoaders at Scale At Yelp, we use GraphQL to provide data for our React webapps . The GraphQL Server is deployed as a public gateway that wraps hundreds of internal HTTP endpoints that are distributed across hundreds of services . GraphQL Request Diagram DataLoaders DataLoaders provide an important caching/optimization layer in many GraphQL servers. If you aren’t already familiar with this pattern, check out this excellent blog post by Marc-André Giroux that covers it in more detail. Without DataLoaders Here’s what a Yelpy GraphQL request might do without DataLoaders: Resolver logic without DataLoaders 😱 Here we can see resolver methods naively making individual upstream requests to\nfetch their data. Without the batching and caching features that DataLoaders provide, we could\nimagine getting into a sticky situation where we issue too many requests and\naccidentally DoS ourselves. With DataLoaders So let’s wrap these HTTP endpoints with DataLoaders: Resolver logic with DataLoaders 😎 In this world, resolvers talk to DataLoaders instead of directly making a network request. The built-in batching and caching logic allows us to greatly cut down on the number of internal API calls we’re making! 🎉 This approach pairs well with the pattern of leaf node resolvers making their own data fetching calls, as described in this blog post . Scaling it up In general, this pattern works great. When we go beyond wrapping a few endpoints,\nwe risk running into some challenges in managing the DataLoader layer: “ Where do I get data from now?!” “What does the DataLoader interface look like? How do I maintain type safety?” “How do I implement the DataLoader for this endpoint?” Challenge #1 - Where do I get data from now? The data available from our endpoints is well understood. All resources are defined by a swagger specification ,\nallowing developers to browse all available endpoints and the data they expose. Browsing API endpoints with Swagger UI. Image from https://swagger.io/tools/swagger-ui. When implementing a resolver method, this makes it easy to know where to fetch data from - just call the corresponding generated method . Endpoints vs DataLoaders Adding DataLoaders introduces a layer of indirection. If we were to manually\ndefine the DataLoaders and ask developers to call those instead, how would they\nknow which DataLoader to call? Human-defined DataLoaders It could be tempting to create a DataLoader with a nice name like users - but\nwhich of our many endpoints that serve user info would this wrap? And do we end up accidentally wrapping the same endpoint twice (thereby losing\nout on some batching ability)? 📣 Relying on a human-defined structure of DataLoaders that is different to an existing set of endpoints introduces friction and risks loss of batching behaviour. We can fix this by asserting that the shape of DataLoaders is a 1:1 mapping to our resources: DataLoaders with a 1:1 mapping to resource Enforcing that the shape (and method names) of the DataLoader object maps 1:1 with the underlying resources makes things predictable when figuring out which DataLoader to use. 📣 We want developers to think in terms of the already familiar, well-documented\nunderlying resources, instead of DataLoaders. Challenge #2 - What does the DataLoader interface look like? How do I maintain type safety? Resources come in all shapes and sizes. Some endpoints use a batch interface ,\nsome don’t guarantee ordering. Perhaps the response object is nested and needs to\nbe unwrapped etc… Figuring out the argument/return types of the DataLoader and correctly transforming the interface  presents another fun challenge to DataLoader authors. For example, consider the following batch resource that returns a list of users: getBasicUserInfo ({ user_ids : Array < number > , locale : string , include_slow_fields : ? boolean }) => Promise < Array < UserInfo >> In our resolvers, we only need to grab a single user object at a time. We should\nbe able to request a single user object from the corresponding DataLoader. That means we need to map the interface of the DataLoader in the following ways: Underlying Resource DataLoader .load function Argument Object containing an Array of user_ids . Object containing a single user_id Return Array of UserInfo objects. Single UserInfo object. With this approach, the interface for calling the DataLoader version of the\nresource looks like this: getBasicUserInfo . load ({ user_id : number , locale : string , include_slow_fields : ? boolean , }) => Promise < UserInfo > Implementing the interface transformation logic consistently and correctly can be\nhard. Subtle, unexpected differences can cause friction for developers. 📣 We want to maintain a consistent mapping from Resource to DataLoader interface,\nin order to make things predictable for resolver authors. Remember the types If the server codebase uses static types - TypeScript (or in Yelp’s case, Flow),\nwe must also define a type signature for the DataLoader. For our case above, it\nwould look something like this: DataLoader < { | user_id : number , locale : string , include_slow_fields : ? boolean , | }, UserInfo , > Transforming the types to match the implementation logic and keeping them in\nsync presents an additional concern for DataLoader authors. Challenge #3 - How do I implement the DataLoader for this endpoint? Writing the DataLoaders over many endpoints results in a lot of imperative\nboilerplate code that requires an understanding of how the endpoint is\nimplemented. Here’s what the implementation for the DataLoader over our getBasicUserInfo endpoint above might look like: new DataLoader ( keys => { let results ; try { results = userApi . getBasicUserInfo ({ user_ids : keys , include_slow_fields : false , locale : ' en_US ' , }); } catch ( err ) { return Promise . reject ( err ); } // Call a bunch of helper methods to tidy up the response results = reorderResultsByKey ( results , ' id ' ); results = ... return results ; } Yes, we’re totally cheating here! The keys argument here\nrepresents the list of user_ids directly - not a list of objects of arguments\nto the resource. This is a decision we might make for the sake of simplicity in\nimplementing the DataLoader. In addition to calling the underlying resource, we might also have to deal with: Error handling Figuring out the argument(s) to the resource from keys Normalizing the response shape Any other extra logic (e.g. request logging) This can get pretty tedious when copied and pasted hundreds of times for\ndifferent endpoints. And since each endpoint could be implemented slightly\ndifferently, each DataLoader implementation might be subtly different too. Since the DataLoader interface is not guaranteed to be predictable, developers\nhave to understand how the DataLoader is implemented, and how it differs from\nthe underlying resource in order to use it. Dealing with multiple parameters Consider the getBasicUserInfo DataLoader above - we cheated by never including include_slow_fields and just using user_ids for keys. If we want to offer the full flexbility of the underlying resource, we need to\nchange the getBasicUserInfo DataLoader to use the signature suggested above: DataLoader < { | user_id : number , locale : string , include_slow_fields : ? boolean , | }, UserInfo , > Inside of our batch function, keys now becomes a list of objects that looks\nlike this: [ { user_id : 3 , include_slow_fields : false , locale : ' en_US ' }, { user_id : 4 , include_slow_fields : true , locale : ' en_US ' }, { user_id : 5 , include_slow_fields : false , locale : ' en_US ' }, ] We need to coalesce these objects so we can make a batched call to userApi.getBasicUserInfo . It may be tempting to write logic that looks\nsomething like this: userApi . getBasicUserInfo ({ user_ids : keys . map ( k => k . user_id ), include_slow_fields : ???, locale : ???, }) What would you put for include_slow_fields or locale ? We can’t naively assume the value for keys[0].include_slow_fields , because this\nwould be unsafe - the second call to .load didn’t request “slow_fields”, but\nwould receive it, slowing down the whole request - oops! In this case, we’d have to make two calls to userApi.getBasicUserInfo , and chunk up the requests accordingly: userApi.getBasicUserInfo({ user_ids: [3, 5], include_slow_fields: false, locale: 'en_US' });\nuserApi.getBasicUserInfo({ user_ids: [4], include_slow_fields: true, locale: 'en_US' }); 📣 DataLoader authors must take care when coalescing the keys input to ensure correctness in the .load response. Summary of issues We have hundreds of internal HTTP API endpoints we may wish to call in GraphQL.\nManually writing the DataLoader layer proved to be a significant pain point for\nthe following reasons: Implementing the DataLoader batch function logic requires knowledge of the underlying resource and can be hard to implement correctly. There’s no guarantee that the shape of the DataLoaders / names of the loaders map perfectly to the endpoints, making them hard to find. We risk duplicate loaders being made for the same endpoint due to (2) The input / return types of the loaders are manually typed, requiring extra overhead and custom types The interface of a DataLoader is separate to the endpoint it’s wrapping, and can be arbitrarily different. If the endpoint changes upstream, we have to manually update the DataLoader implementation. Yikes! After dealing with this for a while, we eventually decided to do something\nabout it :) Given that there’s a finite amount of variation in the way our endpoints are\nimplemented, this looked it could be a job for codegen … 🤖 dataloader-codegen dataloader-codegen is a tool we\nbuilt to automagically generate the DataLoaders used in our GraphQL Server. We’re excited to announce the release of this tool publicly. Check out the\nproject on GitHub: https://github.com/Yelp/dataloader-codegen https://github.com/Yelp/dataloader-codegen It works by passing in a config file that describes a set of resources - e.g. resources : getUserInfo : docsLink : https://yelpcorp.com/swagger-ui/getUserInfo isBatchResource : true batchKey : user_ids newKey : user_id getBusinessInfo docsLink : https://yelpcorp.com/swagger-ui/businessInfo isBatchResource : false ... From this, we can generate our DataLoaders at build time: By letting a tool do the work of writing the DataLoaders, we can solve the issues\ndescribed above. In particular, dataloader-codegen gives us: 🤖 Generated DataLoader implemenations ✨ Predictable DataLoader interfaces 🤝 1:1 mapping of resource to DataLoader 🔒 Type safety of the DataLoaders is preserved Here we can see Flow complaining about an invalid property passed to a generated\nDataLoader: Overall, dataloader-codegen has allowed us to remove a lot of boilerplate code in\nour server, save time, and reduce friction in dealing with DataLoaders. For more information about how dataloader-codgen works and how to use it in your\nproject, check out the project on GitHub! Need help getting set up or have any feature requests for dataloader-codegen? Let us know by opening an issue! Written By Mark Larah, Software Engineer ( @mark_larah ) Acknowledgments Thanks to the following folks for contributing internally to dataloader-codegen! Ryan Ruan, Software Engineer ( @ryanruanwork ) Jack Guy, Software Engineer Tweet Back to blog", "date": "2020-04-08"}, {"website": "Yelp", "title": "Architecting Restaurant Wait Time Predictions", "author": ["\n        \n  Shweta Joshi, Machine Learning Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/12/architecting-wait-time-estimations.html", "abstract": "Is there a restaurant you’ve always wanted to check out, but haven’t been able to because they don’t take reservations and the lines are out the door? Here at Yelp, we’re trying to solve problems just like these and delight consumers with streamlined dining experiences. Yelp Waitlist is part of the Yelp Restaurants product suite, and its mission is to take the mystery out of everyday dining experiences, enabling you to get in line at your favorite restaurant through just the tap of a button. For diners, in addition to joining an online waitlist, Yelp Waitlist provides live wait times and queue updates. For restaurants, it facilitates table management and reduces stress and chaos by the door by allowing guests to sign up remotely. The flow is simple: diners see the current wait times at a Waitlist restaurant and virtually get in line right from the Yelp app. If you want to know more about the product, check out this related post! Wait estimates are modeled as a machine learning problem. When you request to be seated at a restaurant through Waitlist, a machine learning model is alerted behind the scenes to generate a prediction. The ability of this model to provide reasonable wait estimates is what makes the online waitlist possible, so you have some bit of AI to thank the next time you enter a line from the comfort of your home. The prediction endpoint is part of a larger system that enables the generation of the estimated time. This blog post is aimed at describing the Waitlist machine learning system that bridges hungry diners to their tasty food. The System As you can imagine, the system needs to be as up to date as possible with the state of the restaurant (e.g., how many people are currently in line), and the many other contextual factors that determine an estimate as accurately as possible. For example, you cannot expect the wait time to extend beyond the closing time of the restaurant. Additionally, there are certain latency requirements to serve a high volume of QPS. The system can be broken down into three components: The offline training pipeline where model iteration, data-wrangling, and ETLs happen. Online serving which tracks the current state of the restaurant and responds to requests. Analytics providing model performance reports and analyses. We chose to use XGBoost as the model to generate wait estimates. Offline training happens via Spark and an optimized XGboost Java Library that helps us meet latency requirements is used online. We faced  two main challenges while architecting the machine learning system: The requirement of serving users live predictions from a Spark ML model with an online service in Python. The cold start problem when adding new businesses to the product. Most of the system was initially designed to make the first challenge possible. We slowly added components to enable training and prediction with more features once we felt confident in the system’s ability to work seamlessly on its own. The second challenge was addressed by the use of XGboost, which can make predictions with partial feature-sets. Though these predictions may not be very accurate at first, retraining helps improve them over time. Below is a simplified view of the system: The various components in the above diagram are: Data Warehouse : Source of data for training, backed by Redshift. Offline Service : Service responsible for training the model. This is written in Python and uses Spark for model training due to the quantity of data involved (tens of millions of instances after sanitization). Feature ETLs : Spark-based ETLs for generating additional features. These are non-time-sensitive features which are shared both online and offline. Model Server :  In-house Java service which stores the trained model and is optimized for high-throughput traffic. Online Stores : Available features generated from Spark-ETL, as well as up-to-date restaurant data. This encompasses: Cassandra for storing results from Spark-ETLs MySQL for storing the restaurant’s state Online Service : Service responsible for generating predictions in real time and making calls to the online stores and model server to do so. This service is written in Python. As hinted above, we rely heavily on Spark for building models, as well as for deriving additional features. It’s important to note, however, that the online service does not make use of Spark, which can result in different data access and manipulation patterns before being fed into the model to make a prediction. A lot of care goes into ensuring that the set of features we compute offline match those we compute online. A theoretical example of a mismatched online/offline feature would be different orderings for one-hot encoded feature columns, which, despite having identical raw data, can result in different feature vectors. Figure 2 (below) breaks down the model development, evaluation, and launch pipelines: Model development pipeline: At this stage, the model flows from human intuition/ideation to reality. This encompasses: Feature-extraction ETLs Feature-set blueprints: Feature definitions intended to enforce online/offline consistency (e.g., what subset of features this particular feature-set contains, its data types, etc.) Evaluation pipeline: This ensures that the newly trained model obtains an acceptable performance with regard to business metrics. This pipeline is a combination of automation and human decision making. For example, a metric could track the percentage of diners who waited more than five minutes beyond their quoted estimate. The steps for evaluation include: Running an evaluation batch for the freshly trained model. Comparing performance against previous benchmarks. Evaluating if the new model is a viable candidate for experimentation/release. (Unfortunately not all candidates are viable; this can be attributed to the probabilistic nature of machine learning projects.) The models that pass this stage promise superior performance compared to the status quo model. Experimentation/ Model-Launch pipeline At this stage, we’re convinced of the model’s promise and want to experiment with it in the real world. To maintain confidence that the model will operate in production as it did in offline evaluation, we promote the model to “dark-launch” mode. To do this, we need to be able to reproduce the feature-set in the online service. This means: Each incoming user request contains a partial feature-set. The rest of the features are pulled from online data stores. The feature-set is guaranteed to maintain the same format as the training data (thanks to feature-blueprints). Once we have the ability to make predictions from the online service, we can proceed to the dark-launch phase. Here, we: Surface our candidate model as a ghost/dark model. Enable the model to see live incoming requests and produce estimates for these requests (without surfacing them to the user). Use the event logs generated from the experiment launch to measure the performance of all models across all samples. We’ve seen several benefits from dark-launching our model: Comparing performance across different cohorts of businesses without affecting estimates. Weeding out any differences in online and offline model-pipelines. Since both perform their own set of computations, etc., there’s plenty of scope for mistakes and we can ensure that offline and dark-launch give identical prediction estimates for the same candidate. Checking the latency of the new model and ensuring we don’t violate any SLOs. At any given time, we can have several models launched live, several dark-launched, and several under development. We can typically verify within a few days’ time if the dark-launched model is working as expected; if not, we can begin to investigate any discrepancies. If the results are as expected, we can slowly start rolling out the new model. This slow rollout is intended to capture feedback loops that we’re not exposed to during dark-launch. What’s a Feedback Loop? Whenever we surface an estimate to the user, we set an expectation of the time they’ll be seated, thereby affecting when the user shows up to the restaurant. If, for instance, this causes the user to arrive at the restaurant after their table is actually available, they may have a longer overall wait time (our label) than if we’d given them a shorter estimate. These instances are tracked in our logs and we try our best to reduce such inaccuracies. The feedback loop here happens when our label data is  influenced by our prediction. Factors like this add sensitivity to our system, which underscores the importance of providing accurate wait estimations. Measuring Success Within this problem area are a variety of metrics we can track, and choosing the right ones is always a challenge. We need to cater to the needs of not only the users (by ensuring they wait only as long as expected), but also the restaurant and its staff (not sending enough people to occupy empty tables vs. sending too many people at the same time which puts pressure on the hosts). We can observe a few of these metrics using data streamed into our logs. A few others can be gauged through user-feedback surveys (which in itself has the propensity to be biased), and whatever else that cannot be observed, we hypothesize. We’re constantly trying to collect as much data as possible to improve the coverage of each quantitative and qualitative metric. Measuring success is not trivial, especially given that the set of restaurants we serve is constantly growing and providing more opportunities to observe new behavioral patterns. With each model that we build and deploy, we learn a little more about our system, helping us better measure success. So far this strategy has worked well for us. Conclusion Wait-time estimation is a unique problem we could only begin to address because of the state-of-the-art tooling and support from the wonderful people at Yelp! We continue to make updates to the algorithms and migrate our system to use more efficient tooling to make our estimates as accurate as possible so that you - our customer - don’t have to wait longer than you need at your favorite restaurant. Acknowledgements Huge thanks to my indispensable team for all their contributions: Chris Farrell, Steve Thomas, Steve Blass, Aditi Ganpule, Saeed Mahani, Kaushik Dutt, and Sanket Sharma. Tweet Become a Software Engineer at Yelp Passionate about solving problems with Machine Learning? View Job Back to blog", "date": "2019-12-12"}, {"website": "Yelp", "title": "Streaming Cassandra into Kafka in (Near) Real-Time: Part 1", "author": ["\n        \n  Andrew Prudhomme, Software Engineer and Harshal Dalvi, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/12/cassandra-source-connector-part-1.html", "abstract": "At Yelp, we use Cassandra to power a variety of use cases. As of the date of publication, there are 25 Cassandra clusters running in production, each with varying sizes of deployment. The data stored in these clusters is often required as-is or in a transformed state by other use cases, such as analytics, indexing, etc. (for which Cassandra is not the most appropriate data store). As seen in previous posts from our Data Pipeline series, Yelp has developed a robust connector ecosystem around its data stores to stream data both into and out of the Data Pipeline. This two-part post will dive into the Cassandra Source Connector, the application used for streaming data from Cassandra into the Data Pipeline. Data Pipeline Recap Yelp’s Data Pipeline is an abstraction on top of Apache Kafka (explained in this blog post ) and is backed by a schema registry called Schematizer . It currently serves as the backbone of hundreds of use cases at Yelp, ranging from analytics and experimentation to notifications, ranking, and search indexing. The Data Pipeline Ecosystem at Yelp Here’s a quick recap of the Data Pipeline: Data published into the Data Pipeline must be schematized. In essence, data cannot be published if it doesn’t have a predefined schema. For data backed by data stores, the corresponding streams in the Data Pipeline must conform to the stream-table duality . Every message in the Data Pipeline must contain the full content of an equivalent row in the data store. In addition, UPDATE and DELETE messages must also contain the previous snapshot of the equivalent row before the change. Challenges With Streaming Data From Cassandra Due to the nature of how Cassandra works, meeting the aforementioned Data Pipeline requirements can present some challenges. Achieving Ordering of Writes Cassandra uses multiple replicas of data for availability. However, there’s no actual concept of a global replication stream. Each write is independently replicated, with all nodes eligible to coordinate. As a result, concurrent writes may be processed in different orders on different replicas. Cassandra uses several mechanisms (hinted handoffs, repairs, last write wins) to ensure that data is eventually consistent. Although the replicas eventually agree on the final value of the data, this does not resolve the differences in write order. Thus, the Cassandra Source Connector needs to conform to the write ordering guarantees similar to those of Cassandra. Obtaining Complete Row Content There’s no requirement for Cassandra writes to contain all table columns. Even if this were the case, the current state of the row would depend on both the data in the write and all previously written data that shadows it. Thus, the write data alone is not sufficient to determine the new row state. Obtaining Previous Row Content As is the case when determining new row value, knowledge of the row state prior to a given mutation is required. This prior row state represents the accumulation of all previous writes. Distributed Data Ownership The ownership of data in Cassandra is distributed between the nodes in each datacenter. There’s no special “master”; all nodes are able to coordinate writes. Thus, processing these writes to a cluster involves combining information from multiple nodes. Possible Approaches Several approaches were considered when designing the Cassandra Source Connector. This post by WePay gives a solid description of the primary streaming options available along with the pros and cons of each, including: Writing to both Cassandra and Kafka (“Double Writing”) Writing directly to Kafka and using a Cassandra Sink to load the data in Cassandra (“Kafka as Event Source”) Processing the commit log exposed by Cassandra’s Change Data Capture or CDC (“Parsing Commit Logs”) The use of Kafka Connect’s Cassandra Source was also investigated. This connector  streams data from a Cassandra table into Kafka using either “bulk” or “incremental” update modes. Both modes function by periodically polling the table for data. Bulk mode performs a full table scan, publishing the entire result, while incremental mode queries the rows written since the last sampling. Both modes have their disadvantages: Bulk mode table scans are very expensive on large tables, and each scan publishes a lot of duplicate data. Incremental mode is only viable for a certain type of workload. The writes must be append-only with monotonically increasing columns (such as timestamps) as part of the primary key. Additionally, polling for this data can cause extra cluster load. Ultimately, a solution based on processing Cassandra CDC made the most sense for the connector. High-Level Design Cassandra’s distributed deployment characteristics coupled with both the need to achieve an ordering of writes and meet Data Pipeline semantics made creating a single application quite  challenging. Thus, the Cassandra Source Connector was built as two separate components, each addressing a subset of these issues: Cassandra Source Connector at a High Level CDC Publisher : A service running locally on Cassandra nodes that uses CDC to publish raw Cassandra writes into intermediate Kafka streams. These streams serve as unified commit logs, removing the aspect of distributed data ownership and defining an order of events to process. Data Pipeline Materializer ( DP Materializer ): An application running on Apache Flink which processes raw Cassandra writes produced by the CDC Publisher and publishes them as Data Pipeline messages. CDC Publisher The CDC Publisher produces all writes made in Cassandra tables as serialized partition updates into table-specific Kafka streams. Processing Cassandra Writes with CDC The Change Data Capture (CDC) capability introduced in version 3.8 of Cassandra is used by the CDC Publisher to process writes. Normally (with CDC disabled), writes are stored by Cassandra in the following manner: Client writes are persisted to memtables and the commit log by every node Memtables are periodically flushed to SSTables on disk Cassandra Write Path The commit log is composed of a series of fixed-sized files (defaulted at 32MB) called “commit log segments”. Once the memtables are flushed to SSTables, these segments are discarded by Cassandra. If CDC is enabled, all Cassandra commit log segment files containing writes to a tracked table are flagged. When the files are no longer referenced by corresponding memtables, they’re moved into a separate directory (instead of being discarded). Cassandra Write Path with CDC There are several challenges with using the current implementation of Cassandra’s CDC: Per-node processing: As each node stores only a portion of the complete table data, CDC must be processed on multiple nodes. Replication: The same write is stored on each data replica, resulting in duplicate processing. Partial data: Commit log segments only contain the information from incoming writes and do not have the full view of the corresponding rows. CDC does not contain schema information about the tables. CDC directory size limit: If the CDC directory gets too large in size, the node will reject new table writes. Poorly bounded latency: Commit log segments must be full and no longer referenced by memtables before being made available for processing. For clusters with low write rates, the commit log segments can take a while to fill up, affecting latency. Despite these drawbacks, CDC was used because it is the solution developed by the Cassandra open source community for processing committed data. This also means that any future improvements to the CDC implementation can be leveraged by upgrading Cassandra versions. Wrangling CDC Deployment CDC Datacenter Deployment To ensure that processing CDC doesn’t cause any performance issues on the actual cluster, a virtual Cassandra datacenter is created, which is logically separate from the standard region-specific datacenters. The CDC Publisher is deployed only on the nodes of this datacenter. As all writes go to data replicas in all datacenters, this is sufficient to ensure coverage of all table changes. Additionally, nodes in this datacenter can be provisioned differently as they don’t serve live client read requests. Bounding Latency As mentioned earlier, one of the issues with using CDC is that the latency (defined as the time between the write to Cassandra and the data being made available for processing) is poorly bounded. CDC only allows processing of commit log files that are no longer needed, meaning they should be full and not referenced by an existing memtable. To introduce predictable latency bounds to the connector, the following approaches were adopted: Removing Memtable References Memtables are periodically flushed by Cassandra to SSTables when they get too large. However, a table with a low write rate will rarely be flushed, thus delaying CDC processing for the whole cluster. To ensure this does not happen, an explicit flush of all memtables is triggered at periodic intervals (typically 5-10 minutes) for nodes in the CDC datacenter. This ensures that a full commit log segment will only wait, at most, one flush interval before it can be processed. As only the CDC datacenter nodes are flushed, there’s no impact to client read performance in the other datacenters. Filling Segments Commit log segment sizes are fixed. If the tracked table has a slow write rate, it may be a while before a segment completely fills up. This fill-up time is bound by creating a process separate from the CDC Publisher which writes to a “filler” table at a predictable rate. This table is replicated only in the CDC datacenter and is fully replicated to all nodes. To limit any performance impact, fewer large writes (~100K) are performed, only a single key is written to, and the data is aggressively TTL’ed. Processing CDC To aid with the processing of CDC commit log segments, the Cassandra library provides a handler interface for applications to implement. This interface allows processing of a stream of all mutations (writes) present in a commit log segment. The Mutation class is the Java object Cassandra uses to represent data, namely: A Mutation contains PartitionUpdate objects for multiple tables A PartitionUpdate contains Row objects for a single partition key value A Row contains data for a single clustering key value Structure of a Cassandra Mutation The primary function of the CDC Publisher is to break these mutations up into individual PartitionUpdate objects. If a PartitionUpdate contains multiple rows, these are further broken down into a series of updates with single rows. Thus, each update contains data only for a single Cassandra primary key. Breakdown of a Mutation into Individual Row Objects Each of the resulting PartitionUpdate objects is serialized for publishing to Kafka streams. Serializers provided by the Cassandra library are used for serialization before publishing. Publishing to Kafka The PartitionUpdate payloads are used to build messages to publish to the intermediate Kafka stream. Each message includes: The serialized PartitionUpdate The Cassandra messaging version used for serialization Metadata for auditing (host, file, position, etc.) The messages are then published to table specific Kafka streams. A stream can have multiple partitions for scalable publishing; in which case, messages are routed to Kafka partitions based on the Cassandra partition key. Thus, all writes for a single partition key will end up in the same topic-partition. Publishing CDC to a Multi-Partition Kafka Topic Intermediate Kafka Streams The resulting Kafka streams contain all writes to the tracked Cassandra tables. As all updates to a primary key reside in the same topic partition, this sets an ordering of writes for each key. While there’s no guarantee events will be in writetime order, there’s also no guarantee that writes will commit to a Cassandra replica in writetime order. Additionally, there will be a duplicate write copy for each data replica. Even though this is the case, the intermediate streams act as unified commit logs for the tables. They provide an order of events per key that can be deterministically processed into the ordered stream of row updates needed for publishing to the Data Pipeline. Stream Consistency Given that the connector uses the Cassandra write path, the consistency of the resulting Kafka stream will not be more consistent than the underlying datastore. As writes are published from each replica in their local commit order, the processed stream should initially be no less consistent than reading from a single replica. As data from additional replicas is processed, the stream becomes eventually consistent. When all replicas have published updates, the consistency will be equivalent to a read covering all CDC datacenter nodes. The time-boundness of this eventual consistency is determined by the write consistency level used by the Cassandra clients. If the update has to immediately show up in the stream, a high consistency level (e.g., EACH_QUORUM) must be used to ensure commits to nodes in the CDC datacenter. If a lower/local consistency is used for writes, the PartitionUpdate may not appear in the output stream (in the worst case) until the next table repair. Note that this is in line with the guarantees given to clients reading Cassandra directly. What’s Next? At this point, the intermediate Kafka streams contain Cassandra PartitionUpdate objects partitioned by keys and in a loosely ordered manner. These objects must now be deserialized, converted into ordered Data Pipeline messages, and published into the pipeline. This is done through the DP Materializer. The DP Materializer is covered in the second half of this two-part post. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Back to blog", "date": "2019-12-05"}, {"website": "Yelp", "title": "Streaming Cassandra into Kafka in (Near) Real-Time: Part 2", "author": ["\n        \n  Harshal Dalvi, Engineering Manager and Andrew Prudhomme, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/12/cassandra-source-connector-part-2.html", "abstract": "The first half of this post covered the requirements and design choices of the Cassandra Source Connector and dove into the details of the CDC Publisher. As described, the CDC Publisher processes Cassandra CDC data and publishes it as loosely ordered PartitionUpdate objects into Kafka as intermediate keyed streams. The intermediate streams then serve as input for the DP Materializer. Data Pipeline Materializer The DP Materializer ingests the serialized PartitionUpdate objects published by the CDC Publisher, transforms them into fully formed Data Pipeline messages, and publishes them into the Data Pipeline. The DP Materializer is built on top of Apache Flink, a stream processing framework. Flink has been used in production at Yelp for a few years now across various streaming applications. It  provides an inherent state backend in the form of RocksDB, which is essential for guaranteeing inorder CDC publishing. In addition, Flink’s checkpoint and savepoint capabilities provide extremely powerful fault tolerance. The application has two main phases: Schema Inference (or the “bootstrap phase”) ETL (or the “transform phase”) Schema Inference During the bootstrap phase, the avro schema necessary for publishing to the Data Pipeline is derived from the Cassandra table schema. The process begins by building the Cassandra table metadata objects ( CFMetaData ) used by the Cassandra library. Loading this metadata is required to use library functionality to act on the serialized Cassandra data from the CDC Publisher stream. The metadata object contains information on the table primary key, column types, and all other properties specified in a table CREATE statement. This schema representation is processed to produce an avro schema where each Cassandra column is represented by an equivalent avro type. As the DP Materializer is deployed outside of the Cassandra cluster, it cannot load the table metadata from files on the local node (like the CDC Publisher). Instead, it uses the Cassandra client to connect to Cassandra and derive the CFMetaData from the schema of the table being streamed. This is done in the following steps: Once connected to a cluster, the create table and type (for UDTs) statements are retrieved. Cassandra’s query processor is used to parse the retrieved create statements into the table metadata objects. Information about columns previously dropped from the table is retrieved and added to the metadata built in the previous step. Loading the dropped column information is required to read table data created prior to the column being dropped. Loading Table Metadata from Cassandra Once the metadata is loaded, the DP Materializer builds the avro schema from the metadata. A couple of key things happen in this derivation phase: The table’s partition key and clustering key(s) are mapped as the primary keys of the avro schema. All other columns in the table (except the partition and clustering keys) are created as nullable. In the event of schema changes in the table, this guarantees that the corresponding avro schemas are always compatible to their previous versions (except when re-adding a column with a different type, which in itself can cause issues ). Schema generation currently supports nearly all valid Cassandra column types (except when prohibited by Avro), including collections, tuples, UDTs, and nesting thereof. Schema Change Detection As the above schema inference is part of the bootstrap phase, the DP Materializer needs the ability to detect Cassandra schema changes online and update the output Avro schema automatically. To achieve this, it implements Cassandra’s schema change listener interface, provided by the Cassandra client, to detect when a change is made to the schema of the tracked table. Once detected, the corresponding Cassandra metadata is updated and the avro schema is rebuilt from the updated metadata. ETL (or Consume, Transform, and Publish) This phase of the DP Materializer is where the serialized PartitionUpdate objects from the CDC Publisher are consumed, processed, and transformed into Data Pipeline messages for publishing into the Pipeline. The consumer and publisher are provided out-of-the-box by Flink, so this section primarily focuses on the transformer portion of the DP Materializer. Data Pipeline Materializer State Architecture The transformer is backed by Flink’s RocksDB state. This state is abstracted as a collection of map objects, with each map corresponding to a partition key from the Cassandra table. Each map object has, as its keys, the clustering keys from that partition in Cassandra. A PartitionUpdate, containing at most one row, is stored as the value for its corresponding clustering key in the map. For tables which do not have defined clustering keys, each map contains a single entry with a null key. State Structure State loading and memory management is handled internally by Flink. In addition, Flink’s stream keying mechanism guarantees that all updates for a partition key will be routed to the same worker and processed against the same map object persistently across application restarts. Note that the PartitionUpdate objects from the CDC Publisher can be both duplicated multiple times and out-of-order (by writetime). In addition, oftentimes a PartitionUpdate may not contain the full content of a Cassandra row. The Transformer The central piece of the application is the transformer, which: Processes the Cassandra CDC data into a complete row (with preimage) for the given avro primary key (Cassandra partition key + clustering key[s]) for publishing to the Data Pipeline. Produces final output message with the appropriate Data Pipeline message type. The transformer uses the row (PartitionUpdate) saved in the map objects in the state, along with the incoming PartitionUpdate objects from the CDC Publisher to generate the complete row content, the previous row content (in the case of UPDATE, DELETE messages), and the type of the output message. This is achieved by deserializing the input PartitionUpdate and merging it with the saved PartitionUpdate. This is done using the same PartitionUpdate merge functionality Cassandra uses to combine data from SSTables during reads. The merge API takes in two PartitionUpdate objects, one from the Flink state and the other from the CDC Publisher’s output stream. This produces a merged PartitionUpdate which is used to build an avro record with the schema derived during the bootstrap phase. If the previous row value is needed, it is derived from the saved PartitionUpdate in the Flink state. In the end, the state is updated with the merged PartitionUpdate. Determining Row States This process handles duplicate and out-of-order PartitionUpdate objects. The use of Cassandra’s merge functionality results in the same “last write wins” conflict resolution as a Cassandra read. To avoid publishing duplicate messages, it is verified that the input PartitionUpdate changes the row state. This is done by computing the md5 digests of the saved and merged PartitionUpdate objects. If the digests are the same, the PartitionUpdate is ignored. The merge, update state, and publish logic can be summarized below: The incoming PartitionUpdate is merged with the saved PartitionUpdate (if it exists) and the corresponding Data Pipeline message is determined: If the merged PartitionUpdate contains live (non-tombstoned) data and the saved does not, a CREATE message is published. If both the merged and saved PartitionUpdate objects contain live data, an UPDATE message is published if the md5 digests of the objects are different. If the merged PartitionUpdate contains tombstoned data but the saved one contains live data, a DELETE message is published. If the md5 digests of the saved and merged PartitionUpdate objects are different, then the merged PartitionUpdate is saved in the state. Thus, at the end of the transform phase, a message with the appropriate Data Pipeline message type and the full row content is ready to be published into the Data Pipeline. Supporting Backfills Bootstrapping a Stream A limited amount of CDC logs can be stored on a Cassandra node . Thus, when a table is set up to be streamed by the connector, only the data available in the CDC directory at the time (and going forward) will be processed. However, to maintain the stream-table duality, all of the existing data in the Cassandra table needs to be replayed into the stream. To achieve this, the backfill bootstrap process reads through the data stored on disk as SSTables. To ensure that the set of SSTable files are not modified by compaction during the backfill, the table’s snapshot is taken and the SSTables are processed off of that snapshot. The Cassandra SSTable reader returns the scanned data as a series of PartitionUpdate objects. The CDC Publisher processes these PartitionUpdate objects in the same way as commit log segments and publishes them into Kafka, where they’re subsequently transformed into Data Pipeline messages by DP Materializer. This process is followed whenever a Cassandra table is first set up to be tracked by the connector. This is also done if there’s a need to rebuild the state in the DP Materializer. Rebuilding a Stream If a tracked table’s output stream becomes corrupted or is deleted (unlikely but possible), the stream can be rebuilt by replaying the stored state of the DP Materializer. As all of the serialized PartitionUpdate objects are stored in the state, there’s no need to republish data from the SSTables. Limitations and Future Work Partition Level Operations The current system design processes each row change independently. A single input message to the DP Materializer will emit at most one message into the Data Pipeline. Changes at a partition level that affect the value of multiple rows are not currently supported. These include: Full partition deletion (only when using clustering) Ranged tombstones Static columns There is, however, a potential path to support. The DP Materializer stores all rows in a single Cassandra partition as entries of the same map object during processing. It is conceivable to also store the partition level state separately. When this state changes, the DP Materializer could iterate through the entire map (Cassandra partition) and produce Data Pipeline messages for all affected rows. TTL TTL’ed data is currently not supported by the connector. TTL values are ignored and data is considered as live based on its writetime. Dropping Tombstones There’s no support to drop tombstones from DP Materializer’s Flink state. They will remain there indefinitely unless overridden with new data. It may be possible to drop old tombstones when updating row state, similar to the gc_grace_seconds parameter on tables. However, this would not help for rows that are never updated. In addition, great care would need to be taken to ensure backfilling or repairing a table does not create zombie data in the output stream. Publishing Latency As mentioned earlier, commit log segments must be full and no longer referenced by memtables before being made available for processing by Cassandra. In spite of the CDC log filler implementation, some latency is introduced in publishing to the Data Pipeline. This limitation should be overcome in Cassandra 4, which introduces the capability to read live commit log segments and will thus ensure that the publishing latency is as close to real time as possible. Learnings The Cassandra Source Connector has been running in production at Yelp since Q4 2018. It supports multiple use cases, which have helped in surfacing some quirks about its design choices: Avro as a Serialization Format The maximum number of cells (rows * columns) allowed by Cassandra in a single partition is two billion. This means that a row could potentially have two billion columns. However, Avro serialization and deserialization becomes a bottleneck once the number of columns starts going into the hundreds and cannot hold up with the potential maximum number of columns. Horizontal scaling might be needed for consumers depending on the throughput requirements and size (in number of columns) of the Cassandra table being streamed. In addition, a few Cassandra data types (such as DECIMAL) don’t have intuitive Avro data type equivalents. In such cases, either the columns cannot be supported or custom avro data types have to be defined. Flink State Size As every single row from the table is stored as a serialized PartitionUpdate in the state, the state size can grow up to be huge for large tables. The state size becomes a bottleneck for code pushes and maintenance as it has to be reloaded for every deployment and restart of the application. Additional work is required for minimizing the time for saving and loading state for huge tables. TL;DR? Yelp presented the Cassandra Source Connector at Datastax Accelerate 2019. You can watch it here . This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Become an Engineer at Yelp Interested in solving problems like these? Apply! View Job Back to blog", "date": "2019-12-18"}, {"website": "Yelp", "title": "Automated IDOR Discovery through Stateful Swagger Fuzzing", "author": ["\n        \n  Aaron Loo, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/01/automated-idor-discovery-through-stateful-swagger-fuzzing.html", "abstract": "Scaling security coverage in a growing company is hard. The only way to do this\neffectively is to empower front-line developers to be able to easily discover,\ntriage, and fix vulnerabilities before they make it to production servers. Today, we’re excited to announce that we’ll be open-sourcing fuzz-lightyear : a testing framework\nwe’ve developed to identify Insecure Direct Object Reference (IDOR) vulnerabilities through stateful Swagger fuzzing ,\ntailored to support an enterprise, microservice architecture. This integrates with\nour Continuous Integration (CI) pipeline to provide consistent, automatic test\ncoverage as web applications evolve. The Problem As a class of vulnerabilities, IDOR is arguably one of the most difficult to\nsystematically defend against in an enterprise codebase. Its ease of exploitation,\ncombined with its potential for impact, makes it a high-risk vulnerability that we\nwant to minimize as much as possible. In the security industry, there are two main approaches to defending against threats.\nFirst, try to prevent them from happening. If this isn’t possible, make sure\nyou can detect them for fast remediation. The problem with IDOR is that it’s difficult to do either one. Hard to Prevent The main problem with preventing IDOR vulnerabilities is that there’s no system\nthat can be easily implemented to mitigate it. For Cross Site Scripting (XSS) , attacks, you can\nleverage an effective templating system. For SQL Injection attacks , you can use parameterized\nqueries. For IDOR, a common industry recommendation is to leverage a mapping\n(e.g., random string) to make it harder to enumerate values as an attacker. However,\npractically speaking, this is not as easy as it seems. Maintaining a mapping leads to two categories of caveats: Cache Management Let’s assume you have an endpoint that’s currently vulnerable to IDOR attacks: /resource/1 . Now, you want to implement a mapping that masks this ID in the URL\nwith a random string: /resource/abcdef , where abcdef maps to 1. In this contrived example, you may be tempted to deprecate the old endpoint and just\nuse the new one. However, this may break browser caches, user bookmarks, and pages\nindexed by search engines. Imagine taking an unexpected SEO hit when trying to roll\nout your IDOR-prevention system! The alternative is that you can redirect traffic from the old endpoint to the new\none, and let it bake in production for an extended period of time. However, for\nthe time the redirect is in place, you would still be susceptible to IDOR\nvulnerabilities. Furthermore, this mapping is publicly harvestable during this\nperiod, so there’s a chance that someone may store and use it at a later time\nto perform the same attacks – just with less enumerable values. Handling Internal References ID references are littered throughout many different internal systems: various\nlogs, Kafka messages, and database entries to name a few. When you transition\nfrom one reference method to another, how do you make sure that none of these\nsystems break? One good approach is to only use the mapped string for public-facing assets and its\nnumeric counterpart for internal references. However, how do you enforce this to be\ntrue? There will always be more data ingresses, and the problem space might be\nreduced to a whack-a-mole approach of either translating it at the new data\ningress or handling both types of IDs downstream. Another common industry recommendation is to merely perform access control checks\nbefore manipulating resources. While this is easier to do, it’s more suitable for\nspot-fixing, as it’s a painfully manual process to enforce via code audits.\nFurthermore, it requires all developers to know when and where to implement\nthese access control checks. For example, if you put it at the ORM level, you may\nneed to consider legitimate administrative cases for when you need to “bypass”\nthese checks. If you put it at your view layer (assuming MVC layout), you may find\nyourself duplicating code everywhere. How can you ensure all developers are actively thinking about this attack vector, and know how to mitigate it? Slow to Detect Detection strategies for this class of vulnerabilities are also somewhat lackluster.\nWhile manual code audits are effective, they don’t scale and are often expensive.\nOff-the-shelf static code analyzers prove more noisy than they’re worth, and a\ncomplicated taint analysis model would be required due to the various number of\nplaces that access control checks can be done. Traditional API fuzzing may seem like another valid option, but this is not the\ncase. The issue with traditional fuzzing is that it seeks to break an application\nwith the assumption that failures allude to vulnerabilities. However, this is not\nnecessarily true. As a security team, we care less about errors that attackers\nmay or may not receive. Rather, we want to identify when a malicious action succeeds , which will be completely ignored by traditional fuzzing. The Solution In February 2019, Microsoft released a research paper that describes how stateful Swagger fuzzing was able to detect common vulnerabilities\nin REST APIs, including IDOR vulnerabilities. The premise of this strategy is as\nfollows: Have a user session execute a sequence of requests. For the same sequence of requests, have an attacker’s session execute them.\nThis is to ensure that the user and the attacker are able to reach the same state. For the last request in the sequence, have the attacker’s session execute the\nuser’s request. If this is successful, a potential vulnerability is found. Detecting IDOR in a hypothetical sequence of requests Stateful Fuzzing vs. Traditional Fuzzing Generally speaking, the art of using fuzzing requests to find vulnerabilities\nrelies on one core assumption: applications should be able to handle any input\nthrown at them . This means that when the application breaks due to “malformed”\ninput, it’s indicative of a potential exploit and warrants further investigation. The issue with this approach is that as a security team, we care less about whether\nan application breaks for a specific user and more about successful requests in\nsituations where they should have failed. Swagger , as a standardized API specification, is fantastic\nfor programmatically defining the rules of engagement for the fuzzing engine.\nFurthermore, by making it stateful, we can simulate user behavior through\nproper API requests/responses which keep state between each response. This state\ncan then be used to fuzz future request parameters so that a single request\nsequence is able to accurately simulate a user’s session, enabling chaos engineering testing . Finally, user session testing allows for carefully crafted scenarios to assert\nvarious security properties of a given API. In this case, we leveraged this to\ncheck whether users are able to access private resources that don’t belong to them. The simplicity of this concept was profound. It provided a means to scale IDOR\ndetection in an automated fashion through integration with our CI pipeline.\nHowever, while our solution was inspired by Microsoft’s research, we encountered\nseveral issues when adapting it to our ecosystem. Issues Infrastructure Dependencies With a microservice architecture, services often have dependencies on other\nservices. This means that in order to fuzz a given service, we would need to spin\nup its dependent services along with any other nested dependent services. To\naddress this, we leveraged Docker Compose to spin up a sandbox environment so we could perform acceptance testing with the\nservice. Acceptance testing is the practice of treating your service as a blackbox and testing\nwhether the entire system as a whole behaves as expected. Through a microservice\nlens, this differs from integration tests (that mock out external dependencies),\nas acceptance tests spin up sandboxed instances for more realistic end-to-end\ntesting. Since fuzz-lightyear identifies potential IDOR vulnerabilities by analyzing\nsuccessful requests, it complements this framework nicely. Running tests in\nsandboxed instances also prevents leaving after-effects on staging or production\ndatabases so we don’t pollute our data with fuzzed, random input. Acceptance\ntests are typically integrated into CI/CD pipelines but can also be run locally\nby developers. One popular tool we use at Yelp to facilitate running acceptance tests is Docker\nCompose. This allows developers to define service dependencies in one single YAML\nfile and enables them to start/stop them easily. By leveraging this tooling, we\ngain two advantages. First, we empower developers by seamlessly integrating into\ntheir established development/testing workflow. Second, it integrates effortlessly\nwith our existing CI pipeline to provide continuous coverage, and also tests for\nIDOR vulnerabilities in a generated sandboxed environment with all the new changes. Incomplete Resource Lifecycle A fundamental assumption in the original research paper is that the tested\napplication supports all CRUD (Create, Retrieve, Update, Delete) methods. This\nallows for stateful fuzzing, as any resource can be created and manipulated\nwithin the application’s API. However, this is not the case at Yelp. Often, services only provide interfaces\nto retrieve and update resources directly corresponding to that service, but rely\non other services to create such resources. This means that stateful fuzzing\nwould not be effective–since there’s no way to test the retrieval of a resource\n– if we didn’t create it within the request sequence. For example, service A has an endpoint X which takes a business_id as an input,\nbut service A itself doesn’t have the ability to create businesses. By itself,\nthe stateful fuzzing algorithm would never be able to test endpoint X since we\nhave no way of generating a business! We can’t just tack on another service’s API to the request sequence generation\nprocess, since this would expand the search space of the algorithm too much.\nTherefore, our solution is to provide developers the ability to define factory\nfixtures that can be used while fuzzing. This is what a fixture looks like: @fuzz_lightyear.register_factory('userID')\ndef create_biz_user_id():\n  return do_some_magic_to_create_business() This registers create_biz_user_id as a provider for the userID resource, so\nthat if fuzz_lightyear needs a userID resource in a request, it can use the\nfactory to generate it. This fixture system makes it easy for developers to\nconfigure fuzz_lightyear to generate vulnerability-testing request sequences\nby reducing the complexity of creating dependency resources. Expected Direct Object Reference Not all endpoints that allow a direct object reference need to be authenticated.\nThey could simply be providing non-sensitive information about the object\nbeing queried. For example, consider our open-sourced Yelp Love app. This endpoint requires authentication, but the details which it returns are not sensitive in\nthe context of the app. Thus, it doesn’t make any sense to check for IDOR\nvulnerabilities in this case. To address this, we implemented an endpoint whitelisting system to configure\nwhich endpoints should be excluded from a fuzz_lightyear scan. This allows\ndevelopers to configure the testing framework to only alert off high-signal\nendpoints, therefore minimizing test flakiness. Takeaways Automated IDOR detection is a difficult task. Even with the Microsoft-inspired\nstateful fuzzing approach, there were still limitations to applying this concept\nin a microservice ecosystem. To address these issues, we designed a testing\nframework that allows developers to easily configure dynamic tests and integrate\nthem smoothly into our CI pipeline. In doing so, we can achieve continuous,\nautomated IDOR coverage, as well as empower developers to be able to address\nthese issues independently. Curious to check it out? View more details on fuzz-lightyear on our Github page. Contributors I would like to credit the following people (in alphabetical order) for their\nhard work in building this system and in continuing to bolster Yelp’s security. Aaron Loo Joey Lee Victor Zhou Tweet Security Engineering at Yelp Want to transform industry-leading ideas into actionable, scalable solutions to help keep the Yelps secure? Apply to join! View Job Back to blog", "date": "2020-01-16"}, {"website": "Yelp", "title": "Streams and Monk – How Yelp is Approaching Kafka in 2020", "author": ["\n        \n  Flavien Raynaud, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/01/streams-and-monk-how-yelp-approaches-kafka-in-2020.html", "abstract": "We launched our very first Kafka cluster at Yelp\nmore than five years ago. It was not monitored, did not expose any metrics, and\nwe definitely did not have anyone on call for it. One year later, Kafka had\nalready become one of the most important distributed systems running at Yelp,\nand today has become one of the core components of our infrastructure. Kafka has come a long way since the 0.8 version we were running back then, and\nour tooling (some of it open-source ) has\nalso significantly improved, increasing reliability and reducing the amount of\noperational work required to run our clusters. We can now scale clusters with a\nsingle configuration push, load balance and decommission brokers, automatically\ntrigger rolling-restarts to pick up new cluster configuration, and more. Our\nproduction Kafka clusters transport billions of messages every day, with little\nto no operational load on the team. While most of the improvements to our streaming infrastructure have been gradual\nand incremental, we recently went through a major redesign that significantly\nchanged the way we manage our clusters and how we interact with them. Why the Change? One of the core values of Yelp infrastructure engineers is developer velocity:\ndevelopers should be able to use the technology we provide with as little effort\nas possible. Providing Kafka as a service by hosting and maintaining Kafka\nclusters that other teams could directly access was our first approach, and\nallowed us to quickly power many critical use cases. However, one of the first issues we encountered with this approach was that not\neveryone is (or should be) a Kafka expert. Configuring producers, consumers, and\ntopics to get exactly the reliability and delivery semantics that developers\nwant imposes a significant load on any team using Kafka. Additionally, the right\nconfiguration is a moving target, as new parameters are constantly being added\nand new best practices discovered. Upgrading Kafka has also proved to be a challenging endeavour, especially with\nhundreds of services–spread across different client library versions and\ndifferent languages–depending on it. Our deployment also had a few significant obstacles. We had one main cluster per\ngeographical region, where the majority of our topics were stored, and while\nthis approach worked well initially by simplifying discovery, we soon ran into\nits limitations: Clusters were getting big and difficult to maintain. Operations such as rolling\nrestarts and computation of the best partition allocation for cluster balancing\nbecame increasingly slow. A very wide variety of critical and non-critical topics were sharing the same\nresources. Information on the criticality of topics was not always available,\nmaking it hard to understand the impact of having some of those topics degraded\nduring incidents. It eventually became clear that giving direct access to Kafka clusters and\nconfiguration was not going to scale well in terms of data volume, developers,\nand clusters. Some fundamental changes had to be made. Know Your Streams To increase isolation between clients and clusters, we decided to create a level\nof abstraction in the middle that we call a logical stream . Here is a high\nlevel comparison between topics and logical streams: Topic Logical Stream Lives in a specific cluster Cluster-agnostic Low-level topic configuration High-level stream properties Low-level consumer/producer configuration Producers and consumers configured automatically The most important aspect of the new abstraction is that we now know what\nhigh-level properties a stream needs. Given these properties, everything else\nfalls into place. So, what are they? We have identified a few that can be used\nto classify streams and will discuss the two most important ones, policy and priority , in the next section. Policy All streams are not created equal: some favor consistency over availability and\nare willing to trade performance for certainty of delivery. As a hypothetical\nexample, consider a food ordering scenario where Kafka is used to store\ntransactions: the backend service that takes care of handling the order needs to\nbe sure that the order transaction was securely stored for later processing. We\ndefinitely do not want our customers getting “hangry” while waiting for an order\nthat will never arrive! If Kafka is not able to reliably store the message, we\nwould rather show an error message to the user than fail silently. We call this\ncategory of streams “acked.” The other big category contains streams that favour availability over\nconsistency, where performance is more important than being able to react to\nfailure. As a simple example, think of a user checking\nin to a local\nbusiness: if Kafka is not available, we want to be able to store that event\nsomewhere and flush it to Kafka when it recovers. If Kafka cannot receive data,\nthere is not much the producer code can do other than retrying, and we\ndefinitely do not want to block the user until the click has been registered. We\ncall these streams “fire and forget.” Policy Available vs. Consistent Result Callback Enqueue Latency Acked Consistent Yes ~50 ms Fire and Forget Available No ~100 μs Priority Another fundamental property of a stream is its business criticality, in terms of\nboth impact on revenue and user experience. For example, consider two streams:\nuser check-in and debug logs from an experimental service. They both belong to\nthe “fire and forget” policy, but data loss or delay on the former has a much\nbigger impact than on the latter. We define the check-in stream as high\npriority , while the debug log would be low priority . Higher priority streams are automatically allocated to clusters with better SLOs\n( Service-level\nObjectives ). Since\nproviding better SLOs is more expensive (higher provisioning, isolated ZooKeeper\nclusters, etc.), knowing which streams are actually high priority allows us to\nallocate resources more efficiently.  Having priorities also means that if one\ncluster is unavailable, we know what kind of stream will be impacted and can\nreact accordingly. In the new architecture, we group clusters according to the properties of the\nstreams they handle. Here is a (very simplified) graphic representation of our\nclusters: Figure: Representation of clusters, based on properties of the topics they host Having multiple clusters within the same group enables us to scale horizontally\nwithout risking ending up with huge and difficult-to-maintain clusters.\nAdditionally, since clusters are similar to one another, rolling out new changes\nis safer. Accessing Streams So now that we are more familiar with our streams and how they are mapped to our\nclusters: how does a client actually produce to or consume from them? First of\nall, a stream must be declared; this can happen programmatically or via a\nconfiguration file. Our streaming infrastructure is schema-centric ,\nand has been extended to allow stream configuration to be included as part of\na schema declaration. An example configuration could be: schemas:\n  - name: user_checkins\n    schema: <schema definition>\n    policy: fire_forget\n    priority: high\n    owner: userx_team When a client wants to access a logical stream for production, it sends a\nrequest to the Stream Discovery and Allocation service (SDAA - this is actually\ntightly coupled to our schematizer service), which will return the coordinates of a specific topic that transports\nthat stream, together with the configuration that the producer should use. Figure: Interactions between clients and the SDAA service Once the developer declares which policy and priority they want for their\nstream, the system has all the information required to allocate it in the\nappropriate cluster and configure it. Producers and consumers will automatically\npick up this information and configure themselves accordingly. Abstract all the things! While this stream abstraction has allowed us to reduce the cognitive load on\ndevelopers using Kafka for their streaming use cases, it could also add\ncomplexity to the client libraries provided to said developers. Yelp operates\nhundreds of services, most of them relying on streaming or logging in some way,\nso operations such as upgrading Kafka (some with breaking changes) can generate\nenormous migration costs. For many years, Yelp has been using Scribe as its log ingestion\npipeline, with an in-house aggregator called Sekretar (more details are\navailable in the Scaling the Yelp’s logging pipeline with Apache Kafka\npresentation ).\nThe Scribe model has made management of our logging infrastructure easier;\ndevelopers only need to write to a Scribe leaf, and need not worry about Kafka\ninternals.  Upgrading Kafka is also seamless for developers: Sekretar is the\nonly entrypoint to these Kafka clusters and is thereby the only part of the\nsystem that has to incorporate Kafka configuration changes and upgrades. We decided to take this approach one step further, using this model for all\nKafka interactions at Yelp, and abstracting production away from developers and\ninto our new unified stream ingestion pipeline: Monk . From a high-level perspective, Monk is made of two main components: the Monk\nleaf, and the Monk relay. A Monk leaf is a daemon that runs on almost every host at Yelp (mostly PaaSTA\nhosts). This daemon is responsible for handling all streaming traffic generated\nby the host, PaaSTA services, and other system daemons, trying to ensure the\ntraffic eventually makes it to Kafka within a reasonable time. Monk leaves speak the Thrift protocol, and listen\non the same port on every host, removing the burden of discovery for developers.\nFor most use cases, the Thrift sockets are blocking the caller until Monk\nreturns an acknowledgement (or times out). As mentioned above, streams can choose between two policies: ACKED and FIRE_FORGET , which have different latency and delivery guarantees. The\ninformation on stream policies is available within our Stream Discovery and\nAllocation (SDAA) service, which Monk integrates with to determine which “route”\nevents from a stream should take. Let’s dive into how Monk handles each of these use cases. Acked Policy When a Monk leaf requests metadata about a stream to the SDAA service, it gains\ninformation about the policy and Kafka cluster it should produce to. Events are\nenqueued to an in-memory queue and a Kafka producer is responsible for picking\nevents up from the queue and sending them to Kafka. Once Kafka acknowledges\nthese events, the Monk leaf returns its acknowledgement to the caller. Fire and Forget policy The above scenario involves blocking callers until Kafka acknowledges events\nthat need to be produced. FIRE_FORGET traffic does not need to wait for Kafka to\nacknowledge every single event, trading this acknowledgement for an enormous\nreduction of the time spent being blocked by Monk. For some use cases this is\nessential: imagine a 50ms latency hit per logging event generated while serving\na web request! When a Monk leaf receives events for FIRE_FORGET streams, they are immediately\nenqueued to a buffer and an acknowledgement is sent to the caller. The leaf then\nuses a hybrid buffer, where each stream gets up to 1MB of in-memory space to\navoid overloading disks, then falls back to buffering on disk to keep memory\nusage low. In order to increase isolation between streams, each stream gets its\nown buffer, decreasing the impact of noisy neighbours. A pool of egress worker threads then picks up events from the buffer (in\nround-robin fashion, to increase fairness) and sends them onto the next stage. Figure: Monk leaf hybrid buffer This is where Monk relays come into the picture (as replacements for the\nSekretar service). The Monk leaf daemon can run on any host at Yelp across heterogeneous hardware:\nlarge instances, small instances, dedicated EC2\ninstances, spot fleet EC2 instances,\nSSD-backed instances, HDD-backed instances, etc. The team responsible for Monk\nhas limited control over these hosts. For this reason (among others, see below), we intend to move all streaming\ntraffic off of leaf hosts and onto Monk relays as fast as possible. Monk relays act as physical buffers for FIRE_FORGET events that are waiting\nto be flushed into their Kafka cluster. Even though Kafka has proven reliable,\nit is still a system that can fail and have periods of unavailability. In case\nof a Kafka outage, data is buffered inside the Monk relays until availability in\nKafka is recovered. This has proven to be useful for traffic from leaves running\non EC2 Spot instances, which only have two\nminutes of notice before being terminated, losing any data not stored elsewhere. Monk\nrelay clusters can easily be scaled up or down to withstand varying loads. Buffers in a Monk relay use a similar design as buffers in a Monk leaf, only\nwithout the memory component. Every event is appended to a buffer on disk. Not\nrelying on memory for the buffer increases the resilience of Monk relays so that\nin case the process is restarted/killed, little to no data is lost. Each stream\ngets its share of the buffer to increase isolation. Monk relays are also integrated with the SDAA service, which is used to\ndetermine the destination Kafka cluster and producer configurations. Figure: High-level Monk architecture Interactions For an engineer wishing to publish events and have them eventually loaded into\nKafka, the process is as simple as sending Thrift-formatted bytes to the right\nport on localhost .\nOur team provides client libraries for languages with a wide adoption within\nYelp (Python, Java/Scala). producer = MonkProducer ( client_id = \"monk-example\" ) producer . send_messages ( stream = \"user_checkins\" , messages = [ user_checkin ]) results = producer . send_messages ( stream = \"online_orders\" , messages = [ online_order ], timeout_ms = 300 , ) Client libraries also handle retries (keeping track of timeouts) and in-memory\nbuffering if the daemon is temporarily unavailable (e.g., if it is restarting).\nAs you may have noticed, sending messages to Monk requires just three or four\nhigh-level parameters, a good improvement over the dozens available for a\nregular KafkaProducer .\nCluster discovery is handled automatically by Monk and the SDAA service. The vast majority of traffic handled by Monk uses the FIRE_FORGET policy.\nEven if an acknowledgement from Monk is not a guarantee that events will always\nmake it to Kafka, the lower latency guarantee makes it very appealing. Figure: m5.4xlarge EC2 instance - 1 KiB events for 120 seconds (Open in New Tab for more details) We are also working on a service to measure the amount of data lost per stream,\nand exposing it to developers. Stay tuned for a future blog entry! No Pain, No Gain Developing, deploying, and adopting Monk was not easy. Large infrastructure\nchanges require blood, sweat, and tears coordination across teams, extensive\ntesting, and countless hours of debugging. During the initial rollout of some of our high-value streams, events were sent\nto both Scribe and Monk. We compared the events in each pipeline before moving\nforward. The first results showed that Monk was producing fewer events than\nScribe, indicating data loss. We quickly observed a very high number of\nconnection and production timeouts from the Monk thrift sockets. The initial version of the Monk server was synchronous all the way to writing\nevents to disk. When events were published to a FIRE_FORGET stream, the\ncaller was blocked until just before events were appended to our disk buffer in\nthe leaf. This meant that, even if we were not actually blocking callers, Monk\nserver threads were blocked until events were (successfully or not) written to\ndisk. Recall that Monk leaves run on varied hardware: we were at the mercy of\npage cache flush, or just increased I/O from whatever else was running on the\nhardware. During this blocking, Monk server threads were unable to accept\nincoming events from other callers, which were then timing out. Monk leaves now\ndo all of their I/O in separate threads, and clients no longer see timeouts. Once the initial rollout was complete, we noticed Monk leaves using a lot of\ndisk resources on each host, sometimes preventing other processes from using the\ndisk (e.g., due to the limit on the number of IOPS in AWS EBS). This was the main motivation for introducing a 1MB in-memory buffer\nfor each stream in Monk leaves. This has proven to be sufficient in reducing the\nI/O usage of Monk leaves, as most streams flowing through Monk leaves rarely\nhave to rely on the disk buffer. KafkaProducer instances buffer events in memory in order to batch them before sending or\nretrying, thereby increasing throughput. One of the aims of Monk is to provide\n(as much as possible) isolation between streams. Using more Kafka clusters is\none way of achieving this, but does not solve the issue of noisy neighbours\nwithin the same (policy, priority) guarantees. Consider a spike of events\ncoming from a single stream (e.g., a service entering a crash loop and logging\nexceptions): if this happens, Kafka may not be able to immediately handle the\ntraffic increase. Timeouts start occurring and events start accumulating in the\nbuffer of the specific KafkaProducer, preventing events from other streams with\nthe same guarantees from being appended to the buffer. So much for isolation! To circumvent this scenario, we use a simple logic before sending events to\nKafka: limit the number of bytes “in-flight” per stream. Whenever an event is\npicked up from the buffer, the “in-flight” bytes counter for the corresponding\nstream is increased. When the event is acknowledged by Kafka, the counter is decreased. If a stream has too many “in-flight” bytes\n(this can be configured on a per-stream basis, and reloaded at runtime), events\nwill not be picked up from the buffer until Kafka acknowledges the in-flight\nevents. This sliding window logic is simple, easy to comprehend, and has been sufficient in avoiding\nKafka buffer-related issues for now. Producing Directly to Kafka ACKED production in Monk leaves is configured for low-latency ( linger.ms is\nusually set to 1) but relatively low throughput, while FIRE_FORGET production\nis configured for high throughput but may result in data loss without a\nguaranteed ordering. While MonkProducer is the solution to most streaming use\ncases we have, specific applications still require something extra. The output\nof ETL jobs is a good example of such an application; it requires high\nthroughput and rarely tolerates out-of-order events. Instead of banning direct\nproduction to Kafka, we integrated the SDAA service with a few supported client\nlibraries, such as Flink and Spark KafkaProducers, so that they follow the\nguidelines of our Data\nPipeline . Consumers Running smaller Kafka clusters reduced the burden on infrastructure engineers,\nbut this change would not be complete without a way for consumers to read from\nmultiple clusters. Most “regular” Kafka consumers are able to consume from a\nsingle cluster, but what about the other multi-cluster use cases (e.g., joining\ntopics of different policies/priorities or aggregation across regions)? Yelp has widely adopted Apache Flink for stream\nprocessing use cases, as it can easily consume events from multiple Kafka\nclusters. Instead of writing an equivalent of Monk for Kafka consumers, we chose\nto treat Flink as our abstraction for consumers. Since our Flink consumers are\nall integrated with the SDAA service this reduces the burden of configuration\nand cluster discovery on developers. A more detailed explanation of the Flink infrastructure can be found in our\npresentation Powering Yelp’s Data Pipeline Infrastructure with Apache\nFlink . Looking Further Because a proper blog post cannot end without a utopic future vision, let’s have\nat it! In the last few years, Kafka at Yelp has come a long way. From a single,\nunmonitored, three-broker cluster in production, to numerous clusters and\nproduction brokers, it has become the backbone of our data infrastructure. This\nwide adoption would not have been possible without scaling the infrastructure\nsitting on top of it. The introduction of Monk and the SDAA service has served\nour two principal goals: increasing developer velocity and making infrastructure\nmore reliable. Our Data\nPipeline is now integrated with the SDAA service with a good part of it powered by Monk.\nDevelopers do not need to worry about Kafka specifics, and infrastructure\nengineers feel more empowered to operate on production clusters knowing that\nmaintenance operations will be shorter and safer. Shorter and safer operations are a big benefit, but automation is arguably a\nbetter one. Now that cluster operations are invisible to developers, why not\nhave non-human operators perform them? The future includes automating these operations by making Kafka\njust another service run by PaaSTA (our\nplatform as a service) on Kubernetes , reducing the\nload on infrastructure engineers. Acknowledgements Brian Sang, Federico Giraud, Jonathan Zernik, Manpreet Singh, Piotr Chabierski\n& the Data Streams group as contributors; The fancy Latency Heat\nMaps from Brendan Gregg. Tweet Data Streams Platform Engineering at Yelp Want to build next-generation streaming data infrastructure? Apply here! View Job Back to blog", "date": "2020-01-22"}, {"website": "Yelp", "title": "Breaking Down Technical Interviews", "author": ["\n        \n  Nina Yang, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/09/breaking-down-technical-interview.html", "abstract": "Finding that first job in the tech industry can be a daunting task. You might not get a response to your application, or maybe you’ll move forward with the interview, but it doesn’t pan out in the end. You might wonder, “How are other people successful at getting offers? What do others do differently? What’s the secret to getting through this arduous process?” The answer is pretty simple: lots of practice. While there’s never a guarantee of getting an offer, following these recommendations can increase your chances of successfully going through the interview process and potentially landing your dream job! Know the Basic Flow When you start interviewing, expect to follow this basic structure. Below is Yelp’s interview process, but many companies have something similar: Online coding challenge Technical video interview Onsite interview Review CS Fundamentals Before you begin the interview process, you’ll need to have a good grasp of fundamental CS knowledge. If it’s been a while, review data structures and algorithms. I highly recommend Introduction to Algorithms as a good resource. If textbooks aren’t for you, you can find an abundant amount of public resources online. Keep in mind that when a company decides to not proceed at any stage of the interview process, they will likely have you wait 6 months or more before you can reapply. It’s important to be prepared before you apply so you can get the most out of your experience. Coding Practice For coding exercises, most companies will allow you to choose your programming language. Even if your preferred language is different from the company’s primary coding language, use it! This is your opportunity to demonstrate your coding knowledge and show your best work to the interviewer. Pro tip! The interview process is not the time to attempt a new language. Stick to your strengths. If you don’t have a strong preference, consider the trade-offs of different programming languages. For example, one of the biggest constraints for any interview is time, so using a scripted language like Python over a compiled language like Java could be an advantage. Statically typed languages like Java and C++ are considered very verbose, which can take some time to transfer from your head onto a screen or whiteboard. In addition to being less verbose, scripting languages like Python have more flexibility with data structures such as slice notation. Now that you have the fundamentals down, it’s time to practice! Out of all the options available online, many engineers favor LeetCode . Leetcode categorizes problems by difficulty, which is helpful when you’re preparing for an interview. The first technical interview typically covers easy to medium level questions, and onsite interviews medium to hard, so it’s important to practice all levels. From personal experience and online suggestions, you should do 100-200 Leetcode questions to be prepared. Practicing coding will help you at every stage in the interview process. However, you’ll want to make sure you also prepare for the online coding challenge, which is typically the first step. Take advantage of any practice tests made available to you on the website that the company uses (for Yelp, it’s currently HackerRank ). Practice will allow you to familiarize yourself with the UI and the environment, so those won’t be barriers when taking the timed version. When you’re ready to take the test, be sure you’re are in a quiet place without interruptions and that you have a reliable internet connection. You don’t want technical difficulties affecting your performance. Prepare for the First Interview Once you’re done with independent study, it’s time to mimic a real life interview! There are many online resources, such as Pramp , to help you get comfortable with interview situations. Doing mock interviews will let you practice coding while explaining your thought process. It’s important to know that during an interview, you are evaluated on your technical thought process, and not just your code. This is often what distinguishes an outstanding candidate from an average candidate. There is no substitute for practicing saying your thoughts out loud— you don’t want your actual interview to be the first time you try. During your first interview, there are at least three times when you should speak up: At the beginning, always reiterate the problem back to the interviewer to make sure your understanding is aligned. This is your chance to ask follow-up and/or clarifying questions. It is important that you ask clarifying questions instead of making assumptions, as some interviewers will purposely omit criteria to test that candidate will ask the proper questions. When it comes time for a solution, it’s best to offer two or three options and discuss the pros/cons of each with your interviewer. Once you’ve reached a consensus, be sure to either explain the components you plan to code, or make comments, before moving on to actually writing code. Once you’re done writing code, walk through each line to look for errors. If everything looks good, you can verbally unit test your code. Depending on the difficulty of the problem, your interviewers may ask follow up questions or provide additional parts to the problem. Remember that in an interview, timing matters! Don’t assume that the first question that the interviewer asks will be the only question and be sure to keep a steady pace throughout the interview. I hope all of this was helpful as you embark on the interview process! See below for key takeaways and additional resources. Key Takeaways CS Fundamentals Whether you have old class notes, pick up a book to review, or study online, know your fundamentals before jumping into the process. Coding Practice Use your preferred coding language in practice and in interviews. Practice as many coding challenges as you can, ranging in level of difficulty. It helps to browse the website of the company’s online coding challenge to get familiar with the set up. Technical Interview Practice sharing your technical thought process out loud. Reiterate the problem and state assumptions. Tell the interviewer why you’re choosing the solution that you chose. At the end, walk through the code for errors and explain what you see. Always keep an eye on time and keep the conversation moving. Tweet Become an Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2019-09-09"}, {"website": "Yelp", "title": "Modernizing Ads Targeting Machine Learning Pipeline", "author": ["\n        \n  Rahul Dubey, Machine Learning Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2020/01/modernizing-ads-targeting-machine-learning-pipeline.html", "abstract": "Yelp’s mission is to connect users with great local businesses. As part of that mission, we provide local businesses with an ads product to help them better reach out to users. This product strives to showcase the most relevant ads to the user without taking away from their overall search experience on Yelp. In this blog post, we’ll walk through the architecture of how this is made possible by using one of the largest machine learning systems at Yelp: Ads Targeting System . The Ads Targeting System is a machine learning (ML) system designed to serve only the most relevant ads to users based on their intentions and context on Yelp. There are two primary types of ML models in the ads targeting domain: Click Through Rate (CTR) prediction, and Objective Targeting (OT). Both help determine the likelihood of downstream actions, such as calling a business after clicking on an ad. In this post, we’ll primarily focus on architecting ML systems at scale rather than on algorithmic details or feature engineering. For more info on the algorithmic side of our CTR prediction model, check out one of our previous posts where we discuss optimizations made to the XGBoost prediction library. Historical Context on Ads Targeting System Below is a simplified version of the Ads Targeting and Delivery System: The Ad Delivery service is a low-latency online service written in Java that processes incoming ad requests. It generates features for the incoming request using the Ad Feature Generation library, loads the model from the Ad Model Store , generates CTR prediction, and then ranks ads accordingly. The Ad Targeting service is a batch processing service written with MR Job : Python Map-Reduce library open-sourced by Yelp. This is the service that we’ll discuss and redesign in this blogpost. Its main features include processing logs using the same Ad Feature Generation library, training ML models, and storing them in the Ad Model Store . MRJob also comes with a feature that allows you to call Java code from Python to carry out map reduce operations (as can be seen here ). Using the same Feature Generation library ensures that all feature computation, both on and offline, remains consistent. This blog post on CTR prediction illustrates how the Ads Targeting Machine Learning Pipeline used to look: We processed Ad Event JSON logs, downsampled them in Spark, and extracted features from the set of logs with Hadoop MR jobs. We then proceeded to Model training with XGBoost and model evaluation with Hadoop MR jobs using AWS EMR as the compute infrastructure. This pipeline served us well and helped us iterate in an ad hoc fashion to create newer and better ad targeting models. That being said, we did face several issues as the system matured due the following: As all stages of the pipeline were closely coupled, failure in any of the intermediate steps required restarting the pipeline This close coupling also meant that changing the feature generation logic or sampling strategy required running the entire pipeline The pipeline was closely linked with certain EMR instance types and AMI images that restricted either our upgrade to newer versions of Java or trying newer EMR instances (e.g., upgrades in other Java dependencies and online Java services wouldn’t work with the current Ad targeting service, making it impossible to retrain a model or add a new feature) As our system matured and we started adding more models to our ads targeting system, the cost of training grew Redesigning the Ads Targeting Pipeline with Spark To solve the above issues, we decided to re-architect the Ad Targeting Service and its interaction with the other main components of the Ad Targeting and Delivery Systems. Keeping an eye on the big picture and setting goals is very important when re-designing a system as large as this. For us, that meant focusing on: Making it easy to retrain existing models Making feature generation cheaper, easier, and faster Leveraging Yelp’s internal Spark tooling and infrastructure (rather than relying on EMR) Improving monitoring and alerting, and providing easy promotion of models in production We decided to use Spark as the underlying engine for this ML system as it allowed us to leverage our own in-house Spark on Mesos infrastructure. This infrastructure provides us with a quick and cheap way to spin up clusters and get started with writing big data workflows. Moreover, moving away from Hadoop map-reduce jobs on EMR increased speed and cut costs. This, coupled with the availability of PySpark (the official Python API for Spark), made the decision even easier, since most of our code and infrastructure is built with Python. Armed with better infrastructure and tooling around Spark and its natural fit to our big data ML use-case, we decided to rewrite the Ads Targeting Service in PySpark. The new service now contains the same stages as before: Sampling -> Feature generation -> Training -> Evaluation , just with all the stages computed with PySpark. Overview of Modernized Architecture Based on Spark Above is the current machine learning pipeline powered by the updated Ad Targeting Service. Three significant changes were made here: Use Spark as the Compute Infrastructure Spark batches were more efficient both in terms of time and cost. Moving to Spark allowed us to leverage the existing infrastructure at Yelp that enabled us to write ETL jobs and carry out distributed machine learning with XGBoost. This was a very cost-effective move since now we only pay for spot EC2 compute resources (and not for the EMR stack on top of it!). Decouple the ML Pipeline into Stages The batches we created process logs, perform sampling, and generate features that are scheduled to run daily and checkpoint results on S3. Decoupling these batches gave us flexibility: we now have different feature generation strategies on top of the same sampling output, whereas in the older architecture each new feature generation strategy required re-computing sampling output. It also made the system relatively robust; since failure in later stages (say training/evaluation) didn’t disrupt the whole pipeline, engineering and operating costs were reduced. Automated Monitoring and Alerting We leveraged Yelp’s modernized Data Landscape ( 1 , 2 , 3 ) and built our monitoring capabilities on top of this infrastructure.  Instead of manually running Jupyter notebooks to monitor metrics, we computed these in batches, loaded them to our AWS Redshift data warehouse, and created Splunk dashboards on top of it. This made it really easy for PMs and engineers to make model promotion/deployment decisions. Feature Generation with Java and PySpark The online Ad Delivery Java service and the offline Ad Targeting Python service share the same Ad Feature Generation Java library. From there the question then arises: how are we leveraging PySpark to generate features? Hint: What language is Spark written in? Let’s unpack this! DataFlow in PySpark: Spark is written in Scala (a JVM language), and PySpark is a Python wrapper on top of it. PySpark relies on Py4J to execute Python code that can call on objects that reside in the JVM. To do that, Py4J uses a gateway between the JVM and the Python interpreter, and PySpark sets it up for you with SparkContext. This SparkContext has access to the JVM and all packages and classes known to the JVM. You can see where this is heading… To carry out distributed feature generation via PySpark, all we had to do was add our feature generation JAR to the Spark JVM and use SparkContext to refer to these classes. Since Yelp executes Spark within Docker, we added the JARs to our service’s Docker images, then loaded the image in Spark drivers and executors.  We then had a feature generation Java library accessible via PySpark! The diagram above, taken from the PySpark Wiki , illustrates the above design. As you can see, Py4J essentially carries out all data communication with the JVM. Implementation: You can imagine a simple example of a Java class with a method that prints “Hello World!” that is then called from PySpark to get the printed string: “Hello World!”. This implementation was illustrated in this blog (in Scala, but the same principle applies for Java), so we won’t get into it here. Instead, we’ll demonstrate how to apply this principle to our use-case. Say we have some JSON logs containing information about ads that can be read in PySpark as PythonRDD. Now, we want to extract/transform features from these logs using our Java library. One way of doing this is via Java UDF (as is illustrated here ). However, there’s a limitation to this approach: it requires Java classes to have a zero-argument constructor. This can be seen in the official Spark UDF Registration code. Since for our use case we wanted to have the ability to parameterize our classes, this approach didn’t work for us. Hence, we went with the following: first, we created a Java class that extends flatMapFunction interface. This allowed us to generate any number of output rows per row of the input RDD by passing an object of this class to Spark’s mapPartitions function. The Java class also gets a list of Java mappers that we want to apply to the input RDD to generate the output fields. One of these Java mappers calls into the feature generation library to extract the transformed features. The  library itself essentially consists of simple Java classes that can extract features by applying simple transforms or business logic on raw JSON logs. Now that we have all the Java classes ready, we can do the following on the Python side: from pyspark.mllib.common import _java2py from pyspark.mllib.common import _py2java # Step 1: First convert the PythonRDD object into a java RDD object. java_rdd_object = _py2java ( python_rdd . ctx , python_rdd ) # Step 2: Get the Java class that implements flatMapFunction interface, initialize it,\n# and pass some mappers to it to apply on the Java RDD java_flat_map_function_object = flat_map_function_package . ClassWithFlatMapFunctionInterface ( initParamA , initParamB , [ MapperA ( arg_a ), MapperB ( arg_b ), MapperForFeatures (), MapperForLabels () ] ) # NOTE: As one can see above, we can parameterize our mappers as opposed to JavaUDF functions # Step 2: Call mapPartitions on that java object (effectively calling java code)\n# and get the output as a java RDD instance. mapped_java_rdd_object = java_rdd_object . mapPartitions ( java_flat_map_function_object ) # The above mapped_java_rdd_object now consists of results of all the 4 mappers above applied # Step 3: Convert the java RDD object back into a python RDD object. mapped_python_rdd = _java2py ( python_rdd . ctx , mapped_java_rdd_object ) Voila! Now we have a PythonRDD of features that was generated via the Java feature generation code. Model Training with Distributed XGBoost on Spark We use MLFlow-tracking to track and log our model training runs. This provides us with a lot of visibility into our model training metrics, an easy way of logging and visualizing hyperparameters and even sharing the model-training reports. Another cool feature of MLFlow-tracking is the ability to query the model-training runs and retrieve the best models based on metrics. We leverage this feature to automate our evaluation and monitoring pipelines. To train our ads targeting models, we heavily rely on XGBoost . However, distributed training with XGBoost on Spark took some work to accomplish. Since the official library (version <= 0.9) doesn’t provide a Python/PySpark interface, we wrote our own wrapper on top of XGBoost4J-Spark . We also implemented a SparseVectorAssembler instead of using the VectorAssembler provided by Spark, since the default implementation doesn’t integrate well with XGBoost on Spark ( issues dealing with missing values). Another limitation of XGBoost on Spark is that it’s not as fault-tolerant as native Spark algorithms. This becomes an issue when trying to use AWS Spot instances for model training, since at times when the spot instances became unavailable, the training job dies. Thus, we created a separate pool of on-demand resources to carry out large-scale distributed training with XGBoost on Spark. Automated Retraining, Monitoring, and Alerting We use Yelp’s Tron scheduler to schedule our batch processing jobs. The entire new pipeline is scheduled via Tron, where log-processing and feature generation batches run daily and model-training runs every few days. Through running A/B experiments, we’ve observed that pure retraining of models with newer data leads to ~1% improvement in our primary metric, which then compounds over time. While scheduling model retraining is simple, deployment, monitoring and alerting can be a more difficult process. To instill confidence among developers and PMs to go in production with newly trained models, we developed a solid monitoring infrastructure that does the following: Daily model evaluation that replays traffic for all models in production and models yet to be deployed in production(this helps us capture model drift and decays) Live Splunk dashboards of business, online model, and offline model evaluation metrics Scoring verification systems that verify online and offline scoring matches and ensures that features don’t drift between online and offline modes Having a good monitoring infrastructure improves developer velocity in deploying newly trained models. It’s analogous to having a good CI/CD infrastructure for code deployment. With this newly designed service and regular retraining-deployment cycle, we’ve seen a vast improvement in our model metrics that has further translated to improving business metrics such as click-through-rate, sell-through-rate, and lower cost per clicks for our advertisers. This means that we’ve not only improved serving more relevant and useful ads to our users, but have also reduced the cost for our advertisers to serve ads, making Yelp a more cost-effective platform for their business. Conclusion Designing large ML systems is hard due to additional complexities introduced by data and models, but it’s especially important when it’s a big part of your product Sometimes ML systems need to evolve (from Hadoop MR to Spark); ML engineers shouldn’t shy away from this just because it’s infrastructure and not modeling Decouple system components and checkpoint data often so that each component can be independently worked and improved upon Create infrastructure such that training+evaluating+monitoring models are easy and automated. Make this infrastructure instill confidence in developers to deploy newly trained models Retraining models with newer data can provide good gains with almost zero effort, so take advantage of it! Acknowledgements A huge thanks to engineers from the Applied ML, Core ML, and Ads Platform teams, without whom such a broad cross-team collaborative effort wouldn’t have been possible. Credit to the contributors: Chris Farrell, Jason Sleight, Aditya Mukherjee, Abhy Vytheeswaran, Vincent Kubala. Tweet Become a Machine Learning Engineer at Yelp Want to build state of the art machine learning systems at Yelp? Apply to become a Machine Learning Engineer today. View Job Back to blog", "date": "2020-01-30"}, {"website": "Yelp", "title": "Hosting Our First Awesome Women in Engineering Summit in SF", "author": ["\n        \n  Dorothy Jung, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/10/first-awe-summit-sf.html", "abstract": "Last month, we held our first Awesome Women in Engineering (AWE) Summit at our headquarters in San Francisco. AWE’s mission is to build a strong community for women and allies in our engineering and product departments by facilitating professional career-building activities, leadership, and mentorship opportunities. As a resource group, we provide support and organize activities targeted towards professional growth for women, helping them to maximize their potential at Yelp and beyond. The summit was an internal, half-day event for women and allies in engineering and product at Yelp. We had previously hosted a summit for our EU offices , but this was our first time organizing the event stateside. During the event, we shared our experiences with one another and learned about the amazing work done at Yelp by women through a packed agenda of technical and career-based talks, workshops, and a round table discussion. Overview of the Day and Session Highlights We kicked off the summit with lunch and an introduction from Rachel Z., a group engineering manager and leading member of AWE at Yelp. We followed with a series of sessions in parallel tracks, which ran the gamut from lightning talks to hour-long workshops. Some sessions were highly technical, including a lightning talk on PaaSTA , our open-source, distributed platform-as-a-service, by Qui N., and a workshop on machine learning and data mining using the Yelp dataset by Xun T. Other sessions were focused on career growth, diversity, and inclusion. For example, Maria C., one of Yelp’s group technical leads, detailed her career path as an individual contributor, and Jenni S. led a workshop targeted toward allies that focused on real-world scenarios in which an ally could take action to promote a more inclusive workplace environment. The round table discussion, facilitated by Annie W., presented opportunities for women to have open, honest discussions about their experiences and any challenges they were facing. We closed out the day with refreshments and a few parting words from our SVP of Engineering, Sam E. In Our Own Words “I was proud to see my coworkers–women and men–coming together to discuss and learn about these important topics. Only with everyone on board can we make a change towards a more equal industry.” “My favorite part was the round table discussion! I felt at ease to share the difficulties I’d faced. It was very enriching to share career and personal development tips with others.” “I feel inspired to use the takeaways from the [technical leadership talk] as a springboard for leading my own projects. It was clear how the takeaways emerged from practical situations.” “As an ally, I’m glad to be able to participate in the event, it was great! My favorite part was the Ally Skills Workshop discussions and hearing different viewpoints.” Conclusion Holding this event allowed women across different technical departments at Yelp to come together, feel a stronger sense of belonging, and walk away feeling empowered and inspired. We plan to hold this event again in the future, and are proud of the progress that has grown AWE from a small social group when it was founded in 2013, to a thriving organization with hundreds of members today. Organizing this event with the brilliant, motivated women in engineering and product has been a highlight of my time here at Yelp. This event further demonstrated Yelp’s ongoing commitment to diversity and inclusion, as well as the importance for women to have opportunities to connect with others in the workplace to learn and grow. For more information about how Yelp supports women in tech, check out our website ! Tweet Interested in joining the awesome women in engineering and product at Yelp? We're hiring! Check out our Careers page for more open positions. View Job Back to blog", "date": "2019-10-04"}, {"website": "Yelp", "title": "Discovering Popular Dishes with Deep Learning", "author": ["\n        \n  Anna F. and Parthasarathy Gopavarapu, Machine Learning Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/10/discovering-popular-dishes-with-deep-learning.html", "abstract": "Introduction Yelp is home to nearly 200 million user-submitted reviews and even more photos. This data is rich with information about businesses and user opinions. Through the application of cutting-edge machine learning techniques, we’re able to extract and share insights from this data. In particular, the Popular Dishes feature leverages Yelp’s deep data to take the guesswork out of what to order. For more details on the product itself, check out our product launch blog post . The Popular Dishes feature highlights the most talked about and photographed dishes at a restaurant, gathering user opinions and images in one convenient place. In this post we’ll explain how we used machine learning to make this possible. Problem Definition Given everything we know about a business, the data for the Popular Dishes feature boils down to the following output: A ranked list of popular menu items Reviews mentioning each item Photos associated with each item Sometimes Yelp already knows a business’s menu. It may have been uploaded by the business itself or provided by partners. In such cases, the problem is simple: match items from the menu to reviews and photo captions and rank by the number of matches. This matching can be as simple as Python’s string.find method, or can incorporate fuzzy matching and more complex NLP techniques. The basic pipeline However, there are many businesses without menus. In order to provide a better and more consistent experience to users, we wanted to be able to display Popular Dishes even at businesses whose menus we didn’t have. So, we used machine learning to predict them. The Inferred Menu Model We wanted to use the vast review content from Yelp’s businesses to create menus we didn’t have. Since going straight from a list of reviews to a menu is hard, we split this into two steps: information extraction and aggregation. First, we extracted all mentions of potential menu items from a business’s reviews, and second, we aggregated these mentions to obtain an Inferred Menu for the business. Machine Learning Problem Recognizing menu items in unstructured text is very similar to the Named Entity Recognition (NER) process. NER is usually solved by classifying each token in the text by whether or not it belongs to a named entity. In our case, we classified each token in a review by whether or not it belonged to a menu item. For this, we used the BIESO labelling scheme (introduced as BILOU in Ratinov & Roth 2009 ), where: All irrelevant (i.e., not part of a menu-item) tokens received the label ‘O’ (Outside) All single-word menu items received the label ‘S’ (Single) All multi-word menu items started with the label ‘B’ (Begin) and ended with the label ‘E’ (End), and any tokens in between got the label ‘I’ (Inside) Example formatted review (with legend) Once we classified each token as one of these five classes, we could extract menu items by gathering token spans with the following labels: S (Single token items) BI*E (Regular expression for multi token items) Data To train such a sequence classification model, we needed a dataset of reviews where each token in each review was tagged with one of the five classes (BIESO). Manually labelling reviews for this task would’ve been quite costly since training text models requires a significant amount of data. To solve this problem, we took advantage of the menu data that Yelp already has. Since an ideal Inferred Menu for a business would be similar to its Yelp menu (when available), we inverted the problem to create our “Gold Data.” We performed a variant of fuzzy matching to tag the reviews of businesses where we already had a menu (provided by partners/business owners) to create our Gold Dataset. Menu Tagged Reviews The Chicken Burrito The Vegetarian Burrito Nachos (Their O) (Chicken B) (Burritos E) (were O) (good O) (. O) (We O) (had O) (the B) (vegetarian I) (burrito E) (. O) (The O) (nachos S) (were O) (salty O) (. O) Matching Heuristics While generating our training data, we applied some heuristics to account for the variety of ways reviewers talk about menu items. For example, a business that offers a “tofu and vegetable momo” on its menu may have reviews that mention: Misspellings like “tofu and vegtable momo” Different inflections with the same lemmas , like “tofu and vegetables momo” Partial mentions like “tofu vegetable momo” or “tofu momo” Synonymous mentions like “tofu veggie momo” The fuzzy matcher attempted to accommodate this diversity of speech. Issues with the Gold Data While the large number of reviews we have can provide a large enough dataset to train our model, the data does come with some caveats, even after using matching heuristics: Menus can be incomplete : This creates false negatives in the data when reviewers talk about an item that’s served at the restaurant but isn’t present on the menu. Users can be creative : As discussed above, reviewers can refer to the same item in different ways. This creates false negatives when the menu only has one representation of the item and users mention it in other ways. The leniency we introduce in matching heuristics can introduce false positives. For example, a mention of “New York” (the city) in a review can be tagged as a menu item since it’s close to “New York Cheesecake.” These issues with the Gold Data can cause a few problems: Issues with training data can result in a sub-optimal model Issues with validation data can result in unreliable metrics As a result, some post-processing was required; this is discussed later on in the Evaluation and Improvements section. Model We used a Hierarchical GRU model described in Yang et al . Model Components Word Representation We combined pre-trained word (trained on Yelp review data using gensim ) and learned character embeddings to represent each word in the model. The embeddings of each character in a word are passed through a bidirectional GRU to provide a learned character-based representation of the word. This representation is concatenated with pre-trained word embeddings to provide a single representation of each word in the review. Multi Layer RNN We used GRUs as our base RNN unit. The word representation from above (pre-trained word embeddings + learned character embeddings) is passed through two layers of bidirectional GRUs. Each bidirectional GRU has a forward and a backward GRU, which parses the tokens in the review in the forward and reverse order, respectively. The output of the forward and backward GRUs are concatenated to represent the output of the bidirectional GRU layer. CRF Layer One way to predict the class labels for each word in the review would be to use a dense (or linear classification) layer with softmax activation on top of the bidirectional GRU layers to predict one of the five classes. This approach doesn’t account for dependencies between the labels assigned to a sequence of tokens. By the definition of BIESO tagging, there are some invalid sequences (e.g., OBSO, OBOEO, OIO, ...). If we use a dense layer that doesn’t have the context of class probabilities for other tokens in the review, we may end up predicting invalid sequences. To avoid this, we used a Conditional Random Field (CRF) which can model dependencies between the labels assigned to each token in the review. The output of this CRF layer is the final classification assigned to each token in the review. Aggregation The final step was to transform these classifications into an actual menu. We extracted all matching spans (BI*E and S) and counted their occurences. Based on language heuristics, some mentions were collapsed into each other. For example, mentions of “mac and cheese” were combined with those of “macaroni & cheese.” The final result was an Inferred Menu: a list of items that we’re confident appear on the menu. Evaluation and Improvements Motivation Now that we had an Inferred Menu, we were able to plug it back into the same matching pipeline that we used with regular menus. The pipeline for businesses with and without menus Since ML is fuzzy and never perfect, we made sure to evaluate the performance of the model before shipping. We settled on a few evaluation criteria. Our focus was on achieving a high precision , as mistakes would look bad and hurt user trust. There are several questions we wanted to answer about our “Popular Dishes” before showing them to users: Are they actually dishes? Are they popular? Are we finding all their matches in photos and reviews? Are we matching anything we shouldn’t be? For a textbook machine learning task, you assemble a set of labelled data for training and testing. The output of your model is compared with this golden data, and you can calculate metrics like precision, recall, and accuracy. Under this paradigm, we’d manually find the Popular Dishes and their mentions at a restaurant, then compare this gold data with the output of our models. However, collecting this data would’ve been prohibitively expensive, and potentially impossible given the domain knowledge required. Hiring a human to read a business’s reviews, come up with a menu, and find these matches was impractical. Approach Thankfully, our focus was on precision, which was much easier to calculate. Given a “Popular Dish” or a mention, it’s fairly simple to decide whether it’s good or not. As a result, we turned to human-in-the-loop machine learning to both evaluate and improve our model. Our process is summarized in this flowchart: First, we generated Popular Dishes using the model. Then, we asked taskers from an online tasking service to answer the following questions about each item: Next, we calculated precision metrics from the results of this task. If they were good enough, we declared the task done. If not, we incorporated data from these gold labels into improving the model. For example, examining false positives revealed problems with our matcher. We were also able to extract blacklists of common non-main-dish items by combining tasker judgements. The blacklists were incorporated into training data generation for the Inferred Menu model and used to post-process its predictions. Through repeated iterations of evaluation and improvement, we significantly increased the model’s precision. The heatmap below displays our average precision for sample businesses in different (business review count, business category) strata before and after iteration. During the entire process, we kept an eye on coverage as a proxy for recall. Deployment The Popular Dishes backend is currently deployed as a handful of PySpark batches. Every day, all the data we have about our businesses is gathered and run through an NLP pipeline powered by the open source spaCy package. In this way, new mentions of dishes quickly become available for users to browse. Inferred Menus are regenerated periodically to pick up new dishes. Tweet Become a Machine Learning Engineer at Yelp Want to build state of the art machine learning systems at Yelp? Apply to become a Machine Learning Engineer today. View Job Back to blog", "date": "2019-10-08"}, {"website": "Yelp", "title": "Redesigning Yelp for Apple Watch with SwiftUI", "author": ["\n        \n  Thibaud Robelain, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/10/redesigning-yelp-for-apple-watch-with-swiftui.html", "abstract": "At this year’s WWDC, Apple unveiled SwiftUI , a framework that helps developers build declarative user interfaces. At Yelp, we were immediately excited about it and were looking for a way to start adopting it. We decided that our Apple Watch application was the perfect candidate for modernization using SwiftUI and were excited to explore a redesign with this new framework. At Yelp, one of the things we pride ourselves on is the quality of our content. Yelp users have posted hundreds of millions of reviews and photos. As we set out to re-imagine the user interface for our Apple Watch app, we knew that our gorgeous photos should be the star. Here is a side-by-side comparison of the old interface and the new one. Star ratings as of October 16, 2019 As you can see, we’ve adopted an interface similar to the Audiobooks and Music apps which put a very strong emphasis on the thumbnail image. Users of the Apple Watch Series 5 will also see a compass that will allow them to see the direction and distance to each business in their search results. We hope this will help users in their search for great local businesses near them. Star ratings as of October 16, 2019 Building a scrollable card stack with SwiftUI In contrast with WatchKit, SwiftUI gives us much more freedom when building our user interface. It feels much more like developing for the iPhone, with the added constraint of designing for a small screen. One thing that’s notable about the design of the search listings is the simplicity of it in code. This scrollable card stack took less than 120 lines of code, animations included! The magic of it resides in the custom view modifiers you can create to apply to your SwiftUI views. Let’s dive into a simplified example. Here is a slightly simplified modifier that shifts the cards vertically and doesn’t take care of any scaling down or rotation. Given a cardOffset that represents the difference between the current index and the card’s index, we return a custom view modifier that offsets the view’s origin on the y-axis, and modifies its opacity if it goes to the background. Our own implementation also takes care of adding a scale effect for the depth impression, and a zRotation effect, to give the animation more flavor when the cards are scrolled off-screen. Now that we have view modifiers, let’s create the scrollable stack. We create a ZStack that will fill out the remaining screen space left out by the Spacer. We then compute the cardOffset needed for returning the correct view modifier, and apply the modifiers on their respective cards. SwiftUI is able to smoothly interpolate animation parameters for the offset and opacity whenever the modifier changes for a given card. This means the animation logic is all handled for us if the current index is changed within an animation block. Since this code hooks into the digitalCrownRotation modifier and passes the animated binding that represents the current index, the animation will be automatically performed when the crown is rotated. How convenient! This redesign made us eager to see where Apple is going to take the framework, and what we’re going to be able to build with it in the upcoming years. We’re thrilled to launch the new Yelp for Apple Watch application today and hope you will love it as much as we do! Tweet Become an iOS Software Engineer at Yelp Want to build more great looking products? Come join us! View Job Back to blog", "date": "2019-10-21"}, {"website": "Yelp", "title": "Open sourcing spark-redshift-community", "author": ["\n        \n  Luca Giovagnoli, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/10/open-sourcing-spark-redshift-community.html", "abstract": "At Yelp, we are heavy users of both Spark and Redshift. We’re excited to announce spark-redshift-community , a fork from databricks ’ original spark-redshift project. spark-redshift is a Scala package which uses Amazon S3 to efficiently read and write data from AWS Redshift into Spark DataFrames. After the open source project effort was abandoned in 2017, the community has struggled to keep up with updating dependencies and fixing bugs. The situation came to a complete halt upon release of Spark 2.4 which was sharply incompatible with the latest spark-redshift. Developers looking for a solution turned to online threads on websites like StackOverflow or Github. Answers strayed far from even a simple workaround. At Yelp, it was only a matter of time before we jumped into action. The inability to upgrade Spark from 2.3.3 to 2.4 meant that: We could not use highly sought-after features from Spark 2.4, Our move on to Kubernetes was endangered. In order to move our infrastructure to run on Kubernetes, we needed Spark on 2.4: “Spark can run on clusters managed by Kubernetes . This feature makes use of native Kubernetes scheduler that has been added to Spark [2.4].” 1 The spark-snowflake open source project is a stable spark-redshift fork for Snowflake. We considered adapting spark-snowflake to work with Redshift but the time estimate was higher than forking and upgrading the original spark-redshift. Upon suggestion from databricks, we did exactly that. We focused on porting the functionalities that we use the most, like performant reads from Redshift. We had to make tradeoffs in supporting a subset of features due to the timeline and workload. While some made the cut (reading from Redshift, various data types parsing, implementing an InMemoryS3AFileSystem for testing), others didn’t (Postgres driver support, AWS IAM Authentication, some SaveMode options).  We have already seen great internal adoption, and several teams are unblocked in their progress on moving to Spark 2.4. Our plans for the future include supporting the project by focusing on the features we use the most, in the hope that the community could carry forward features they find useful. spark-redshift-community is an edition for the community. Any support in the form of Github issues or pull requests is greatly welcomed. https://spark.apache.org/docs/latest/running-on-kubernetes.html ↩ Tweet Become a Backend (Big Data) Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2019-10-25"}, {"website": "Yelp", "title": "Beyond Labels: Stories of Asian Pacific Islanders at Yelp", "author": ["\n        \n  Tenzin Kunsal, Nivedita Mittal,Gabe Ramos, Julie Truong, Wing Yung\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/10/beyond-labels-stories-of-asian-pacific-islanders-at-yelp.html", "abstract": "During Asian Pacific American Heritage Month , ColorCoded (a Yelp employee resource group) hosted a panel discussion called “Beyond Labels: Stories of Asian Pacific Islanders (API)* at Yelp.” We heard stories from five API Yelpers about their cultural backgrounds, identities, and thoughts on what it means to be an API in today’s world. Their stories helped us understand that identity is both multilayered and contextual, and that individuality goes beyond labels. Read more from their unique perspectives below. Tenzin Kunsal, Events + Partnerships, Engineering Recruiting From a young age, I knew the concept of “home” was complicated. Like many refugees, my family called multiple countries home. My grandparents left my first home, Tibet, in the 1960s, after it was taken over by China. My second home, India, is where I was born and where I grew up, in a Tibetan refugee community. I was not automatically granted Indian citizenship, so for the first few years of my life, I was state-less, born without a country. That was until 1996, when Minneapolis became my third home. Soon after, I became an American citizen and finally officially “belonged” to a country. Growing up, this was all very confusing. I never felt like I fully fit in anywhere. It wasn’t until college that I started to accept the multifacetedness of my identity and that it’s okay to call multiple places “home.” Nivedita Mittal, Software Engineer, Reader Experience I moved to the U.S. four years ago to get my Master’s in Computer Science. Since then, it’s been a journey of self-discovery. When I moved from Mumbai to Boston, I always said “I’m from Mumbai, India.” Then, after moving to San Francisco, it became “I’m from Boston.” Something that has always stuck with my identity is how my immigration status defined whether I “belonged.” Whether it’s finding a job that sponsors your H-1B visa, or filling out your green card, defining who you are and whether you belong in the first place is an ongoing insecurity. It didn’t help that during grad school, every conversation I had with other international students revolved around my visa situation. The same applied to recruiting conversations with companies—I would always get questions like, “Did you get your H-1B yet? Did they file your green card already?” Once this is all said and done, I wonder if I’ll finally find that sense of belonging, or whether it’ll still be a conscious thought in my head to remind people that I belong here. Gabe Ramos, Director, CorpEng I identify as Filipino American, a person of color, and a Hapa. “Hapa” is a Hawaiian word that’s used to describe people who are part Asian and part Caucasian. Growing up in the Bay Area, I bounced around schools that had different ethnic make-ups. People often can’t tell what race I am. When I was in a predominantly Black and Latino school, classmates teased me for being “white.” When I was in a mostly white Palo Alto public school, classmates teased me for being “Japanese” because they didn’t know what race I was. I felt like I was between worlds because I didn’t pass for white yet often didn’t feel Filipino enough. Learning about different racial identities in college was pivotal for me. I have a liberal arts background, and my education really helped me learn about other Asian Americans’ experiences, the history of racial violence in the U.S., and anti-miscegenation laws. This helped me gain more of a sense of shared history. Most importantly, this empowered me to feel more ownership over my opinions of my own racial and cultural identity. Julie Truong, Software Engineer, Restaurant Plan From my last name, you may assume that I’m Vietnamese; I’m actually Chinese. My family immigrated from China to Vietnam (and later to the U.S.), and in order to blend in, my paternal grandfather changed our last name. My family is a mix of Chinese and Vietnamese cultures. At any given family gathering, you can hear English, Cantonese, and Vietnamese—all within the span of a couple minutes. I grew up in a primarily Latinx/Black/Samoan/Fillipino neighborhood in the East Bay. When I was younger, I had an idea of what being a “cool Asian” entailed, and Chinese people weren’t necessarily portrayed in this light. So I actually wished I were Fillipino, just like the cool kids in school. Now, as an adult living in the Bay Area, I feel I’m actually quite privileged. There’s a large Asian American population here, and I don’t have to think about my cultural identity very often. Interestingly, I find I have to think more about my gender and sexual orientation and how these parts of my identity show up in my personal and professional life. Wing Yung, Vice President, Engineering I grew up near Arcadia, California, in a community with many other Asian Americans. Most of my classmates in public school were like me—our parents immigrated here, and we were born here. I can speak three dialects of Chinese (poorly): Mandarin (which I learned through lessons), Cantonese (which my parents speak at home because they grew up in Hong Kong), and Wenzhounese (my grandparents’ dialect). Throughout college I became more aware of my Asian identity, but didn’t seek out opportunities to explore it. Early on in my career at IBM, one of my managers sent me to an Asian leadership development program. In retrospect, it was one of the first times I became aware that leadership comes in many forms. I’m very much aware of the fact that I’m often the only (or one of the few) Asians in leadership settings. It’s important to me to be a role model for others so that they know there are paths to these roles. Conclusion What ties all of these stories together is a sense of belonging that impelled us to redefine our identities on our own terms. Finding the right communities and support groups was critical for our journeys of self-discovery. The process of preparing for this panel was in itself extremely empowering, as it allowed us to dig deeper and reflect on what makes us who we are. Opportunities like these provide a platform to learn about others’ experiences and to realize how much representation influences our lives. It’s important to remind ourselves that sharing these stories makes us stronger and is an important part of cultivating community. Want to be a part of the dialogue? Here are a few steps you can take right now! Join a resource group/meetup/support group that focuses on diversity and inclusion. We have employee resource groups employee resource groups here at Yelp, including Colorcoded, Diverseburst, and Awesome Women in Engineering (AWE). For a more personal conversation, grab coffee with someone who identifies as an API to hear more about their journey. *In the context of this conversation, API stands for Asian Pacific Islanders—people with origins in Asia or the Pacific Islands. Tweet Engineering at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2019-10-28"}, {"website": "Yelp", "title": "Winning the Hackathon with Sourcegraph", "author": ["\n        \n  Mark Larah, Dennis Coldwell and Kevin Chen\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/11/winning-the-hackathon-with-sourcegraph.html", "abstract": "Visualizing how code is used across the organization is a vital part of our engineers’ day-to-day workflow - and we have a *lot* of code to search through! This blog post details our journey of adopting Sourcegraph at Yelp to help our engineers maintain and dig through the tens of gigabytes of data in our git repos! Here at Yelp, we maintain hundreds of internal services and libraries that power our website and mobile apps. Examples include our mission-critical “ emoji service ” which helps translate and localize emojis, as well as our “ homepage service ” which… you guessed it, serves our venerable homepage, yelp.com! Yelp homepage Don’t Break the Website Imagine you’re a developer tasked with implementing an exciting new feature. Perhaps you need to change the interface of the “ getBusinesses ” API endpoint to power a dedicated Find Desserts Near Me button on the homepage. “Piece of cake!” you say to yourself, as you add new parameters to alter the response of the shared resource. In order to not break the rest of the website though, you figure it’s best to see where other code is calling this endpoint so you can create a design that works for all use cases and doesn’t break existing call sites. We have over 100,000 Python files alone to power Yelp - that’s a lot of code to search through! In order to figure out a safe rollout plan, we need to scan through all of our existing code to understand where and how the method is being called across multiple git repositories. So how can we do this? Combined, our git repositories amount to tens of gigabytes of data. So cloning everything down locally whenever you want to perform a search is not a viable solution. Instead, we do this in the background as a scheduled process on a subset of our development machines, powered by all-repos . Some folks use this workflow, stringing together xargs and git grep, etc. into many homegrown bash scripts. A web interface (historically cgits and opengrok) is generally a more convenient go-to tool for browsing and searching code. Tools like this are essential to our workflow. And since we’re always on the lookout for ways we can improve the developer experience at Yelp, we want the best-in-class tool for the job! Discovering Sourcegraph Code Search We first heard about Sourcegraph at a React meetup hosted at Yelp. There was a discussion around how different companies view and search code, and Sourcegraph was introduced as an interesting-looking new search tool. One of the participants pulled up sourcegraph.com to demonstrate its capabilities. We tried a couple of searches using the repo and file regex filters and jumped around the codebase using the Jump to Definition feature. Coming from other tools and homegrown scripts, this was a huge step up in the developer experience! It stood out as a clear win on that front, and we decided to look into it some more and see how we could maybe bring Sourcegraph to Yelp. We validated the idea to see if it was worth pursuing by first setting it up locally. Sourcegraph is conveniently distributed as a docker image, so we were able to get a proof-of-concept running quickly and share it out with a small group of people. The feedback was positive! After using it regularly for a few weeks, we felt that the code browsing experience had been improved significantly and we pushed on to try and roll it out to the rest of Yelp! Productionizing Sourcegraph At Yelp, we run a biannual Hackathon – an opportunity for engineers to “scratch their creative itch” on projects outside of their day-to-day work. It was during one of these Hackathons that we started to productionize Sourcegraph at Yelp - which meant graduating the Sourcegraph instance from running on a local machine to being deployed on our PaaS platform, PaaSTA . By the end of the three days, we had Sourcegraph ready for the whole company to try out. The feedback was great, and Sourcegraph was well received. We even won an award! A coveted Hackathon trophy Showing off Sourcegraph to Yelpers at the Hackathon “Science Fair” Once Sourcegraph was up and running at Yelp, we had to decide whether we wanted to invest more in the product to get features such as Code Intelligence. To come to this decision, we surveyed developers on how they liked Sourcegraph compared to other code search/viewing tools we were using, and the results were heavily favored towards Sourcegraph. 70% of developers rated Sorcegraph as very good, and 51% percent of developers were already using Sourcegraph exclusively as their preferred code analysis tool. As a result of this feedback, we decided to make Sourcegraph the singular supported tool at Yelp for code search and viewing! Shipping Code Faster with Sourcegraph Sourcegraph empowers developers at Yelp to ship code faster and more reliably than ever before. Code intelligence features such as Go-to-Definition and Find References are heavily-used features that enable developers to understand the plethora of microservices and libraries in our code base. When making large changes, Sourcegraph is the way to discover how your code is being called throughout the rest of the code base. Sourcegraph has also been helpful for onboarding new hires and introducing them to the code base. Sourcegraph has proven to be one of the most useful tools for making mass code migrations and deprecations. A quick search can help scope out the magnitude of the change and the difficulty of implementing it, while also providing an easy way to track the progress of long-running migrations and deprecations. Sourcegraph’s GraphQL API has also proved to be useful for tooling we have built in-house. Developers at Yelp have used the Sourcegraph API to power services such as our internal npm registry and flaky test analysis engine, both of which heavily utilize source control metadata. Daily active users of Sourcegraph at Yelp Future Work We are evaluating running Sourcegraph as a clustered deployment. While we are currently able to serve all Sourcegraph usage on a single host, we are looking into running all of Sourcegraph’s different services individually. This would allow us to scale up more resource-intensive instances of Sourcegraph’s services. We are planning to put it on Kubernetes, an initiative that is underway for a lot of Yelp’s infrastructure. Written By Mark Larah, Software Engineer ( @mark_larah ) Dennis Coldwell, Engineering Manager Kevin Chen, Software Engineer Tweet Become an Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2019-11-01"}, {"website": "Yelp", "title": "Migrating Kafka's Zookeeper With No Downtime", "author": ["\n        \n  Toby Cole, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/01/migrating-kafkas-zookeeper-with-no-downtime.html", "abstract": "Here at Yelp we use Kafka extensively. In fact, we send billions of messages a day through our various clusters. Behind the scenes, Kafka uses Zookeeper for various distributed coordination tasks, such as deciding which Kafka broker is in charge of assigning partition leaders and storing metadata about the topics in its brokers. Kafka’s success within Yelp has also meant that our clusters have grown substantially from when they were first deployed. At the same time, our other heavy Zookeeper users (e.g., Smartstack and PaasTA ) have increased in scale, putting more load on our shared Zookeeper clusters. To alleviate this situation, we made the decision to migrate our Kafka clusters to use dedicated ZooKeeper clusters. Since we rely so heavily on Kafka, any downtime due to maintenance can cause knock-on effects, like delays in the dashboards we show to business owners and logs backing up on our servers. And so the question arose… can we switch Zookeeper clusters without Kafka and other ZooKeeper users noticing? Zookeeper Mitosis After a few rounds of discussion and brainstorming between the teams that look after Kafka and Zookeeper, we figured out an approach that seemed to allow us to achieve our goal: Migrate the Kafka clusters into their own dedicated Zookeeper clusters without any Kafka downtime. The procedure we came up with can be compared to the process of cell mitosis in nature: At a high level, we replicate the Zookeeper hosts (our DNA), and then divide the duplicated hosts into two separate clusters using firewall rules (our cell walls). Major events in mitosis, where chromosomes are divided in a cell nucleus Let’s delve a bit deeper into the details, step by step. Throughout this post we’ll be referring to the source- and destination-clusters, with the source representing the cluster that already exists and the destination the new cluster that Kafka will be migrated to. The examples we’ll be using are for a three-node Zookeeper cluster, but the process itself works for any number of nodes. Our examples will use the following IP addresses for the Zookeeper nodes: Source 192.168.1.1-3 Destination 192.168.1.4-6 Stage One: DNA Replication First, we need to fire up a new Zookeeper cluster. This destination-cluster MUST be entirely empty, as the contents will be wiped out during the migration process. We’ll then take two of the destination nodes and add them to the source-cluster, giving us a five-node Zookeeper cluster. The reason for this is that we want the data (originally stored by Kafka on the source ZooKeeper cluster) to get copied onto the destination-cluster. By joining the two destination nodes to the source-cluster, the copy is performed automatically by ZooKeeper’s replication mechanism. Nodes from the source- and destination-clusters are combined Each of the nodes’ zoo.cfg file looks something like this now, with all of the source nodes and two of the destination nodes in the cluster: server.1=192.168.1.1:2888:3888\nserver.2=192.168.1.2:2888:3888\nserver.3=192.168.1.3:2888:3888\nserver.4=192.168.1.4:2888:3888\nserver.5=192.168.1.5:2888:3888 Notice that one of the nodes from the destination ZooKeeper cluster (192.168.1.6 in the above example) remains dormant during the procedure; it does not become a part of the joint-cluster,and ZooKeeper is not running on it. The reason for this dormancy is to maintain quorum in the source ZooKeeper cluster. At this point the joint ZooKeeper cluster has to be restarted. Make sure you perform a rolling restart (restart a single node at a time with at least 10-second intervals between each) starting with the two nodes from the destination-cluster. This order ensures that quorum is not lost in the source ZooKeeper cluster and ensures availability for other clients, e.g. Kafka, while the new nodes are joining the cluster. After the rolling restart of ZooKeeper nodes, Kafka has no idea about the new nodes in the joint-cluster, as its Zookeeper connection string only has the original source-cluster’s IP addresses: zookeeper.connect=192.168.1.1,192.168.1.2,192.168.1.3/kafka Data being sent to Zookeeper is now being replicated to the new nodes without Kafka even noticing. Now that the data is in sync between the source and destination Zookeeper clusters, we can update Kafka’s Zookeeper connection string to point to the destination-cluster: zookeeper.connect=192.168.1.4,192.168.1.5,192.168.1.6/kafka A rolling restart of Kafka is required for it to pick up the new connections, but this does not require any downtime for the cluster as a whole. Stage Two: Mitosis The first step of splitting the joint-cluster is restoring the original source and destination ZooKeeper configuration files (zoo.cfg), as they reflect the desired final state of the clusters. Note that none of the Zookeeper services should be restarted at this point. We use firewall rules to perform our mitosis, splitting our joint ZooKeeper cluster into distinct source and destination-clusters, each with their own leader. In our case, we’re using iptables to achieve this, but any firewall system that you can enforce between the hosts in your two Zookeeper clusters should suffice. For each destination node, we run the following commands to add iptables rules: $source_node_list = 192.168.1.1,192.168.1.2,192.168.1.3\nsudo /sbin/iptables -v -A INPUT  -p tcp -d $source_node_list -j REJECT\nsudo /sbin/iptables -v -A OUTPUT  -p tcp -d $source_node_list -j REJECT This rejects any incoming or outgoing TCP traffic to the source nodes from the destination nodes, achieving a separation of the two clusters. The source and destination clusters are separated by firewall rules and then restarted The split means that we now have two destination nodes on their own. Since they think they’re part of a five-node cluster and cannot talk to a majority of their cluster, they will not elect a leader. At this point, we simultaneously restart Zookeeper on every node in the destination-cluster, including the dormant node that was not part of the joint-cluster. This allows the Zookeeper processes to pick up their new configuration from step two. It also forces a leader election in the destination-cluster so that each cluster has its own leader. From Kafka’s perspective, the destination-cluster is unavailable from the moment the network partition is added until the leader election completes. This is the only time ZooKeeper is unavailable to Kafka throughout the whole procedure. From now on, we have two distinct Zookeeper clusters, and there is no return! At least, not without copying data between clusters and possible data loss or downtime for Kafka. All we have to do now is clean up after ourselves. The source-cluster still thinks it has two extra nodes, and we have some firewall rules in place that need to be cleared up. Next, we restart the source cluster in order to pick up the zoo.cfg configuration containing only the original source-cluster nodes. This allows us to safely remove firewall rules, as the clusters are no longer trying to talk to each other. The following command drops the iptables rules: $source_node_list = 192.168.1.1,192.168.1.2,192.168.1.3\nsudo /sbin/iptables -v -D INPUT  -p tcp -d $source_node_list -j REJECT\nsudo /sbin/iptables -v -D OUTPUT  -p tcp -d $source_node_list -j REJECT Building Confidence Distributed Stress Test The main approach we took to test the correctness of the migration procedure was a distributed stress test. The script runs tens of instances of Kafka producers and consumers across multiple machines while the migration procedure is in progress. After traffic generation finishes, all consumed payloads are aggregated into a single host to check for data loss. The distributed stress test works by creating a set of Docker containers for Kafka producers and consumers and running them in parallel on multiple hosts, the list of which is passed as one of the parameters of the experiment. All produced messages contain serial numbers that can be used to check for message loss. Ephemeral Clusters In order to prove that this migration procedure was going to work, we wanted to build some clusters specifically for testing. Rather than manually building Kafka clusters and tearing them down again, we built a tool to spin up new clusters within our infrastructure that could be torn down automatically, allowing us to script the whole test procedure. The tool connects to the AWS EC2 API and fires up several hosts with specific EC2 instance tags, permitting our puppet code, via External Node Classifiers , to figure out how to configure the host and install Kafka. This ultimately allowed us to run and re-run our migration scripts, simulating the migration multiple times. The Ephemeral Cluster script was later reused to create ephemeral Elasticsearch-clusters for our integration testing, proving to be an invaluable tool. zk-smoketest We found phunt’s simple Zookeeper smoketest scripts extremely useful in monitoring the state of each Zookeeper cluster while we were migrating. Throughout each stage of the migration we ran a smoketest in the background to ensure the Zookeeper clusters were behaving as expected. zkcopy Our first (naive) plans for the migration involved simply stopping Kafka, copying a subset of the Zookeeper data over to the new cluster, and starting Kafka with an updated Zookeeper connection. A more refined version of this procedure, which we called ‘block & copy’, is still used for moving Zookeeper clients to clusters with data stored in them, as the ‘mitosis’ procedure requires a blank destination Zookeeper cluster. A great tool for copying subsets of Zookeeper data is zkcopy , which copies a sub-tree of one Zookeeper cluster to another. We also added transaction support, enabling us to batch Zookeeper operations and minimize the network overhead of creating one transaction per znode. This sped up our usage of zkcopy by about 10x! Another feature that was core to our speedup was ‘mtime’ support, which allowed us to skip copying any nodes older than a given modification time. Through this, we were able to avoid a majority of the work for the second ‘catch-up’ copy required to get our Zookeeper clusters in sync. The downtime needed for Zookeeper went from 25 minutes to less than two! Lessons Learned Zookeeper clusters are pretty lightweight. When possible, try not to share them between different services, as they may start to cause performance issues in Zookeeper, which are hard to debug and usually require downtime to repair. It is possible to migrate Kafka to a new Zookeeper cluster without any Kafka downtime, but it’s certainly not trivial. If you can schedule Kafka downtime to do your Zookeeper migration, it’ll be a lot simpler. Credits Piotr Chabierski - Initial draft & reviews Raghu Prabhu - Reviews Toby Cole - Typing furiously & photoshoppery Tweet Become an Engineer at Yelp Come and work with the teams in London that wrangled these various moving parts! View Job Back to blog", "date": "2019-01-17"}, {"website": "Yelp", "title": "Yelp Dataset Challenge: Round 11 Winners", "author": ["\n        \n  Yelp Dataset Challenge team\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/02/yelp-dataset-challenge-round11-winner.html", "abstract": "The eleventh round of the Yelp Dataset Challenge ran throughout the first half of 2018 and we received many impressive, original, and fascinating submissions. As usual, we were struck by the quality of the entries: keep up the good work, folks! Today, we are proud to announce the grand prize winner of the $5,000 award: “Generalized Latent Variable Recovery for Generative Adversarial Networks” by Nicholas Egan, Jeffrey Zhang, and Kevin Shen (from the Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science). The authors used a Deep Convolutional Generative Adversarial Network (DCGAN) to create photo-realistic pictures of food by training on images from Yelp. Through use of Gaussian priors on the latent variables in the images and other standard technique improvements, they managed to train DCGANs that perform better than past models. Their DCGANs can also be adapted to multiple applications in the industry. The three authors presented at Yelp’s weekly Engineering Learning Group last month to further describe their work and its various utilizations. This entry was selected from numerous submissions for its technical and academic merit by our panel of data scientists, data mining engineers, and software engineers. For a list of all previous Yelp Dataset winners, head over to the challenge site . Thanks to all who participated! Tweet Back to blog", "date": "2019-02-06"}, {"website": "Yelp", "title": "Autoscaling Mesos Clusters with Clusterman", "author": ["\n        \n  David R. Morrison, Compute Infra Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/02/autoscaling-mesos-clusters-with-clusterman.html", "abstract": "Here at Yelp, we host a lot of servers in the cloud. In order to make our website more reliable—yet cost-efficient\nduring periods of low utilization—we need to be able to autoscale clusters based on usage metrics. There are quite a\nfew existing technologies for this purpose, but none of them really meet our needs of autoscaling extremely diverse\nworkloads (microservices, machine learning jobs, etc.) at Yelp’s scale. In this post, we’ll describe our new in-house autoscaler called Clusterman (the “Cluster Manager”) and its magical\nability to unify autoscaling resource requests for diverse workloads. We’ll also describe the Clusterman simulator, which\nwe use not only to verify that our code is working correctly, but also to predict operating costs based on hypothetical\ndata. And just to pique your interest, we’d like to note that Clusterman is now the de facto autoscaler at Yelp, and is\nable to begin autoscaling new clusters with just a couple hours of work! So you want to run a Mesos cluster; would you like autoscaling with that? First, let’s start with some preliminaries: there are lots of different models for distributed computation out there,\nand not all of them can be autoscaled in the same way. For the purpose of this post, we’ll focus on autoscaling\nstrategies for distributed clusters running Apache Mesos. Apache Mesos is an open-source project that abstracts away the specific properties of\nservers in your cluster, and essentially allows developers to treat that cluster as one large computer. In order to make\nour clusters ready for autoscaling, we’ve designed our applications to be relatively fault tolerant so they can easily\nhandle the loss of a compute node in one of our Mesos clusters. Now, let’s consider some different types of workloads we might want to autoscale. At Yelp, we have numerous  types of\ndistributed computation that we want to perform, ranging from long-running services (such as the web service hosting\nthis very blog!), to periodic batch jobs (for doing image classification or analyzing ad revenue), unit and integration\ntests, machine learning, and many more! Each of these applications have a different usage pattern and level of fault\ntolerance, so it’s important to have an autoscaler that can handle them all. Finally, let’s think about this from an operational standpoint for a minute. At Yelp, we know that people make mistakes\nall the time, and our goal is to provide safeguards that minimize the impact of those mistakes. The problem with\nautoscaling is that it’s invisible when it works, catastrophic when it doesn’t, and nearly impossible to test except on\nproduction workloads. Ideally, we’d like to design a solution that catches errors before they take down our website. Enter Clusterman: 50% autoscaler, 50% simulation environment, 100% love There’s a lot of prior art on how to build an autoscaler. At Yelp, we built at least two different autoscalers before\nClusterman (the “Cluster Manager”) was born! This new autoscaling project has allowed us to consolidate our engineering\nefforts in one place, while simultaneously handling a wide range of distributed computation workloads. The key idea behind this design is to separate the data from the business logic, as shown in the following diagram: Figure 1: The Clusterman architecture diagram. Yelp's infrastructure is hosted on AWS, but in principle\n     Clusterman can integrate seamlessly with any cloud provider. We’re a data-driven organization at Yelp, so the most important component of our autoscaler is the actual data itself.\nWe collect all kinds of data about our clusters, ranging from the number and type of machines in the cluster, to the\namount of resources allocated by Mesos frameworks running on the cluster, as well as many application-specific metrics\nsuch as, “How many developers are trying to run unit tests on this cluster right now?” Much of this load is periodic,\nwhich we can take advantage of for autoscaling purposes (Figure 2). All of these data points are collected per minute\nand stored in Amazon’s DynamoDB, which works great for storing timeseries data (Figure 1, Step 1). Figure 2: The CPU load range for our biggest service, yelp-main. Next, this data is passed into autoscaling signals (Figure 1, Step 2), which indicate how many resources each\napplication thinks it will need in the upcoming minutes. The key insight here is that every application will need a\ndifferent signal, and we can’t know the ins and outs of every application running on the cluster. So, at Yelp, these\nsignals are owned by the teams that running the applications, and we use configuration files to specify what versions of\nwhich signals we want to use for each cluster. The input and output of these signals is specified through a simple API,\nwhich allows a team to plug-and-play various signals as they please. The use of “pluggable” signals is the secret sauce that makes it easy for new teams and projects to take advantage of\nClusterman. Each signal can be as simple as just a few lines of Python code or as complex as a machine-learning-driven\nprediction algorithm. For example, the default signal used by Clusterman looks at the current utilization (CPUs, memory,\nand disk) of the cluster and scales up or down to maintain a constant load fraction; however, CPU utilization is not a\ngreat metric for other types of workloads, so our unit-testing cluster instead uses the number of developers currently\ntrying to run tests and bases its autoscaling requests on that number. Even more recently, we’ve added a signal that\nallows our cluster that runs Spark jobs to be autoscaled based on the number of Spark jobs in flight and the amount of\nresources they’ve requested. And because all the signals are independent, nobody has to worry about what anyone else is\ndoing on the cluster. The output from each signal is collected and aggregated by the core Clusterman autoscaler (Figure 1, Step 3), which is\nthen responsible for analyzing all of the signal requests and deciding what to do (scale up, scale down, or do nothing).\nOnce it’s made a decision, it communicates with the cloud provider (Figure 1, Step 4) to provision or terminate machines\nas needed (in our case, the cloud provider is AWS, but this is modularized so there’s no reason you couldn’t use\nsomething else). So this solves half of the equation: how to handle different types of workloads with one common system. But there’s\nstill the issue of how to make sure your autoscaling will do the right thing on production traffic. This is exactly\nwhere the Clusterman simulation environment comes in! To put it simply, we’ve developed an entire simulated ecosystem that allows you to test out changes to the autoscaling\nlogic before deployment. You can use randomly-generated metrics for this simulation, replay production metrics from a\nprevious time period, or use any combination of the two that you like. Not only does this allow you to check for bugs in\nthe code against production data, but you can also start answering hypothetical questions like, “How much more money\nwould I spend if our website traffic doubled next week?” Enough talk, show me pretty pictures! Let’s take a look at some actual data from one of our clusters. This cluster runs Jolt , our\ninternal distributed unit and integration testing framework. To give you a sense of scale, the code powering yelp.com\nhas about 100,000 unit and integration tests. Running these back-to-back would take a couple of days, and we run all of\nthem about 400 times a day with less than 30 minutes per run. So this cluster definitely earns its keep!  Here’s a graph\nfrom our internal monitor showing the usage on the cluster during the month of December 2018: Figure 3: Cluster capacity for Jolt (our unit- and integration-testing cluster) in December 2018. The green\n     curve shows the total CPUs in the Mesos cluster and the purple curve shows the CPUs allocated to tasks. This graph shows some nice trends: in particular, we see that we’re almost always overprovisioned on the cluster, and\nmoreover, starting on 9 December there’s a period where a bug in the autoscaler prevented us from scaling down. There\nare two main disadvantages in the above graph: first, our monitoring aggregates a lot of data points, especially for\ndata that’s more than thirty days old; and second, we can’t provide it with “fictional” input data and ask it what the\ncluster would do. Now let’s take a look at the same data as represented by Clusterman: Figure 4: Actual cluster usage for our Jolt (unit- and integration-testing) workload in December 2018. Figure 5: Actual cluster capacity (i.e., number of available CPUs) in the Jolt cluster in December 2018. Figures 4 and 5 show the respective number of CPUs allocated by Jolt tasks and provisioned in the cluster, as reported\nby Clusterman for this same time frame. Each dot represents one minute of real time, and the brightness of the dot\nindicates a higher capacity or utilization. Clusterman’s data collection does not average or aggregate, and we store\ndata for up to two years; so, unlike other more general monitoring tools, we can do 100% accurate simulations for events\nfrom  two years ago, or compute year-over-year comparisons of usage or cost. You can definitely see some patterns here that weren’t immediately obvious in Figure 3. For example, it’s quite clear\nthat most people run tests during the day rather than at night (we encourage a healthy work-life balance at Yelp!).\nMoreover, it’s still obvious, even just qualitatively, that our cluster is overprovisioned. Finally, using EC2 pricing\ndata from AWS, Clusterman can look at the above usage data and tell you how much you’re spending and when your costs\npeak. While there are lots of other tools out there with the same functionality, only Clusterman can break the costs\ndown to a per-application level, enabling us to perform cost attribution to various teams at Yelp. The two above graphs are based on actual data from Clusterman and Jolt. Let’s look now at a simulation to see how we\nmight do better (or worse). Here’s an example of a really basic signal called the “Constant Signal.” As you might\nimagine, it always requests the same number of resources. What would happen to the above workload if we used this\nsignal? Let’s specifically look to see how many unused CPUs we have sitting around: Figure 6: Simulation of what would happen if we replaced our sophisticated autoscaling signal with one that\n     requests the same amount of resources at all times. Oh no! What are all those red dots?? Well, based on historical utilization, we know these are cases when the number of\nCPUs allocated by Jolt Mesos frameworks would have exceeded the capacity of the cluster. This means that if we deployed\nthis signal into production, we would run out of capacity practically on a daily basis, and our unit test jobs would be\ndelayed. This might be what we want if we’re spending too much money during the day, but in all likelihood is probably a\nbad sign. Fortunately, this is just a simulation! The use of this simulator has proven invaluable for our autoscaling efforts; it’s allowed us to catch several bugs that\nwould have made it into production otherwise, and additionally has given us useful data for where to focus our\ncost-saving efforts. We’ve been able to translate results from our simulator into real changes to signals, resulting in\naround 10% cost savings (at no loss of reliability) for our Jolt cluster! Wrapping Up: What’s next for Clusterman? Have no fear: just because we wrote this blog post doesn’t mean we’re done working on Clusterman. We’ve got a lot of\nexciting things in store for the future. First up, we’re currently investing in building Kubernetes support into\nClusterman. Due to its modular design, it will take just a few weeks of effort before seeing the benefits of Clusterman\nfor free on our nascent Kubernetes clusters. We also have a bunch of improvements we want to make around the simulation\ninterface, namely with regards to making it easier to do data analysis. Finally, we’re just scratching the surface of\nwhat we can do with our signals. We’re planning to start looking at machine learning models to better predict the\nincoming demand for our clusters, and balance that automagically with our cost constraints. Watch this space for more\nexciting info about Clusterman in quarters to come! Tweet Become an Infrastructure Engineer at Yelp Want to do data analytics on our infrastrucure?  Apply here! View Job Back to blog", "date": "2019-02-19"}, {"website": "Yelp", "title": "Tech Intersections Conference", "author": ["\n        \n  Mariana Gonzalez, Technical Diversity Recruiting Partner\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/03/tech-intersections-conference.html", "abstract": "This year Yelp sponsored the second annual Tech Intersections conference in Oakland, CA. It was a great opportunity to celebrate womxn of color in tech and to come together and learn from each other’s successes, challenges, and experiences. The conference, which featured ALL womxn speakers and attendees, highlighted topics ranging from tech entrepreneurship to self-care and career skills. Kelly Greenia, Engineering Recruiter, with some Yelp Swag! Two members of Yelp’s Awesome Women in Engineering (AWE) group attended the conference and below are some of their takeaways. This past weekend, we had the opportunity to attend the Tech Intersections conference at Mills College. It was an empowering experience to be surrounded by so many strong, intelligent, and supportive womxn of color. The talks were a great balance of technical skills, career development, and self-care. We attended an amazing session by Maryanna Quigless , a PM leader of the Ads Creation and Guidance team at Facebook. She walked us through a product development framework she uses with her teams called PURSUIT, which really encourages you to take a step back and focus on the problem you’re solving and the user you’re solving it for. Another session we attended, Leveling Up, was a panel of women discussing their journeys in tech. One of the most powerful parts of this was when Ei-Nyung Choi , a software engineer at Slack, introduced herself to the audience. First she told us her polished success story which highlighted the high points of her career–building products for 20 years, being a tech co-founder, etc. We were all obviously impressed by her long list of accomplishments. Then she told us the “real story” and talked through all of the challenges she faced along the way. Aditi Ganpule, Product Manager & Banafsheh Derayat, Software Engineer It’s so easy for us to only focus on the highlights reel of other people’s journeys; we tend to forget that everyone faces failure and that’s perfectly okay. As two young women just starting out in our careers, this was an invaluable perspective for us to have. We also attended a session about toxic work environments and practicing self-care at the workplace. One of the activities in the session was to come up with “I am” statements describing yourself in positive ways. This was interesting to us since it’s a simple concept but something that’s often neglected. Overall, the organizers of the conference did a remarkable job curating the sessions and inviting inspiring womxn. We want to thank them, all the volunteers, and Yelp for giving us the chance to attend this conference. -Aditi Ganpule, Product Manager & Banafsheh Derayat, Software Engineer Tweet Are you an awesome womxn of color? Checkout engineering jobs at Yelp! View Job Back to blog", "date": "2019-03-04"}, {"website": "Yelp", "title": "Jira & Ansible: Scaling Jira Server Administration for the Enterprise", "author": ["\n        \n  Jose Martin de Vidales Biurrun, Application Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/04/Scaling-Jira-Server-Administration-For-The-Enterprise.html", "abstract": "In 2017, Yelp had over 40 Jira administrators to allow different teams across the organization to perform administrative tasks. With lots of admins came lots of changes, which lead to our Jira environment accumulating hundreds of orphaned workflows, screens, and schemes. To solve this problem, we built a scalable solution that empowers our engineers to create Jira projects themselves using code and source control – ensuring 100% standardization across all engineering projects and making Jira easier to manage, simpler to use, and better performing. This blog post will cover Yelp's use of Ansible to manage Jira server project creation, updates, and archive functionality. Behind the Scenes We first needed to decide which technologies to utilize to solve this problem. We settled on an open-source tool called Ansible as the main framework because of its procedural language and client-only architecture. While Puppet is used across Yelp, not having to deal with a complex infrastructure to run Ansible was key for us. In our implementation, Ansible is responsible for interacting with Jira's REST API to perform every step of the project lifecycle. We identified three critical project lifecycle elements, each with its own playbook: Project creation Project archival Project updates We use Git to host our configuration while maintaining a full revision history. These configurations are maintained across three different repositories: Ansible_configs: Hosts Ansible configuration, playbooks, and roles. Ansible_precommithooks: Hosts configuration for the pre-commit hooks that were used in Ansible_configs repository. Ansible_jira_projects: Hosts YAML files defining each Jira project and its configuration. Each project is represented by a single YAML file which describes the configuration in an easy-to-read way: PROJECTBETA : - key : beta - leader : darwin - board_type : agile - description : the new revolutionary project - security_schema : engineering We then use the powerful templating feature from Ansible to transform the YAML configuration files into a JSON object that can be interpreted by Jira's REST API. Using Jinja2, we created custom templates that transform the YAML files into a JSON payload of the different Jira Server API requests. Here's an example of one of our Jinja2 templates we used to create a project: { \"key\" : \"\" , \"name\" : \"\" , \"lead\" : \"\" , \"projectTemplateKey\" : \"\" , \"issueSecurityScheme\" : \"\" , \"permissionScheme\" : \"\" , \"notificationScheme\" : \"\" } To verify that our engineers only push valid project manifests, we use a framework for managing and maintaining multi-language pre-commit hooks called \"pre-commit.\" Each time an engineer makes a commit, our defined pre-commit hooks are automatically run to verify the manifest and identify any issues, such as invalid Jira project keys, names, or illegal characters. Pre-commit has a lot of pre-built hooks available out-of-the-box . For this solution, we used ones like check-yaml and sort-simple-yaml, but also wrote our own custom hooks to ensure that all secrets were properly encrypted with Ansible Vault, and all existing and new YAML files only contained allowed keys/values and followed Yelp's Jira project standards. Here's an example of one of our custom hooks that we created to validate our YAML syntax: #!/usr/bin/python import sys import argparse import yaml import re VALID_BOARD_CONFIGS = [ 'kanban' , 'agile' ] def main ( argv = None ): retval = 0 parser = argparse . ArgumentParser () parser . add_argument ( 'filenames' , nargs = '*' , help = 'Jira project files to check.' ) args = parser . parse_args ( argv ) argv = argv if argv is not None else sys . argv [ 1 :] for filename in args . filenames : try : YAML_PROJ_CONF = yaml . safe_load ( open ( filename )) PROJECT_KEY = YAML_PROJ_CONF . keys ()[ 0 ] except yaml . YAMLError : print ( 'Error parsing: {}' . format ( filename )) retval = 1 continue if 'key' in YAML_PROJ_CONF [ PROJECT_KEY ]: if not re . match ( r \"^[A-Z]{2,10}$\" , YAML_PROJ_CONF [ PROJECT_KEY ][ 'key' ]): print ( \"{} isn't a valid key. The project key MUST be 2-10 characters, only A-Z\" . format ( YAML_PROJ_CONF [ PROJECT_KEY ][ 'key' ])) retval = 1 else : print ( '{}: is missing project key' . format ( filename )) retval = 1 if not re . match ( r \"^[a-z]{2,10}$\" , YAML_PROJ_CONF [ PROJECT_KEY ][ 'lead' ]) and 'svc-' not in YAML_PROJ_CONF [ PROJECT_KEY ][ 'lead' ]: print ( \"{} isn't a valid lead. The lead MUST be 2-10 characters, only a-z\" . format ( YAML_PROJ_CONF [ PROJECT_KEY ][ 'lead' ])) retval = 1 if YAML_PROJ_CONF [ PROJECT_KEY ][ 'board_config' ] not in VALID_BOARD_CONFIGS : print ( \"{} isn't a valid board config. You can choose between the following options {}\" . format ( YAML_PROJ_CONF [ PROJECT_KEY ][ 'board_config' ], VALID_BOARD_CONFIGS )) retval = 1 return retval if __name__ == '__main__' : sys . exit ( main ()) Jenkins (our continuous integration tool) puts all the pieces together by merging the three Git repositories, verifying that all new and existing YAML files pass validation through pre-commit hooks, and then executing in Ansible. The payload that’s built is based on the information included in the new manifest files and is executed against Jira’s API to process the changes. Our Jenkins pipeline consists of four different stages: Review validation: In order to avoid engineers pushing code without a code review, we’ve customized our Gitolite permissions to ensure engineers only push integration branches. By doing this, we avoid rogue changes in our repositories that haven’t undergone proper review from other team members. Manifest validation: Runs pre-commit hooks against all YAML and configuration files and validates that they pass. Playbook execution: If the first two stages pass, Jenkins executes the proper playbook based on the changes that were made to the repository. Notification: If the Ansible project creation/update/deletion is successful, it notifies the requestor that the changes are now live. Implementation Now that you’re familiar with the technologies involved, let’s talk about implementation. This chart illustrates how each of these components works together to deploy changes automatically: When an engineer wants to create, update, or delete a Jira project in our main production instance, they simply clone the “ansible_jira_projects” repository, which contains all existing project YAML files. At this point, engineers have the option to manually update an existing project YAML file or to use one of our self-service scripts to automatically generate a new project manifest based on prompted details: Pre-commit hooks are run when an engineer makes a Git commit to submit the changes for review. At this point, the engineer submits a code review. Included in the review are the results of all tests run in pre-commit hooks. Once the code review receives a “ship it!” from one of our team members, the engineer can merge the changes to a deploy branch using a simple script. Jenkins will get a notification of new changes in our Git repositories. Benefits By using Ansible, Jenkins, pre-commit hooks, custom written Python scripts, and easily comprehensible YAML we were able to: Ensure 100% standardization across all engineering projects, making Jira easier to manage, simpler to use, and better performing. Establish a rich auditing trail by ensuring that every action taken during the Jira project lifecycle is captured, reviewed, and logged. Eliminate repetitive tasks for our Jira administrators. Reduce the turnaround time of Jira project creation/update/deletion while also freeing our Jira admins to focus on other important tasks. Reduce the number of Jira global administrators. Tweet Back to blog", "date": "2019-04-03"}, {"website": "Yelp", "title": "Inside TensorFlow", "author": ["\n        \n  Bronek Kozicki, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/11/inside-tensorflow.html", "abstract": "Inside TensorFlow It’s probably not surprising that Yelp utilizes deep neural networks in its quest to connect people with great local businesses. One example is the selection of photos you see in the Yelp app and website, where neural networks try to identify the best quality photos for the business displayed. A crucial component of our deep learning stack is TensorFlow (TF). In the process of deploying TF to production, we’ve learned a few things that may not be commonly known in the Data Science community. TensorFlow’s success stems not only from its popularity within the machine learning domain, but also from its design. It’s very well-written and has been extensively tested and documented (you can read the documentation offline by simply cloning its repository ). You don’t have to be a machine learning expert to enjoy reading it, and even experienced software engineers can learn a thing or two from it. Building TensorFlow You can start using TF without the extra build steps by installing the Python package from pypi.org . Doing it this way is straightforward, but also means you won’t have access to any optimization features. Here’s an example of what this can look like in practice: $ python3 -c 'import tensorflow as tf; tf.Session().list_devices()' 2>&1 | grep -oE 'Your CPU .*' Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA If you want to hack TF (the second part of this post explains how), then in order to test your changes, you’ll have to build the package yourself. So, assuming you’re interested in building TF for your own requirements, or perhaps with your own code changes, here’s a compilation of hints on how to make it a relatively painless experience. Note: this is not a step-by-step recipe; obvious points (like “copy the sources ”, and “read the documentation ”) are not included! We recommend building TensorFlow inside containers like Docker or Podman . The TF project uses Docker for both continuous integration and official images . You’ll find Dockerfiles and documentation for the latter in the tensorflow/tools/dockerfiles directory. However, it is a Continuous Integration (CI), which is of more interest in the context of building TF, so make sure to read tensorflow/tools/ci_build/README.md and check out other files in this directory. Using containers to build TF makes it easier to consistently install all required packages and helps ensure the builds are reproducible (a critical requirement of CI). A major required package for building TF is the Bazel Build system (it’s possible, but not recommended, to use make instead of bazel. For instructions see tensorflow/contrib/make/README.md ). In addition to Bazel, other TF dependencies can be found inside the configure.py script (in the project root directory). TF also depends on a number of Python packages, all of which are listed inside the tensorflow/tools/pip_package/setup.py file (look for REQUIRED_PACKAGES ). Important among those is NumPy, which may require you to install an extra package in the operating system, such as the libatlas3-base package for Ubuntu users. Additionally, if you want to build TF for GPU, you’ll need either CUDA with cuDNN (for NVIDIA) or ROCm (for AMD, which we have not tried) installed inside your container. The simplest way to ensure that all CUDA dependencies are present is to use the official nvidia images as your container base, as demonstrated in the tensorflow/tools/ci_build/Dockerfile.gpu file. You’ll need to execute configure.py before the actual build. The script will ask many questions, such as “Please specify which C compiler should be used.” For a scripted build, the answer to all questions can be automated with “ yes | ” (as demonstrated in tensorflow/tools/ci_build/builds/configured ). Also, if you read the configure.py source, you’ll quickly discover that individual questions can be suppressed with environment variables, such as HOST_C_COMPILER . Among these, a very useful variable is CC_OPT_FLAGS , which by default contains “ -march=native -Wno-sign-compare ”. If you want to use the resulting package with a model of CPU different than the one where you run your build, you should replace “native” with a more appropriate value . The output of configure.py is the .tf_configure.bazelrc file, which you may want to look into. After the initial configuration step, you’ll need to run “ bazel build ” with options to build TF binaries (but not its Python wheel - yet!). The selection of Bazel options can be a little tricky, but the script tensorflow/tools/ci_build/ci_build.sh may give you some ideas. The build typically takes between 30–60 minutes (or longer when CUDA is enabled) on 40 CPUs - it is quite a large project! After this step is completed, you still need to build the Python wheels. As explained in the documentation, this step is actually performed by the “ build_pip_package ” binary you’ve just built! Here’s an example of what the above steps may look in a Dockerfile: RUN curl -L https://github.com/bazelbuild/bazel/releases/download/ ${ BAZEL_VER } /bazel- ${ BAZEL_VER } -installer-linux-x86_64 .sh --output bazel.sh && bash bazel.sh --prefix=/opt/bazel &&\n    rm bazel.sh ENV PATH ${PATH}:/opt/bazel/bin RUN curl -L https://github.com/tensorflow/tensorflow/archive/ ${ VERSION } .tar.gz | tar xz --strip-components = 1 ENV TF_NEED_CUDA 0 ENV CC_OPT_FLAGS -mtune=intel -march=haswell -Wno-sign-compare RUN tensorflow/tools/ci_build/builds/configured CPU RUN cat .tf_configure.bazelrc RUN bazel build --config = opt  //tensorflow/tools/pip_package:build_pip_package RUN bazel-bin/tensorflow/tools/pip_package/build_pip_package /tensorflow This of course implies that you’ll want to actually build TF with a “ docker build ”. This may seem counterintuitive at first (running Bazel in the context of “ build run ” will be a more natural choice to some, and in fact will be required for the incremental build), but is actually quite useful as it lets you re-run the build very quickly if no changes have been made, and you don’t have to worry about the build directory. Just remember to “ build run ” with --user option to copy your Python wheels out of the container image afterwards. TensorFlow project structure There are two important top-level directories in the TF project: tensorflow and third_party . The latter contains TF dependencies (which you may want to check out). While the list is rather extensive and some third-party libraries can alternatively be brought in as system dependencies (you may see them inside third_party/systemlibs/syslibs_configure.bzl ), our focus is going to be on the tensorflow directory. It may not be immediately apparent, but most of the TF functionality is, at the lowest level, implemented in C++. This is what the tensorflow/core directory is for. Next, this low-level functionality is exported as a public API to various programming languages inside directories named after each language. Most TF users are familiar with the Python API inside the tensorflow/python directory, but there are also subdirectories for C, C++, Java and Go. Knowing your way around the Python subdirectory can help you find useful pieces of information without the need to seek external documentation. For example, to find the constants used by selu activation, you can look in tensorflow/python/keras/activations.py . Another useful Python subdirectory is debug . If you’ve ever wondered what the computation graph of your deep learning model looks like, then file tensorflow/python/debug/README.md is a good start. There are also some very useful tools inside the (you guessed it!) tensorflow/python/tools directory. Some C++ functions are imported by Python with the SWIG file tensorflow/python/tensorflow.i , which in turn includes *.i files in various subdirectories. As you’ll see, most of these files have an accompanying *.cc with implementation, which in turn include headers from the tensorflow/core directory (and also from the tensorflow/c public API directory). However, SWIG is only used for low-level functions, and TF focuses mostly on high-level operations. These are coded and registered in the tensorflow/core directory as so-called “ops” (look for REGISTER_OP macro; the majority of ops are inside the ops subdirectory). Ops are imported by language APIs using their name. Note that in Python, the spelling of each op is changed, replacing CamelCase with snake_case (for example, ApplyGradientDescent from tensorflow/core/ops/training_ops.cc is imported inside tensorflow/python/training/gradient_descent.py as apply_gradient_descent ). Other language APIs refer to ops using the original CamelCase names. The C++ implementation of each op is coded in the so-called “kernel” (there can be separate kernels for CPU and GPU as demonstrated in tensorflow/core/kernels/fact_op.cc ), which is then mapped to an op with a REGISTER_KERNEL_BUILDER macro. Most kernels reside inside the tensorflow/core/kernels directory. For example, ApplyGradientDescent is implemented in tensorflow/core/kernels/training_ops.cc . Unit tests for kernels are written in Python and reside either inside the tensorflow/python/kernel_tests directory or next to their Python API wrapper, in “*_test.py” files. For example, unit tests for ApplyGradientDescent are coded in tensorflow/python/training/training_ops_test.py . A complete list of ops is available in two locations: the tensorflow/core/api_def directory and the tensorflow/core/ops/ops.pbtxt file. As you can see, TF defines a considerable number of ops which explains the large size of its binary. When building TF, you can minimize its size by enabling only selected ops. This is documented inside the tensorflow/core/framework/selective_registration.h file (note, this is an experimental feature). Interestingly, you don’t need to maintain a fork of TF if you want to add your own custom ops. Instead, TF’s design allows for an external project to extend TF with a new functionality. This is demonstrated in the TensorFlow Addons project . Finally, you may want to check the content of the tensorflow/core/platform directory. There, you can find files not specific to TensorFlow, but rather low-level operating systems or network protocol functionalities. Files shared by all platforms reside in this directory, but there are also several platform-specific subdirectories. For example, if you’re troubleshooting an S3-related issue, there’s an “ S3 ” subdirectory to help you. This code is very well-written and potentially useful outside of the TF project (but please do check the license first!). Finally, for a high-level overview of the TensorFlow architecture, we recommend you check the official documentation . We hope you’ll find this collection of hints useful when playing with TensorFlow or deploying it in your machine learning workflow! Note Neither Yelp nor the author of this post are affiliated with Google or TensorFlow authors. Tweet Become a Machine Learning Engineer at Yelp Want to build state of the art machine learning systems at Yelp? Apply to become a Machine Learning Engineer today. View Job Back to blog", "date": "2019-11-08"}, {"website": "Yelp", "title": "Introducing Bento", "author": ["\n        \n  Tyler Argo, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/05/introducing-bento.html", "abstract": "Today we’re proud to introduce Bento , an open source framework for building modularized Android user interfaces, created here at Yelp. Over the past year, we’ve seen great developer productivity gains and product design flexibility from using Bento on our most critical screens. In this post we’ll explain a bit about how Bento works, why you might want to use it, and where we want to go next. What is Bento? We named this framework after the wonderfully compartmentalized Japanese lunch container. A Bento box is a container with dividers to separate different food items from each other. If you squint really hard and maybe take a step back, it also looks like the way most apps are designed these days: a list of colorful components arranged in a somewhat staggered grid. Many Android apps that have a list-based user interface use a RecyclerView to display their views. At a basic level, the RecyclerView works by referencing an ordered list of data and creating a view on screen for each item of data in that list. That works really well if your list consists of homogenous data types, but can quickly become unruly when you need to manage an unbounded number of data and view types in a list. It also becomes an issue if you need to use the same view type in a user interface other than a RecyclerView, such as a ViewPager or ListView. Bento aims to fix these issues by providing a framework to manage the complexity of handling different view types and the dynamic position of each view in the list. Bento can also be used to manage views in other parent view types, such as ViewPagers and ListViews, all while keeping the benefits of RecyclerView best practices (like view holders and view recycling). How Does Bento Work? Bento groups different view types and the logic associated with displaying and interacting with those view types into “Components”. A Component can be anything from a simple text view to a horizontal carousel comprised of other components. At its core, a Component is a self-contained element that provides a data item. An associated ComponentViewHolder class will inflate a view and bind the data item provided to the inflated view. The view holder will also typically bind the Component (or some Presenter ) to the view to handle any user interactions. To demonstrate how a Component works, here’s a diagram of the data flow of a component to be displayed on screen. First, the underlying page view needs to display something, so it asks the ComponentController for a view to render. The ComponentController needs to return an updated view to the underlying page view;so based on the internal list of components the controller maintains, it creates a new ComponentViewHolder by calling getHolderType on the component in the list at the position that the page view needs. This method returns a ComponentViewHolder class which is then instantiated through reflection. Since this is the first time the component is creating a view, the layout needs to be inflated. The ComponentController calls the inflate method on the newly created ComponentViewHolder to create the view. Next, we need to populate the view with data and make sure it will react to user input. The bind method is called on the ComponentViewHolder instance that was created. This method is provided with a data item and a presenter. These are generated through the ComponentController calling the getPresenter and getItem methods of the corresponding Component . The presenter is any object that does some business logic or handles user interactions, In many cases it is the Component class itself. The data item is usually a data class with view properties and strings to display to the user. The view is updated with the data item and event listeners are bound to the presenter. The view is then passed back to the underlying page view to be rendered. The order of a Component in its parent view relative to other components is determined by the ComponentController . This interface is the magic soy sauce in our Bento box that allows us to add, remove, and insert components dynamically into the ordering as if we were manipulating values in a simple list data structure. It also provides an abstraction we can use to apply this functionality to different view types, such as RecyclerView, ListView, ViewPager, and potentially many others. For example, the RecyclerViewComponentController handles the complex choreography of communicating with the RecyclerView class and adapter to determine spans and positions, making it very simple to manage diverse sets of components in a list. We can also create groupings of different components using a ComponentGroup , which is also a Component itself, to keep logical groupings of components together in the list. The Bento framework makes it easy to break down complex interfaces into a set of easy to understand, modular, dynamic, and testable components. An Example Let’s take a look at an example of how to build a very basic component that just renders some text. Here’s an example of a very simple Component class: class ExampleComponent ( private val text : String ): Component () { override fun getCount () = 1 override fun getPresenter ( position : Int ) = this override fun getItem ( position : Int ) = text override fun getHolderType ( position : Int ) = ExampleViewHolder :: class . java } Here we can see we’ve overridden some methods of the abstract Component class. Let’s take a look at each one: getCount - Components can be internally made up of a series of items. In our simple case, we only have one item. Each item in the component at each position can have its own presenter, data item, and view holder type if we wanted, but it’s usually best to break it into different Components , unless all items have an identical view holder and presenter. getPresenter - The presenter is the brains of the component that knows how to respond to user interactions and do other complex state-driven things. In a way, each Bento component is it’s own MVP ecosystem where the Component is the Presenter, the ComponentViewHolder is the View, and the data item is the Model. getItem - The item is the data that’s associated with the component at the specified position. In this case, our data is the text that we want to display. getHolderType - The holder type is a class that is instantiated by the Bento framework through reflection. It’s responsible for inflating the component’s layout and binding the data item to the view. Let’s take a look at our ExampleViewHolder class: class ExampleViewHolder : ComponentViewHolder < ExampleComponent , String >() { private lateinit var textView : TextView override fun inflate ( parent : ViewGroup ) = parent . inflate < TextView >( R . layout . example_component_layout ) . also { textView = it } override fun bind ( presenter : ExampleComponent , element : String ) { textView . text = element } } Much like the view holder pattern we see when using RecyclerViews, Bento’s view holders are separated into an inflate and a bind method. Let’s take a look at what these methods are doing: inflate - Here we inflate a layout file which, at its root, contains nothing but a simple TextView element. We then return that inflated view and store a reference to it in textView so we can use it later when binding data. bind - This method is called whenever an item in the Component is ready to be shown on screen. It is called once for each item as defined by getCount in the Component class. The bind method provides a reference to a presenter and the corresponding data item at the position in the component that this view holder represents. In other words, the presenter argument is obtained from calling getPresenter(i) at some position i. The element argument is also obtained from calling getItem(i) for the same position i. NOTE: The bind method is often called as views are recycled in the list, so performance should be a high priority for this method. Great! So now we have a Component and a ComponentViewHolder that will take some string and bind it to a TextView to show the user. So how do we actually use the component? We need to create a ComponentController that organizes all of the components. For this example, we’ll use the simple RecyclerViewComponentController . Here it is in an example activity: class ExampleActivity : AppCompatActivity () { private val componentController by lazy { RecyclerViewComponentController ( recyclerView ) } override fun onCreate ( savedInstanceState : Bundle ?) { super . onCreate ( savedInstanceState ) setContentView ( R . layout . activity_recycler_view ) componentController . addComponent ( ExampleComponent ( \"Hello World!\" )) } } Here we create a regular activity whose content view layout is just a RecyclerView with an id of recyclerView . We lazily initialize the ComponentController the first time it is referenced, and create it by passing in the instance of the RecyclerView. From there, we call addComponent and pass in a new instance of our ExampleComponent with a text string of Hello World to display. Here’s what the app looks like when rendered: Nice! Bento also has helper classes to avoid boilerplate code. Our Example component class was actually pretty simple, and so we can write it as a SimpleComponent : class SimpleExampleComponent ( private val text : String ): SimpleComponent < Nothing >( ExampleViewHolder :: class . java ) { override fun getItem ( position : Int ) = text } It’s still using our ExampleViewHolder from before, but now we don’t need to worry about the count or the presenter, and the view holder type is specified in the super constructor. There are also many variations of components included with Bento, including a ListComponent for repeating views, a PaginatingListComponent for lazy loading, and even a CarouselComponent for collections of components that can be scrolled horizontally. Features Modular As you can tell from the above example, Components are pretty modular. The component exists as a cohesive whole without any dependencies on the environment into which it’s inserted. That’s nice for a lot of reasons, most of which are covered in this section. Testable Since Components don’t rely on the Android framework, it’s very easy to unit test their logic. It’s also easy to test the logic of a Component view holder using the ComponentViewHolderTestCase that inflates the view and injects it into a testing activity where the data is bound. Then, Espresso can check that everything is displayed properly and the correct methods are called on the presenter during user interactions. From an integration testing standpoint, the bento-testing module provides some BentoInteraction s to test an entire screen of components as the user would see it. Reusable A component created for one environment can be reused across many different screens that are using a ComponentController of any kind. That means it’s easy to drop the same component into a RecyclerView, ListView, or ViewPager. Progressive Bento was made to be progressively introduced into an existing application. You don’t need to rewrite your app from scratch or rethink your entire application architecture. We’ve been integrating it into the Yelp consumer and business owner apps for almost a year now. For example, on the nearby screen of the consumer app, everything below the header (outlined below in red) is a Bento component. We’ve also incorporated some tools to make the transition easier for existing apps. For example, for those of you still stuck using ListViews (this is a judgement-free zone), you can use the ListAdapterComponent to wrap your existing list into its own component, and start converting the items in the list into their own separate components. Scalable Bento is scalable from a technical and organizational standpoint. There’s no limit to the number of components a project using Bento can have. Because it’s easy to keep components isolated from one another, it’s also easy to separate them across modules for faster build times. Since we can use RecyclerViews as a backing view for Bento, it’s also very performant when using a large number of heterogeneous components. More modular user interface components also mean that we can assign ownership of a component to a particular team for maintenance. Instead of one screen being one team’s problem, now other teams can own components on that screen, meaning the weight of software maintenance can be more evenly distributed and bugs more easily triaged. Low Overhead Bento doesn’t use type annotations. That means there’s a very low compile-time overhead since no annotation processing needs to happen. Also, since Bento is only a handful of classes, it has a very small storage footprint. The aar library file is only 105.6 KB. Where to Next? Bento has helped our Android app development scale and allowed us to execute new features efficiently and reliably. But Bento is still growing, and is by no means perfect or complete. We have several areas which we’d like to improve, mostly centered around performance. We currently don’t have asynchronous layout inflation, where each layout is inflated off of the main UI thread. We also don’t have automatic diffing when notifying of a change in a ComponentGroup. Parts of the framework are still written in Java while others are written in Kotlin. That being said, we’re always looking for new contributors to the project! If you’d like to contribute to the project, please check out the repo on GitHub at github.com/Yelp/bento and follow the contributing steps in the readme. Tweet Want to build next-generation Android application infrastructure? We're hiring! Become an Android developer at Yelp View Job Back to blog", "date": "2019-05-02"}, {"website": "Yelp", "title": "Autoscaling AWS Step Functions Activities", "author": ["\n        \n  Devesh Chourasiya, Jingjing Gu\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/06/autoscaling-aws-step-functions-activities.html", "abstract": "In an ongoing effort to break down our monolithic applications into microservices here at Yelp, we’ve migrated several business flows to modern architecture using AWS Step Functions . Transactional ordering at Yelp covers a wide variety of verticals, including food ( delivery/takeout orders ), booking , home services , and many more . These orders are processed via Step Functions, where each is represented as an execution instance of the workflow, as shown below. Figure 1: Illustrative Step Functions Workflow for Transactions Orders Each step in the above workflow is an “activity,” and Yelp implements these activities as batch daemons, which interact with AWS Step Functions via an API integration that fetches tasks and submits activity execution results. We run\nmultiple instances of these batch daemons, which we deploy via PaaSTA , an\nin-house deployment platform. Each workflow execution, i.e., processing of an order, is time bound. We do this\nby enabling the timeout capabilities provided by Step Functions. Each activity\nwithin a workflow needs to be completed within a specified time, such that the\nsum of the time taken by all activities in a workflow is within the limits of\nthe workflow timeout. We also use activity retries , which are configurable per\nactivity, to achieve resiliency in cases of intermittent recoverable failures. Figure 2: Zoom-in view for SubmitOrder activity execution (Step Functions' internal steps shown in blue color) If we zoom in on a specific activity (SubmitOrder in this case) for each workflow execution, each execution will be queued in the ActivityScheduled state until it’s picked up by one of the “activity workers.” Since the total time per activity (including execution and wait time) is bound, tasks with longer wait times get less time for execution. These tasks may need retries, and cascading effects from multiple activities could hit a workflow timeout threshold. As AWS provides aggregated metrics on the wait time for these tasks ( ActivityScheduleTime ), in order to maintain the desired success rate and latencies for workflow processing, we need to have a healthy count of activity workers. Why Activity Instances Need Auto-Scaling Transactional flows at Yelp experience repetitive traffic patterns over the\ncourse of the day and week. Before the new autoscaling system, we used Step\nFunctions cloudwatch metrics to tune the activity instances count to meet\nservice-level objectives. In these cases, we provisioned a static count of\nactivity workers to handle peak traffic, which led to a lot of unused compute\ncapacity during non-peak hours, making transactional flow an ideal use case for\nauto-scaling. Auto-scaling from PaaSTA is based on pyramid uwsgi metrics and CPU usage,\nwhereas Step Functions(SFN) workflows rely on a timely execution\n( ActivityScheduleTime metric ) of the activities. The Step Functions Autoscaler\nbridges the gap between these two systems to manage and control activity\ninstances. Autoscaler Architecture Figure 3: Different components of the Autoscaler and it's interactions\n   with AWS services and PaaSTA The Autoscaler system consists of three components: AWS Services, the Autoscaler\nservice, and the PaaSTA system. At a high level, the Autoscaler first fetches\nthe scaling configuration, then gathers scaling demands from the AWS side,\ncomputes the scaling decision, and, lastly, invokes the PaaSTA api to adjust the\ninstance count. AWS Components We utilize Step Functions Metrics ( ActivityScheduleTime metric) and Cloudwatch Alarms to detect any scaling-worthy events, and SNS and SQS services for\nrelaying scaling messages. More specifically, there are two cloudwatch alarms for each activity: scale up and scale down . When an alarm detects a breaching condition, it sends out\n“ALARM” notifications to be polled by the Autoscaler. It will then send an “OK”\nnotification when the ActivityScheduleTime metric is back to normal. Scaling Brain Upon receiving scaling messages, the Autoscaler will validate the message,\nparse, and comprehend the request, and then compute a concrete scaling decision\nfor a specific activity. It considers two major factors for scaling decisions:\nscaling configurations (e.g., min/max count and scaling gradient) and scaling\nrecord (e.g., the last alarming time and the last scaling time). Figure 4: Steps involved in the scale-up process, highlighting\n   repeated scaling. Design Considerations Repeatedly Scaling When traffic ramps up, the scale-up alarm fires off an “ALARM” notification and\nthe Autoscaler repeatedly scales up the activity workers until the ActivityScheduleTime metric is back to the normal threshold. When traffic\nsettles down, the cloudwatch alarm sends out an “OK” notification. With the “OK”\nmessage, the autoscaler will begin to clean up the previous “ALARM” notification\nand wrap up the cycle of scaling for that activity. Avoid Scale Flapping Flapping (continuous churn of scale-up and -down events) is a typical challenge\nfor any auto-scaling system. Here are a few highlights from our design made to\nhandle this challenge: We support a scale-down cool-off time to prevent two consecutive scale-down\nactions within a certain amount of time. This value is configurable by service\nowners. We validate incoming scaling signals to guard against any malicious, delayed, or\nduplicated scaling notifications. This is achieved through qualifying the alarm\nname, maintaining the last alarm time, and examining the scaling configuration\nbefore every scaling action. Conservative scaling down is based on historical statistics for scale-down\nalarms so that they’re less susceptible to triggers and never occur during peak\nhours. Rollout Story We’ve rolled out the Step Functions Autoscaler in production for 85% of our\ntransaction ordering and have already seen positive results in the first few\nmonths. Figure 5: Graph showing number of activity workers for a given\n   activity in last 7 days. Blue line represent the static count of workers\n   before Autoscaler integration. The above graph shows the instances of number changes for a production activity\nin a one-week range. The blue line is the instance number we would have without\nthe Autoscaler and the orange line is with autoscaling. You can clearly see that\nthere are periods where an activity could use fewer instances, and we’ve seen a\n~34% savings in compute cost per activity per week, even with a very\nconservative scaling down. We’ve enabled autoscaling for ~11 Step Functions activities which processed ~2\nmillion tasks within one week. The entire rollout took about 2 weeks, not\nincluding downtime for the activities. Below are some suggestions and lessons\nlearned as part of this rollout. Revisit historical data for setting cloudwatch alarm threshold We used historical data for the initial values of scale-up and scale-down\nthresholds, and adjusted them accordingly during the rollout phase. Note: there\nwill be a non-zero activity wait time ( ActivityScheduleTime metric) even during\noff-peak hours, so a scale-down alarm threshold should be set accordingly. Gradually reach the ideal minimal instance count with large scale-up increment\nsteps For a safe rollout, we kept the minimum and maximum instance bounds close in the\nbeginning and gradually widened the gap throughout. High scale-up gradients can\navoid degradation during burst traffic, while instance count is kept at a\nminimum. Be aggressive on scaling up and conservative on scaling down We used cool-down times, small scale-down gradients, and larger evaluation\nperiods for conservative scale-downs, as opposed to large scale-up gradients and\nsmaller evaluation periods for aggressive scale-ups. Monitoring Among Step Functions cloudwatch metrics , close monitoring of activities and\nworkflow metrics like ActivityScheduleTime , ActivitiesTimedOut and ExecutionsTimedOut , helped us during the rollout phase. As for next steps, we look forward to rolling out Autoscaler for other Step\nFunctions use cases at Yelp. We’re also exploring proactive scaling strategies\nbased on heuristics like time of day, historical trends, ad-hoc demands, etc. Tweet Join us to build a Commerce Platform at Yelp We are building a comprehensive and APIs driven Commerce Platform that enables teams at Yelp to build Subscription and Transaction products. If you are curious to learn more, Apply here! View Job Back to blog", "date": "2019-06-26"}, {"website": "Yelp", "title": "Maptype — fast doc-value lookups for map data in Elasticsearch", "author": ["\n        \n  Guenther Starnberger, Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2019/07/maptype-fast-doc-value-lookups-for-map-data-in-elasticsearch.html", "abstract": "The ability to quickly serve search results is essential for Yelp. Ranking\nperformance has a significant impact on the time it takes to process a search\nrequest, and it’s crucial for fast ranking that we can quickly look up the data\nthat’s fed into our machine learning models.  Per-document map-like data is\nespecially challenging in this regard, as traditional approaches often read and\ndecode the full map only to look up a single value within that map later on.\nMapType is a custom Elasticsearch datatype that provides an optimized look-up\napproach for such data. Let’s start by discussing a concrete use case using map-like lookups: For each\nstored business that offers delivery, we want to store a mapping from the\ngeobox that the business delivers to, to the average time it takes to complete\na delivery to that geobox. This allows us to provide the average delivery time\nfor a particular address as input to our ranking model. Figure 1 shows how that\ndata is stored in the index. Figure 1: Example of how businesses are represented in the search index. One main concern regarding map storage is how to find an efficient storage\nformat that can quickly search data for a particular map key. In this blog\npost, we’ll start with a short introduction into how Elasticsearch internally\nstores data, and then further discuss how maptype enables low-latency lookups\nfor this particular type of use case. Lucene doc-values While a regular inverted index maps from a term to a document id (encoded in\nthe postings list), doc-values map from document id to doc-value of that\ndocument. Internally, doc-values are stored in a column-store on a per-segment\nbasis. As an example, let’s see how the doc value for the rating of a business\nis represented: Business ID Rating 1 5 2 3.5 3 4.2 Due to the columnar property of doc-values, the actual physical layout looks\nlike [5, 3.5, 4.2] — with each doc-value for a particular field sorted by\ndocument id and placed next to each other. Internally, Lucene uses a set of compression techniques to reduce the amount of\nstorage required for doc-values. For example, in the case of numeric\ndoc-values, Lucene tries to find a common denominator by which it can divide\nall of the values, and only encodes the offset from one number to the next,\nfurther reducing the amount of bits required to store that number. Serializing maps for efficient lookups Our first high-level goal is to find an approach that allows us to serialize\nthe map data-structure into a field that we can store as doc-value. One\nimportant characteristic of our use case is that we usually only look up one\nkey per deserialized map. Because of this, using a traditional encoding format\nsuch as JSON, Protocol Buffers, or Avro is not a good choice, as they would\ndeserialize the whole map, only to have to look up single elements later on. Instead, we would use a format similar to FlatBuffers or Cap’n Proto, as they\nsupport “zero-copy” deserialization. Instead of first decoding the serialized\ndata into memory, they directly execute operations on the serialized data. For\nexample, in the case of a map lookup, only the accessed element would have to\nbe serialized, thereby considerably improving performance in cases where only a\nsingle element is required. Our first prototype was implemented based on Cap’n Proto, but two issues\nconvinced us that a custom implementation would be a better choice. First, we\nwould need to pre-compile our schema. This works well if we know beforehand\nwhich types we want to support, but doesn’t work so well if we want to allow\nusers to choose the types during index creation. Second, we have a very\nspecific use case for which we can optimize our format, compared to general\nencoding formats that need to support a much wider range of use cases. As a consequence, we implemented our own serialization format from scratch, but\nreused some of Elasticsearch’s functionality to, e.g., efficiently encode a\nnumber in VLong or ZLong format. The next sections describe our format and\nimplementation. Maptype format The maptype format is based on multiple layers: The bottom layer provides an efficient way to store variable length\nbyte-arrays in a list and to enable random-access lookups of individual list\nitems. The middle layer uses two lists to implement a map data-structure from a\nvariable length byte-array to another variable length byte-array: The first\nlist stores the keys of the map in sorted order and the second stores the\nvalues of the map in the order that corresponds to the sorted key. The top layer allows us to serialize and deserialize custom data-types (such\nas integer values and geohashes) from and to the underlying byte-arrays that\nare used as keys and values in the lower layers. Encoding a list of variable-length byte arrays There are two cases for encoding a list of potentially variable-length\nelements. In the first case, all variable elements are the same size, so we\nhave a header section (as depicted in Figure 2) that stores (i) the total\nnumber of elements and (ii) the number of bits required by each element. This\ninformation allows us to calculate the offset value when we want to address an\nelement with a given index. Figure 2: All elements have the same length. The second case, as shown in Figure 3, is more complex: If elements have\nvariable lengths, we cannot calculate the offset position based on the index\nand size of each element. Instead, we must use an array of fixed size pointers\nthat point to the last byte of each element. While the pointers are a fixed\nsize, we use the minimum number of bits required to represent the last pointer.\nFor example, if the last pointer points to byte 700, we would use 10 bits per\npointer (which allows us to represent numbers up to 1024). We point to the last\nbyte of each element instead of the first since it’s implicitly known that the\nfirst byte of the first element starts at the position right after the\npointers. Figure 3: Elements have variable lengths. To simplify the handling of this logic in our code implementation subclasses, we\nimplement AbstractList<byte[]> , which allows us to use Java Collections\nfunctionalities such as Collections.binarySearch. Encoding a map of variable-length byte arrays keys to variable-length byte array values The next layer uses two instances of our AbstractList<byte[]> implementation\nto provide a map interface. The basic idea is that we can store sorted keys in\none list and values in the other in an order that corresponds to the order of\nthe keys. If we want to look up an element, we first search for the key via a binary\nsearch in the key list. If we find the key, we look up the value with the same\nindex as the key and return it. Mapping byte arrays to custom data types The top layer allows us to map between types such as strings, geoboxes, and\nbyte arrays used as the key and value type in the underlying map. This can best\nbe explained with a concrete example: Let’s assume the key consists of two elements: A geohash and an integer number\nrepresenting the time of day (as hour from 0-23). The value also consists of\ntwo elements: The time a delivery takes to a geohash at that particular time,\nas well as the average rating (from 1-5) for that delivery. Using JSON during\nindexing and query-time, here’s how an instance of this map could look: { \"9qdex|10\" : { \"avg_delivery_time\" : 25 , \"avg_rating\" : 4 }, \"hmnpx|20\" : { \"avg_delivery_time\" : 15 , \"avg_rating\" : 5 } } We immediately notice two things: First, although the key of the map contains two different elements, it is\nrepresented as a single string. This is required, as we need to be able to\nencode the map as JSON during index and query time. However, on an internal\nlevel, the two elements are split and encoded as their respective types. Second, we assign a label to the elements in the value, representing the value\nas something similar to a struct or namedtuple. This simplifies the usage of\nthe data-type, as fields are available under a descriptive label instead of a\nrandom offset. Internally, the offset added by the labels is negligible: The\nvalues are still stored concatenated to each other in a byte array, but the\norder of the values is given by the sorted labels. Putting it all together — ES maptype plugin The implementation of the ES maptype plugin takes all of the encoding logic\ndescribed in the previous section and wraps it inside an Elasticsearch plugin\nthat stores the encoded byte array as a doc value. Here’s an example schema for a maptype encoded field: \"properties\" : { \"example_field\" : { \"type\" : \"maptype\" , \"doc_values\" : true , \"key_types\" : [ \"geohash\" , \"vint\" ], \"value_types\" : { \"avg_delivery_time\" : \"float\" , \"avg_rating\" : \"float\" } } } To index values to that field, we can directly post the JSON-encoded version as\nshown in the previous section. If we want to access the average delivery time for geobox 9qdex at 10 am in\na painless script, we can access that value via the following statement: doc [ 'example_field' ]. get ( '9qdex|10' ). get ( 'average_delivery_time' ) As our underlying code only implements a Map interface without decoding all of\nthe values into a, e.g., HashMap, this statement only needs to decode the values\nfor the given key: First 9qdex|10 is split into the geohash 9qdex and\nthe number 10, then both values are serialized into bytes with their respective\nencoders. Afterwards, we look up those serialized bytes via a binary search and\n— if they exist — find the bytes for the corresponding key and deserialize the\nkey with the respective decoder for each type. Current status MapType is currently in production use at Yelp and an open-source release of\nthe plug-in is in the works. If you want to learn more about MapType or the\nYelp ranking platform in general, we’ll be hosting a meetup on August 1, 2019\nat Yelp HQ in San Francisco: https://www.meetup.com/Elasticsearch-San-Francisco/events/263053170/ Tweet Join us to build the Ranking Platform at Yelp We are building a reliable and standardized platform for search and ranking applications at Yelp. If you are curious to learn more, Apply here! View Job Back to blog", "date": "2019-07-30"}, {"website": "Yelp", "title": "Yelp Dataset Challenge: Round 10 Winners And Round 12 Announcement", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/08/yelp-dataset-challenge-round10-winner.html", "abstract": "Round 10 Winners The tenth round of the Yelp Dataset Challenge ran throughout the second half of 2017 and we received many impressive, original, and fascinating submissions. As usual, we were struck by the quality of the entries: keep up the good work, folks! Today, we are proud to announce the grand prize winner of the $5,000 award: “Understanding Hidden Memories of Recurrent Neural Networks” by Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, and Huamin Qu (from the Hong Kong University of Science and Technology). These authors developed a visual analytics method for understanding and comparing Recurrent Neural Network (RNN) models used in Natural Language Processing (NLP) tasks.\nDespite their impressive performances and wide use in deep learning problems, RNNs are still “black boxes” that are difficult for humans to understand. The authors developed a method to, amongst others, facilitate the fine-tuning of RNNs by making it easier to analyze what the different layers and units are doing. This entry was selected from numerous submissions for its technical and academic merit by our panel of data scientists, data mining engineers, and software engineers. For a list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Thanks to all who participated! Round 12 Announcement Round 11 just closed (stay tuned for the winners announcement!), and another one opened: round 12 of the Yelp Dataset Challenge is now live. It will run from August 1, 2018 to December 31, 2018. The Yelp Dataset Challenge gives college students access to reviews and businesses from 10 metropolitan areas scattered over 2 different countries. This time around, there are close to 6 million reviews written by about 1.5 million users about 188,500 businesses, as well as 157,075 check-ins and 1.2 million tips left by these users. Moreover, we have added even more photos about these businesses in a separate file, for convenience. With such a trove of data, the sky (or the processing power you have access to) is the limit. Remember, if you are a student, you’ll have the opportunity to win a $5,000 award if your submission is selected as the winner. Want to try your hand at our dataset? Head to yelp.com/dataset to download and use it for personal, educational, and academic purposes. And to see what else we’re up to with Yelp data, check out the Yelp blog’s data section . Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset. These examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Tweet Back to blog", "date": "2018-08-02"}, {"website": "Yelp", "title": "First AWE EU Summit", "author": ["\n        \n  Alina Radu and Yenny Cheung, Security Engineer and Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/08/first-awe-eu-summit.html", "abstract": "At Yelp we pride ourselves on employee inclusiveness and building employee resource groups. We are excited to share with you the learnings of our first Awesome Women in Engineering (AWE) Summit in Europe, a one-day event on diversity and inclusion, leadership, and tech. We have two engineering offices in Europe, located in Hamburg and London. The AWE groups from both offices recently joined forces and co-organized the summit. The event was open to all Yelpers and the sessions were recorded so that we could share them with other Yelpers  who weren’t able to attend. The summit opened up public speaking and professional development opportunities for our AWE members while engaging the whole office on the awareness of diversity and inclusion and the importance of allyship. The summit featured the following sessions: Becoming a software engineer with a non-CS background Speaker: Sen Sun, Software Engineer As the enthusiasm for information technology grows, more and more people enter the field without an educational background in Computer Science. Our speaker, Sen has an urban planning background and is a self-taught software engineer. Sen shared with us how she was drawn to tech during the times of the tech boom, when multiple tech companies went public. To acquire the necessary skills, she started by reading, practicing on online platforms, and following the tutorials of popular web frameworks. One piece of advice she shared about changing to a career in Software Engineering was to get feedback early. For Sen, she applied for programming jobs and interviewed to see what was actually needed in  the job market. She started a part-time job in coding after only two months of studying programming. When it came to getting a full time job, getting her foot in the door required her to be tenacious and she had to put in extra effort in resume building. One way of doing that was creating her personal website, which helped her land her first full-time job in programming. DARs attacks - anatomy of a DAR Speaker: Sophie Matthews, Site Reliability Engineer Everytime Yelp has an outage or when the site reliability is at risk, the Operations teams calls a DAR. DAR is short for Darwin, our CEO’s dog’s name at the time, who once ate through our network cables and caused a panic when our CEO thought the site had gone down! In her talk, Sophie went through her team’s incident response process, how it is applied and how it can be improved. She explained the three types of DARs, when each is raised based on the severity of the situation, and the roles that are assigned to people on the team (coordinator, investigators, communicator). She also described the team’s mandatory and blameless post-mortem process, which is a great learning opportunity for  the people involved in the incident as well as  all engineers at Yelp. Fireside chat Speakers: David Kiger, Engineering Manager and  Bill Hewitt, Engineering Manager Moderator: Verena Berg, Technical Recruiter This panel included the heads of our European offices, experienced managers, and diversity advocates  who discussed their career paths, leadership, and topics around diversity and inclusion. We had a mix of questions from the moderator and the audience. The panelists shared experiences from their careers, what helped them learn and grow, talked about important people they’ve met along the way, and advice for people in engineering or leadership positions. The audience’s questions were focused on the differences between the HQ office and the European offices and how to further support diversity and inclusion in Hamburg and London. Unconscious bias Speaker: Yenny Cheung, Software Engineer In one form or another, everyone is biased. These biases help us make decisions based on our past experiences. However, when it comes to interviewing candidates and working with our co-workers, these bias can harm minorities in the workplace if they are not addressed and corrected. Yenny shared with us how affinity bias (the tendency to warm up to people like ourselves) and confirmation bias (seeking out evidence that confirms our initial perceptions) can be a deterrent for us to hire and retain the best people. For these reasons it’s important that we focus on unlearning them. She also shared actionable steps to help overcome them such as acknowledging biases, putting the facts on the table in the evaluation process, and attributing work to the right person. Microaggressions and how we can promote a more inclusive culture Speaker: Alina Radu, Security Engineer Small things can make a big difference. Alina explained what microaggressions are, how they differ from standard aggressions and why are they unhealthy in our personal and professional lives. She covered a few scenarios where seemingly innocent questions could be perceived differently from  intended. She then discussed the impact of this behaviour and addressed topics like impostor syndrome and psychological safety. Alina encouraged the audience to chime in during the talk and transformed it into a fruitful discussion where people shared personal experiences and lessons. Our Awesome Women in Engineering employee resource group (AWE) aims to build a strong community for women in tech and their allies at Yelp by facilitating professional career building activities, networking, leadership and mentorship opportunities. The AWE Summit in Europe acted as a professional amplifier and offered networking opportunities for AWE members. At the same time, it strengthened the offices’ commitment to create an inclusive culture for all employees. Check out our website if you wish to know more about how Yelp supports women in tech to grow! Tweet Back to blog", "date": "2018-08-03"}, {"website": "Yelp", "title": "The Yelp Production Engineering Documentation Style Guide", "author": ["\n        \n  Chastity Blackwell, Site Reliability Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/10/yelp-production-engineering-documentation-style-guide.html", "abstract": "Documentation is something that many of us in software and site reliability engineering struggle with – even if we recognize its importance, it can still be a struggle to write it consistently and to write it well. While we in Yelp’s Production Engineering group are no different, over the last few quarters we’ve engaged in a concerted effort to do something about it. One of the first steps towards changing this process was developing our documentation style guide, something that started out as a Hackathon project late last year. I spoke about it when I was giving my talk on documentation at SRECon EMEA in August, and afterwards, a number of people reached out to ask if they could have a copy. While what we’re sharing today isn’t our exact style guide – we’ve trimmed out some of the specifics that aren’t really relevant, done a bit of rewording for a more general audience, and added some annotations – it’s essentially the one we’ve been using since the start of this year, with the caveat that it’s a living document and continues to be refined. While this may not be perfect for every team (both at Yelp and elsewhere), it’s helped us raise the bar on our own documentation and provides an example for others to follow. So, without further ado, here’s the… Yelp Production Engineering Documentation Style Guide To make sure we provide consistently good documentation that’s also easy to find, this guide lays out a number of standards to use when writing your documentation. Make Your Documentation Discoverable One of the most important parts of writing documentation is making sure it’s discoverable. This is not the same as searchable; discoverable means that someone should be able to find the document without knowing exactly what they’re looking for. For this reason, Production Engineering has decided that our Wiki will be our primary repository for documentation. Here are a few methods you can use within this framework to make your documentation more discoverable. We also provide a more specific doc for the steps to take when migrating docs from elsewhere to our Wiki. Create a portal. If you’re working on a large project or collecting projects for a team, you can have a portal, like the Production Engineering Home Page, that gives easy access to the most frequently used or most important documents of that project/team. When you do this, try to make sure that the portal is no more than one page, and that you’ve arranged it so that the most essential docs are towards the top of the page (check out the inverted pyramid style ). Group similar documents together. Let’s say you have a new service and need to create a topical guide, how-to, and runbook for it. One way of grouping these so that the connection between them is obvious is to make the how-to and runbook child documents of the topical guide. These should also include links so that it’s easy to navigate between them. In general, the best groupings are based on subject, rather than the type of document. If I’m trying to solve a problem with Puppet, for instance, and I’m looking at its runbook, the other documents I’m probably most interested in looking at next are other Puppet documents. Keep titles short and descriptive. If you’re writing a runbook for the “Helper” service logs, calling it “Runbook: Helper Logs” is much better than “Runbook: Helper 1” or “Runbook: Logs” (even if it’s connected to the Helper service document). Documentation doesn’t necessarily need to be dry and boring, but it should be functional, first and foremost. So, try to avoid giving it a name like “Help! What do I do with Helper Logs?” or something else where the intent isn’t clear. Avoid making titles too generic. If you name your page “Docker,” chances are someone else will choose a similar name, and they’ll all show up in the same search. “Docker for Production Engineering” or “Docker for Service X” is a much narrower title. Also, keep in mind that links will be organized in the left side table of contents alphabetically, so try to make your first word (or the first word after the “How-to:” or “Runbook:” prefix) specific; i.e., “How-to: Load Balancer Configuration” rather than “Howto: Configuring Load Balancers.” Types of Documentation Within Production Engineering, we have a number of different categories for documentation. Each category serves a different purpose and sometimes, a different audience. This determines what should be in the document; regardless of audience and content, all documentation should be in the Production Engineering Wiki space, if at all possible. Topical guides provide an overview of how a system or service works, as well as why certain technical decisions were made. The goal of a topical guide is to give the reader a thorough understanding of the topic at hand. They should come away feeling confident that they understand how the system or service functions and how to further investigate any issues that may fall outside the scope of a how-to or runbook. Configuration files, important directories, Puppet modules, and other components of the service should be discussed (or at least mentioned), as well as any upstream or downstream dependencies. Any complex behaviors should be demonstrated with concrete examples of how the system would behave under various conditions. Diagrams can be helpful, but keep in mind that adding any sort of graphics adds to the load required to revise the document, so use them sparingly. In general, these should be on the Wiki (though the initial draft can be done in Google Docs, which has a better collaborative model; this should then be deleted after the final version is put into the Wiki to avoid confusion). For open-sourced projects, this documentation may be elsewhere (such as readthedocs.org ). How-tos describe how to do common tasks for a system or service. They should be more streamlined than topical guides, directly addressing the steps to actually complete the task(s) described. However, there should also be some context for commands. If relevant, you can talk briefly about why we made the decision to do things this way (especially if you want to reassure the reader under what circumstances this procedure is appropriate, or if the procedure seems counterintuitive). Keep in mind that these documents will probably be the first thing that newcomers read. Follow the inverted pyramid style and address the most important or most common situations at the top of the document. Keep the main document reasonably short (3-4 pages at most); if you need to split the document because it’s too long, keep the most common use-cases in the main document and split the special cases. If the how-to addresses a common topic of interest to outside groups, consider adding a link to the document from the Production Engineering Home Page or one of the sub-portals linked off of that (like the PaaSTA or Puppet Homepages). The title for all How-to docs should start with “How-to:,” as in “How-to: Writing Docs.” Runbooks describe how to diagnose and remediate issues with a system or service. They should seek to address specific questions: How do I diagnose an issue with this service? What does this alert mean and how do I fix it? When organizing the runbook, follow the inverted pyramid style , placing the most common and/or important questions at the top of the page, and avoid making the runbook too long; it should be ~2-3 pages at the most. If it needs to be longer, find a way to break it up, but try to keep related topics together. Do not include long explanations for each action, a sentence or two will do. For anything longer, link to another, more comprehensive technical document. If you’re addressing specific alerts, make sure to include their names in the document so that they’re easily searchable. Specific command lines and expected outputs are good to include in these; avoid screenshots and other large graphics which can make the document too long or disjointed. The title for all Runbooks should start with “Runbook:,” as in “Runbook: Fixing Docs.” Note that How-tos and Runbooks are both more specific versions of the “runbook” type document referred to in the SRECon talk that inspired this post. A Note on How-tos and Runbooks One consideration when writing How-tos and Runbooks is that they should be seen as the first step on the route to automating these processes. Because of this, the more specifics you can include and the more explicit steps you can define, the easier it will be to automate the process. For more on this, see Tom Limoncelli’s ACM Queue article “ Manual Work Is A Bug ”. Writing Documentation General Writing Tips Command Lines Especially when writing runbooks and how-tos, you should include exact command lines that people can actually use. When writing example command lines, you should be sure of two things: that they actually work and that they are benign. The first is self-explanatory, the second means that if someone takes the command line and cuts-and-pastes it into a terminal, it will not cause an unwanted behavior. For instance, if you were writing an example of a command intended to delete a user account, you’d want to make sure your example uses a nonexistent user so that a cut-and-pasted action would not impact any real users. When adding command lines to documents, they should be added in {code} blocks, like so: $ /usr/bin/do_the_thing -o now Linking to Other Docs Linking to other docs is one way you can keep runbooks and how-tos short. However, in order to avoid sending someone down an ever-expanding link hole, try to give a one or two sentence summary of the relevant material in the original doc so that they don’t need to reference another one. In addition, when you link to another document, try to use the title of the document as your link. Using Graphics In general, you want to use graphics sparingly. They take up a lot of space and can make your document longer than necessary. If you do use them, make sure they clearly show what you’re trying to illustrate, and include alt-text for accessibility. You could also consider providing a thumbnail or link to the graphic, which would provide a way for people to see the resource without impacting the surrounding text as much. If you create a diagram, if possible, attach its source to the document you’ve included it in so that it can be easily edited later on. Spelling, Grammar, and Language In general, when it comes to basic spelling, grammar, and language, we follow the Yelp Brand Style Guide. This is an internal style guide written by our Marketing department copywriters that tackles many common language issues, as well as how to use specific Yelp-branded terms. Your organization probably has one too! Acronyms When using acronyms, be sure that its first appearance is in expanded form, so that new readers understand what it means. Jargon Be very careful about using company-specific jargon; keep in mind that your documentation may be the first thing a new hire reads on the topic. If you can use more general or industry-wide language, you should. If you do use company-specific jargon, make sure its meaning is clear. (That doesn’t mean just linking to a glossary.) Documentation Resources This section might not seem that important, but people really like having a place to go to learn more about things, and it can be really helpful for gathering momentum – especially having a dedicated chat channel. Here’s a collection of resources which can help you improve your documentation writing. Having a specific Slack or company chat channel for documentation discussion is also a helpful tool to ensure everyone understands company best practices for documentation. Articles: 10 Tips For Making Your Documentation Crystal Clear (Ben Cotton) A Primer on Documentation Content Strategy (Stephanie Blotner) What to Write / Technical Style (Jacob Kaplan-Moss) Talks: The 7 Deadly Sins of Documentation (Chastity Blackwell) Scalable Meatfrastructure (Alice Goldfuss) Traps and Cookies: A Mystery Package from Your Former Self (Tanya Reilly) Tweet Site Reliability Engineer (SRE) Feel strongly about reliability? So do we. Check out the Site Reliability Engineer positions on our careers page. View Job Back to blog", "date": "2018-10-16"}, {"website": "Yelp", "title": "How We’re Keeping Our Android UI Consistent", "author": ["\n        \n  Nicola Corti, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/10/how-we-are-keeping-our-android-ui-consistent.html", "abstract": "Every day, we ship the Yelp experience to millions of users through our website and mobile apps. Our design team is committed to shaping the best interfaces to make Yelp easier to understand and more practical for users. Writing a review should be as simple as searching for a restaurant or reserving a table on Yelp. The Yelp experience must be consistent across all products and platforms where we ship our applications. For this reason, every app has to comply with our trusty Yelp style guide. This post will explain how we’ve built an Android library around our style guide and will focus on how we design, build, and share our reusable components. Yelp’s Style Guide The Yelp style guide ( yelp.com/styleguide ) is the source of truth that all designers, product managers, and developers use to build our elegant Yelp UI. It’s a collection of common patterns and components that makes frontend development easier and consistent on all fronts. The style guide contains specifications about components, typography, illustrations, and color palettes used for both mobile and web applications. We like to think of it as the alphabet to Yelp’s visual language. The Android style guide is the library responsible for implementing those design specs into real code. This library plays a critical role in our environment and dozens of Android developers rely on it every day. We pay special attention to making sure the components are accessible and also comply with our style guide. Yelp’s Android Style Guide Library The Android Style Guide Library (ASG) is a repository of components and resources available to every Android developer here at Yelp. To better understand this library, let’s start from scratch and focus first on how components are created. This process usually consists of three phases: API Design: Define how developers and designers will interact with the component. Build: Implement the component and make sure it’s in harmony with our ecosystem. Share: Ensure the component is reusable, documented, and frustration-free! API Design The API design phase defines the rules and interactions between our components and our client developers, and is the first step of the component lifecycle. In this phase, we define how flexible our components are and how we can expect developers to interact with them. We’re pretty strict on this step because a method with the wrong visibility (say public ) may result in a @Deprecated method that will end up polluting our codebase. The first question we ask ourselves during this phase is, “Can it be reused?” And if so, how? Adding a new component to the shared library comes with some costs, especially since components have to be flexible and reusable. The ASG is not just a folder with a lot of Android custom views, but a collection of components with well defined use cases, and understanding those use cases is the first step to designing a solid component. Attributes and Styles The definition of attributes sparks a lot of discussion. In its simplest definition, an attribute represents a mutable property of a component. Adding more attributes means giving more freedom to developers and designers to edit the look-and-feel of each component. To make sure our library is consistent, we want to restrict the attribute set so that it provides only those that are absolutely needed. As an example, we can take a look at our user passport , a component used to display a user’s information in our Android app (e.g., on top of a review or next to an uploaded picture): As we can see, there are several mutable fields in this component: User photo Username and description Elite badge Counter for friends, media, and check-ins The above fields will be mapped into one or more attributes inside the library resource file. All attributes will be declared inside a <declare-styleable> block: <resources> <declare-styleable name= \"UserPassport\" > <!-- Control user's name --> <attr name= \"user_passport_name\" format= \"string\" /> <!-- Control user's description --> <attr name= \"user_passport_description\" format= \"string\" /> <!-- Control color tint of the icons and icons count (default orange) --> <attr name= \"user_passport_tint\" format = \"color\" /> ... </declare-styleable> </resources> Example 1: Defining attributes for the UserPassport component We ensure that every attribute is usable from both XML and Java/Kotlin: <UserPassport android:layout_width= \"match_parent\" android:layout_height= \"wrap_content\" app:user_passport_description= \"...\" /> Example 2: Setting the description attribute from the XML var description : CharSequence get () = descriptionTextView . text set ( value ) { descriptionTextView . text = value // .isGone is defined in Android KTX core descriptionTextView . isGone = value . isBlank () } Example 3: The description attribute from the Kotlin code point of view Occasionally, updating an attribute may trigger several side effects. In Example 3, we can see a setter for the description property. When setting an empty description, we also want to hide the TextView (updating the .isGone property). This kind of logic should only live inside the setters for every attribute, and shouldn’t spread around the component code. This helps us keep our components clean and organized, and is practical since our components’ constructors share a common structure: init { // Inflate the layout and retrieve the views. context . withStyledAttributes ( attrs , R . styleable . UserPassport , defStyleAttr , R . style . UserPassport ) { description = getText ( R . styleable . UserPassport_userPassportDescription ) ?: \"\" // Other attribute initialization here. } } Example 4: A section of the UserPassport constructor; we’re also using Android KTX to keep our constructors clean and more idiomatic We generally adhere to the following four steps: inflate the layout, bind the views, retrieve the attributes, and call the setters. In Example 4, we’re actually retrieving the attributes and calling the description setter at the same time. We also need to define the component’s styles . A style allows us to fix a value for one or more attributes of a component. We want our components to come with a good set of styles to cover all major use cases. First, we define a default style for every component. As the name suggests, this style will be applied by default whenever a component is used. As a result, developers won’t need to provide a value for all the attributes, just the desidered customizations. On top of the default styles, we provide other styles to address each essential use case. A good example is our UserPassport component and its styles. The .White style defines how the passport should look in a dark environment (like a media viewer). Thanks to the Android styles’ dot notation , the .White style will inherit all the attributes from the parent attribute (UserPassport in this case) so we don’t need to redefine all the values, but just override the one needed to obtain the desired appearance. <!-- The default style for a User Passport --> <style name= \"UserPassport\" > <item name= \"user_passport_tint\" > @color/orange_dark_interface </item> ... </style> <!-- White style, suitable to be used in combination with a Dark theme  --> <style name= \"UserPassport.White\" > <item name= \"user_passport_tint\" > @color/white_interface </item> </style> Example 5: Two of our UserPassport styles Defining attributes and styles plays a fundamental role in the API Design phase. When defining attributes, we are essentially defining the features of every single component. This will have a deep impact on how flexible our library is, and to which degree of freedom we want to provide the client developers. Colors and Icons When defining styles and attributes, we often have to fix colors and icons that will be used by our components. This step is very critical: assets and hex color strings can be lost among Slack messages, and exporting icons for every density and platform can be tough and can cause us to easily lose consistency. To overcome these kinds of issues, we developed two tools to automate this process: Yelpdesign and Yelpicons. Yelpdesign is a tool used to automate the handoff of designer “tokens.” A designer token is basically a constant that can be used by frontend developers, such as a color or a padding value. All tokens are bundled together into archives that can be consumed by every platform. E.g., for Android, we obtain an .aar containing a set of resource files. Yelpdesign is the tool we use to convert the color palette defined in our style guide to a resource accessible from the Android environment. As you can see from the style guide website, we indicate which colors are safe to use for mobile and which will be exported in a colors.xml file. Yelpicons is a tool used to automate the handoff of assets. Yelpicons works in a similar way as yelpdesign: illustrators upload SVGs to Yelpicons, which get bundled for every platform. For Android, assets are created for every density and placed in the proper drawable-*dpi folder. You can read more about how we automate these process in this blogpost from our Yelp design team: Automating Consistency . Build Once the API for our component is defined, it’s time to implement and integrate it into the ASG library. When adding a new component, we want to make sure the component is well documented and tested . The component should also work well with the Android Studio’s designer preview tool. We ultimately want every widget to work out of the box with just a simple drag-and-drop action. We want every developer contributing to the library to be aware of the impact of their pull requests. For this reason, we use a simple template to populate each pull request with a set of questions that every developer must answer (e.g., “Have you added tests?” “Have you documented your component?”). This could come off as a weak form of enforcement, but in actuality is already catching a lot of common mistakes and in the process, is educating developers on how to contribute. The ASG library is hosted on its own repo and exposed to client developers as a maven artifact through our internal repository. We use semantic versioning to inform developers if the next release contains new components, bug fixes, or breaking changes. Some of our components are designed to replace the Android framework component (e.g., we have a custom button). For those components, it’s good practice to write a custom lint check , which warns developers to use their style guided counterpart rather than the framework component. Lint checks turn out to be really handy. Since they’re integrated into Android Studio, the developer’s code is immediately highlighted to mark the warning. From our experience, lint checks are a great tool to advocate for new components. Furthermore, they’re integrated into our CI system and run on every build. In our infrastructure, we set all the warnings as errors, and abort all builds that return errors. This means that a developer’s build will be broken if they don’t use the proper component. This approach could sound a bit stricter than necessary, but is actually a great way to encourage library usage. Adding a new lint rule using such strict settings could sound hard. Let’s say that we add a new button with a lint check to raise a warning for every usage of a legacy button. We have hundreds of buttons in our codebase, so if we run this lint check, we will break the build for everyone. To overcome this problem, we use a lint baseline file. A baseline file is a snapshot of all the current lint warnings. Lint will check if the warning is contained in the file and suppress it if necessary. This allows us to add new lint checks easily and without breaking legacy code. lintOptions { abortOnError true warningsAsErrors true lintConfig ( \"lint.xml\" ) baseline ( \"lint-baseline.xml\" ) } Example 6: One of our lint configuration blocks Share After we’ve coded the component and made sure it works properly within our infrastructure, it’s time to share it with the developer and designer community at Yelp. Lint checks are a great tool to enforce usage, but components should also be easy to find and use. First, we need to provide proper documentation . Every component detail should be documented, in particular: Attributes and styles should be documented with an XML comment Classes should be documented with a KDoc/Javadoc Public methods should be documented with a KDoc/Javadoc comment We use Dokka to build our documentation from the Kotlin/Java files. The generated documentation will then be populated with comments from the XML to make sure all attributes/styles/code/methods are on the same page for every component. This will allow every developer and designer to understand the particular capabilities of each individual component. Someone once said “A picture is worth a thousand words.” This is why we also include several screenshots with every component to aid in understanding the different styles and attributes of each one. They’re also a great tool to improve discoverability and are added to the same widget page. Finally, we also developed a small companion app called the styleguide test app . This application is a repository with a list of all available components. This is the best tool to see all of our components in action, and especially to appreciate all the animations and see how they actually look on a real device. Our test app is also rich with playgrounds that designers can use to see how every attribute change transforms the component. Conclusion In this blog post, we presented our Android Style Guide Library, a collection of reusable components used by our Android developers. We followed the lifecycle of every single component to better understand how to fix the nitty gritty details and build great, reusable components. While building a components repository is no easy task, we’re deeply committed to achieving our final goal: providing a solid and consistent Yelp experience to our users! Tweet Software Engineer - Mobile Android Come join us in helping build an awesome Android experience at Yelp View Job Back to blog", "date": "2018-10-08"}, {"website": "Yelp", "title": "TTL as a Service: Automatic Revocation of Stale Privileges", "author": ["\n        \n  Aaron Loo, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/11/ttl-as-a-service.html", "abstract": "Security and usability are often at odds with one another, a fact that is best\nillustrated by access control. Deny everyone, and you’ll have a super secure\nsystem that no one can use; allow everyone, and you’ll maximize usability at\nthe cost of security. The Principle of Least Privilege exists to balance both security and usability by giving users only the minimum\namount of access they need to do their job. This reduces the attack surface by\npreventing attackers from leveraging a compromised user’s important, albeit\nunused, privileges for vertical/horizontal escalation. The Problem That said, there are a few key reasons why least privilege is hard to enforce : No one asks for their access to be taken away. Developer velocity is important to us. As long as there’s an audit trail,\nwe generally allow people access to the resources necessary to do their job.\nHowever, once their task is complete, be it one day or several years later,\nthey move on. This often results in people accumulating access like scout\nbadges throughout their tenure. It’s much more common for people to complain about not having access, rather\nthan having too much access. That’s just human nature. There is no singular governing system of access control. Enterprises are comprised of many different systems, including external\nvendors and internal builds/hosts, and each of these may have its own\naccess control management system. An employee’s holistic set of\nprivileges includes their access to each one of these different systems. While it may technically be possible to have one single, centralized system\nthat maps every user to all things they have access to, this can quickly\nbecome unruly given the level of granularity that a solid access control\nsystem should provide. Audits are painfully manual . Manual audits are a necessary burden to ensure that the state of the world is\nas we know it to be. However, they are also very time consuming and becoming\nincreasingly more difficult to scale as the amount of privileges grows in a\ncompany. The Solution To address this issue, we designed “TTL-as-a-Service” (Time-To-Live): a\nsystem to identify and flag users and their stale privileges . The premise is\nsimple: if you haven’t used your access in X days, you probably don’t need it\nanymore and won’t notice if we take it away. At its core, this requires two things: Knowledge of every time a user has used a given privilege. The ability to revoke access upon detecting staleness. Embracing the UNIX philosophy of portability and minimalism, this system is designed to simply ingest logs, then\nperform a daily scan to process and identify stale privileges. Upon detecting\nstaleness, it will fire off an alert to execute custom integrations or\nautomatically generate tickets to revoke the identified user’s stale privilege. The architectural diagram below provides a clearer image of our implementation: We use Splunk as both a log ingestor and alerting mechanism, powered by savedsearches . Splunk ingests logs from various sources, including our access\ncontrol system and osquery. These upstream log providers are also configured\nto log upon permission usage. On a daily basis, our customized saved searches are triggered to perform two\nthings: Aggregate daily use and throw them in a summary index . This allows us to perform more efficient searches, since we merely\n need to know whether a person has used a permission in a given day,\n rather than every single time they use it. Query the last X days to detect new stale permissions. This rolling report is the essence of this solution, as it enables\nus to minimize the amount of manual effort necessary for periodic audits\nthrough automation. If stale permissions are detected, actions are automatically triggered. By\ndefault, this results in JIRA ticket creation, however, it can also be\nuploaded to an S3 bucket for downstream consumption. An example of downstream consumption is a batch worker for our access control\nsystem. On a daily basis, this pulls the latest changes from S3 and\nsubsequently revokes access for the (user, permission) pairs listed. This system can be easily applied to a variety of different access\ncontrol systems by merely feeding the access log and receiving\nactionable alerts. These alerts can be further expanded through optional custom\nintegrations that read from S3, and revoke privileges appropriately.\nHolistically, this allows us to assert that anyone with a given privilege has\nactively used it within the last X days. Issue: Cold Start The Cold Start issue occurs when a system has not processed enough data to make accurate judgements\non an individual user level. In the case of least permissions, this occurs when\na user is first granted new permissions, or when a permission is exercised\ninfrequently or irregularly. How do we know when the right time is to remove\na privilege with no prior knowledge of expected use cases? To address cold start issues, we leverage anomaly detection techniques and try\nto bootstrap our knowledge of an individual by comparing their permission usage\nagainst the rest of their team and the company as a whole. For example, we\nattempted to identify “unusual” permissions by aggregating a given team’s\npermission set. If 95% of the team has a given permission, it would suggest\nthey need it for their job. On the flipside, if only 1% of the team has a given\npermission, it might suggest an anomaly that should be more closely investigated. With the additional assistance of on-the-ground managers to process and validate\nthis data analysis, we’re able to answer the following questions: Which employees currently have privileges they should not need to do their job? Given these usage statistics, what seems like an appropriate upper bound for a\npermission to be considered stale for the entire team? Though we’re unable to completely avoid manual processing, this solution has\nhelped ensure that we only have to do it once. For future potential improvements,\nwe can also train a machine learning (ML) model to better improve the performance\nof our statistical analysis. Issue: Other Edge Cases No project implementation is complete without a few hiccups along the way. Some\nedge cases to consider include: Emergency-only Privileges There are certain privileges that are only used in an emergency or rare,\ntime-sensitive situations. By definition, these will be flagged by the system\nas “stale,” yet may not be advisable to be removed if it would require\nadditional overhead when they’re actually needed. However, this varies from case to case and is implementation-dependent, as\nit depends on the system’s ease of acquiring a permission when necessary. Periodic Usage Some activities are only done periodically, e.g., once a quarter. By definition,\nthis may also exceed the X days configured for your staleness definition.\nTherefore, depending on your implementation, you can either revoke immediately\n(requiring the user to request the permission again every period) or create an\nexception for these privileges. In general, we found that a smooth, auditable process to quickly and securely\nreinstate an employee’s privileges was incredibly helpful, allowing us to be\nmore aggressive in revoking privileges. For example, if it only takes a couple\nhours to restore revoked privileges after ninety days of non-use, people are\nmore willing to give up stale access. Takeaways The ability to quickly provision user access is important, especially for a\nhigh growth company. In the same way, it’s important to be able to quickly\ndeprovision users when access is no longer needed. Unfortunately, the latter\nis a lot harder to manage and scale. Using this system, we’re able to identify and subsequently revoke stale\nprivileges without hindering developer velocity. This allows us to confidently\nassert that users will not have unused access longer than X days, thereby\nsystematically enforcing least privilege with minimal manual effort. Finally, through the process of building and rolling this out, we learned that\nit is also beneficial to have a smooth, speedy process in place for restoring\nrevoked privileges, as it will reduce friction when trying to establish this\nnew process. Contributors I would like to credit the following people (in alphabetical order) for their\nhard work in building this system and in continuing to bolster Yelp’s security. Aaron Loo Joey Lee Kevin Hock Tweet Security Engineering at Yelp Want to build automated systems to reduce manual effort, and help keep the Yelps secure? Apply to join! View Job Back to blog", "date": "2018-11-19"}, {"website": "Yelp", "title": "Joinery: A Tale of Un-Windowed Joins", "author": ["\n        \n  Vipul Singh, Technical Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/12/joinery-a-tale-of-unwindowed-joins.html", "abstract": "Summary At Yelp, we generate a wide array of high throughput data streams spanning logs, business data, and application data. These streams need to be joined, filtered, aggregated, and sometimes even quickly transformed. To facilitate this process, the engineering team has invested a significant amount of time analyzing multiple stream processing frameworks,  ultimately identifying Apache Flink as the best suited option for these scenarios. We’ve now implemented a join algorithm using Flink, which we’re calling “Joinery.” It is capable of performing un-windowed one-to-one, one-to-many, and many-to-many inner joins across two-or-more keyed data streams. So, how does it work? In the simplest terms, developers provide a config file describing the desired join, and the Joinery service executes a joined keyed output stream. Background: What Problem Are We Trying to Solve? Since the advent of streaming pipelines, the gap between streams and tables has been greatly reduced. Streaming pipelines allow for computationally intensive data operations like joins, filtering, and aggregation to be performed on high throughput data streams. While most streaming pipelines support joins within time-bounded windows, there are many that require joins on un-windowed data. One such use case is Salesforce . Salesforce is a downstream data store we use at Yelp to empower sales teams. It contains data about the businesses supported on the platform, such as purchased advertising packages and business owner profiles. The data is stored in separate tables in a relational database, but is also denormalized in Salesforce to help prevent expensive real-time join operations when sales people need access to data on-the-fly (e.g., while pitching to clients). To support this use case, we implemented a real time stream joiner that joins data across multiple data streams and presents the normalized tables in the relational database into one stream that feeds into the denormalized table in Salesforce. In the figure below, each inbound stream represents a table in the relational database. The stream joiner consumes messages from these inbound streams and creates fully joined messages based on a key before publishing them to outbound streams. For example, in the stream joiner below, the key used to join messages is the business-id, which represents the primary key of the business and advertisement tables and the foreign key of the business owner table. Previous Approach Historically, Yelp Engineering has built Paastorm spolts to solve similar problems. However, when datasets grew to the tens of gigabytes, spolts incurred a higher maintenance cost to  recover. Another issue was that they were not designed for stateful applications, so using Paastorm spolts for stateful solutions meant having to implement state management from scratch. To cite an example, one spolt that uploaded results to Salesforce stored several tens of millions of messages at any given time, and in case of a crash, took several hours to recover! This resulted in delays in the overall pipeline and required manual intervention, which ultimately hampered engineering productivity. This historical use mandates that any approach to joining unbounded streams must scale to be fault tolerant. A Join Algorithm? Our past experience in building data pipelines and aggregation led us to the following joiner algorithm: Algorithm: Shuffle/sort messages into equi-join partitions based on message keys. Insert every message into its corresponding hash table within the multi-map . Construct the output by taking the Cartesian product of all the multi-map’s lists. Filter and project based on what’s required in the final output. The above algorithm can be summed up into three key parts: Update phase Join phase Projection phase Let’s discuss these phases in more detail. Update Phase For each input, the algorithm creates a hash table of schemas, and then maps datapipeline messages to keys in these streams. For every new incoming message, we check the message type (analogous to MySQL LogType - log, create, update, delete) and apply the create/update/delete messages to their corresponding hash tables. Join Phase Next, we probe the above hash tables to generate a cross-product of all messages. This generates all possible permutations of the new message with the tuples of the other relations. The joined messages are then published to the target stream. Note that a joined message (one for each row of the joined result) is published to the target stream when there are inbound messages (one on each inbound stream) with the same key. The join phase of the algorithm here performs an inner join. Projection Phase During creation of the output message, aliases can be used to project fields in the output schema to prevent naming collisions. Fields can also be dropped entirely if unnecessary to downstream consumers. This algorithm only works on log compacted, schematized keyed streams. Using a log compacted stream prevents unbounded growth and ensures that a consumer application will retain at least the last known value for each message key within the kafka partition. These constraints imply the algorithm works with data change log streams as opposed to regular log streams. In the diagram below, the input streams are represented on the left, with messages coming from different input sources. The figures depict the cartesian product computed for the input streams. In the join phase, we perform stream aggregation that emits a tuple when records with the same key (id in this example) are detected from the input sources. In other words, the algorithm checks if the keys in the input stream have a mapping in all hash tables (streams), and only if there is, move to the projection phase. This schematic illustrates how the algorithm emits records: This Is Cool, but What About the Memory Footprint? Since Joinery computes joins on unbounded streams, its internal state could potentially grow very large. Having a large in-memory state is costly and does not allow fast recovery. To alleviate this, Joinery keys data streams by different keys, which helps distribute the memory footprint across nodes. However, this doesn’t necessarily keep the state size from growing beyond the total available heap memory on its nodes, which may lead to OOM errors. Therefore, we needed a way to spill data to disk while maintaining a relatively low memory footprint. By utilizing Flink’s incremental checkpointing with RocksDB, we can persist the application state to an external storage. This results in a low memory footprint and allows for a faster recovery time (as compared to our spolt implementation), in a matter of minutes. For a more thorough understanding of Flink and RocksDB, check out this article . So Far So Good, but Do You Have an End-To-End Example? Let’s talk about a hypothetical scenario where Joinery joins two streams: user review and business. user review:\n  - biz_id\n  - content\n  - review_id\n  - user_id user review stream business:\n  - business_id\n  - name\n  - address\n  - state business stream We want to generate an output stream that joins the above two streams based on the business id. The Joinery configuration for this would be as follows: join:\n    - schema_id: 12345\n      join_keys: [biz_id]\n      exclude_fields: [content, review_id]\n    - schema_id: 23143\n      join_keys: [business_id]\n      aliases:\n        - from: business_id\n          to: biz_id\n      exclude_fields: [address, name]\n  output:\n    namespace: joinery_example\n    source: business_review_join\n    Doc: Join of business table and review table\n    pkey:\n      - business_id Joinery Configuration The above configuration guides Joinery to join the two streams of the biz_id key across the input streams. One important thing to note here is that even though we don’t have the same key names in both streams, we can utilize aliases to map keys (similar to traditional SQL aliases). An example of this join is provided below: Future Work One of the main challenges we’ve faced and are looking to tackle in the future is maintaining data integrity during upgrades and state migrations. A truly robust streaming application deployed in production should be resilient to restarts and state recovery should work consistently without any significant time lags. Blackbox testing and auditing an application like Joinery is hard. Yelp has built tooling like pqctl (custom docker compose environment) that helps infrastructure teams have testbeds to implement repeatable, simple unit tests. By leveraging this tooling and developing an extensive acceptance test suite, we look to test more end-to-end joins while inducing failures scenarios. Some of this is in progress, but there is still more work to be done to ensure that we can repeatedly verify states after restarts, particularly on version upgrades of Joinery. Appendix: MJoin algorithm Acknowledgements Thanks to Justin Cunningham, Semir Patel, Alexandru Malaescu and Sharvari Marathe who contributed to this project, in addition to members of the Stream Processing team for their advice and support. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Become an Engineer at Yelp We work on a lot of cool projects at Yelp, if you're interested apply! View Job Back to blog", "date": "2018-12-11"}, {"website": "Yelp", "title": "Scaling Collaborative Filtering with PySpark", "author": ["\n        \n  Shafi Bashar and  Alex Gillmor,  Machine Learning Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/05/scaling-collaborative-filtering-with-pyspark.html", "abstract": "Here at Yelp our core mission is to connect users with great local businesses. On the engineering side, that requires us to tackle big data and machine learning problems. One of the natural ways to apply machine learning to our core mission is to deliver relevant and personalized business recommendations. This helps us deliver great content to users. For example, we can let users know when a hot and new coffee shop we think they will love opens in their neighborhood through push notifications or features like collections . This is an area of active research and development at Yelp, and there are many promising ways to think about these problems. One of the techniques that we have found to be useful is collaborative filtering. In the first part of this blog series, we present a case study of creating a large scale personalized business recommendations framework using collaborative filtering and Apache Spark and discuss our experience working on this project and lessons learned. There are tens of millions of users interacting with tens of millions of businesses on Yelp. To train a collaborative filtering model of this size, a distributed framework like Apache Spark seemed a natural choice for us. Our tool of choice was PySpark - the Python API for Spark. A widely-adopted approach for building a collaborative filtering model is matrix factorization. The Spark ML library contains an implementation of a collaborative filtering model using matrix factorization based on the ALS (Alternative Least-Square) algorithm. In the matrix factorization model, we start with a matrix in which each user is represented as a row and each business as a column, and entries represent the user’s interactions with a specific business. For example, to find a dinner reservation for a special occasion, user A may swipe through photos of restaurant B of their liking. So the entry corresponding to the row/user A , column/business B , i.e. M AB = 1 . Since a user interacts with only a few of the tens of millions of businesses on Yelp, this matrix is sparse. The objective of the matrix factorization model is to decompose this sparse user-business matrix into a user matrix U (where each row represents a latent factor representation of that user, also known as user-embeddings) and a business matrix B (where each column represents a latent factor representation of that business, also known as business-embeddings) as shown in the following figure: Users can express their interest in different businesses on Yelp through various interactions. These interactions vary and can be either explicit feedback, such as writing reviews or rating a business, or implicit feedback , like checking in at or bookmarking a business, adding it to a collection, viewing a business page, browsing through business photos, or using features like request a quote (RAQ) . In our matrix factorization model, we used the implicit feedback signals because the amount of implicit feedback signals exceeds the amount of explicit signals. Spark’s ALS module worked seamlessly to train the user-business matrix factorization model. The implicit feedback model has a few hyperparameters - rank or dimension of the latent space, regularization parameters, alpha or weight on the positive interaction terms. In order to determine the proper set of hyper-parameters, we performed grid-search hyper-parameter tuning over a subset of the input data. We chose the set of hyper-parameters that maximizes our desired metric in the validation set. Once the matrix factorization model is trained, we can determine a user’s affinity or interest for a business as the inner-product of the user-embedding ( u ), and the  business embedding ( b ) found from the model. user-business affinity-score = u ⋅ b To serve the exact top-K business recommendations for all users, we need to calculate the affinity-scores for all (user, business) pairs. This is a special case of maximum inner product search (MIPS) problem. Serving exact top-K recommendation efficiently is still an active area of research. Spark ML has a built-in function to calculate the exact top-K item recommendations for all users. In our experience, however, the function doesn’t  scale well for our dataset. To predict for all users, the built-in function took a significant amount of time. We also found the function would frequently throw OOM (Out of Memory) errors. Challenges While digging into the source code of the built-in function , we discovered a few potential issues: To get all combinations of (user, item) pairs, the function uses the crossJoin operation between user DataSet and item DataSet. The crossJoin operation is very expensive, resulting in lots of shuffling and is usually not recommended. For example, a crossJoin between 1000 user rows and 1000 item rows will produce 1,000,000 = (1000 x 1000) combined rows. Even though the function groups the user and item sets into blocks to reduce the possible combinations per crossJoin operations, for a large dataset like ours, the crossJoin operation can still explode . The function calculates the inner-product for each (user, item) pair individually. Instead, optimized BLAS libraries can take advantage of the cache hierarchy of modern  CPUs/GPUs when performing block matrix multiplications. To solve these problems, we implemented a top-K prediction algorithm in PySpark using  a block matrix-multiplication based technique as shown in the figure below: The idea behind the block matrix multiplication technique is to row-partition the tall and skinny user matrix and column-partition the short and wide business matrix. Once partitioned, we can parallelize matrix multiplications over these partitions. We decided to use PySpark’s mapPartitions operation to row-partition and parallelize the user matrix. Each spark executor (located in worker nodes) will then operate on a partition, aka a chunk of rows from the user matrix. The next challenge is to find  a mechanism to make the business matrix available across all worker nodes without using the expensive crossJoin operation. One possible solution is to use the broadcast operation to distribute the business matrix. However, to use broadcasting, the entire matrix first needs to be loaded in the memory of the master node. In our use case, this wasn’t an option because the business matrix was too large in size. Instead, we copied the business matrix file directly from S3 to the worker nodes using the addFile operation. For a given number of worker nodes, the  trade-off we need to be aware of here is the number of executors vs. available resources for each executor. Ideally, we want as many executors as possible so that we can compute large number of recommendations in parallel. However, the more executors we assign to a worker node, the less resources (CPU, memory etc.) each of these executors receive. We want to ensure that each executor has sufficient resources to load the business matrix file in memory and perform fast matrix multiplication. To speed up the computation and reduce the memory requirements in each executor, we can column-partition the business matrix and split it into multiple files (shown in the figure above). During the mapPartitions operation, each executor will then sequentially load these business matrix files one at a time into memory, perform block matrix multiplication with the user matrix block, determine and maintain a rolling top-K recommendations. As a bonus, using PySpark makes it possible to leverage popular python libraries like NumPy and SciPy , which come with very fast implementations of matrix multiplication. With proper setup of the BLAS and LAPACK modules, block matrix multiplication can outperform individual dot product calculation. The pseudo-code of our implementations steps are summarized below: Step 1: Divide and save business matrix into p different files Step 2: Add these files to worker nodes sc . addFile ( BUSINESS_MATRIX_FILES_S3_PATH ) Step 3: Load user matrix row as rdd user_row_rdd = sc . textFile ( USER_MATRIX_S3_PATH ) Step 4: Use mapPartitions operation to calculate the exact top-K business recommendations for all users user_topK_biz_recs = user_row_rdd . mapPartitionsWithIndex ( get_topK_biz_recs_for_users ) Step 5: Save results to S3 user_topK_biz_recs . saveAsTextFile ( OUTPUT_S3_PATH ) The pseudo-code of the logic inside the mapPartitions is provided below: In summary leveraging the Spark ML library, we built a distributed collaborative filtering model for our large user-business interactions dataset. Even though in our experience, the out of the box exact top-K recommendation function didn’t scale well, we were able to leverage PySpark’s highly parallel framework in conjunction with NumPy and SciPy to implement the well-known trick of block matrix multiplication to produce exact top-K recommendations for all users. In the next post in this series, we will discuss our experience with deploying PySpark code in production as well as how we unit test our Spark code. In the meantime, check out some of the ML and Spark tooling we’ve open sourced. Acknowledgements We would like to thank the following for their feedback and review: Eric Liu, Niloy Gupta, Srivathsan Rajagopalan, Daniel Yao, Xun Tang, Chris Farrell, Jingwei Shen, Ryan Drebin, Tomer Elmalem. Tweet Interested in working on Apache Spark and scaling machine learning? We're hiring! Check out our current job openings. We’d like to hear from you! View Job Back to blog", "date": "2018-05-07"}, {"website": "Yelp", "title": "PySpark Coding Practices: Lessons Learned", "author": ["\n        \n  Alex Gillmor and Shafi Bashar,  Machine Learning Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/05/pyspark-coding-practices-lessons-learned.html", "abstract": "In our previous post , we discussed how we used PySpark to build a large-scale distributed machine learning model. In this post, we will describe our experience and some of the lessons learned while deploying PySpark code in a production environment. Yelp’s systems have robust testing in place. It’s a hallmark of our engineering. It allows us to push code confidently and forces engineers to design code that is testable and modular. Broadly speaking, we found the resources for working with PySpark in a large development environment and efficiently testing PySpark code to be a little sparse. By design, a lot of PySpark code is very concise and readable. As such, it might be tempting for developers to forgo best practices but, as we learned, this can quickly become unmanageable. In the process of bootstrapping our system, our developers were asked to push code through prototype to production very quickly and the code was a little weak on testing. As our project grew these decisions were compounded by other developers hoping to leverage PySpark and the codebase. We quickly found ourselves needing patterns in place to allow us to build testable and maintainable code that was frictionless for other developers to work with and get code into production. This was further complicated by the fact that across our various environments PySpark was not easy to install and maintain. Early iterations of our workflow depended on running notebooks against individually managed development clusters without a local environment for testing and development. Test Something by Mocking Everything? Our workflow was streamlined with the introduction of the PySpark module into the Python Package Index (PyPI). Prior to PyPI, in an effort to have some tests with no local PySpark we did what we felt was reasonable in a codebase with a complex dependency and no tests: we implemented some tests using mocks. However, this quickly became unmanageable, especially as more developers began working on our codebase. As result, the developers spent way too much time reasoning with opaque and heavily mocked tests . Our initial PySpark use was very adhoc; we only had PySpark on EMR environments and we were pushing to produce an MVP . To formalize testing and development having a PySpark package in all of our environments was necessary. PySpark was made available in PyPI in May 2017. With PySpark available in our development environment we were able to start building a codebase with fixtures that fully replicated PySpark functionality. PySpark Fixtures One element of our workflow that helped development was the unification and creation of PySpark test fixtures for our code. One can start with a small set of consistent fixtures and then find that it encompasses quite a bit of data to satisfy the logical requirements of your code. In our service the testing framework is pytest . Our main SparkContext fixture: And similarly a data fixture built on top of this looks like: Where business_table_data is a representative sample of our business table. And an example of a simple business logic unit test looks like: While this is a simple example, having a framework is arguably more important in terms of structuring code as it is to verifying that the code works correctly. PySpark Coding Conventions As often happens, once you develop a testing pattern, a correspondent influx of things fall into place. We love Python at Yelp but it doesn’t provide a lot of structure that strong type systems like Scala or Java provide. And - while we’re all adults here - we have found the following general patterns particularly useful in coding in PySpark. Separate your data loading and saving from any domain or business logic. We clearly load the data at the top level of our batch jobs into Spark data primitives (an RDD or DF). For most use-cases, we save these Spark data primitives back to S3 at the end of our batch jobs. Any further data extraction or transformation or pieces of domain logic should operate on these primitives. We try to encapsulate as much of our logic as possible into pure python functions with the tried and true patterns of testing, SRP , and DRY . Be clear in notation. We make sure to denote what Spark primitives we are operating within their names. Spark provides a lot of design paradigms, so we try to clearly denote entry primitives as spark_session and spark_context and similarly data objects by postfixing types as foo_rdd and bar_df . We apply this pattern broadly in our codebase. A pattern we’re a little less strict on is to prefix the operation in the function. E.g. the signatures filter_out_non_eligible_businesses(...) and map_filter_out_past_viewed_businesses(...) represent that these functions are applying filter and map operations. Do as much of testing as possible in unit tests and have integration tests that are sane to maintain. Hopefully it’s a bit clearer how we structure unit tests inside our code base. However, we have noticed that complex integration tests can lead to a pattern where developers fix tests without paying close attention to the details of the failure. So what we’ve settled with is maintaining the test pyramid with integration tests as needed and a top level integration test that has very loose bounds and acts mainly as a smoke test that our overall batch works. Final Thoughts In moving fast from a minimum viable product to a larger scale production solution we found it pertinent to apply some classic guidance on automated testing and coding standards within our PySpark repository. These best practices worked well as we built our collaborative filtering model from prototype to production and expanded the use of our codebase within our engineering organization. Acknowledgements We would like to thank the following for their feedback and review: Eric Liu, Niloy Gupta, Srivathsan Rajagopalan, Daniel Yao, Xun Tang, Chris Farrell, Jingwei Shen, Ryan Drebin, Tomer Elmalem. Tweet Passionate about large scale data problems and high quality software engineering? We're hiring! Check out our current job openings. We’d like to hear from you! View Job Back to blog", "date": "2018-05-14"}, {"website": "Yelp", "title": "A Guide to Software Engineering for the Visually Impaired", "author": ["\n        \n  Abrar Sheikh, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/10/a-guide-to-software-engineering-for-the-visually-impaired.html", "abstract": "Introduction My name is Abrar Sheikh, and I’m a backend engineer on Yelp’s Distributed Systems team. Our team enables real-time data transfers between microservices and different data stores by building streaming infrastructure on top of Kafka using technologies like Python, Scala, and Apache Flink. I suffer from a genetic disorder called Albinism , which is mainly characterized by two things: Lack of pigmentation in the body, resulting in a white skin and hair tone. Severe loss of vision, which limits the ability to perform routine tasks such as driving, reading, or using computers. Growing up, I was fascinated by computers. Luckily my parents led me to pursue a career in software engineering. I’ve faced many challenges along the way, but through deliberate practice and employing some simple tricks, I’ve been able to successfully grow my career. In this blog, I’ll cover the following topics: Understanding what low vision really means. How low vision impacts certain population groups and its effects on society. Common challenges that people suffering from low vision face in education and employment verticals. Simple and effective strategies to overcome these challenges. We’ll start off by discussing which aspects of vision impairment apply to people with low vision, both regardless of profession and in software engineering specifically. This is by no means a comprehensive list of problems nor a complete set of recommendations; this is solely based on my personal experience. My ultimate hope is to convince you that software engineering doesn’t have to be a difficult profession for someone suffering from severe vision loss. What is Low Vision? Snellen's eye chart. http://www.optometrial.com/snellen-vision-chart-downloadable-graphic-free This chart will likely look familiar to most of you. It’s called Snellen’s eye chart and is used by optometrists to measure visual acuity. Essentially, the further down the chart you’re able to read, the better your vision is. A visual acuity lower than 20/70 may be considered low vision, and would likely require someone to use alternative methods to engage in regular visual activity. However, keep in mind that this definition of low vision is subjective and does not universally apply to every individual. Personally, I prefer a more functional definition of low vision: uncorrectable vision loss that interferes with daily activities. In other words, “not enough vision to do whatever you need to do.” This can vary from person to person. For example, a visual acuity of 20/70 would be  perfectly acceptable for a truck driver, whereas a visual acuity of 20/70 would be too low for a medical surgeon. Stats Here are some interesting numbers I pulled from the 2015 American Community Survey and Report published by the National Federation of the Blind: Maximum Education Attainment Unemployment Rate As seen from the charts above, a significant portion of the American population suffering from low vision drops out of the education system early on, and the unemployment rate for this group is significantly higher than those with normal vision. What are some of the reasons for this distinction? Lack of a supportive learning environment in classrooms for students with low vision. Restrictive access to reading and learning materials for students with low vision. Lack of documentation around access to technology for people with visual impairment. Lack of structure and awareness for creating an inclusive working environment for people with visual impairment in a professional work setting. In the following sections, I’ll outline some solutions that have worked for me to overcome these challenges. Education Let’s see what challenges exist in an academic setting for a person with low vision and how they can overcome them. Students studying in classroom. http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2014/20140219114514-1_0.jpg?itok=yyWbC65W Our current education system is centered around classroom learning, and for decades this has been the primary way of consuming knowledge. The problem with this system for someone with low vision is that, in most situations, even with the best possible corrective lenses and proximity to the front of the class, it’s not possible to easily decipher the content on the board or projector. These limitations can be alleviated with some simple strategies: Reach out to your student registrar: They can help facilitate appropriate accommodations like extra time for exams and assignments, access to larger font notes and textbooks, classroom assistants for taking notes, etc. Make your professor aware of your condition: They can get you access to recorded lectures from previous years, allow you to audio record the lecture, or provide access to slides and lecture notes beforehand. Get a monocular : This is a visual aid device that can help read distant texts. Specs on a book. http://eliaspelcastre.com/wp-content/uploads/2015/09/book-min.jpg It can be difficult to read certain materials like books and research papers due to their small font size. This can be remedied by using a good magnifying glass; one with at least a 3x-5x optical zoom should sufficiently offset any reading difficulties. As simple as this sounds, it’s often overlooked because it takes some time to get used to. Reading content on a computer screen can be more challenging than books, as the content on the screen is significantly more dynamic in nature. To elaborate: the layout on a computer screen is less predictable and changes often; text rarely follows a prescribed format and can differ among applications. These factors pose challenges for someone with low vision. At the same time, it’s hard to imagine a career in software engineering without easy access to technology, something that’s become increasingly true across most professional fields. Fortunately for us, the early developers of operating systems built their software with accessibility in mind. Both Windows and MacOS come with their own versions of on-screen magnifiers, which work great. This is how it looks in reality:: Computer screen with magnifier turned on. You’re probably wondering how I’m able to swiftly navigate my way through the screen while using a magnifier? I get asked this question a lot, and it can be easily explained with a driving analogy. Driving with GPS. http://www.navionika.com/images/stories/products/drive61lmt.jpg Imagine you just moved to New York and have no idea what the city’s map looks like. The only way you can get from point A to point B is by using a navigation system. This navigation gives you a broader view of the city while you use the windshield of your car to actually look around and make driving decisions. This is exactly how a person with low vision uses the on-screen magnifier to engage with applications on computers. The zoom area of the screen is analogous to the windshield of the car, while the rest of the screen is analogous to the navigation system. A person with low vision can see a localized area of the screen clearly while still being able to see where it fits in the broader context of the entire screen. Getting Hired When I began interviewing with companies, one of the biggest battles I faced was my own insecurities. I would constantly analyze my limitations and conclude that there was no way in the world anyone was going to hire me. It almost sounds silly to say that now, but the fact is, to some degree we all find ourselves in that same mindset. If that situation sounds familiar to you, this is what I want you to remember: Your hiring manager typically only cares about the answers to the following three questions: Do you have exceptional analytical and problem-solving abilities? Do you have the right leadership skills to drive important business decisions? Are you a strong team player? I know it sounds simple, but in my experience, this has always been the case. One thing I can say with confidence is that your visual limitation has little to no impact on your ability to fulfill these job requirements. If you do need certain accommodations during the hiring process due to your disability, please don’t hesitate to bring it up with your recruiter. From personal experiences, after having spoken to a lot of technical recruiters, I can attest to the fact that your physical limitation is definitely not one of the factors that goes into the hiring decision. When interviewing a candidate with the low vision, the interviewer should keep the following in mind: Ask the candidate if they need extra time. Ensure that all coding questions can be conducted on a whiteboard instead of a computer. How Can Your Peers Help? As a manager , you can help provide appropriate workplace accommodations for engineers with low vision. These can vary from person to person, but here are a few things that I’ve found helpful: Having a desk that’s away from sunlight, as extremely bright environments make it difficult for me to read the screen. Sharing the slide deck with me during each team meeting since I often don’t have visual access to the content on the projector. As a peer , if you’re working on a problem with a low-vision engineer, using a whiteboard for discussions and tickets or documents for sharing problem context makes a huge difference. Engineers with low vision have a certain way of using computers with accessibility turned on; while they can work seamlessly on their own computer, they may not necessarily be able to on someone else’s. As a tech community , we should all strive to follow consistent coding standards, test coverage, and solid code documentation. This is important because while an engineer with normal vision can read about 10 lines of code without moving a visual muscle, an engineer with low vision can only read about three lines in the same amount of time. Recap and Conclusion Gather Your Tools Adopt the usage of assistive devices like monocular and magnifying glasses to gain access to classroom education and reading material. Use an on-screen magnifier to access computers. Practice and Perfect Your Skill Spend time familiarizing yourself with the above mentioned devices, as it’s likely going to take more time for you to reach the same level of efficiency to perform routine tasks as a person with normal vision. Ask for Help It’s in your best interest to make your peers aware of your limitations so that they can step in and help whenever necessary. Demo The content in this blog was first presented at PyOhio 2018. Join Us Yelp fosters an inclusive work culture that brings out creativity and enables self development for all. To learn more about working at Yelp, I highly encourage checking out our Careers page. If my blog post has left you curious and with lots of questions, or if you just want to say “hi,” I’m reachable via twitter at @abrarsheik . Tweet Back to blog", "date": "2018-10-29"}, {"website": "Yelp", "title": "All About Yelp Hackathon", "author": ["\n        \n  Alexandra Phillips, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/11/all-about-yelp-hackathon.html", "abstract": "All About Yelp Hackathon By: Alex Phillips It’s time for our fall Hackathon! At Yelp, Hackathons are two-day events that provide unstructured time for our engineering and product teams to work on whatever may scratch their creative itch! Hackathon truly embodies our company values of “Playing Well with Others” and “Being Unboring,” as it invites us to participate in so many different ways. Engineers have the liberty to work on projects related to or completely outside the box of the Yelp product. We’ve seen many types of projects over the years from music videos and new photo classification algorithms to baking workshops, custom video games, and so much more! It’s a great outlet for collaboration and innovation that really helps foster teamwork and creativity. Ready, set, hack! For the past several weeks, we’ve been hard at work preparing for the final Hackathon of the year: number 27! This year will be our ninth year running, with each year traditionally hosting three hackathons. This pace enables the engineering team to have reliable and regular outlets for their creativity and to take advantage of  several opportunities throughout their career to work on a variety of different project types. We’re particularly focused on the celebration of building something together , and in an effort to recognize that, have come up with six different awards: Useful, Funny, Cool, Hardcore, Unhack, and Spotlight. The Spotlight award in particular rotates its theme every Hackathon; Hackathon 27 we’ll be spotlighting “Inclusion” which is an important facet of Yelp culture. We’re hoping to this inspires a broad range of projects and activities bringing awareness to how important inclusion is in workplace culture. Hackathon planning is a collaboration between our awesome Engineering, Engineering Event Planning, and Engineering Recruiting teams. There’s a lot of orchestration involved in selecting the theme, arranging the catering, helping engineers find or evangelize their ideal projects, designing the swag, and of course, planning the Ridiculousness! Plenty of hacking fuel! In the true spirit of being unboring, Ridiculousness is the center of fun and games during Hackathon. Need a break from hacking? Come on by to paint, build, draw, or play interactive games with your fellow engineers! Team connectedness is something that transcends both our SF and Hamburg Engineering teams and is celebrated by sharing a Hackathon kickoff toast and awards ceremony. Ridiculousness! I’ve had the amazing opportunity of seeing so many unique, creative projects that have been the product of hard work and collaboration of our engineering and product teams. I’d like to share just a few with you! One of my favorite projects coming out of Hackathon is “AWE the Book.” AWE is our Awesome Women in Engineering employee group at Yelp, who champions and facilitates initiatives to improve inclusion and diversity within Yelp Engineering. “AWE the Book” is a collection of interviews from over 60 women in Engineering and Product, with each page speaking to their childhood aspirations, what they love about Yelp, and their pathway into the tech industry. It was an amazing demonstration of people coming together to work on a project they’re passionate about. Read more about it in this blog post ! One useful Hackathon project that’s now embedded into Yelp culture is Yelp Love, an app that allows any employee to send kudos to one or several colleagues at a time. Yelp Love has become the defacto way to say “thank you” to a coworker that really went above and beyond, and it helps us all live by our “play well with others” value. Hackathon Science Fair - Winner of the Hardcore award, Neon Incident Pager One of the most hardcore projects was the “Neon Incident Pager project.” This was a physical neon light and LED display that integrated with our incident paging system to create a bright and eye-catching display when an incident is triggered! This project took on a creative, fun, and yet hardcore challenge to produce something really remarkable! Hackathon Science Fair I grow more and more excited as we head into Hackathon 27 as I’m reminded of some of my favorite aspects of Hackathon at Yelp: meeting new people, learning new things, and building! Hack on! Tweet Engineering at Yelp Want to experience Hackathons for yourself? Join us at Yelp! View Job Back to blog", "date": "2018-11-07"}, {"website": "Yelp", "title": "Performance Improvements for Search on The Yelp Android App - Part 2", "author": ["\n        \n  Tyler Argo, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/05/android-search-perf-improvements-part-2.html", "abstract": "In our previous blog post , we established a performance improvement lifecycle and explored the first step by defining what metrics we wanted to measure and establishing a baseline on which we can improve. In this blog post, we’ll cover how we improved our two metrics of initial render timings and scroll performance when rendering search results in the Yelp Android app. Step 2: Making Performance Improvements Not all changes we made were specific to only one of our goals. Often times, a change that was expected to contribute to one goal also had an effect on the other. We used a few different performance techniques to achieve these goals: Offloading Work Onto Background Threads The main thread (or UI thread) on Android is the most important thread with respect to performance. This is the thread that is in charge of executing drawing functions that render views to the screen. If this thread is busy doing other work, like setting up a network request to relay analytics to the backend, then it won’t be able to send new instructions to the GPU to update the display when that 16ms refresh mark rolls around. It’s a good idea to offload any work that isn’t on the “critical path” to rendering pixels on the screen to another thread. The easiest way to identify the work that needs to be done before your view can be drawn is to look at the work that’s already being done on the main thread before your view is rendered. The Android Studio CPU profiler is a great tool to use for this and helped us identify many potential improvements. Once you’ve identified work that’s being done on the main thread in the critical path to rendering that doesn’t actually contribute to the final result, you can move it to another thread. RxJava makes it incredibly easy to offload work to a different thread pool. Here’s how easy it is: Observable . fromCallable ( new Callable < Void >() { @Override public Void call () throws Exception { // Do work on io thread here. } }) // Specify which thread the callable is to be executed on. . subscribeOn ( Schedulers . io ()) // Specify which thread doOnNext is to be executed on. . observeOn ( AndroidSchedulers . mainThread ()) . doOnNext ( new Action1 < Void >() { @Override public void call ( Void aVoid ) { // Update UI on main thread. } }) . subscribe (); Even though it’s relatively simple to offload work to another thread, there is a tradeoff being made. It increases the complexity of your codebase since you now need to make sure that the work that’s being done off of the main thread is thread safe. Asynchronous View Inflation As client developers, one of the things we can take advantage of for performance gains is the time it takes for a request to resolve on the backend. All those milliseconds (or in some cases, just regular seconds) that we spend waiting for a response from the server is free time for us. We can use that time to prepare as much as possible for the arrival of the response and use pre-loading techniques to make the rendering process as quick as possible. One of these techniques that we employed involved inflating the views used in the search list ahead of time as opposed to letting the layout manager inflate them on-the-fly. Normally, while waiting for data from the backend, no view inflation takes place. Once some data is returned and we add it to the list’s backing data adapter, then all of the views that would be initially visible in the viewport are inflated and bound to the data. Waiting on this inflation increases the amount of time perceived by the user for search results to load and be shown on screen. This also has an impact on scroll performance. As a user scrolls down the list, new views enter the viewport. If the list doesn’t have enough views already inflated and offscreen to be recycled for these new views, then more view inflation needs to occur which impacts scroll performance. This is an issue that affects both of our key performance metrics, but we can kill both of these proverbial birds with one stone by inflating our views ahead of time while we wait for the request! We started by using Android’s LayoutInflater to inflate views without actually showing them to the user by setting “attachToRoot” to false in the inflate method. We then stored these views in manually managed view pools (lists of inflated views) that we could draw from at render time rather than rely on the list itself to inflate them. Unfortunately, the LayoutInflater inflates views on the main UI thread. This meant that even if we were inflating views ahead of time and solving our initial performance issues, we were still introducing dropped frames that were visible to the user as a noticeable stutter in the loading animation. To fix this, we went one step further and offloaded the inflation of each view to another thread using the AsyncLayoutInflater (introduced in API 24). Each call to AsyncLayoutInflater’s inflate method inflates a view on a new thread, freeing up the main ui thread. Each time that this is done, a small overhead is incurred in scheduling that inflation work on a different thread. While it might be practical to do this for a one-off view inflation, creating view pools of multiple views with the AsyncLayoutInflater.inflate() means that the small overhead in passing work onto a new thread for each view eventually adds up and still causes performance issues on the main thread. Instead of making multiple calls to the AsyncLayoutInflater on the main thread (one for each view in the view pool), we offloaded all of these calls to another thread with RxJava. Now we have one RxJava call to offload work to a new thread, keeping our main thread free while the AsyncLayoutInflater can inflate all the views we need it to. The view inflations now have little to no impact on our main ui thread during loading, rendering and recycling of search result views! Now when the response is returned from the server, we can quickly bind the data to the views from the view pools and render them on screen instead of having to wait for the views to inflate before binding and rendering. This improvement alone saved us 50-80ms on initial rendering time of our search results. Reducing Overdraw and Layout Hierarchy Depth When it comes to performance, it’s important that you don’t do any work that you don’t have to do. Sounds pretty obvious right? The tricky part is figuring out what work you don’t actually have to do. One example of duplicate work is painting white pixels on top of white pixels. If the background of the application is white by default, and each of our list items also has a white background, then we’re making the GPU do work that it doesn’t have to do. The more layers there are to paint on screen, the more work the GPU has to do as layers are painted from back to front. Each time the GPU has to draw over a layer that it has previously drawn before we incur a performance overhead. This overhead is known as overdraw . On the left we can see the Yelp search page before any improvements were made to reduce overdraw. Most of the screen is some shade of color which means the GPU is doing extra work to render each frame. To render anything on screen, the GPU has to make at least one pass, which is demonstrated by the color blue. Two passes is green, three is pink and four or more is red. To improve overdraw we removed the background color and drawables on many of our views. Just doing this got us 80% of the way to what you see on the right. The other 20% came from different tricks to avoid overlapping views. Instead of using a background drawable to add a border to each list cell and causing the GPU to pass over the entire cell, we removed the background entirely, allowing the white background of the app to show through and added a small 1dp view to act as the divider. When using a background drawable to add a border, even if only part of the drawable is visible and the rest is transparent, the GPU still needs to do a full pass over the area that the drawable is displayed, contributing to overdraw . By removing the background drawable, the GPU only needs to worry about rendering the view that represents the border, and not the entire background of the list item. To make the measure and layout phase of the Android render pipeline more efficient, we also switched to use a ConstraintLayout for each list item. This change allowed us to flatten the view hierarchy and reduce the number of nested views as much as possible. As compared to the other improvements we made as part of the effort to improve search performance on Android, improving overdraw and flattening the layout hierarchy seemed to have had very little effect on our performance metrics. We tried to measure them at a device level but found too much variance between test runs to claim it as a successful improvement. We expected to see more of an impact as reducing overdraw and layout hierarchy are commonly prescribed methods to improve performance in Android apps, but as we tested we found no significant improvement in the number of dropped frames on scroll or in the time to render search results. Because of this, we decided not to invest more time in these improvements and focus our efforts on larger opportunities. Caching Results in a View Model On the search page, each business in the list is bound to a business model object. This business model is received from the backend as part of the search response. The model does not map 1:1 with what we display to the end user. We do some calculations on the client to build labels with information that’s useful to the user and localized to their device’s locale. For example, we convert kilometers to miles, format strings for proper pluralization and determine how many minutes there are until the business’ closing time. Some of these calculations and resource loading take a significant amount of time (10-20ms) and were done on the main UI thread. We were doing these costly calculations each time we needed to bind the business information to a business list item view, and the performance impact was evident, especially on older devices. Instead of doing these calculations each time a view needed data, we decided to move the results of these calculations into a view model so that they only needed to be done once. We created a view model builder class to handle all of the logic for populating the fields of the view model. This builder takes in a business network model object and returns a business view model object with all of the properties calculated and string resources loaded. We then stored these view models in a cache where each business id would map to a view model object that we could use to bind data to the view. The cache only persists across one page of search results since it’s possible that new properties need to be computed depending on the search vertical. /** This method is called for each item in the list at bind time. */ public BusinessListItemViewModel getViewModel ( YelpBusiness business , Collection < DisplayFeature > features , int position , int offset ) { // The result position that the user will see. int resultPosition = offset + position + 1 ; String cacheKey = business . getId (); BusinessListItemViewModel viewModel = mBusinessListItemViewModelMap . get ( cacheKey ); if ( viewModel == null ) { viewModel = mBusinessListItemViewModelBuilder . setBusiness ( business ) . setListPosition ( position ) . setResultPosition ( resultPosition ) . setDisplayFeatures ( features ) . build (); mBusinessListItemViewModelMap . put ( cacheKey , viewModel ); } return viewModel ; } Not only did this drastically improve scroll performance, but it also made our code a lot easier to test. The view model builder was created to not have any dependencies on the standard Android libraries, meaning we would be able to run unit tests instead of integration tests that require the entire Android app to start and run. Where we previously relied on a handful of espresso tests to verify view binding logic, we now have excellent unit test coverage that covers all aspects of creating a view model from a network business model and runs in a fraction of the time. Here is a comparison of the scroll performance before and after the view model introduction: As you can see, scrolling on the right hand side not only feels faster, it is faster! The GPU profiler showing frame render time bars on the screen illustrates this. We are admittedly still dropping some frames under heavy load, but we are dropping far fewer than before and when we do, they span a much shorter time frame. Through our measurements using the FrameMetrics API, we also found that the percentage of dropped frames while scrolling dropped from 33% of all frames to 17%. When the user scrolls down for the first time, the view model for each business is being constructed for the very first time. This is something that is still pretty computationally intensive. We could probably take this one step further by taking advantage of the time before the user starts scrolling to build the view models ahead of time, but as it is now, the view model is constructed as each new view enters the viewport. While it is quicker than previously on the initial scroll, the performance improvement is most evident when the user scrolls back up the list. At that point the view model for each business is already constructed so no new computations are needed. That’s why you see a much smaller impact (shorter bars) on the right when compared to the left when the user scrolls up. Next Steps for Performance Improvements There are a few more ways we can improve the performance of search on the Android app. In case you didn’t notice, we didn’t provide any timing details for the timespan between the user pressing the button and the search request being executed. Unfortunately, we actually didn’t take that time into consideration when improving our timing and it was assumed to be negligible. Upon further investigation we have found that it’s non-trivial amount of time since it involves waiting for the activity containing the search list to be created. As a next step we want to add this time to the search_results_loaded timing metric and accurately gauge just how much time is spent on the client before the request is executed (represented in green below). To offset this time, we can fire the search request as soon as the user interaction to initiate the search happens. If we do that, then we can do the work to create the activity containing the search list while the request is in flight which would make our diagram look more like this: This makes the search_results_loaded timing metric smaller which means less time waiting for our users! The sooner we’re able to execute the request after the interaction, the more time we end up saving. Another step we could take to improve performance would be to persist the business view models for a longer period of time by taking advantage of our app’s data layer. The data layer follows the repository pattern and we use it to cache the response for many network requests, including search. If we also cached the view models associated with each business in the data layer, then a repeat search could be rendered extremely quickly! Also, if a business shows up in more than one search (say after toggling a price filter) then we wouldn’t need to reconstruct the view model for this business. Conclusion Through our performance work on the Android client, we were able to reduce perceived search performance from 350ms at the 50th percentile and 656ms at the 90th percentile to 190ms and 394ms respectively. The percentage of dropped frames while scrolling to the bottom of the search page also improved from 33% to 17%. We’ve completed step 2 of the improvement lifecycle by implementing some changes that have improved our performance metrics, the next step is to ensure that our improvements won’t be unwittingly undone by future engineers. In the next blog post we’ll cover our client-side performance monitoring and how we can be confident we’re always moving towards a more performant app, even when performance is not at the forefront of our next project! Tweet Want to build next-generation Android application infrastructure? We're hiring! Become an Android developer at Yelp View Job Back to blog", "date": "2018-05-17"}, {"website": "Yelp", "title": "Yelp's Secret Detector: Preventing Secrets in Source Code", "author": ["\n        \n  Aaron Loo, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/06/yelps-secret-detector.html", "abstract": "We are always looking for new ways to bolster our security posture to keep our\nusers and businesses safe. Today, we’re happy to announce that we will be open-sourcing our detect-secrets framework to prevent\nsecrets from being committed to our codebase. This aligns well with our value\nto always Protect the Source and\nadds to our growing collection of secure-by-default frameworks which bolster\nweb security without compromising employee productivity. The Problem Secrets in source code are points of weakness in an otherwise secure system\nbecause they are available to any and all repository contributors, cloned,\ncopied and distributed. And each one of those copies provides authorized access\ninto your system. Can you imagine if your keys to the kingdom are one data leak\nor one mis-configuration away? It’s easy to chalk this up to “better developer education”, but here at Yelp,\nwe know that accidents happen. The question is how can we put safeguards in\nplace so that: It’s harder to accidentally commit a secret, and If something does happen, we become be quickly aware of it, and can patch it\nbefore it is exploited. Build vs Buy This is not a new problem and we hate to reinvent the wheel so we spent time\nresearching what solutions were already available. In our investigations of\nexisting related work ( truffleHog , git-rob , git-secrets , Repo Supervisor ), we discovered\nthey didn’t quite meet our needs. Specifically, we were looking for a solution\nthat: Supports a whitelisted baseline of secrets We acknowledge that there may be existing secrets in our codebase or git\nhistory, however, we didn’t want this fact to block the deployment of this\nservice. We would rather assert that all commits after a certain point of time\nare free from secrets. At a later time, we could review all secrets before\nthis service was deployed, and retroactively roll and improve their storage. Integrates with the Software Development Lifecycle (SDLC) An optimal solution needs to work well with developers’ workflow because\nsecret management is an ongoing process, rather than enforced through\nperiodic audits. To enable this, we needed to make sure our solution was\nfast and didn’t scan files unnecessarily. Is compatible with the pre-commit framework we use We heavily leverage our pre-commit framework to run a suite of static code\nassertions before committing to the codebase. Therefore, an optimal solution\nwould build off this framework, and maintain backwards compatibility with\nour existing hook installations. Is lightweight and modular We need a system that is easy to customize to our needs and fits within the\nlarger ecosystem. Is language agnostic The solution should perform equally on all repositories, regardless of\nprimary language, to support different clients (eg. python, puppet,\njavascript, php, java, etc.) With these requirements in mind, we set out to build our own secret detection\nand prevention engine. The Solution Our solution leverages a client-side preventative method and a server-side\ndetection method to make sure no secrets are accidentally shared. Both methods\nrun through the same secret detection engine so that updates to the engine\nwould conveniently affect both methods. To prevent committed secrets, we leverage our existing pre-commit framework to\nscan through the commit diff for any “secret looking” strings (determined by\nthe engine’s rules). For users who may not have the pre-commit hook installed,\nthis also integrates into our continuous integration (CI) pipeline as a\nprecautionary measure. What about those secrets outside the CI pipeline, and in projects that don’t\nhave the pre-commit hook installed? For these cases, we use the same engine to\ndetect committed secrets by periodically scanning tracked code bases for updates,\nand secrets within those diffs. Through string analysis (rather than examining\nthe Abstract Syntax Tree), we’re able to have a faster, more flexible solution\nwhile also being language agnostic. This ultimately allows us to scan all\nrepositories frequently, increasing our response time without using much\ncomputing power. How We Identify “Secrets” We use regex rules to scan, and help identify the following types of secrets: API Keys AWS Keys OAuth Client Secrets SSH Private Keys …and other high entropy strings, calculated using Shannon Entropy. Naturally, this may cause false negatives with inherently bad passwords , as\nShannon Entropy assumes that there is a component of randomness to the secrets\ndetected. However, with proper employee education, the prevalence of these\nscenarios will be reduced. Takeaways It should be noted that this is not a fix-all solution. This is a heuristical\nattempt at enforcing stronger security practices. However, paired with good employee education, this framework equips a growing\ncompany with tools to more easily enforce better secret management practices.\nWe are excited to share this with the wider community, so that we can\ncollectively make our systems a little safer. Tweet Back to blog", "date": "2018-06-11"}, {"website": "Yelp", "title": "Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch", "author": ["\n        \n  Dmitriy Kunitskiy, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/06/fast-order-search.html", "abstract": "Since its inception in 2013 , Yelp has grown its transactions platform to tens of millions of orders. With this growth, it’s become slow and cumbersome to rely solely on MySQL for searching and retrieving user orders. Transactions at Yelp are built around integrations with dozens of external fulfillment partners across different verticals, from food ordering to spa reservations. In most of these integrations, Yelp takes care of payment processing. Given this, the way we store order details prioritizes facilitating stateful order processing in a uniform way. However, other access patterns like finding popular orders by category or fetching a user’s full order history were not scaling well and were becoming pain points when writing new features. With this in mind, we decided to leverage Yelp’s real-time data pipeline to duplicate transactions data into Elasticsearch, which could provide the enhanced performance and powerful search capabilities we wanted, while being completely decoupled from the critical order processing workflow. This blog post will look at the pipeline to ingest orders in near real-time into Elasticsearch, and the decisions we made along the way. In particular, we’ll look at Elasticpipe, a data infrastructure component we built with Apache Flink to sink any schematized Kafka topic into Elasticsearch. The correctness of Elasticpipe is validated by an auditing system that runs nightly and ensures all upstream records exist in Elasticsearch using an algorithm that requires only O(log(number of records)) queries. Finally, we’ll look at some client-side efforts to migrate to this more performant, eventually-consistent system. The Problem One example of what was wrong with our legacy order search backend was the deteriorating performance of the food order history page. Food order history on iOS Over time, the latency of this page started to increase, climbing to more than several seconds for some users. When we profiled the code, we saw that a lot of time (> 1s) was spent in database lookups: Simplified cProfile trace graph Because order history information is split across multiple tables, each request required a join between the Order table, the OrderLine table (order lines are the individual items in an order, like “2 samosas”), the Address table, and the CancellationPolicy table. This was becoming expensive, especially considering that the order history page was not the only consumer of this data. More and more features across Yelp needed to access order history and, in many cases, the service calls to fetch it were some of the slowest made by clients. We realized that we had been asking MySQL to do double duty. We needed: A relational data store with ACID guarantees to power the complicated finite state machine underlying order processing. A distributed key-value store for quickly retrieving order history that could be used in latency-sensitive, high-traffic applications. On top of this, we became increasingly interested in having a backend that acts as a general purpose search engine with full text search, allowing us to do things like find the most commonly ordered dishes for a restaurant, or even the most popular burger in a particular area. Although some of these problems, like speeding up the order history page, could have been solved by more aggressive MySQL indexing or materialized views, we knew these would be temporary solutions that wouldn’t solve our needs more broadly and wouldn’t grow with our future product goals. It became clear that we’d need a different tool to solve this new class of problems. New Datastore After considering a few alternative NoSQL datastores, we chose Elasticsearch because it could accomodate all of our use cases, from retrieving a single user’s order history, to full text search over order items, to fast geolocation queries. It is also widely deployed at Yelp as the backend for new search engines, so a lot of tooling and infrastructure for it already exists. The most attractive attribute was the flexibility to make new queries that we may not have anticipated when first designing the data model - something that’s much more difficult with datastores like Cassandra . The next step was to determine the schema of the denormalized order document that we’d want to store in Elasticsearch. Although it sounds trivial, it required surveying the current clients and thinking of possible future uses cases to determine the subset of fields that would form the denormalized order from over 15 tables that store information pertaining to orders. The tradeoffs here are clear. On one hand, there was no cost to adding fields, especially if we may need them in the future. On the other hand, we don’t want to bloat the order object with unnecessary information, since: Each new field makes the Elasticsearch document schema more brittle and sensitive to changes in the MySQL schema. Adding a field that requires joining on a new database table in constructing the order document adds latency to the pipeline. We don’t want requests to Elasticsearch to return very large response bodies, requiring us to paginate them when we might not otherwise have needed to. After some discussion, we developed a schema that includes the high level order state, all the order lines, the user id, information about how the user entered the order flow, plus some high level partner details. We chose to exclude details of the business itself, all details of the user aside from the delivery address, and any details about the payment processing. Change Replication Pipeline With the datastore and schema determined, the next question would be how we’d funnel our MySQL data into it. The most straightforward approach might be a batch that periodically queries MySQL for new orders and uses the Elasticsearch REST API to insert them. This has its share of problems, including: The latency between an order being placed and being picked up by the batch. Lack of clarity for how to track updated orders since the last batch run without polluting our MySQL schemas with new flags to mark unflushed updates. Inability to quickly rebuild our data in Elasticsearch from scratch in the case of data loss or corruption. Another solution we considered was writing into Elasticsearch as a part of order processing anytime an order enters a terminal state such as “completed” or “cancelled”. As part of this solution, we would also run a one-time backfill batch to insert historical orders. The most significant drawback was what to do if the write request to Elasticsearch failed. Failing or stalling order processing is clearly unacceptable, but the alternative of ignoring the write failure results in missing data in Elasticsearch. Fortunately, we didn’t have to use these ad-hoc solutions and were able to leverage Yelp’s realtime data pipeline , which persists incoming MySQL data to Kafka by scraping MySQL binary logs. This allowed us to build an asynchronous processing pipeline for Elasticsearch writes out of band with order processing itself. One new difficulty that presented itself was how to make sense of many independent Kafka streams of change events, with each table’s changes written to a different Kafka topic. As mentioned above, orders go through multiple intermediary states, with state changes reflected across multiple tables and written out in large database transactions. As an example: you’re ordering a pizza, but immediately after ordering you decide to add extra toppings and deliver to a friend’s address. In our order processing state machine, credit card transactions are processed asynchronously. An update like this is persisted in MySQL as a new Order row with a “pending” status and foreign key to a new Address row, in addition to a new OrderLine row for the new pizza. In this way, the history of an order is maintained as a linked list, with the tail of the list being the most current order. If the credit card transaction succeeds, yet another Order row will be created in a “completed” state - if it fails, the order will be rolled back by adding a new row which duplicates the order state prior to the attempted update. Here’s how just the first part of this operation, the creation of new Order row in a “pending” state, would look when seen as a series of independent events in the data pipeline: Yelp's data pipeline persists MySQL writes to Kafka Since each high level update can result in dozens of data pipeline events, reconstructing high level order updates and materializing them at the appropriate times is a challenging problem. We considered two approaches: Independently Insert Each Table Stream Select One of the Table Streams as a Signal of High Level Changes Independently Insert Each Table Stream The first approach was to run independent processes for inserting events from each table’s data pipeline topic. Given that we wanted a single Elasticsearch index of order documents, a process inserting events from a supporting table (like Address ) would need to query to find which orders would be affected by an event and then update those orders. The most concerning challenge with this approach is the need to control the relative order in which events are inserted into Elasticsearch. Taking the example of related address and order events in the above diagram, we would need an order event to be inserted first, so that the address event has a corresponding order to be upserted into. Another difficulty with this approach is more subtle: resulting records in Elasticsearch may never have existed in MySQL. Consider an incoming update that altered both an order’s address and order lines in one database transaction. Because of the separate processing of each stream, an order in Elasticsearch would reflect those changes in two separate steps. The consequence is that a document read from Elasticsearch after one update has been applied but not the other is one that never existed in MySQL. Select One of the Table Streams as a Signal of High Level Changes The second approach is a result of analyzing our data model and finding a common pattern in high level order changes: ultimately, they make a change to the Order table. In database terminology, our schema was a Snowflake and Order was a Fact table . Meaning that, if we could figure out how to map events from the Order table’s data pipeline stream to high level operations on the order, we could watch for changes just to this table. When seeing a row created or updated event, we could first check whether or not the row was for an order in a finalized state. If it was, we could query the database for this row’s foreign keys and use that to create a full denormalized order document, ready to insert into Elasticsearch. Ultimately we chose this approach because it was simple and eliminated the problem of partial updates from MySQL being inserted in Elasticsearch. However, this has some downsides: Not easily generalizable. If we introduce an update pattern which affects a Dimension table without affecting the Fact table, this mechanism wouldn’t work. Adds extra read load on the database. Since we don’t utilize the row contents in data pipeline messages except to filter out rows in intermediary stages, each order update ending in a final state results in additional reads to create the order document. The approach led us to design an order ingestion pipeline with two main steps. The first consumes data pipeline events from the Order table and constructs denormalized order documents. These are written to a downstream Kafka topic, which is then read by the second step that writes them into Elasticsearch. This architecture is shown in the following schematic: Ingestion pipeline Order Assembler The order assembler is a Paastorm instance (our in-house stream processor) which consumes the Order table’s data pipeline stream, reads the required additional fields from the database, and constructs order documents for insertion into Elasticsearch. Since we don’t care about maintaining orders in intermediary states, the assembler skips events for order rows in transient states. The resulting documents are written to an intermediary Kafka topic, which we refer to as the assembled_order topic. Having a two-step design with an intermediary topic in between allows us to separate the concerns of assembling the denormalized order in the representation we want and writing it to Elasticsearch into isolated parts, with the latter done by Elasticpipe (described below). Additionally, setting infinite retention on this intermediary topic turns out to be an effective backup in the case of data loss or corruption in Elasticsearch. The assembled_order topic cuts down disaster recovery time by an order of magnitude because writing the entire assembled_order topic to Elasticsearch is a lot faster than recreating that topic from events in the data pipeline Order table stream (a few hours versus a few days). This is because the order assembler processes records one at a time, requiring a database read across multiple tables for every order, whereas sinking records to Elasticsearch over the network can be done in large batches. Lastly, the assembled_order topic is log compacted by Kafka, allowing us to retain only the last order version for each order (although we don’t assemble intermediary order states, an order can go through multiple final states, such as being completed, then updated, and then cancelled). Elasticpipe The last step of the pipeline is a data connector we wrote using Apache Flink . Rather than writing a single-purpose app for order search, we made one that could work with any schematized Kafka topic. Elasticpipe solves the problem of writing all records from a schematized Kafka topic to an Elasticsearch index. Because Flink comes with an Elasticsearch connector that takes care of generating bulk requests and handling API failures, the only remaining parts to solve were: Bridging the possible gap between the Elasticsearch mapping and the schema of the topic we wanted to sink Forming idempotent insert requests Implementing offset recovery on restart In dealing with the potential for divergence between the topic schema and the Elasticsearch mapping, we get help from the following Elasticsearch feature: type coercion . This means that types in the mapping can differ somewhat from those of the Avro schema (we use Avro to serialize Kafka messages), and Elasticsearch will do the conversion at write time. When starting to sink a new topic, we require a developer to manually create a corresponding index in Elasticsearch with a compatible mapping. We also require the mapping to include an extra field to hold document metadata which gets added to every document. For example, an order record is actually indexed as: { \"order\" : { \"total_price\" : 17.95 , \"user_id\" : 12345 , \"order_lines\" : [ { \"name\" : \"meat-lover’s pizza\" , \"description\" : \"sausage, pepperoni and pancetta in a hearty tomato sauce\" , ... } ] ... } \"metadata\" : { \"cluster_name\" : \"uswest2\" , \"cluster_type\" : \"datapipe\" , \"topic\" : \"assembled_order\" , \"partition\" : 0 , \"offset\" : 55 , \"doc_id\" : \"ab12cd34ab12cd34ab12cd34ab12cd34\" } } Storing this extra state in Elasticsearch allows Elasticpipe to do stateless offset recovery on restarts and helps with auditing. Currently, the index creation process is done manually. Since setting the options for how fields are stored and analyzed is critical in obtaining utility from Elasticsearch, we are okay leaving this to the developer for now. However, this process is also prone to human error so we’re considering writing a command-line script that will generate a “starter” mapping from a topic’s Avro schema which a developer could then tweak to suit his/her needs. Schema auto-generation is something we commonly do in other data connectors and have found it valuable in reducing uncertainty about how different datastore schemas interact. For Elasticpipe, we could generate a starter mapping using a type translation table like: Avro type Elasticsearch type boolean boolean int integer long long float float double double bytes keyword string text array nested map object Of these, the array type is the most interesting. By default, arrays don’t work as expected in the Elasticsearch data model because array item fields are dissociated from each other. We faced this problem in the context of order lines, which are an array of the items in an order. Had we used the default behavior, we would have been unable to query for “orders containing 3 meat-lover’s pizzas”, for instance, because the quantity field and name field of an order line item would no longer be associated with each other. Instead, we chose the alternative, which is the “nested” datatype. It creates new documents for each array element, and transparently joins them at query time. The downside is that this is more resource intensive, especially for high cardinality arrays. Turning to the question of how to structure insert requests, we chose to make every request an upsert based on the document id, where the id is the same as the message key in the Kafka topic. This ensures that all index requests are idempotent and that we can replay the Kafka topic from any offset. What about unkeyed Kafka topics? Consider a topic that records some client-side event (i.e. button-clicks). If a developer decides to pipe these to Elasticsearch, we still need inserts to be idempotent. To accomplish this, records from unkeyed topics are inserted under a pseudo-key, which is a hash of the message’s Kafka position info - that is, a hash over the cluster info, topic name, partition, and offset. Lastly, we needed a way for Elasticpipe to pick up where it left off after a restart. One of the strengths of Flink is its built-in state-saving mechanism, which is extended to the Elasticsearch connector. However, saving and reading state, especially on an eventually consistent store like Amazon S3, extends the surface area for failure. In this case, we saw that we could make the application stateless by querying Elasticsearch on startup to find the last seen offset for a topic. Since the last executed bulk write could have succeeded only partially, and since writes are idempotent, we actually restart at an offset which is the max seen in Elasticsearch minus the bulk write size. In practice, this way of doing offset recovery has proven very reliable. Auditor With a new component like Elasticpipe, we wanted to build confidence in its correctness, so we set up an auditing mechanism to ensure that it’s not silently skipping records without our knowledge. Additionally, Elasticsearch doesn’t make ACID guarantees and can lose data in the case of network partitions so we wanted to periodically ensure that our index still has all the records we expect if we plan to rely on it as a NoSQL database. Having such an auditor allows us to avoid periodic full re-indexing, a common but heavy handed solution to the potential for data loss in Elasticsearch. The most basic approach of scanning the upstream topic and checking that every document exists in Elasticsearch, even in bulk, would be as slow as a full reindex of the upstream topic. We wanted a more performant solution that would scale more favorably than O(n) in terms of number of network requests per upstream document. One of the existing ideas for this problem is using histogram aggregations to find missing records . However, this doesn’t work when your document ids are uuids instead of continuous integer ranges. So, for our application, we developed the following auditing algorithm which uses binary search to find missing documents: Get the upstream topic’s low and high watermarks Get all message keys in the upstream topic between the low and high watermark De-duplicate and sort the keys into an in-memory array Sleep for Elasticpipe’s max bulk-write time interval, to allow for documents to be flushed Query Elasticsearch to find possibly-missing keys, which are those that don’t exist in the index under the restriction that we only query for records whose offset (which is part of the metadata we include with each document) is between the low and high watermarks from step 1. (More on this below) Without the high watermark offset limit, query to find which of the possibly-missing documents are actually missing. This is the final result. Once we have a sorted and deduped list of all the document ids which should exist in Elasticsearch by scanning the upstream topic (after step 3), we can use binary search to find the possibly missing documents with O(log(n)) range queries , which Elasticsearch supports both over strings and numeric types. (A word of warning: versions over 5.0 of Elasticsearch do not support range queries over the _id , which is why we have duplicated the id in the document body explicitly as a field in the metadata.) For example, if the low and high watermarks are L and H , the list of keys has n documents, with the first and last (in lexicographic order) documents having keys k1 and k2 , respectively, we can query Elasticsearch for the number of documents between k1 and k2 with an offset between L and H . If the result is equal to n , we’re done. If it’s less than n , we can split the list into a left and right half and binary search recursively until either the number of documents in the range matches what we expected, or all the documents in the range are missing (typically when the range reaches size 1). At this point, why are the documents we find only “possibly missing”? Because some documents in the list from step 3 could have been updated while the auditing is underway, which would get upserted into Elasticsearch with an offset higher than H . Because these documents aren’t actually missing, this requires a final step to explicitly find which of the possibly missing documents is actually missing by querying for the possibly missing document ids without the high watermark offset limit. This two-step process is necessary to ensure that brand new documents added to the index after auditing has started don’t pollute the range queries we make, causing us to undercount missing documents. The diagram below, which visualizes documents in Elasticsearch sorted by offset, explains this further: Documents being audited, ordered by increasing offset. The arrow is an upsert that happens while the auditor is running. Although the auditing algorithm doesn’t detect all possible failure cases, such as corruptions inside document bodies, running it daily gives users a good degree of confidence in the continuous integrity of data in their indices. Eventual Consistency Lastly, let’s look at some difficulties of migrating existing applications that had previously been querying MySQL to an eventually-consistent data store. The difficulty stems from the lag of 10-20 seconds between a change being committed in a master database and that change being visible in Elasticsearch queries. The lag is primarily driven by the bulk writes to Kafka of the data pipeline MySQL binary log parser. These bulk writes ensure max throughput when processing hundreds of thousands of writes per second, but it means that data pipeline table streams, and therefore the order search pipeline, is always behind the truth of what has been committed in MySQL. We’ve found two strategies for making a migration. The first is dark-launching, which entails sending requests to both MySQL and Elasticsearch at the same time for a period of a few days to a few weeks and then comparing their output. If we find only a negligible number of requests get differing results, and those differences do not adversely affect users, we can feel confident about making the switch. The second is rewriting clients to be aware that the data they receive might be stale. This strategy requires more work, but is superior because the clients are in the best position to take mitigating actions. One example is the new order history client in Yelp’s iOS and Android apps, which now keeps track of recent orders placed using the app, and sends the id of the most recent order to the backend. The backend queries Elasticsearch and, in the case that the most recent order is not found, will make a supplementary query to MySQL. This way we still get low latency times for the vast majority of queries, while also ensuring that the pipeline latency does not result in users not seeing their most recent order. Acknowledgements Thanks to Anthony Luu, Matthew Ess, Jerry Zhao, Mostafa Rokooie, Taras Anatsko, and Vipul Singh, who contributed to this project, in addition to Yelp’s Data Pipeline and Stream Processing teams for their advice and support. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Become an Engineer at Yelp Want to work on challenges that span applications and infrastructure? Apply below! View Job Back to blog", "date": "2018-06-01"}, {"website": "Yelp", "title": "How Yelp Modularized the Android App", "author": ["\n        \n  Sanae Rosen, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/06/how-yelp-modularized-the-android-app.html", "abstract": "When the Yelp Android App was first built it was a monolith, with almost everything inside a single module. Single modules are easier to deal with; there are no complex dependency graphs to account for and some details of how the app is managed, such as how testing is implemented, are simpler. Eventually the app and company structure grew to a size where the monolith made code changes and testing harder, where modularization would help the app scale. After some discussion we decided to break the app into modules, thinking carefully about how we structured the app.  We’ll discuss our experiences and the lessons we learned modularizing our applications and how they can help inform similar decisions for other Android applications. Why modularize? Modularization helps the app scale - builds are faster, tests are faster, and it’s easier to understand how the app works. If the app is modularized in a logical way, then individual modules will be the responsibility of a particular team, and when working on a feature, only a few modules would be affected, maybe even only one. This impacts how effectively developers can work on the app. For example, developers no longer have to build the entire app when they make a change. We’ve recently introduced other optimizations, such as only running tests for a specific module (discussed more later). Furthermore, it helps conceptually group related code in the app, making the app easier to understand. In essence, instead of needing to deal with problems at the scale of the entire app, you can now spend most of your time only considering a small amount of the codebase, which will become increasingly valuable as the app scales. We’ve seen some pretty substantial performance benefits from modularization. Currently, a typical build from a minor change in one module takes about 12 seconds to build. A similar change in a previous version of the app which was partially modularized - no feature modules - took 47 seconds to build. While these numbers are relatively small, we have well over a thousand builds a day and those time savings add up. We’ve also seen performance benefits for running tests. The time savings are not as dramatic in relative terms, because our tests run in parallel so test times are determined by the longest-running test. In general, though, running tests selectively will tend to decrease the longest running test since it’s likely that a small subset of tests won’t happen to have the longest running test in the larger subset. We have a job that, among other things, runs all tests, that developers frequently run when working on a feature, and which runs when code is merged in. Previously, this would take about 16-20 minutes to run. Now, a test in just one module takes about 13 minutes to run. App structure The main consideration when dividing code into modules is how to structure dependencies. Module dependencies must form a directed acyclic graph ; there can be no cycles in the dependency tree. Apps as they are written do not necessarily naturally fall into this structure. Thus, when modularizing, it’s a question of how to untangle code into modules with the desired dependency structure, while managing to divide code into conceptually related chunks. There are two approaches that we explored for dividing up the app. “Horizontal” modules The first is to divide the app into horizontal modules. Broad categories of functionality are placed into an individual module, then stacked on top of each other in dependency relationships. You have all the UI elements near the top, and utility functions near the bottom, with the dependency relationship going from the top to the bottom. In the middle you have modules such as the data repository (a module through which the network and the cache are accessed), networking and analytics, and models. The precise modules you have would depend on how your app is designed - in practice, our app has tens of modules, multiple modules at the same layer, and not all modules are dependent on modules at lower layers. Feature modules The second is to have vertical feature modules. In these modules, for a given feature, the UI code, data repository, models, and everything else, are all in one module. An example of a feature module we created is one for onboarding. A major advantage of this approach is that developers will often work on specific features, and having a corresponding module means that the code they touch is restricted to one (or a few) modules. It’s also possible that a developer will focus on a horizontal module — revamping how analytics are done, for instance – but this is less common at Yelp and likely at most companies. Furthermore, modules are more independent of each other: modules affect any module upstream, so having many independent modules at the same level with no connections between them gives more of the benefits of isolation between modules. It should also be possible to have feature modules be at the top (or almost at the top) of the module hierarchy, meaning that many changes will only affect one module. The big advantage of the horizontal modules is that well-structured code falls into these horizontal slices with clear layering one above the other: for instance, UI code depends on network code, but network code probably shouldn’t depend on UI code. We thus started by dividing the app into horizontal modules, untangling our code base and separating out related functionality. This puts us in a good position to make feature modules later, and in the meantime, there are still savings in build speed if we can separate out less commonly modified parts of the code into their own modules. There are also some cases where we will probably keep code in the horizontal modules. For instance, it will probably be easier to keep the Data Repository code in its own module. It includes some relatively expensive annotation processing we want to restrict to one module, and with the way we manage caches, it’s convenient to have them be in the same module. Circular dependencies If an app hasn’t been substantially modularized yet, there are likely circular dependencies that will need to be addressed. A circular dependency is where class A references class B, and class B references class A, and it makes sense to put the classes in different modules. There can also be more complex circular dependencies (e.g. A->B->C->A). In this case, there is a problem: dependencies between modules must form a directed acyclic graph. Ideally, these dependencies could be broken. Perhaps some classes can be moved around so that they’re in the same module. Perhaps it makes sense to break apart a class, or pass a different value into a method. Ultimately, though, there will be circular dependencies that can’t be removed. One way to resolve this is to make these indirect dependencies using an interface (as shown in the figure below). We found a few common cases where this would happen in many parts of the app, and created a Base module to address them. This module sits at the base of the dependency tree. The class that is higher in the dependency tree then is made to implement an interface that sits in the Base module. Classes can then refer to that interface, which at runtime resolves to the higher class. You don’t want every single class to have a corresponding interface, and we generally try and keep these interfaces to a minimum, finding ways to refactor to avoid needing to make these references where possible. What should a feature module look like? When modularizing the app, one team (Core Android) took the lead in the initial modularization work, in particular setting up the horizontal modules and some example feature modules. The goal is for feature teams (e.g. the team focused on Search) to be able to build on this groundwork and make the modules that make sense for them. A good feature module should be whatever structure the feature teams find to be beneficial. We don’t want to set strict guidelines on the size of modules. Modules will hold features or logical pieces of features, and these are likely to vary greatly in size (although a one class module is probably too small). However, we want to have modules be the smallest self-contained unit of functionality that a person is likely to work on at a given time. That way, a developer sees the highest benefit: they see the smallest possible build time and test time for the amount of code they’re working on. Another consideration is that you want to avoid having dependencies between your feature modules. Dependencies mean that if you change one feature module, other modules have to be built or tested. While dependencies can be eliminated using the base class trick above, it’s something you want to keep to a minimum - it somewhat obscures how the code works and adds an invisible dependency that might come back to haunt you later (for instance, if you run tests based on what modules appear to have changed). Where should you start with feature modules? A challenge to keep in mind when modularizing is that moving code to a module that’s being very actively developed results in many conflicts when you merge your modularized branch back in. Of course, putting the most important code in modules gives you the greatest benefits, so it has to be done at some point. However, when you’re creating your first feature modules, you may need to experiment with different approaches and it’s best to do so with a less frequently used part of the codebase. On the other hand, make sure your first modules touch most of the important systems in your app. When deciding what to put in a feature module and what to keep in a horizontal module, it makes sense to think about what’s compiled and modified often. It might make sense to leave code that could fit in a feature module in a horizontal module if it’s almost never changed. One example of this are our strings. Furthermore, we have specialized tools for handling them (for internationalization) and keeping them all in one module made the most sense with the way our tools work. There’s also the question of how much to modularize. When pulling code out into feature modules, there were a lot of places where the horizontal modules could be broken down further to make the separation of code in the app more elegant. In particular, our UI module had a lot of utility code that could probably have been pulled out into different modules. In the end, though, it didn’t make sense to do so: aside from being more elegant, the actual benefits were fairly small. When modularizing, it’s important to have a goal for what you want to achieve. Faster build times and test times are a good goal, as is empowering feature teams to pull their code out into modules. There are advantages to creating very fine-grained modules, but modularization is a time-consuming process, and there’s a tradeoff between that and the benefits of modularization. What next? After seeing the initial benefits of modularization, there is a lot of enthusiasm among feature developers to modularize their features. Older parts of the code will probably take longer to be modularized, but new features will most likely be built in new modules. By setting the groundwork for modules to be created, we make it possible for any Android developer to easily modularize their code. One question we’re exploring is whether we can use modules to limit the number of tests we run. Modules structure your app in a way that you know what code depends on what other code. In doing so, you can greatly reduce the number of tests you need to run: you can run only the tests that are actually affected by a change. If you’ve made a change to how Reviews are displayed, for instance, it’s unlikely you need to run tests on the ordering platform (if the ordering platform doesn’t use Reviews). If they are both in modules, it’s easy to determine, by looking at the module dependency graph, that these two features are unrelated. Currently, we run tests on every merge, and developers frequently run the full set of tests as they work on a feature. This is a big win if you do much UI testing. UI testing can be expensive, in terms of both time and resources, and if developers have to wait twenty minutes for all the UI tests to run, they get frustrated. As the app gets bigger, this becomes more and more of a problem. Modularization can allow rigorous testing to scale as your application grows. One concern, though, is the indirect dependencies above. At runtime, modules can depend on code from other modules in a way that isn’t reflected in the dependency graph. In our particular app, we looked through these dependencies to understand the potential impact of running tests selectively, and concluded that, with the way the app is structured and the types of indirect dependencies we have, it’s unlikely to be a problem. Fortunately, we have a limited number of base interfaces, making it possible to reason about the impact of these indirect dependencies. Furthermore, we run the full set of tests on the master branch after every commit (in addition to before every merge), so we expect to catch any false negatives (see our previous blog post here about how we run tests). However, as we deploy the selective running of tests, we’ll carefully monitor to see if these assumptions are correct. Why not modularize? Modularization has a lot of advantages, but it comes at a cost and may not be for everyone. In particular, it’s not a magical fix for all your problems. For instance, reduced build times were a major motivator for modularizing our app. As described above, we did see some pretty substantial benefits, but after profiling our build we found other approaches that had an even more dramatic effect (for instance, our virus scanner made file i/o slow). Don’t just assume that because modularization helped someone who wrote a blog, that it suits your use case! We found modularizing took a substantial amount of engineering effort. Resolving circular dependencies is time-consuming, and changes that affect large portions of the app run into problems with extensive merge conflicts. When the largest module was broken out of our app, the engineer involved merged in his code on a weekend. Otherwise it would have been impossible to deal with the merge conflicts. Completely modularizing the app could easily take months. Furthermore, our testing, merging and release infrastructure was based around code being a single module and there was large overhead in getting it to work with multiple modules, particularly our testing infrastructure. There are also corner cases you have to deal with, especially around tests. We had a lot of code shared between tests, but code can’t depend on code in the test folder of another module. This made splitting test code into modules more difficult. Furthermore, many of the tools (particularly those related to tests) assumed that the main code was in one module, and so they had to be updated. There is probably a point at which the app grows too big to be one module. The bigger the app, the more important that it be modularized. On the other hand, the bigger the app and the more developers, the larger the engineering effort to modularize (due to the number of circular dependencies to resolve and the number of merge conflicts respectively). There’s no easy answer as to the optimum point in time to modularize the app. If the app isn’t growing much, though, and you’re fine without modules as it is, it’s probably best to leave it as it is. Some apps may never reach the point where modules make sense, and you want to avoid prematurely wasting effort that could better be spent elsewhere. Whether to start out with modules when creating a new app is an interesting question. Since our experience was to first create a monolith and then break it up, it’s hard to authoritatively claim that starting with modules is a good idea. The argument has been made that usually apps are more successful when you start with a monolith - there is an interesting discussion of the topic here . Overall, there isn’t an easy answer to that question. The goal of this blog post isn’t to convince people that modularization is necessarily the right decision, but to bring forth the considerations  when deciding whether and how to modularize. Tweet Become an Android Infrastructure Engineer at Yelp Interested in these kinds of problems?  The Core Android team is hiring! View Job Back to blog", "date": "2018-06-04"}, {"website": "Yelp", "title": "Performance Improvements for Search on The Yelp Android App - Part 3", "author": ["\n        \n  Tyler Argo, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/06/android-search-perf-improvements-part-3.html", "abstract": "In our previous blog post , we made changes to the app that improved the initial rendering performance and scroll performance of the search results in the Yelp android app. In this blog post, we’ll cover how we monitor the performance of these changes and future changes to the app to ensure no performance regressions occur and undo all of our hard work! Step 3: Monitoring Performance Improvements Making changes that improve performance is great, but there’s no point in making these improvements if future changes by engineers could undo all the hard work you’ve done to get to this point. That’s why it’s important to monitor your application’s performance and be able to identify performance regressions. A big part of our search performance monitoring involves tracking the search_request and search_results_loaded timing metrics mentioned in part 1 of this series. We collect these metrics and analyze them through Splunk not only to measure the baseline performance of our search results, but also to monitor them in case of a regression. If the search_results_loaded timing changes by 5% between app releases, an alert is sent via email to notify us of the change. Over the course of this project, we received two such alerts to notify us of performance changes, but luckily for us they were performance improvements rather than performance degradations. Another, more experimental angle that we’ve taken to monitor performance is to introduce automated performance tests that use the FrameMetrics API . At Yelp, we run our Android instrumentation tests using espresso and distribute the test runs across multiple virtual devices using Firebase test lab. As part of our continuous integration process , we run these integration tests before merging any feature branches into master. If the tests don’t pass, then the branch is not merged. To monitor application performance, we’ve introduced performance instrumentation tests that run alongside our integration tests. These performance tests are different from our integration tests in a few important ways: There is no success case, so if the test fails it does not block the continuous integration process We enable animations in the tests to get an accurate performance profile This means tests take longer to run and are more prone to flaking through espresso actions We run these tests on physical devices instead of emulators to be as close to the end user experience as possible In these performance tests, we analyze the performance of the device in rendering each and every frame. To accomplish this, we took advantage of the new FrameMetrics API to log frame stats during the execution of an instrumentation test. The API gives us a whole bunch of information about each frame that is rendered on the device. The most important piece of information that we record is the duration that it takes for each frame to render. If this duration is less than 16ms we consider it a “fast frame”. If it’s slower than 16ms, then we consider it a “dropped frame”, meaning that the display didn’t update when it was time to refresh, so the frame was “dropped”. For dropped frames we also record additional information to see if we can pinpoint the reason why the frame was dropped. Here’s an example of the output from one of the performance tests: [\n  {\n    \"SearchListPerfTest.test_ScrollToBottom_ToCapturePerformanceProfile\": {\n      \"droppedFrameAverageTotalDuration\": 69.27096400000002,\n      \"droppedFrameMetrics\": [\n        {\n          \"ANIMATION_DURATION\": 868385,\n          \"COMMAND_ISSUE_DURATION\": 91301468,\n          \"DRAW_DURATION\": 29282346,\n          \"FIRST_DRAW_FRAME\": 0,\n          \"INPUT_HANDLING_DURATION\": 59271,\n          \"INTENDED_VSYNC_TIMESTAMP\": 1432112745057,\n          \"LAYOUT_MEASURE_DURATION\": 30366514,\n          \"SWAP_BUFFERS_DURATION\": 6132188,\n          \"SYNC_DURATION\": 5824896,\n          \"TOTAL_DURATION\": 619208032,\n          \"UNKNOWN_DELAY_DURATION\": 455240776,\n          \"VSYNC_TIMESTAMP\": 1432562745039\n        },\n        ...\n      ],\n      \"fastFrameAverageTotalDuration\": 12.279790835820895,\n      \"numDroppedFrames\": 35,\n      \"numTotalFrames\": 102\n    }\n  }\n] We write this output to a json file that is saved as an artifact of the test on google cloud storage. From there, our Jenkins continuous integration job collects all of the output files from these tests and aggregates the data into events that are then ingested by Splunk. That lets us create alerts for when performance degrades significantly. Step 4: Repeat Just like a software product, performance work is never really “done”. It is an iterative process that involves building on previous work and avoiding regressions. We were able to make significant improvements to our search result load times on Android purely through changes on the client. These improvements were made over a series of changes that built on each other. At some point you will see that there is a diminishing return on your investment of effort. If it takes a week to reduce your timings by only a few milliseconds, then it might be a good time to stop and re-evaluate where your efforts are best spent. Next Steps for Performance Monitoring We’ve considered using the FrameMetrics API to monitor aggregate dropped frames in production. While this would give us a great view into real-world client side performance, we still haven’t investigated to see what the performance overhead would be. Conclusion Over the past three blog posts, we’ve outlined how we have established a system of reliable measurement through our metrics collection, improved performance by offloading work off of the main thread, asynchronously inflating views and taking advantage of caching a view model for each search result. We’ve also introduced a monitoring framework and alerts to help avoid regressions and have identified potential improvements that warrant further investigation. Onwards to a faster Yelp Android app! Tweet Want to build next-generation Android application infrastructure? We're hiring! Become an Android developer at Yelp View Job Back to blog", "date": "2018-06-21"}, {"website": "Yelp", "title": "Happy 10th Birthday to Yelp on iOS!", "author": ["\n        \n  Bill Meltsner, Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/07/happy-10th-birthday-to-yelp-on-ios.html", "abstract": "Yelp hit a big milestone last week: ten years ago, Yelp launched on the Apple App Store! On July 10th, 2008, Apple officially launched what was then known as the iPhone App Store, giving iPhone users their first-ever opportunity to run third-party native apps on their devices. It launched with a whopping 552 apps available for download, including the Yelp app! At the time, Yelp was only available in the U.S. (we would launch in Canada a few weeks later), and offered an incredible breadth of functionality: you could search for businesses, read reviews, and even bookmark! Staggering. It’s not a birthday without baby photos. It was our very first foray into mobile development which, at the time, was a huge unknown. Neither we nor the industry had any real experience to draw upon, nor knowledge of whether this would be a worthwhile investment. Luckily, it was a hit and we kept it up; we launched a BlackBerry app a year later, and an Android app soon after that. (Some of which have aged more gracefully than others.) The app’s 10-year journey has been long and winding, incorporating greatly expanded functionality, new usability research, changing platform standards, and the unique sensibilities of our designers and PMs. You can trace that journey, as well as the much shorter journey of our app icon, in this retrospective from 9to5Mac (we’re at the bottom). That’s a great article, but we’ve also included the images below for your reading convenience: The yellow buttons from 2010 were our version of JNCOs and frosted tips. Truly revolutionary. I’ve been at Yelp for almost 6 years, and it’s been genuinely inspiring to work exclusively on the app and watch it grow and evolve in ways I never could have expected. It’s developed from a window into Yelp’s business data into a place users can post reviews and photos, order food, and hire a plumber. Not to mention what was just one app is now part of a whole family of apps across iOS and Android! Of course, it’s fun to look back, but we’re even more excited by what Yelp will look like ten years from now… here’s to another successful decade of connecting people with great local businesses! Tweet Become an iOS Engineer at Yelp Come join us if you’re interested in helping shape the future of Yelp on iOS! View Job Back to blog", "date": "2018-07-16"}, {"website": "Yelp", "title": "Celebrating the Women of Yelp: AWE the Book", "author": ["\n        \n  Grace Jiras Yuan, Technical Recruiting Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/02/celebrating-the-women-of-yelp-awe-the-book.html", "abstract": "As a recruiter, I talk to a lot of people about what it’s like to work at Yelp. Most often, I find myself answering questions about the work environment and individual growth opportunities. During my four and a half years at Yelp, I would summarize the people here as very sharp and intelligent, while also humble and open minded. This spirit has fostered an environment that encourages individuals to learn by trying things for themselves (new hires get to push code out their first week!) and empowers them to ask questions. This collaborative work culture invites tremendous opportunity and gives employees the power to participate in the projects they are most passionate about, whether it’s specifically related to work, technology, community or otherwise. This attitude is core to Yelp’s values and allows employees from every walk of life and every skill set find opportunity and growth. In an effort to foster community and build relationships across the organization, Yelp has a number of Employee Resource Groups. One of the groups that I’m actively involved with is Awesome Women in Engineering (AWE). AWE is focused on building a strong community for women in engineering and their allies by facilitating professional career building activities, networking, leadership, and mentorship. Over the years, I’ve had the privilege of  learning more about this groups’ experience working in the tech industry as well as their experiences here at Yelp. As I heard more stories, I felt inspired to collect and share them with others outside of Yelp. Thus began the start of our hackathon project: AWE the Book. As with any hackathon , the idea of creating a book was pitched to a group of engineers and those interested joined to help turn this idea into a final product! Our team came together to craft compelling interview questions ranging from childhood aspirations, what they love about Yelp, and helpful advice they wish they had received earlier in their career. We had over 60 women in Engineering and Product volunteer to participate! Along with the interviews we had (very talented) Yelpers take portraits of each individual. After Hackathon we had even more volunteers, men and women, across the entire engineering organization help transpose, type, and edit the interviews. It was so much fun seeing everyone work together to make the book come together. The finished product was incredible to read - over 120 pages of stories, experiences, and advice shared by women professionals in tech. Here are some of my favorites! Pictured above from left to right: Yue Wu, Product Manager; Ya-Lin Huang, Software Engineer; Marianne Gosciniak, Software Engineer How did you get into tech? I’ve always been interested in tech, a lot of my favorite products are tech products. I used to be in consulting and data, working on heavy data driven roles. Then I went to business school to try to find a career change outside of consulting. I found a lot of good opportunities in tech. This is an area I’m very passionate about. I had my favorite products as a consumer and Yelp was right on the top. Being able to work on one of my favorite products ever is awesome! Yue Wu, Product Manager What did you want to be when you were a kid? When I was a child, my parents wanted me to become a teacher because they thought it would be a stable job. It’s a very regular job from 8-4 and you deal with kids - what possible trouble could kids get into?! At that time, though, I didn’t think too much about being a teacher. I played a lot of video games, a lot of Super Mario, so I wanted to be either the character Super Mario, or someone who creates Super Mario. Ya-Lin Huang, Software Engineer What is the most exciting thing you have worked on at Yelp? I really loved my internship project. I added addresses and street suggestions to the location suggest service. When I was presenting during the intern project presentations, a lot of people said they were wondering why we never had this. It was really exciting to help make a feature that a lot of people had been wanting and then later got to use. Marianne Gosciniak, Software Engineer Pictured above from left to right: Alex Phillips, Engineering Manager; Jen Wang, Software Engineer; Ellen Heirbaut, Technical Recruiter What do you love about Yelp? When I was looking for a job, I was looking for four things that I really valued in a job. One was mentorship - I would love learning from other people’s experiences and I find that I can do that a lot at Yelp. There are so many different people with diverse backgrounds and diverse experiences that I feel like I learn something new every day. Two is problem solving - within Yelp and within Yelp Reservations, it’s a fun problem - It’s relatable. Thirdly, I think the culture we have at Yelp is really extraordinary. The friendliness, the collaborative nature, the kindness, the relatability, and the welcoming feeling. We’re always helping each other learn - that’s so wonderful to have. The last thing was opportunities to grow and I think that’s something that Yelp has been really focusing on lately. Those are the main reasons why I chose to work at Yelp and why I continue to work at Yelp. Alex Phillips, Engineering Manager Have you ever broken Yelp? Oh, lots of times. I’ve caused lots of codedeploy rollbacks. I tanked SEO for awhile. I’ve also recommended changes that ended up breaking Yelp. I won’t go into the specifics here, but one of the things I like the most about Yelp is the empathetic environment. It is OK to make mistakes, as long as you take responsibility for them and learn from them and don’t keep repeating them. Better yet, teach others how to not make the same mistakes or build tools that safeguard against them. I’ve broken Yelp many times, but I’ve learned from all of them and made my peace. None of them give me nightmares now. Well, except, maybe the SEO one. Jen Wang, Software Engineer How do you balance a personal life and work life? I’m a working mom, so I always feel like I should be doing more. I do a lot of events outside of work, like the Expat Women meet-up. It’s a lot of time management and balancing priorities. Some days I will have my son and on those days I have to leave right on time. If I’m not on time, they can kick us out of preschool, which is not an option! I feel like Yelp makes it easier for me. My manager is very understanding when I have to leave early, and other days I work longer. The nice thing is I have a very happy, fulfilling life with many different aspects to it. Ellen Heirbaut, Technical Recruiter Make sure to ask the see the book in person whenever you come by our HQ in SF! For more information about roles at Yelp check out: careers.yelp.com. Tweet Back to blog", "date": "2018-02-14"}, {"website": "Yelp", "title": "Introducing LogFeeder - A log collection system", "author": ["\n        \n  Yonny Tonui, Information Security Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/03/introducing-logfeeder.html", "abstract": "Introduction The collection and processing of logs is essential to good security. One of the primary functions of a security team is to keep organizations safe by eliminating blind spots in infrastructure. Breach investigations without logs result in a lot of guesswork. Worse, the activities of an attacker can easily remain undiscovered without adequate logging. To ensure we have a robust log storage and visualization platform, we use Elasticsearch, Logstash and Kibana ( ELK ). These tools form part of the toolset that we use in our Security Incident and Event Monitoring (SIEM) solution. ElastAlert is the primary means by which alerts are generated from the event data that is stored in Elasticsearch. It is the best thing since sliced bread, check out these blog posts to learn more . A majority of the log collection software that is currently available today focuses on collecting logs from syslog, which is good for systems that we control directly. Unfortunately, today’s solutions fall short when it came to infrastructure we depended on heavily but do not control, such as Duo or Cisco Umbrella. We felt there was a need for a system that would enable us to collect logs for the different SaaS (Software-as-a-service) systems that we subscribe to. We needed these logs in our SIEM so that we could have visibility on various actions, create alerts for threat detection, and comply with the relevant regulations that guide log storage and auditing. The basic problems we faced were: We needed to ingest logs generated by very different systems These systems each had different log output formats and authentication schemes We needed to curate, normalize and standardize certain fields in the logs in a form that would make it easy to query e.g username@domain, domain\\username etc Service providers didn’t keep logs as far back as we wanted Introducing LogFeeder In searching for a log collection and ingestion tool that fit our needs, we developed LogFeeder . This service enabled us to collect logs from different systems and ingest them into our SIEM for alerting. The architecture is very modular and enables the ingestion of new SaaS log sources by writing new plugins. Writing a new plugin is as simple as extending the BaseFeeder class and adding the specific log retrieval API shared by the service provider. It currently uses Elasticsearch as a datastore and Amazon SQS for queueing, but the architecture is modular enough that any other datastore or queue could be integrated. LogFeeder allows for the tokenization of the event log fields in such a way that makes querying easy. The Class Architecture The service is written in Python. The class architecture takes advantage of standard Python inheritance to model input and output plugins. The classes are detailed below: BaseFeeder - The base class, sets up the common logic for example command-line argument parsing, tracking the number of processed records, timestamps etc S3Feeder - The class enables reading from Amazon S3 buckets SqsOutput - The class enables writing out data to an Amazon SQS queue EsOutput - The class enables writing out data to Elasticsearch Sample Scenarios We have open-sourced feeders that we use for Cisco Umbrella , Duo and OneLogin . In the following sections we will go through some practical sample scenarios on how you can leverage the existing feeders and also demonstrate how you can extend the platform to support feeders for systems that you already use in your organization. OneLogin User Authentication Logs OneLogin provides single sign-on (SSO) solutions to organizations. They allow the extraction of various types of logs, but the logs we are concerned with are user authentication logs. These logs are invaluable to an analyst responding to a stolen-credentials scenario that might be related to phishing or any other type of attack. The diagram below shows how the input one gets from OneLogin is transformed by LogFeeder. Input Output { “id”: 999999999, “created_at”: “2018-12-19T02:02:39.276Z”, “account_id”: 55555, “user_id”: 88888888, “event_type_id”: 13, “notes”: “password”, “ipaddr”: “11.111.11.111”, “actor_user_id”: 7777777, “assuming_acting_user_id”: null, “app_name”: null, “group_name”: null, “actor_user_name”: “John Doe”, “user_name”: “jdoe”, “policy_name”: null, “otp_device_name”: null, “operation_name”: null, “directory_sync_run_id”: null, “directory_id”: null, “resolution”: null, “client_id”: null, “resource_type_id”: null, “error_description”: null, “proxy_ip”: “127.0.0.1” } { “@version”: “1”, “@timestamp”: “2018-12-19T02:02:39.276Z”, “type”: “logfeeder”, “event_time”: “2018-12-19T02:02:39.276Z”, “onelogin_data”: { “id”: 999999999, “created_at”: “2018-12-19T02:02:39.276Z”, “account_id”: 55555, “user_id”: 88888888, “event_type_id”: 13, “notes”: “password”, “ipaddr”: “11.111.11.111”, “actor_user_id”: 7777777, “assuming_acting_user_id”: null, “app_name”: null, “group_name”: null, “actor_user_name”: “John Doe”, “user_name”: “jdoe”, “policy_name”: null, “otp_device_name”: null, “operation_name”: null, “directory_sync_run_id”: null, “directory_id”: null, “resolution”: null, “client_id”: null, “resource_type_id”: null, “error_description”: null, “proxy_ip”: “127.0.0.1” }, “logfeeder_account”: “example.org” “logfeeder_subapi”: “auth” “logfeeder_type”: “onelogin” “org_username”: “jdoe” } Duo 2FA Logs Duo provides two-factor authentication solutions to organizations. Attackers are getting craftier and more innovative by the day. Even with two-factor authentication, some users are unfortunately convinced to give the attackers access. In these cases, two-factor authentication logs become important for scoping the extent of a compromise. Two-factor authentication logs can also be used to detect anomalies (or limit post-exploitation) with regard to login location, time of day and other indicators of compromise (IOC) parameters. Other log types of interest may be those related to a change in a user privilege. Remember context matters in most cases; if a change is expected and all the change procedures have been followed, then everything is fine. If the change is unexpected and a potential breach is being investigated at the same time - then it just may be malicious. Duo also has a nifty feature that allows users to “report as fraud” pushes that were not initiated by them. These logs and corresponding alerts are very helpful in identifying compromised users. The diagram below shows how the Duo logs are transformed by LogFeeder. Input Output { “username”: “jadmin”, “description”: { “phone”: “+1(234)-56789”, “email”: “jdoe@example.org”, “role”: “Admins”, “name”: “jdoe”, “hardtoken”: null, } “timestamp”: 1512020011, “object”: “jdoe”, “host”: “za.duo.com”, “eventtype”: “administrator”, “action”: “admin_create” } { “@version”: “1”, “@timestamp”: “2018-12-19T02:02:39.276Z”, “type”: “logfeeder”, “event_time”: “2018-12-19T02:02:39.276Z”, “duo_data”: { “username”: “jadmin”, “description”: { “phone”: “+1(234)-56789”, “email”: “jdoe@example.org”, “role”: “Admins”, “name”: “jdoe”, “hardtoken”: null, } “timestamp”: 1512020011, “object”: “jdoe”, “host”: “za.duo.com”, “eventtype”: “administrator”, “action”: “admin_create” }, “logfeeder_account”: “example.org” “logfeeder_subapi”: “auth” “logfeeder_type”: “duo” “org_username”: “jdoe” } Cisco Umbrella Logs Nowadays employees access the organization’s network from a myriad of places. A security analyst needs visibility into domain resolutions wherever they happen. DNS records greatly assist in mapping out affected users in a breach and identifying the presence and origin of malware in a network. Cisco Umbrella makes this information available in a comma separated values (CSV) format. LogFeeder parses this information (as shown below) and makes it available in Elasticsearch; ready for querying whenever a situation strikes! Input Output “2018-01-22 14:14:23”, “COMPUTER_001”, “COMPUTER_001”, “112.11.21.31”, “172.16.1.1”, “Allowed”, “1(A)”, “NOERROR”, “gimmeyourpassword.com”, “Search Engines” { “@version”: “1”, “@timestamp”: “2018-01-22T14:14:23.276Z”, “event_time”: “2018-01-22T14:14:23.276Z”, “opendns_data”: { “domain”: “gimmeyourpassword.com”, “external_ip”: “112.11.21.31”, “response_code”: “NOERROR”, “internal_ip”: “172.16.1.1”, “query_type”: “1(A)”, “action”: “Allowed”, “identities”: “COMPUTER_001”, “categories”: “Search Engines”, “most_granular_identity”: “COMPUTER_001” }, “logfeeder_account”: “example.org”, “logfeeder_subapi”: “opendns”, “logfeeder_type”: “opendns” } Sample Feeder Now we shall look at extending LogFeeder to import logs from a sample service Foo. In this example we shall assume that the service provider has shared a python library ( foo_client ) that allows for the extraction of timestamped logs. We shall also assume you want to write the data out to an Elasticsearch cluster (writing out to any other data store is as simple as extending BaseFeeder and overriding the send_records function ) and as such FooFeeder will extend EsOutput in order to take advantage of the EsOutput class that writes out to Elasticsearch. The following methods will need to be overridden in order to leverage the functionality offered by the BaseFeeder: read_api_creds - This function enables the reading of Foo’s API credentials from a file. It enables you to define how the credentials will be read out and it is a good place to initialize Foo’s API client. convert_dt_to_api_timestring - This function enables the formatting of timestamps in a format that is compatible to Foo’s API date format. convert_api_timestring_to_iso8601 - This function enables the standardization of timestamps when outputting the logs to Elasticsearch. _make_queries - This is where all the magic happens, the core function that is expected to return the log records read from the Foo API. Two variables are provided to enable you to limit the start and end times of the specific time range you would like to retrieve logs for. The sample feeder for Foo’s service logs is shown below. #!/usr/bin/env python\n# -*- coding: utf-8 -*- \"\"\"\nReads events from the Foo API and sends them to Elasticsearch\n\"\"\" from __future__ import absolute_import from datetime import datetime import foo_client import simplejson from pytz import utc from log_feeder.utils import datetime_to_unixtime from log_feeder.utils import format_dt_as_iso8601 from log_feeder.utils import lock_and_load from log_feeder.utils import read_yaml_file from log_feeder.base_feeder import parse_base_feeder_options class FooFeeder ( EsOutput ): \"\"\"Reads Activity Data from the Foo API and feeds it to Elasticsearch\"\"\" def __init__ ( self , ** kwargs ): super ( FooFeeder , self ). __init__ ( ** kwargs ) APP_NAME = 'foo' TIMESTAMP_KEYNAME = 'timestamp' def read_api_creds ( self , config_file ): \"\"\"Read Foo credentials necessary to access activity data from the Foo API\n\n        Args:\n            config_file: file containing the service’s credentials\n        Returns:\n            Foo credentials object in a dictionary, or None on failure\n        \"\"\" yaml_content = read_yaml_file ( config_file , [ 'api_key' , 'secret_key' , 'api_hostname' ]) foo_api = foo_client . FooApiClient ( api_key = yaml_content [ api_key ], host = yaml_content [ 'api_hostname' ]) return { 'foo_api' : foo_api } def _make_queries ( self , start_time , end_time , api_info_dict , sub_api_name ): \"\"\"Makes queries to the Foo API for records in the specified time range\n\n        Args:\n            start_time (string): starting time (as a string recognized by the\n            API) for log ingestion\n            end_time (string): ending time (as a string recognized by the\n            API) for log ingestion\n            api_info_dict: a dictionary containing a Foo API access object\n            sub_api_name (string): the name of the sub_api to be called (if\n            the API has sub_apis)\n        Yields:\n            An enumerable of dicts containing data from Foo API\n        \"\"\" foo_api = api_info_dict [ 'foo_api' ] # Call the Foo API query_results = foo_api . get_auth_logs ( start_time , end_time ) for qr in query_results : yield qr def convert_dt_to_api_timestring ( self , datetime_obj ): \"\"\"Converts a datetime object to a time string accepted by the API\n\n        Args:\n            datetime_obj (datetime): A datetime object\n        Returns:\n            A string representing the datetime that is accepted by the API\n        \"\"\" return datetime_to_unixtime ( datetime_obj ) def convert_api_timestring_to_iso8601 ( self , api_timestring ): \"\"\"Converts a time string output by the API to a iso8601 time string\n\n        Args:\n            api_timestring (string): A time string output by the API\n        Returns:\n            A time string in the iso8601 format\n        \"\"\" return format_dt_as_iso8601 ( datetime . utcfromtimestamp ( api_timestring ). replace ( tzinfo = utc )) def parse_options (): parser = parse_base_feeder_options () ( options , args ) = parser . parse_args () return options def main (): options = parse_options () lock_and_load ( FooFeeder ( options = options )) if __name__ == \"__main__\" : main () LogFeeder has recently been open-sourced and is available on GitHub . Happy Hacking, stay secure and we hope you find this tool useful! Tweet Back to blog", "date": "2018-03-02"}, {"website": "Yelp", "title": "CSS in the Age of React: How We Traded the Cascade for Consistency", "author": ["\n        \n  Theresa Ma, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/03/css-in-the-age-of-react.html", "abstract": "With hundreds of engineers, developers and designers working on Yelp, ensuring visual consistency across Yelp is a challenging task. We’ve been migrating our web components from Yelp Cheetah to React to increase designer and developer productivity while ensuring visual consistency across our web app. Along the way, we built Lemon Reset - a package containing consistent, cross-browser React DOM tags, powered by CSS Modules. Since our design system components are the building blocks of our frontend, we had to port them to React as the first step before our developers could port their features. We made a lot of design decisions about these new components, including how to tackle the problem of CSS in React. In a traditional web stack, where CSS and markup files are independent, we applied styles using selectors that target DOM elements and classes. Those selectors have infinite inheritance thanks to the cascading part of cascading style sheets - arguably the best and worst part of CSS. That means that we had to manually scope our class names using a BEM naming convention to avoid naming collisions and specificity wars. Relying on human conventions such as manual namespace scoping doesn’t scale as an organization and codebase grows. As we added more and more new features at a faster rate, it also became more difficult for developers to track of all the stylesheets they needed to import for their pages. When we write the markup for a button, we assume that button.css will also be loaded whenever we use a Button component. Since our components couldn’t declare dependencies on CSS, it was up to developers to ensure that the styles they needed for their components were included in the CSS package for the template. Template file: <link rel=\"stylesheet\" type=\"text/css\" href=\"web-pkg.scss\">\n\n<button class=”btn”>Wow!</button> web-pkg.scss import 'yelp_styleguide/assets/scss/lib/buttons';\nimport ‘cool_feature’; yelp_styleguide/assets/scss/lib/_buttons.scss @import ‘colors’;\n\n.btn {\n    background: $yelpy-red;\n} _cool_feature.scss .my-feature {\n...\n} To tackle some of these issues in our ever-growing frontend codebase, we created our design system with reusable components . Rather than writing every component from scratch, developers can build UIs by simply gluing together prefabricated components, allowing us to more quickly iterate and build a visually consistent product. But the consistency we wanted was unenforceable due to the cascading nature of CSS. Our consistent class names allowed developers to deviate from our design system by creating new, more specific CSS rules, making it more difficult for us to maintain a consistent visual language. .btn {\n    color: white;\n}\n.btn--primary {\n    background: red;\n} <button class=”btn btn--primary”>Wow!</button> That means that our standard components might not always be so standard in usage, as things like our color guidelines could easily be overridden by one line of rogue CSS. .btn.btn--primary {\n    background: blue;\n} With hundreds of developers across many feature teams, our frontend infrastructure team can’t be on every code review that touches an SCSS file. After all the effort put into standardizing our components, one line of CSS could still sneak its way into the codebase and turn our red buttons into blue buttons! React to the rescue! In a React world, we’re able to package together JSX with the styling it requires. Our React components explicitly declare dependencies on the CSS they need, which means that we can statically extract all the styles needed on a page so our developers no longer have to manually ensure that the CSS their components need is included! Button.js import styles from'./Button.scss';\n\n<Button className={styles.button}/> Button.scss .button {\n    background: red;\n} Additionally, instead of relying on human naming conventions, we can use CSS modules to provide automatic scoping for class names. That means we no longer have to worry about naming and styling conflicts! This all sounds too good to be true, so what’s the catch? If everything in CSS Modules becomes namespaced, how do we share basic styles that we want to include across all our components’ pages? In a markup and CSS world, we provided Meyer Reset as a global style sheet with global styles that targeted DOM elements, such as h1 for the Yelpy red headings we know and love; however, any CSS selector targeting a DOM element instead of a class can’t be scoped, and scoping is the whole point of CSS modules! h1 {\n    font-size: $h1-font-size;\n    color: $yelpy-red;\n} The Yelpy red headings we all know and love A few options we considered initially: We could just do what we used to and include a package with styles that we would want globally, such as Meyer Reset, typography styles, and our grid layout scss. We could declare a dependency for all of our components on the global style package, let Webpack resolve which styles we need, and dump those styles as a global on the page, but having global side effects seems gross when we can explicitly express dependencies on our CSS. Global styles would also pollute the entire page and make it impossible to use a Yelp button in isolation without affecting other styles on the page. Yelp Wifi and Yelp Reservations are separate products from the consumer Yelp app, so it’s important for them to be able to borrow components as they see fit rather than be forced to adopt our entire design system. So what’s a core web team to do? The Solution We needed a solution that would satisfy two key requirements: Components should be able to be used in isolation from one another Components need to be generic enough for developers to use across the site, but consistent enough that they adhere to our design system The solution we decided on was to create components for everything, and to style our components in two layers: Basic components for HTML tags that would handle baseline styles for browser resets. Components for our design system that abstract away the underlying DOM elements and expose a predetermined set of styles for developers to use. Layer 1: Lemon Reset The first layer of components should contain the basic styling that every component needs: a reset stylesheet for browser consistency. We didn’t find any existing open source library that provided basic React components with Eric Meyer’s popular CSS reset stylesheet Meyer Reset styles built in for use with CSS Modules, so we built our own. This first layer of components became Lemon Reset , now open sourced for your convenience! For every element in Meyer Reset, we generated a React Component: export const LemonReset = ({ tag: Tag, children, className, tagRef, ...otherProps }: Props) => (\n    <Tag className={classNames(styles\\[\\`lemon--${Tag}\\`\\], className)} ref={tagRef} {...otherProps}>\n        {children}\n    </Tag>\n);\n\nexport const H1 = ({ children, className, ...otherProps }: TagProps) => (\n    <LemonReset tag=\"h1\" className={className} {...otherProps}>\n        {children}\n    </LemonReset>\n); The “LemonReset.H1” component is styled with the reset CSS for h1 from Meyer Reset, and that’s it. The second layer of abstraction components can then use these reset components internally by passing in all necessary props down to the underlying HTML element. Layer 2: Design System Components Components need to be generic enough for developers to use across the site, but consistent enough that they adhere to our design system. Inspired by React Primitives , we created these Abstraction Components encapsulating Lemon Reset for our developers to use, such as: <Text> <Heading> <Container> <Button> And 65 other components! An example of one of our design system components This allows us to constrain the patterns that appear within our UI. Developers are required to choose values from our design system which we provide for props like font size, color, margin, padding, and background-color. These standards help us create visual consistency across Yelp and surface existing solutions for design needs. Learn more about our design system at yelp.com/styleguide By using CSS Modules and providing abstraction components with built-in styles from our design system for our developers to use, we can increase designer and developer productivity and ensure visual consistency across our web app. Tweet Become a Software Engineer at Yelp Want to help us make even better tools for our full stack engineers? View Job Back to blog", "date": "2018-03-05"}, {"website": "Yelp", "title": "Caching Internal Service Calls at Yelp", "author": ["\n        \n  Arnaud Brousseau, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/03/caching-internal-service-calls-at-yelp.html", "abstract": "Casper is a caching proxy designed to intercept traffic flowing between internal Yelp services. It is built with Nginx and OpenResty at its core and contains some logic in Lua to fit in our ecosystem. Today we’re proud to announce that Casper is opensource and available on Github . To introduce the context in which Casper was created, this post outlines a few basics about Yelp’s SOA, explains the technical decisions behind Casper’s design, and finally exposes concrete problems that we’ve encountered while rolling it out and running it in our production environment. Moving past “Memcached for everything” Yelp has had a solid Memcached infrastructure in place for years. Our main application (aka “yelp-main”) and a few services are still taking advantage of it today to persist data across restarts or cache database/logic-derived data for performance and cost reasons. However, as the number of services and traffic begin to scale, some of Yelp’s services experience a disproportionate amount of load. These are what we call “data” services, or “core” services, containing or processing core data like user, business or review-related information. Our Memcached usage is fairly standard: we have a client library which lets applications interact with a set of Memcached clusters deployed in their AWS region. It’s a very thin wrapper around the Memcached protocol (“GET”, “SET”, “DELETE”, etc). Applications dictate cache keys, values, TTL (Time-To-Live), and invalidation strategies. This flexibility comes at a cost: As Yelp grows the number of services in its infrastructure, it’s extremely hard to understand which pieces of data are cached, how, and where. Tricky forward/backward compatibility of data in the cache with the code being deployed can lead to outages Updates to caching policies require deployments, because application code drives these decisions We also realized that we were limited by a few fundamental Memcached design choices. Namely: Memcached clusters are hard to share fairly because of their global eviction policy. If eviction spikes, it’s hard to tell where it comes from unless custom metrics and instrumentation for all clients are in place. Memcached does not have the concept of key enumeration. This makes cache invalidation hard to perform granularly unless you keep a running tally of keys somewhere. Memcached has no built-in mechanism for data replication. Because Yelp has one cluster per AWS region, this leads to suboptimal hit rates and makes DELETEs hard to perform correctly. From these challenges emerged a list of guiding principles: transparency, to know when and how data is cached in our infrastructure, built-in operational metrics, granular cache invalidation , fail-safe mechanism when our cache isn’t available or misbehaving, and isolation from app deploys , allowing us to modify our caching layer independently from applications or clients. Background: Yelp service calls Yelp has hundreds of small services running in production today. Our overall service infrastructure, PaaSTA , relies on SmartStack (made of Nerve and Synapse ) to perform service discovery and routing. In practice, each host at Yelp runs a local HAProxy instance to route requests to services. When a process wants to contact a service it connects to HAProxy locally, and requests are forwarded to available backend servers transparently. The HAProxy instance running on each host is periodically updated and restarted to reflect backend servers going up or down. This is done without downtime ! Yelp’s first proxy-as-a-service Caching service calls is traditionally done either on the server or on the client, through a thin client library (in pale blue in the picture below): Instead of deploying Casper as infrastructure with an associated client or server library we designed, built and deployed Casper as a separate service . This brought several key advantages. The first is that changes or updates to Casper are completely independent from clients and underlying Yelp services. More importantly, Casper deployments are not special, because Casper is just another Yelp service. PaaSTA deploys and runs Docker containers without caring about what’s inside of them, making it possible to choose libraries and technologies that best fit what we wanted to accomplish. This was important when the time came to decide how we’d build our caching proxy. We decided to use Nginx and the OpenResty platform to build Casper. Nginx is a widely adopted, performant proxy solution, and Openresty is a platform designed to script Nginx with Lua . We’ve decided to adopt OpenResty for three main reasons: Its extremely mature documentation . Everything you might want to know is in a single README file, full of complete examples and references. The OpenResty community is active and has authored dozens of high-quality Lua modules. We’re confident that Nginx-based solutions can be both performant and reliable because Yelp already uses Nginx at its outermost edge (external load balancers). We took advantage of Smartstack’s calling mechanism described above to roll out Casper transparently and make caching available for all Yelp services. Rather than routing to backend servers directly, HAProxy instances running on client hosts route requests to Casper first. As an added bonus, HAProxy does not forward requests to Casper if it’s down or unhealthy, allowing for a natural fail-safe mechanism. How does HAProxy “know” which requests to send to Casper rather than the usual backend server? The answer is configuration, which we cover later on in this post. Read on! Yelp’s first in-memory Cassandra cluster The first iteration (proof-of-concept) of Casper bundled Nginx, OpenResty and Redis (a popular key/value store for caches) in a single Docker container. This is ideal when it comes to read/write latency: everything is available on localhost! The downside is poor hit rates, because this yields N independent caches, where N is the number of running Casper containers. Not to mention that caches are cleared when containers restart, which happens on every deploy. Clearly not a sustainable option! To work around these problems we knew we needed to host our datastore outside of Casper’s container. Scaling Redis to multiple machines is done through cluster mode . Since Casper is built with Nginx/Openresty at its core we needed to find a Redis driver written in Lua supporting cluster mode. Unfortunately, at the time, we weren’t able to find one. Eventually we explored using Cassandra, hosted outside of Casper’s container. Why? A few reasons: Has a well-maintained lua driver, lua-cassandra Offers built-in replication mechanism across AWS regions Maintains the ability to list cache keys (this is important for cache invalidation) Maintains the ability to automatically expire cache entries with a built-in TTL mechanism Another big factor in this decision was performance. We were initially reluctant to look at Cassandra because it persists data to disk by default. But we realized that reads and writes could be made fast enough if we used Cassandra with data and log directories mounted in-memory (in tmpfs). The following table compares different Cassandra drivers, and shows that lua-cassandra gets decently close to what Redis offers out-of-the-box: Cassandra drivers benchmark results Today we’re still running with this setup (in-memory Cassandra, with lua-cassandra), but had to tweak it along the way. More specifically, we: Initialized lua-cassandra with local nodes (within the same AWS region) to avoid timeouts. Since our cluster spans multiple regions, there were cases of cross-region traffic and initialization timing out Decreased the default timeouts and got rid of retries in lua-cassandra to minimize the impact of an unhealthy cluster on traffic flowing through Casper (see cassandra_helper.retry_policy ) Adjusted our Cassandra schema to distribute data fairly between nodes (we now have a “bucket” as our partition key – see cassandra_helper.get_bucket ) Contributed back to lua-cassandra to handle errors that slipped through the default error handling: https://github.com/thibaultcha/lua-cassandra/pull/108 Split our traffic to cassandra across two connections: one for reads, the other for writes. That’s because reads (cache lookups) are performed synchronously in the critical path while writes (for invalidation or cache writes) are asynchronous and performed post-request. Tuned our write consistency level from ALL to LOCAL_QUORUM to avoid blocking clients performing invalidation Decreased gc_grace_seconds from 10 days to 1 day, which forces Cassandra to run compaction more often and helps with read timings. Tuning cache keys Yelp services talk to each other with HTTP (1.1), a text-based protocol. Picking the whole request as a cache key isn’t acceptable since it includes headers that will vary with every request (e.g. “Date”). This would bring the potential cache hit rate to 0 because all keys would be different. For our first iteration, we picked the HTTP URI and method as a cache key. This is pretty typical for an HTTP cache, but it triggered an interesting bug in gzip-enabled services: if Casper proxies a gzip-accepting request first, the cached response will be gzipped. This cached gzipped response will then be served to all subsequent clients requesting that URI, regardless of whether they accept gzip! This made us reconsider our initial decision and we now let developers configure Casper to optionally include HTTP headers in the cache key. This is configured per service. For example, if a service configures caching with vary_headers: \"Accept-Encoding\" , then the Accept-Encoding header along with its value will be included in the cache key. Ultimately it’s up to developers to recognize which cache key is right for them. Keep in mind that adding more headers to the “vary_headers” list is potentially expensive and will bring hit rates down. If a header included in that list has too many possible values (e.g. “User-Agent”), then caching becomes a waste. This Fastly article goes into more details about this problem. Caching bulk endpoints Bulk endpoints: a quick primer Traditionally, REST endpoints return one resource per response: GET /user/42\n{\"id\": 42, \"first_name\": \"Mark\", \"review_count\": 2, ...} Bulk endpoints, on the other hand, typically return multiple resources in one request/response cycle. For example, one of our core services exposes an endpoint to retrieve Yelp user information with /user/v2?ids=<one_or_more_ids> . This endpoint returns one or several user objects per response: GET /user/v2?ids=1,2,42\n[\n    {\"id\": 1, \"first_name\": \"Bob\", \"review_count\": 0, ...},\n    {\"id\": 2, \"first_name\": \"Alice\", \"review_count\": 4, ...},\n    {\"id\": 42, \"first_name\": \"Mark\", \"review_count\": 2, ...}\n] Bulk endpoints are fairly common inside of Yelp’s infrastructure, especially for core data services. They’re useful to avoid network overhead by letting clients minimize the number of round trips necessary to retrieve the pieces of data that they need. Caching bulk endpoints with Casper was non-trivial and led to interesting challenges that we didn’t foresee. Accommodating bulk endpoints in Casper First, let’s talk about why caching bulk endpoints leads to problems with a naive caching strategy. Consider requests being made to /user/v2?ids=1,2,42 , /user/v2?ids=1,42,2 and /user/v2?ids=42,1,2 . They all return the same data (information about users #1, #2 and #42) yet their URLs are distinct. Assuming we enable caching for this endpoint, Casper stores 3 different objects in its cache. Lots of inefficiencies to this scenario: We have 3 objects in the cache instead of one. This occupies unnecessary space in Casper’s datastore If a client comes along and requests /user/v2?ids=42,2,1 (yet another combination!), /user/v2?ids=1,2 (a subset), or /user/v2?ids=1,2,42,43 (a superset), Casper is unable to serve data from its cache and has to forward the entire request (complete cache miss) If user #42 changes (say, they write a review and we have a new review_count value), it’s impossible to perform granular invalidation. We’d have to invalidate all keys where “42” appears, regardless of which other resources are requested alongside of it. We author our internal endpoints somewhat consistently across the board with Swagger and use JSON/HTTP as our encoding/transport of choice. This makes it possible to assume the following when it comes to bulk endpoints: Bulk endpoint URIs contain a comma-separated list of resource IDs (e.g. /bulk/endpoint?ids=1,2,3 ). The param name can be anything (e.g. reviews=1,4,16 ) Bulk endpoint responses are JSON-encoded object arrays, and each object contains a resource id. For example, a request to a bulk endpoint for resources 1, 2 and 3 is in the form \"[{id: 1,...}, {id: 2,...}, {id: 3,...}]\" To cache bulk endpoints responses more efficiently, we parse responses to extract resources and cache them separately. At request time we extract IDs from its URL and use them to fetch the corresponding resource in Casper’s cache. Concrete example and illustration If a request comes in for /user/v2?ids=1,2,3 Casper forwards the request downstream because nothing is in its cache yet. When the response comes back, it’s parsed and Casper extracts user objects from it. This lets us populate the cache with as many entries as user objects present in the response (3 in this case). For subsequent requests, we extract IDs from request URIs and use them to retrieve users from Casper’s cache. Concretely, once we have user #1, #2 and #3 in our cache: GET /user/v2?ids=1,2,3 is a hit. But so is /user/v2?ids=3,1,2 or any permutations of requesting data about users #1, #2 and #3. GET /user/v2?ids=1 is also a hit since we have user #1 in our cache GET /user/v2?ids=1,2,3,4 is a partial hit/miss. In this case, we get users #1, #2 and #3 from cache, but send a downstream request for resource 4 Below is a slide deck summarizing these scenarios (important note: for the sake of clarity, cache writes are depicted as if they happened synchronously, but in reality they happen asynchronously !) (see this deck on SlideShare ) Note that this strategy also solves our invalidation problem: if user #1 changes, we can invalidate precisely that user without affecting the rest of our cache. Keep reading for more information about how Casper’s cache gets invalidated. If you’re curious about how the code behind bulk endpoints works, head to bulk_endpoints.lua . Configuration We mentioned in the previous section that Yelp services had the ability to “configure” bulk endpoint caching and customize their cache keys. How does this work exactly? How does a service opt into using Casper? To enable caching services opt-in via PaaSTA’s SOA configs . We’ve baked that into our platform with a new directive, proxied_through . If a service declares proxied_through: casper.main in its configuration ( smartstack.yaml file), then Synapse generates HAProxy configuration such that requests are forwarded to Casper first, then to a service backend. Curious about how this works in more details? This directive is defined here , and documented there . Another system that we rely on is our “srv-configs” (service configs) system. It’s essentially a set of files distributed on all hosts at Yelp and is the canonical place for configuration used by services (as opposed to SOA configs representing configuration about Yelp services). On each Yelp server, we thus have Casper-specific configuration: srv-configs\n├── casper\n│   ├── service.foo.main.yaml\n│   ├── service.bar.main.yaml\n│   ├── ... This is the place where services declare which endpoints they want cached, whether or not these endpoints are bulk endpoints, and which set of headers should be part of the cache key, among other things. Here is a simple configuration file: cached_endpoints:\n    foo_cache: {ttl: 300, pattern: \"^/foo/bar$\"}\nvary_headers:\n    - Accept-Encoding\nuncacheable_headers:\n    - 'Date'\n    - 'X-Zipkin-Id' This allows for very explicit caching rules, all gathered in a central place, not tied to application code. Invalidating Caches Famously shortlisted as one of the hardest problems in computer science , we anticipated a lot of hurdles around cache invalidation. Our solution today includes two mechanisms for invalidation: through an API that we built into Casper (Casper then talks to its datastore to expire entries, see the code here ), and through a standard TTL system. The invalidation API is a standard, human-friendly HTTP API, which allows for manual cache invalidations in case of emergencies (“I need this cache busted right now!”). We also use this API to reactively invalidate resources instead of letting them expire naturally. A concrete example is user data, which we cache for 1hr. In the absence of reactive invalidation, any update to a Yelp profile (say, an update to your tagline) could take up to an hour to be reflected on your app. To address this concern we’ve built a system based our Data Pipeline to trigger invalidation requests when our database commits changes to our “user” table, bringing down the potential staleness from up to an hour to consistently less than a few minutes. Results Casper has been running in our production data centers for more almost a year now. It currently offloads a big chunk of traffic off of our primary internal API service, saving 20-30% of its compute capacity. Below are 2 graphs showing the rollout of Casper in front of an endpoint called user.retrieve_users_by_ids_v2 . On the top is QPS (query per second) for each AWS region, and on the bottom is hit rate (in blue), raw number of hits (green) and raw number of misses (yellow/brown): Observations from the graphs above: QPS went down severely. This is good! It means less compute resources used overall for that given endpoint (serving responses from our cache is way cheaper) Cache hit rate (blue dotted line on the bottom graph) is above 80%, which means for >80% of requests sent to that endpoint, the response is returned immediately from Casper’s cache instead of being computed in the underlying service. This is a huge win for performance. See the two spikes in traffic from the bottom graph? They were completely “absorbed” by our cache. Most responses delivered during these spikes were served by Casper. In these 2 instances, Casper successfully “shielded” the service from traffic spikes. That’s a win for reliability and performance as well. Aside from the performance wins, let us also celebrate the fact that we can measure per-service and per-endpoint QPS, hit rates and timings really easily now! This is because Casper was built with some baked in metrics reporting: see metrics_helper.lua for the nitty-gritty details on how we do this. A look ahead While we’re truly happy with the way Casper has been helping our infrastructure, we’re aware of a few challenges we aim to consider and address as we move forward. Some have simple resolutions while others will require some creative problem-solving. Static cache keys are not good enough. What we have now (configurable cache keys on a per-service basis) is better than pure URL-based caching, but we’d eventually want to provide services with the ability to dynamically set cache keys per request. This can be done if we supported the “Vary” header (see Yelp/casper#18 ) Lack of generic mechanism to invalidate subgroups of cache objects. What if we wanted to invalidate all entries in all caches related to user #1? This is typically solved by surrogate keys , and we’re looking into building this into Casper ( Yelp/casper#17 ) Casper’s bulk endpoint logic relies on assumptions specific to Swagger and JSON-encoding. This makes our bulk endpoint support less broad than it could be since we exclude Yelp services with different bulk endpoints implementations/assumptions. This is a hard problem to solve, for which we don’t have a solid lead at the moment. As Casper gets more and more popular, it handles more and more traffic and becomes a critical part of our infrastructure. While we’ve initially built Casper to be “fail-safe” it’s becoming increasingly unrealistic to completely shut it off without Yelp users suffering the consequences of an infrastructure under pressure (slow responses, timeouts). Yelp/casper#7 contains a potential avenue to solve this problem. Conclusion I hope that you’ve had as much fun reading this post as I’ve had writing it. Again, Casper is opensource and available on Github today. I’d highly encourage you to open issues or ask questions if you are curious. While we don’t expect Casper to be “plug-and-play” (after all, it’s built with Yelp’s infrastructure in mind!), we hope that it’ll inspire others to replicate its design to make their infrastructure more efficient and speed up user experience! Acknowledgements Casper has had a number of key contributors I’d like to acknowledge for\ntheir respective contributions: Bai L. for the initial prototype and rollout Anthony T. for the design and implementation of our bulk endpoint logic Prateek A. for benchmarking different datastore options for Casper Daniele R. and Avadhut P. for their continued help and contributions, especially during our migration to Cassandra Ben P., Stephen A., Daniele R., Tomer E. and Liina P. for suggestions and comments on this blog post Tweet Want to become one of our performance engineers? Yelp engineering is looking for engineers passionate about solving performance problems. Sounds interesting? Apply below! View Job Back to blog", "date": "2018-03-12"}, {"website": "Yelp", "title": "Introducing Salsa: A tool for exporting iOS components into Sketch", "author": ["\n        \n  Max Rabiciuc, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/04/open-sourcing-salsa.html", "abstract": "What is Salsa? Salsa is an open source library that renders iOS views and exports them into a Sketch file. We built Salsa to help bridge the gap between design and engineering in an effort to create a single source of truth for visual styling of UI. Why use Salsa A few years ago, we started putting together a library of common components that developers and designers could use to build features. Initially, we had to manually maintain consistency between the designs in Sketch and their implementations in code. When we only had a handful components, this wasn’t difficult to maintain. However, as the library grew, it became increasingly difficult to keep the code and Sketch files in sync with one another. Salsa allows us to ensure that designers and developers always have access to the exact same components. One of the  unique advantages of Salsa is that it does not require Sketch to be installed on the machine it’s running on, enabling us to integrate Salsa into our continuous integration flow and generate a new master Sketch file any time a code change is made to the styleguide repository. Salsa also allows us to create lots of permutations of the same component without any additional effort. We can even include loading states when generating components. How it works Running Salsa inside of an iOS simulator will output two things into a specified directory: a .salsa file and an images folder. You can then pass these two inputs into the salsa command line tool to compile them into a .sketch file. Why two steps? Certain macOS-only APIs need to be used to encode text for .sketch files. Having two steps allows us to define our own intermediate file format that’s easier to work with than the full sketch file format. This means we can leverage this file format in the future if we want to expand this tool for other platforms. If you want to create your own .salsa file, you can check out the schema here Installing Salsa pod 'Salsa' brew tap yelp/salsa\nbrew install salsa Using Salsa import Salsa Converting a view to a Sketch Group // Configure the export directory SalsaConfig . exportDirectory = \"/some_directory\" // Convert a view into a group let myGroup = myView . makeSketchGroup () Putting a group into a sketch document and exporting to a salsa file // Create a page containing the generated group, and insert it into a Document let document = Document ( pages : [ Page ( layers : [ myGroup ])]) // Export the document to disk try ? document . export ( fileName : \"my_file\" ) Converting a salsa file to a sketch file In your terminal of choice run the following: $ salsa -f /some_directory/my_file.salsa -e /some_directory/my_file.sketch Creating a Sketch file documenting your standard UI elements We provide some helpers to help you document your elements out of the box. You organize examples of your views into an Artboard by conforming your view class to ArtboardRepresentable . extension View1 : ArtboardRepresentable { static func artboardElements () -> [[ ArtboardElement ]] { ... } } If you would like to also create Symbols of your views to go along with the generated Artboards you can instead conform your views to SymbolRepresentable . extension View2 : SymbolRepresentable { static func artboardElements () -> [[ ArtboardElement ]] { ... } } Create your Artboards and Symbols from these ArtboardRepresentable and SymbolRepresentable views // Configure the export directory SalsaConfig . exportDirectory = \"/some_directory\" // Generate the artboards and symbols let artboardsAndSymbols = makeArtboardsAndSymbols ( from : [[ View1 . self ], [ View2 . self ]]) // Put the artboards and symbols onto their own dedicated pages let artboardPage = Page ( name : \"Artboards\" , layers : artboardsAndSymbols . artboards ) let symbolPage = Page ( name : \"Symbols\" , layers : artboardsAndSymbols . symbols ) // Create a document with the generated pages and export it let document = Document ( pages : [ artboardPage , symbolPage ]) try ? document . export ( fileName : \"my_file\" ) Give it a Try The source code for Salsa is available on github at https://github.com/yelp/salsa . If you’d like to contribute feel free to send us a pull request. We’re excited to see what kinds of cool things we can build together using Salsa! Tweet Back to blog", "date": "2018-04-24"}, {"website": "Yelp", "title": "Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift", "author": ["\n        \n  Jacob Park, Software Engineering Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/04/black-box-auditing.html", "abstract": "Since Yelp introduced its real-time streaming data infrastructure, “ Data Pipeline ”, it has grown in scope and matured vastly. It now supports some of Yelp’s most critical business requirements in its mission to connect people with great local businesses. Today, it has expanded into a diverse ecosystem of connectors sourcing data from Kafka and MySQL, and sinking data into Cassandra, Elasticsearch, Kafka, MySQL, Redshift, and S3. To ensure that the whole ecosystem is functioning correctly, Yelp’s Data Pipeline infrastructure is continually growing its repertoire of reliability techniques such as write-ahead logging, two-phase commit, fuzz testing, monkey testing, and black-box auditing to verify the behavior of various components within the ecosystem. Black-box auditing is an essential technique we employ to verify the behavior of every individual component within Yelp’s Data Pipeline infrastructure. It’s particularly useful because it does not rely on any assumptions about the internals of the components. In this post, we’ll explore how we designed a novel algorithm to black-box audit one of our most critical flows: replicating data from MySQL into Redshift. MySQLStreamer, Application Specific Transformer, and Redshift Connector Before explaining why we designed a novel algorithm to black-box audit end-to-end replication integrity between MySQL and Redshift, it is important to provide a bit of context on how replication between MySQL and Redshift is achieved in Yelp’s Data Pipeline infrastructure: The MySQLStreamer is a single leader, producing Kafka messages from the binary log of a MySQL database. The Application Specific Transformer is an intermediary stage consuming from and producing to Kafka, essentially, as one link in the asynchronous chain replication from MySQL into Redshift. The Redshift Connector is a collection of many workers consuming Kafka messages into a Redshift cluster. In this design, we chose to make the replication between MySQL and Redshift eventually consistent by favoring the availability of a MySQL database, instead of requiring acknowledgements from all downstream consumers, ultimately opting for asynchronous chain replication. In the face of transient logical errors and machine failures, we needed a way to prove replication was reliable between MySQL and Redshift. To address this concern, we designed a black-box auditing algorithm to build trust that the replication was consistent (or eventually consistent to be specific). Anti-Entropy Repair in Dynamo Systems How do other eventually consistent systems verify their data is actually eventually consistent? Dynamo systems such as Cassandra and Riak utilize a process called anti-entropy repair that incorporates Merkle trees to verify and update all replicas, ultimately ensuring that the systems are eventually consistent. The anti-entropy repair algorithm can be summarized by the following steps: Snapshot each replica Partition each replica Hash each replica (e.g. compact Merkle trees) Repair each inconsistent partition by the equality of their hashes This algorithm is very resource intensive in terms of disk I/O as it aims to minimize network utilization. As a result, Cassandra provides incremental repair that utilizes an anti-compaction process that segregates repaired and unrepaired data to successively reduce the cost in performing repairs. What’s Wrong with Anti-Entropy Repair in Dynamo Systems? Why didn’t we use anti-entropy repair to black-box audit end-to-end replication integrity between MySQL and Redshift? As alluded to in the prior section, anti-entropy repair is highly effective in systems which can segregate their data into partitions which are repaired and unrepaired. At Yelp, there is no universal method of snapshotting MySQL and Redshift to align their snapshots for proper segregation into repaired and unrepaired partitions for black-box auditing with anti-entropy repair. Furthermore, as propagation delay in an eventually consistent system reduces the integrity between replicas, a uniform distribution of writes can potentially invalidate all partitions from utilizing hashes for equality, regardless of the size of the partitions. Thus, the anti-entropy repair algorithm would state the data between MySQL and Redshift to be fully inconsistent unless a one-to-one projection of the data to hashes was saved to disk, which is a prohibitively expensive solution. Ultimately, we need to innovate or invent an algorithm that migrates away from a binary notion of replication integrity, utilizing hashes for equality, to a continuous notion of replication integrity utilizing hashes for similarity. This shift relaxes the effects of propagation delay and uniformly distributed writes from invalidating partitioning. With partitioning, the algorithm can execute at scale, with a trade-off in accuracy. New and Improved Anti-Entropy Repair for Replication between MySQL and Redshift To solve the issues of the original anti-entropy repair algorithm, our algorithm to black-box audit end-to-end replication integrity between MySQL and Redshift can be succinctly summarized by the following steps: Snapshot each replica Partition each replica Hash each replica for equality testing Hash each replica for similarity testing Identify each inconsistent partition by their equality testing hashes and their similarity testing hashes Overall, this algorithm is very similar to the original anti-entropy repair algorithm except for the addition of hashing for similarity testing. Hashing for Similarity Testing? To hash for similarity testing, we opted to use MinHash to estimate the Jaccard similarity coefficient between two sets, primarily MySQL and Redshift. Before diving into MinHash, it is important to explain the Jaccard similarity coefficient. It is a measure of similarity between two sets defined by the cardinality of their intersection divided by the cardinality of their union. A coefficient of 0 represents that the two sets are disjoint, while a coefficient of 1 represents that the two sets are equivalent. To minimize bias when calculating the Jaccard similarity coefficient, the cardinalities between the two sets should be approximately equivalent. Finally, MinHash is a technique that estimates the Jaccard similarity coefficient between two sets, by the observation that the Jaccard similarity coefficient is equivalent to the probability of a set of minimum hashes generated from k different hash functions being equivalent across the two sets. This estimation has an expected error of O(1/\\sqrt(k)) . Too Succinct, Where’s the Details? In practice, our algorithm to black-box audit the end-to-end replication integrity between MySQL and Redshift is primarily expressed in SQL, to score whether each partition common between MySQL and Redshift is consistent. Using SQL, the algorithm benefits from data locality to be network efficient by generating scores from MySQL and Redshift in sub-linear or constant space. Furthermore, Redshift’s distributed query execution engine executes the SQL as a highly-parallelized MapReduce job that executes in seconds. More Details Through an Example The following example will provide a sample set of SQL statements that accomplishes Steps 1 to 4 of the algorithm to generate hashes for equality testing and hashes for similarity testing. These hashes will be consumed by another process that will illustrate how inconsistent partitions are identified. MySQL and Redshift Datasets For this example, MySQL and Redshift share a key-value structured table of 32 rows in which 2 rows differ between MySQL and Redshift. mysql > SELECT * FROM `MASTER` ; + ------+-------------+ | id | text | + ------+-------------+ | 0 | ALPHA | | ... | ... | | 16 | QUEBEC | | ... | ... | | 24 | YANKEE | | ... | ... | | 31 | CONSECTETUR | + ------+-------------+ redshift > SELECT * FROM `SLAVE` ; + ------+-------------+ | id | text | + ------+-------------+ | 0 | ALPHA | | ... | ... | | 16 | JPARKIE | -- Different From MySQL. | ... | ... | | 24 | JPARKIE | -- Different From MySQL. | ... | ... | | 31 | CONSECTETUR | + ------+-------------+ SQL Statements The following SQL statements can be executed on MySQL and on Redshift to generate hashes for equality testing and similarity testing per partition. START TRANSACTION WITH CONSISTENT SNAPSHOT ; SELECT `partition` , MIN ( `partition_key` ) AS `min_partition_key` , MAX ( `partition_key` ) AS `max_partition_key` , COUNT ( `partition_key` ) AS `count` , SUM ( CAST ( CONV ( SUBSTRING ( `hash` , 1 , 8 ), 16 , 10 ) AS UNSIGNED )) AS `signature_0` , SUM ( CAST ( CONV ( SUBSTRING ( `hash` , 9 , 8 ), 16 , 10 ) AS UNSIGNED )) AS `signature_1` , SUM ( CAST ( CONV ( SUBSTRING ( `hash` , 17 , 8 ), 16 , 10 ) AS UNSIGNED )) AS `signature_2` , SUM ( CAST ( CONV ( SUBSTRING ( `hash` , 25 , 8 ), 16 , 10 ) AS UNSIGNED )) AS `signature_3` , MIN ( `partial_hash_0` ) AS `min_hash_0` , MIN ( `partial_hash_1` ) AS `min_hash_1` , MIN ( `partial_hash_2` ) AS `min_hash_2` , MIN ( `partial_hash_3` ) AS `min_hash_3` FROM ( SELECT `partition_key` , `partition` , `hash` , (((( `seed` * 1285533145 ) + 1655539436 ) % 2305843009213693951 ) & 2147483647 ) AS `partial_hash_0` , (((( `seed` * 1350832907 ) + 492214603 ) % 2305843009213693951 ) & 2147483647 ) AS `partial_hash_1` , (((( `seed` * 1235092432 ) + 1629043653 ) % 2305843009213693951 ) & 2147483647 ) AS `partial_hash_2` , (((( `seed` * 176256801 ) + 205743474 ) % 2305843009213693951 ) & 2147483647 ) AS `partial_hash_3` FROM ( SELECT `partition_key` , `partition` , `hash` , CAST ( CONV ( SUBSTRING ( `hash` , 25 , 8 ), 16 , 10 ) AS UNSIGNED ) AS `seed` FROM ( SELECT `id` AS `partition_key` , FLOOR ( `id` / 8 ) AS `partition` , MD5 ( CONCAT ( COALESCE ( `id` , 'NULL' ), COALESCE ( `text` , 'NULL' ))) AS `hash` FROM `MASTER` ) AS `PARTITION_HASH_SUBQUERY` ) AS `PARTITION_HASH_SEED_SUBQUERY` ) AS `PARTITION_HASH_PARTIAL_HASH_SUBQUERY` GROUP BY `partition` ORDER BY `partition` ; COMMIT ; The SQL statements accomplish the following actions: Start a transaction for a consistent snapshot Partition each row by the partition key into fixed-sized partitions Map each row of the table to a MD5 hash Map 32-bits of each MD5 hash into a seed Permute k=4 partial hashes from the seed with the h(x) - mod(a * x + b, p) ^ m hash family Split each MD5 hash into a vector of signatures Calculate the minimum partition key for each partition Calculate the maximum partition key for each partition Calculate the number of partition keys for each partition Calculate the vector sum of the signatures for each partition to be the partition’s hashes for equality testing Calculate the k=4 MinHash values for within each partition to be the partition’s hashes for similarity testing Step 1: Snapshot each Replica Action 1 illustrates how each replica is snapshot by utilizing consistent-read semantics provided by a transaction with a consistent snapshot. Step 2: Partition each Replica Action 2 illustrates how each replica is partitioned by floored division with fixed-sized partitions on the partition key. Step 3: Hash each Replica for Equality Testing Actions 3, 6, and 10 illustrate how all the row-level hashes of a partition are mixed by vector summation to generate a partition-level hash to be used for equality testing. Step 4: Hash each Replica for Similarity Testing Actions 4, 5, and 11 illustrate how all the row-level hashes are seeded to be permuted by the h(x) - mod(a * x + b, p) ^ m hash family into partition-level, k=4 MinHash values to be used for similarity testing. MySQL and Redshift Results The following tables lists the results of executing the SQL statements on MySQL and on Redshift. The green highlight illustrates that the values are equivalent between MySQL and Redshift. The red highlight illustrates that the values are not equivalent between MySQL and Redshift. Example MySQL Results Example Redshift Results Analyzing MySQL and Redshift Results Utilizing the signatures S0 , S1 , S2 , and S3 , which were generated for equality testing, we can state the following with confidence: Partitions 0 and 1 are equivalent between MySQL and Redshift Partitions 2 and 3 are different between MySQL and Redshift If these absolute statements were accepted at face value, then 50 percent of the data within Redshift would need to be repaired. In actuality, only 6 percent of the data needs to be repaired. This discrepancy is too draconian if we believe that MySQL and Redshift primarily diverge due to propagation delay. Luckily, the MinHash values MH0 , MH1 , MH2 , and MH3 , which were generated for similarity testing, can be utilized to relax such harsh conclusions by introducing some uncertainty. If we apply the MinHash technique to Partitions 2 and 3, then we would respectively estimate the Jaccard similarity coefficients to be 3/4 or 0.75 and 4/4 or 1.0, both with an expected error of 0.5. These estimated Jaccard similarity coefficients suggest that Partitions 2 and 3 are not drastically different between MySQL and Redshift. Hopefully, such a suggestion would relax operators from eagerly repairing transiently divergent partitions. Scoring Consistency between MySQL and Redshift In practice, to effectively respond to a black-box audit of the end-to-end replication integrity between MySQL and Redshift, we use a bounded score of their consistency/similarity such that we can escalate incident response if the bounded score falls below desired thresholds. The following is the formula we used to calculate the bounded score. Consistency Scoring Formula The following list explains all the symbols within the formula. Let J represent the similarity between a MySQL table and a Redshift table Let A represent the data within the MySQL table Let B represent the data within the Redshift table. Let N represent the total number of partitions shared between the MySQL table and the Redshift table Let m represent the number of partitions where their signatures are equal Let n represent the number of partitions where their signatures are not equal Let h_min represent a permutation in the MinHash of the i th partition Let k represent the number of permutations Let |h_min(A_i) = h_min(B_i)| represent the number of equivalent permutations This score is akin to the Jaccard similarity coefficient. Thus, for the previous example, the actual Jaccard similarity coefficient between MySQL and Redshift is 30/34 or 0.882. Meanwhile, its bounded score is 0.938 bounded by 0.688 and 1.0. Although 0.938 is fairly close to 0.882, such a wide bounded score is unsettling. In practice, we utilize higher values of k to restrict the bounds for increasing accuracy for a trade-off in network efficiency and time efficiency. Conclusion At Yelp, we use either the old or the new anti-entropy repair algorithm to audit end-to-end replication integrity between many MySQL and Redshift tables. By analyzing the distribution of writes per table, we opt to use one algorithm over the other, optimizing accuracy versus performance. These algorithms deliver daily reports that state the overall similarity scores between MySQL and Redshift tables. If an overall similarity score is below a specified threshold, its report lists all the divergent partitions between a pair of MySQL and Redshift tables. These reports empower operators to investigate and resolve potential problems within the Data Pipeline infrastructure. To conclude, I hope you’ve enjoyed learning how we designed our own black-box algorithm to verify end-to-end replication integrity between MySQL and Redshift and encourage you to explore how your own systems can benefit from the use of black-box auditing! Aside: Yelp Internship Program The work discussed within this Yelp Engineering Blog post was completed as one of my projects during my internship with Yelp at the San Francisco office in Summer 2017. Yelp has a phenomenal internship program in which interns can join a wide breadth of teams to work on exciting and relevant work as if you were a full-time software engineer. Yelp’s commitment to help interns enjoy their stay in the company and the city is fantastic. At the time of writing this post, I am completing my third internship with Yelp. For any students actively looking for internships, visit https://www.yelp.com/careers/teams/college-engineering . This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Back to blog", "date": "2018-04-12"}, {"website": "Yelp", "title": "Performance Improvements for Search on The Yelp Android App - Part 1", "author": ["\n        \n  Tyler Argo, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/05/android-search-perf-improvements-part-1.html", "abstract": "#perfmatters At Yelp, we’ve been working hard to improve the performance of our search results on mobile devices (iOS and Android). We know that performance matters from a numbers perspective, but another reason why we’ve decided to invest more in performance is due to a recent increase in user studies commissioned by Yelp. We noticed that most users “grunted or made noises when waiting for search to load”. We took this as a sign that something needed to be done! In our quest to make search faster on the Android mobile app client, we broke down performance into two different categories: perceived search performance and scroll performance. 1. Perceived Search Performance Perceived search performance is the time it takes between when a user presses the search button to when they see the first search result rendered on screen. This is made up of three separate time spans illustrated in the following diagram: For each search request on the Android app, we measure the time it takes the request to be resolved by the back-end and parsed on the client. We record this timing in the search_request timing metric, represented in red above. We also measure the total amount of time it takes from when the request is sent to when the results are rendered on the device with a custom metric that we refer to as the search_results_loaded timing metric. This metric is associated with the same request id as the search_request timing metrics. We can then subtract the search_request timing metrics from the search_results_loaded timing metrics to get the total amount of time spent rendering search results on the device. This is represented by the second blue section and it’s where we’ll be focusing our attention for perceived search performance. Over the past few months, we’ve managed to reduce the perceived search performance timing from 350ms to 190ms at the 50th percentile (and there’s still room for improvement.) 2. Scroll Performance This is the performance associated with how quickly the device can render each new frame as the user scrolls through a list of businesses. Most device screens refresh 60 times per second (60Hz). To get the smoothest possible animation and scroll performance, we need to be able to render 60 frames per second to match that refresh rate. That gives us 16 milliseconds to do all the calculations needed to get pixels drawn on the screen, which is not a lot of time. If we miss this 16 millisecond mark, then scrolling starts to look janky as the display is refreshing but has no new frames to show the user. This delay creates a disconnect between the user’s smoothly scrolling finger on the screen and the pixels underneath, and overall makes the app feel slow. In our performance work we’ve been able to significantly reduce jank and improve the scroll performance of our search results leading to a better user experience: The Performance Improvement Lifecycle Both perceived search performance and scroll performance can be improved using a common set of steps even if they require different types of technical optimizations. These are steps anyone can take on any platform to improve performance and make sure the the improvements are durable. First, you can’t improve if you don’t know where you currently stand so it’s important to record an initial assessment of performance. Second, once you have a baseline measurement of performance for the feature you’re attempting to improve, you can start to take actions to improve this metric to achieve your goal. And last, after the improvements have been made, you want to make sure that there are no regressions in this metric, so it is important to monitor your changes. Because there is never really a limit to how much you can improve performance (only diminishing returns), this is an iterative process. This cycle of performance improvement can be defined as a series of steps: Measure Improve Monitor Repeat We followed these steps in our performance work to great effect and we’ll go over how to implement the “Measure” step using the tools available to us as Android developers. In two subsequent posts we’ll also explain how we were able to improve the performance of our search results and how we implemented a monitoring system to ensure that we don’t regress. Step 1: Measuring Performance One of the advantages of developing for the Android platform is the wide range of tools available for profiling and measuring app performance. Here’s a brief overview of the tools that can be used to profile your app along with some links to the official documentation: Debug GPU information on screen as bars Great for visually determining where performance bottlenecks are occuring Gfxinfo sysdump More detailed format for analyzing performance bottlenecks as a whole Outputs results as a text file through adb FrameMetrics API Extremely detailed and programmable API for analyzing performance and identifying bottlenecks Debug GPU Overdraw A visual tool to see where the GPU is doing duplicate work Systrace A great tool for identifying when best practices are not being followed and pointing towards relevant documentation Good place to start for identifying high level performance issues Traceview A detailed CPU profiling tool that can help identify slow code across different threads Useful for digging into code performance at a lower level than Systrace Android Studio CPU Profiler Very similar to Traceview but with a nicer UI and call charts Choose Your Metrics Wisely It’s important to choose key metrics and see how they change during the improvement stage of the performance lifecycle. These key metrics might vary based on the feature that you’re working on, so it’s important to understand what you want to measure and to ensure that it aligns with your performance goal. In our case, we wanted to improve perceived search performance and scroll performance. For this we chose to measure two simple metrics that aligned with our goals: Initial Search Rendering Performance This was the measurement of the amount of time that was spent on the client rendering the results (the second blue section of the below graph). We were able to accurately measure this using the search_request and search_results_loaded timing metrics mentioned earlier. The metrics are recorded and aggregated in near real-time through Splunk on every search request and provide us with powerful insight into the performance of our search requests and rendering times. We found the baseline for our search results rendering to be 350ms at the 50th percentile. search_results_loaded - search_request = search results rendering Scroll Performance This was measured by examining the percentage of total frames that took longer than 16ms to render as we scroll down the list of search results. In our initial baseline measurements, we used gfxinfo sysdump and manual testing to determine a baseline. We later moved to the FrameMetrics API and automated testing (outlined in the monitoring section) to get an accurate count of the frames dropped during scroll performance. As our baseline for scroll performance we found that 33% of all frames rendered when scrolling down the search page were dropped. That means a third of our frames were taking longer than 16ms to render. Yikes! Testing on Older Devices The Yelp app is installed on a wide variety of Android devices, from top-of-the-line phones to phones that were released many years ago. In establishing our baseline for performance, we wanted to be confident that the great majority of our users’ devices can run the app smoothly and have a solid user experience. Most developers run their application in an emulator running on a high-end computer. If not, then they likely have a test device released in the past 1-2 years. That’s great for feature development (you want the app to load your changes as quickly as possible), but for performance testing you can miss out on a lot. Issues that might not affect your brand new phone might bring an older phone to its knees so testing performance on an older device can help to identify bottlenecks. By fixing performance issues on older devices, you’re also ensuring newer devices will run more smoothly and that your entire user base will have a great experience. In our case, we tested on a Nexus 5 running Android N. The Nexus 5 was released almost 5 years ago, as of today, which is a lifetime in smartphone years. The scroll performance was noticeably worse than the scroll performance on the newer Galaxy S8. On the Nexus 5, as we slowly scrolled down the list of search results, each time a new view scrolled into the viewport, we could see a noticeable dip in performance. This same performance impact was barely noticeable on the S8 and because of this, we used the slower Nexus 5 to test our incremental improvements to scroll performance. No User Left Behind We also measured the performance of older devices by monitoring metrics at scale. We looked at the performance numbers not only at the 50th percentile, which shows us the performance for half of our users, but also at the 90th percentile which gave us an idea of what the user experience might be like for users on older devices. From there, we filtered out poor network performance as a factor for these slower timings by using the search_request and search_results_loaded timing metrics mentioned earlier. By doing this, we were able to determine the amount of time spent rendering on the client for the 90th percentile was 656ms which was reduced to 394ms after our performance work, which is a huge improvement for those with less powerful devices. Conclusion Now that we’ve completed step 1 of the improvement lifecycle by establishing a baseline on which we can improve, the next step is to actually implement the changes that will move us towards our end goal. But where do we start? What changes can we make to improve client-side performance? You might already have a few ideas in mind, but we’ll cover how we improved our performance in the next blog post ! Tweet Want to build next-generation Android application infrastructure? We're hiring! Become an Android developer at Yelp View Job Back to blog", "date": "2018-05-02"}, {"website": "Yelp", "title": "Active Directory Password Blacklisting", "author": ["\n        \n  Leeren Chang, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/04/ad-password-blacklisting.html", "abstract": "Many enterprise professionals use passwords that are weak and easily compromised. Equipped with this knowledge, as well as the exposure of more and more password leaks, dictionary attacks focused on compromised or popular passwords have become increasingly effective. As such, the National Institute of Standards and Technology recommends password blacklisting as a highly-effective means of preventing such attacks. Unfortunately, use of password blacklisting countermeasures has remained a relatively new innovation that has not yet achieved widespread corporate adoption. At Yelp, however, we strive to add the latest and greatest defense mechanisms to our arsenal, which is why we adopted such password blacklisting countermeasures very quickly. Since Yelp uses Active Directory (AD) for all employee authentication and management, implementing our own customized Password Filter dynamic-link library (DLL) was the clear solution. In this blog post we will outline how we built a password blacklisting service out of an existing open source DLL that met our policy and security needs. Why didn’t we use a commercial option Given the plethora of commercial options available, we debated whether to use a pay-for-pricing solution or an existing open-source option. Many of the 3rd party providers in the space offered software solutions that were easy to set up and proven to be effective. However, they often lack transparency in password filtering. We also worried that any fault on the third-party side could cause the Local Authority Subsystem Service to crash, resulting in a Blue Screen of Death and a reboot of all affected Domain Controllers (DCs). The outcome of such an event would greatly impact employees using the system. Lastly, reliance on third-party providers for such sensitive issues would increase Yelp’s attack surface area so we opted for an open-source solution that would give us fine-grained control. Choosing the solution After much research and testing, we found that OpenPasswordFilter (OPF) suited our needs best. We chose a forked version of the original implementation that added the ability to connect with a SQL database rather than comparing hashes in plaintext. One of the biggest concerns with utilizing a customized Password Filter DLL was the risk that any error run by the Local Security Authority (LSA) could potentially crash the running DC server. Another major concern was ensuring that new passwords could be appended quickly and effectively without affecting user experience. The chosen solution solved both these problems by incorporating a service-oriented architecture that decoupled essential LSA thread code from filtering functionality while also integrating an SQL-based DB that could be directly queried by the service. Furthermore, the design was fail-safe. Any triggered exceptions or errors during filtering would cause the system to fail open, mitigating any worrisome DC shutdown possibilities. Given the vulnerability of the LSA to DLL errors, this was a necessary tradeoff, especially knowing that errors would be logged if compromised password resets were made. System Architecture The Password Filtering service is comprised of three places: OPF Service : The module for password verification (talks with the DLL over loopback). OPF DLL : The raw Password Filter DLL which interfaces with the LSA for credential approval. OPF DB : The database holding SHA-1 hashes of all easily-compromised passwords. Whenever a client requests a password change, the request routes through their assigned domain controller that contacts the LSA. If the default password policy is not met (a combination of minimum password length and certain character criteria), the Password Filter DLL is not called and the password is automatically rejected. If, however, the default policy requirements are met, the DLL will be called with the password. The diagram below illustrates the process: Figure 1: AD Password Filter Authentication Flow Client initiates password request change through DC to LSA. The contact from DC to LSA is done by configuring the LSA notification package. If default password policy is met, registered DLLs are called, otherwise the password is denied. The PasswordFilter function, one of the three core methods of Microsoft’s PasswordFilter DLL interface, is called. This function returns a boolean response based on whether or not a reset should be made. The OPF version attempts connection to a specific port on the loopback interface to call the registered service. If this fails, the function returns true due to its fail-safe nature. If a connection can be established, the DLL attempts to send credentials to the OPF Service. A predetermined preamble is sent first, after which credentials are propagated. The hashed credential (SHA-1) is then checked for existence in the DB of easily-compromised passwords. An index on password hashes makes this significantly faster. Depending on whether the hash was found, a message indicating success or failure is returned to the OPFService, which makes its way to the initial DLL as a boolean, ending back in the hands of the LSA. The above process is repeated for all registered DLLs. Assuming all are successful, the password is officially committed to the Security Account Manager (SAM). The PasswordChangeNotify function is then called for each registered DLL for synchronization purposes. Testing Thoroughly testing this process was particularly important, since any errors would have serious impact, such as crashing the Domain Controller. Additionally, we wanted to make sure the system didn’t impose a significant performance overhead. Therefore, before  touching our production environment, we tested the service on a standalone domain controller, and then again in a lab environment. The methodology was as follows: Setup a DC in its own standalone domain or in a lab domain. Configure the default domain password policy settings in accordance to Yelp’s password policy. Download a large dump of the top most commonly used/breached passwords. We chose to sample from this popular security password list. Randomly divide the password dump into four subsets (we used 4 of size 5000 each): Those conforming to default policy but belonging in the blacklist (A) Those conforming to default policy and not belonging in the blacklist (B) Those not conforming to the default policy and belonging in the blacklist (C) Those not conforming to the default policy and not belonging in the blacklist (D) For the above four sets, we would thus expect only B to result in reset successes. Generate SHA-1 hashes for B and C Create enabled LDAP users in the amount of total passwords to be tested Create a CSV file with each row containing a user, password, and category (A, B, C, D) pertaining to the password categorization. Run our custom-made powershell script (next page) for checking verified behavior. Powershell script details: Each enabled user attempts to have their respective password reset The total time required is recorded using a stopwatch Expected reset success is validated based off exit code An AD bind is then performed to verify successful login Expected AD bind behavior is also validated based off exit code Results are aggregated and output by password category (specifically, average time per password reset and number of errors found) The below script does the testing described above. param ([ string ] $file , [ string ] $dc , [ string ] $admin_usr , [ string ] $admin_pwd ) # Default error usage message Set-Variable errUsg -option Constant -Value \" $( $MyInvocation . MyCommand . Name ) [CSV_FILE] [DC] [ADMIN_USR] [ADMIN_PWD]\" # Ensure proper parameters are given $CommandName = $MyInvocation . InvocationName $ParameterList = ( Get-Command -Name $CommandName ) . Parameters foreach ( $key in $ParameterList . Keys ) { $value = ( Get-Variable $key -ErrorAction SilentlyContinue ) . Value if ([ string ]:: IsNullOrEmpty ( $value )) { Write-Host \"Required parameter not found.\" Write-Host \" $errUsg \" exit 1 } } # Check existence of csv file if ( ! ( test-path $file )) { Write-Host \"File $file not found.\" exit 1 } # Set password types and associated expectations (0 = failure, 1 = success) $csv = Import-CSV -Path $file $pwdTypes = @( \"NO_POLICY_NO_DUMP\" , \"NO_POLICY_YES_DUMP\" , \"YES_POLICY_NO_DUMP\" , \"YES_POLICY_YES_DUMP\" ) $expected = @( 0 , 0 , 1 , 0 ) $times = @( 0 ) * 4 $errors = @( 0 ) * 4 $counts = @( 0 ) * 4 # Check provided DC is valid and that admin login credentials valid dsquery user -u $admin_usr -p $admin_pwd -s $dc -q > $null 2 > & 1 if ( $LastExitCode -ne 0 ) { Write-Host \"Invalid remote credentials or DC server specified.\" exit 1 } # Start logging start-transcript -path C:\\nail\\syslog\\DLL_LOG_ $ ( get-date -format 'MM-dd-yyyy-HH-mm-ss' ) . log Foreach ( $el in $csv ) { # Extracted relevant fields $ex = $expected [ $el . Id ] $type = $pwdTypes [ $el . Id ] $usr = dsquery user -samid $el . User -s $dc -u $admin_usr -p $admin_pwd $pwd = $el . Password # Record time in milleseconds for password reset $sw = [ system.diagnostics.stopwatch ]:: startNew () dsmod user $usr -pwd \" $pwd \" -mustchpwd no -d $dc -u $admin_usr -p \" $admin_pwd \" > $null 2 > & 1 $sw . Stop () # Check exit code with expectations to validate reset success $r_s = If ( $lastexitcode -ne 0 ) { 0 } Else { 1 } $t = $sw . get_ElapsedMilliseconds () $times [ $el . Id ] += $t if ( $r_s -ne $ex ) { $errors [ $el . Id ] ++ } # Perform AD bind and check with expectations to verify successful login ( new-object directoryservices.directoryentry \"\" , $usr , $pwd ) . psbase . name -ne $null > $null 2 > & 1 $b_s = If ( $lastexitcode -ne 0 ) { 0 } Else { 1 } if ( $b_s -ne $ex ) { $errors [ $el . Id ] ++ } # Log appropriately in the format of (EXPECTED, ACTUAL) for both the reset and bind $logline = \"( $usr ) : [ $type ] : RESET ( $ex , $r_s ) : BIND ( $ex , $b_s ) : TIME ( $t )\" if (( $b_s -ne $ex ) -or ( $r_s -ne $ex )) { $logline = \"[ERROR] [PASSWORD = $pwd ] $logline \" } $counts [ $el . Id ] ++ Write-Output $logline } # Aggregate final results Write-Output ( \"*\" * 22 ) Write-Output \"RESULTS:\" for ( $i = 0 ; $i -lt $pwdTypes . Length ; $i ++ ) { $type = $pwdTypes [ $i ] if ( $counts [ $i ] -ne 0 ) { $t = $times [ $i ] / $counts [ $i ] } else { $t = 0 } $es = $errors [ $i ] Write-Output \"( $type ) ERRORS: $es AVG TIME: $t \" } stop-transcript Results Testing not only allowed us to verify correctness of our password filtering service, but also visualize data like reset time intervals: Figure 2: AD Password Reset Time Intervals (with DLL) More importantly, the DLL imposed marginal overhead: Figure 3: AD Password Reset Time Intervals (with DLL) Figure 4: AD Password Reset Time Intervals (with DLL) Indeed, on average, with approximately 1 million passwords in our dump, we found the following: Average Rest Time - Standalone Domain Category Time Not in dump 115.7656 ms Yes in dump 83.2938 ms Average Rest Time - Lab Domain Category Time Not in dump 415.8513 ms Yes in dump 377.3411 ms Conclusions All in all, this project allowed us to better understand the underpinnings behind AD authentication and gave Yelp a much stronger layer of security for preventing threats against easily-compromised password dictionary attacks. We strongly recommend employing similar blacklisting measures in your corporate environment. At the very least, ensure strong minimum password policy requirements. But understand that blacklisting offers a more effective means of mitigating against dictionary attacks while not imposing the counterproductive restrictiveness high-complexity password requirements offer. For further readings, please refer to the original author’s open-source fork which inspired our implementation. Tweet Back to blog", "date": "2018-04-16"}, {"website": "Yelp", "title": "Code Review Guidelines", "author": ["\n        \n  Jonathan Maltz, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/11/code-review-guidelines.html", "abstract": "We deeply value code review and feel that it’s crucial to being a high-functioning engineering organization. Code review results in higher quality code that is more broadly understood. It also lets engineers learn from their peers, practice mentorship, and engage in open dialog and discussion about what they build. The benefits of code review align well with Yelp’s value Play Well with Others and support our culture of continually teaching and learning. As our organization has continued to grow, there are certain patterns that have made code reviews more beneficial and keep them from becoming a bottleneck. We’ve been following these principles for years and wanted to share them with the broader community because they work so well for us. Jason Fennell, SVP Engineering Code Review Guidelines These guidelines stem from what code review should accomplish. It’s impossible for us to lay out guidelines which will be applicable to every situation so staying mindful of these goals will help you adhere to “the spirit” of code reviews even when you encounter a situation they don’t cover. Code reviews should: Verify that the code is a correct and effective solution for the requirements at hand Ensure that your code is maintainable Increase shared knowledge of the codebase Sharpen your team’s skills through regular feedback Not be an onerous overhead on developer time Guidelines When posting a code review Pick the right reviewer(s) You should choose reviewers who can confirm that your code is correct, well-architected, and conforms to conventions within a reasonable timeframe. Communication is key to prevent yourself from getting blocked on code reviews. If you find that your reviewers are bottlenecking the process, work with them to find more appropriate reviewers or determine a timeline that works for all of you. Always have a primary reviewer A primary reviewer is responsible for the overall code review. They are as responsible for the final code as the person who wrote it. You should always explicitly have a primary reviewer listed so that everyone knows who has final responsibility. Don’t ship code without approval from your primary reviewer unless you are experiencing an emergency and your primary reviewer is unreachable. ###Clearly define the responsibilities of each reviewer\nTo avoid redundancy when multiple people are reviewing a piece of code, clearly define what section(s) each reviewer is responsible for and who will be the primary reviewer. This allows each person to focus on their area of expertise (in the case of people like DBAs) and keeps discussions manageable. Preparing your code for review Communication is key Give your reviewers context on your change. Ideally, you should speak with them before any non-trivial review or document the changes you’re making inside of the review’s description. Make sure to summarize the change you’re making, why you are making those changes, and  link to a ticket or spec to provide any additional context. As we mentioned early on, set a timeline with your reviewers so they know how quickly you need feedback (see: “Faster is Better” for high-level guidelines). Sometimes the most efficient way to resolve a disagreement is a direct conversation (e.g: offline or in chat). If you find yourself having long discussions on your code reviews, reach out to your reviewers to resolve any disagreements in a timely manner. If you have discussions offline, summarize the discussion and plan of action in the code review to reference later and provide context to other reviewers. Smaller is better Keep your code reviews small so that you can iterate more quickly and accurately. In general, smaller code changes are also easier to test and verify as stable. To help keep your code reviews small, keep code reviews that change logic separate from reviews that change code style. If you have changed both, submit the code style changes as a branch and then follow-up with a branch to change logic. Make your code easy to review Code should contain both high-level and in-line comments. High-level comments explain how all the components fit together and how it handles any exceptional cases while in-line comments describe why the code takes its current shape. This will make your code easier to understand for maintainers and reviewers. If you feel your code review is confusing even after adding documentation, you should specify a starting point for your reviewer and detail which parts of code can be ignored. Read your diff before you send it out for review Before submitting for review, you should review your own diff for errors. Try to do this through the eyes of someone who has never seen the code before. Look for any decisions that may cause confusion and may need preemptive explanation. If you feel the code is too confusing, you may want to further refine your code before sending it out for review. Catching these issues early will save both you and the reviewers time. During a Code Review Avoid major changes during code review Major changes in the middle of code review basically resets the entire review process. If you need to make major changes after submitting a review, you may want to  ship your existing review and follow-up with additional changes. If you need to make major changes after starting the code review process, make sure to communicate this to the reviewer as early in the process as possible. It’s fine to conduct a “drafting” review to solicit preliminary feedback, but make sure to explicitly communicate this with the reviewer to avoid confusion. You’ll then want to communicate with your reviewer when your review has left “drafting” state or open a new code review. Respond to all actionable code review feedback You should understand every piece of feedback from your reviewer and respond to each actionable piece. Even if you don’t implement their feedback, respond to it and explain your reasoning. If there’s something you don’t understand, ask questions inside or outside the code review. See “Communication is key” for more information. Code reviews are a discussion, not a dictation You can think of most code review feedback as a suggestion more than an order. It’s fine to disagree with a reviewer’s feedback but you need to explain why and give them an opportunity to respond. When reviewing code You are responsible for the code you review You are equally as responsible for the code shipped as the person who wrote the code. You are responsible for making sure that the code is correct, well-architected, secure, and maintainable. If you’re not confident that the code meets these standards, ask a teammate to help complete the code review. Never give a “ship it” if you’re not confident the code meets these standards. Time is of the Essence The more quickly you can return a code review to the submitter, the better. Ideally code reviews should be returned within 24 hours to maintain project momentum. This is obviously much more practical with smaller code review (see “Smaller is Better” for more info). For larger-scale code reviews, expectations should be discussed between you and the reviewee. Keep in mind that the entire code review doesn’t need to be finished in one sitting. If the review is large, review a chunk of code at a time and communicate your progress. Never ship code until you have reviewed all of it. Review for correctness When reviewing code, you should make sure that it is correct. Check that the code is bug-free, solves the intended problem and handles any edge cases appropriately. If you are dealing with data serialization/deserialization check that the code is rollback and roll-forward safe. Review for core logic and architecture, not just style Think carefully about the architecture of the code. You should be able to understand each piece and how they all fit together. Confirm that the logic of each component is efficient and that the architecture is flexible but not over-engineered. When in doubt, optimize for readability and maintainability. Review for Adequate test coverage Generally speaking, all code in a codebase should be tested. Spend time reviewing the testing strategy to ensure that all code is well tested. This could include unit tests, integration tests, regression tests, and so on. Make sure that the unit tests are well isolated and don’t have unnecessary dependencies. How to communicate on code review Link to relevant style guidelines If you point out style that needs to be changed to conform to your team’s style guidelines, link to a relevant document that outlines this. If you’re highlighting a style change that isn’t covered in your team’s guidelines, think about whether it should be in the guidelines. If you find yourself commenting on style frequently, you should automate codestyle through a pre-commit hook. This will allow you to focus on review the code’s for correctness rather than style. As mentioned above, communication is key and code reviews are a discussion, not a dictation. Whether you are reviewing code or having your code reviewed, communication is critical and both parties need to address that feedback is a suggestion that’s open for discussion. This may require some compromise and architecture choices. That said, as a reviewer, you should not give the code a “ship it” if you’re unsatisfied with the mitigation of an open issue. Remember your job as a reviewer is to foster discussion so be sure to encourage open communication on and offline. Checklists When posting a code review Communicate context and requirements with reviewers Identify the best person/people to review your change Communicate  your change and what it’s purpose to your reviewers Establish your timeline with all reviewers if you need to ship by a specific date Identify a primary reviewer Define each reviewer’s responsibilities Carefully read your code before publishing Read your diff Make sure your diff clearly represents your changes. Can your code review be broken into smaller chunks? Make sure your code is easy for reviewers to follow Style check your code Make any relevant documentation easily available for reviewers Confirm that your reviewers are aware of any major changes (if any) you plan on making during review Discuss feedback Respond to all code review feedback. Discuss any feedback you disagree with When doing a code review Understand the code Make sure you completely understand the code Evaluate all the architecture tradeoffs Check code quality Verify code correctness Check for well-organized and efficient core logic Is the code as general as it needs to be, without being more general that it has to be? Make sure the code is maintainable Enforce stylistic consistency with the rest of the codebase Verify that the code is tested well Confirm adequate test coverage Check tests having the right dependencies and are testing the right things Make sure the code is safe to deploy Ask if the code is forwards/backwards compatible.  In particular, be on the lookout if the code is changing the serialization / deserialization of something Run through a roll-back scenario to check for rollback safety Check for any security holes and loop in the security team if you’re unsure Answer: If this code breaks at 3am and you’re called to diagnose and fix this issue, will you be able to? Tweet Back to blog", "date": "2017-11-20"}, {"website": "Yelp", "title": "Breaking down the monolith with AWS Step Functions", "author": ["\n        \n  Scott Triglia, Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/11/breaking-down-the-monolith-with-aws-step-functions.html", "abstract": "As we’ve discussed in earlier blog posts , Yelp Engineering has been working hard to break down our largest monolithic code base (yelp-main) for the past few years. We’ve made great progress but some of our oldest, most critical code remains within yelp-main. A great example of an older, more established system is our monthly subscription billing cycle. The system is core to how Yelp collects revenue and has proven technically challenging and risky to transition. The Revenue engineering team knows these older systems should be moved into services, but the challenge of extracting tangled, business-critical code has proven expensive and dangerous. Luckily a new framework was announced by Amazon Web Services at the end of 2016, AWS Step Functions , that’s allowed us to make this transition a reality. This post covers how we’re leveraging Step Functions to achieve escape velocity from yelp-main, better represent our business processes, and build a more reliable, observable system along the way. A subscription billing primer To set the stage a bit, here’s a look at what made this process so technically challenging. The subscription billing process lies at the center of a large nightly chain of batches. This pipeline takes hours to run each night and its ownership spans several teams at Yelp. Subscription billing consists of three conceptual jobs in the center of this pipeline: Billing accounts (how much does each account owe?) Invoicing (rolling up these line items into a single bill) Collections (actually collecting payment for the invoices) Each of these three steps runs over all relevant payment accounts before the next step proceeds. This fact has two implications for the stability and performance of these jobs: Each of these steps is doing significant work, so billing across more than 100k accounts takes hours to run in the best case. Since these steps operate over all accounts at once, one broken or slow account can block the entire rest of the pipeline. Making sure these steps complete cleanly and correctly is incredibly important! As the number of advertisers at Yelp has grown, it has been a challenge to keep this process scaling successfully. We’ve introduced concurrency frameworks into our batch processing libraries, been very conservative when changing the code, and spent a ton of time maintaining the status quo. A few key limitations have kept us from moving this code into services: These processes did not have clear APIs in our monolith. They were invoked by the daily billing pipeline and ran over all accounts in semi-parallel fashion. The data backing these various processing steps is entrenched in yelp-main. Moving the backing data out felt impossibly expensive, but if the data was left in yelp-main it wasn’t clear what value a service would provide. Some steps – like marking invoices as paid – currently leverage very stringent ACID guarantees (via MySQL transactions) to ensure our ledgers are consistent with payments we’ve collected from advertisers. Moving to a service would require devising an alternative way to maintain the same consistency guarantees. These collective challenges prevented extraction of subscription billing. We saw no cost-effective way for a small team to decouple the system without unacceptable consequences. Luckily for us, a solution was right around the corner. AWS Step Functions steps into the breach AWS Step Functions released to the public in December 2016, offering state machine-based workflow coordination as a service. It implements basic primitives for you, including retries, branching, and timeouts. Step Functions delegates tasks to your code for more complex side effects, like writing to a database, or calling an internal service. These are called “activity tasks” when run from your own servers, instances, or containers. Step Functions can also dispatch tasks to AWS Lambda functions. The flexibility of the state machine description language makes it applicable for a variety of use cases, but the core appeal for us came down to a few important features: There are few limitations on how long an activity task can run. If you aren’t ready to break up your code into many bite-sized activity tasks, you can run a few, very large activity tasks Activity tasks are codebase agnostic. This means your workflow can seamlessly coordinate multiple activity tasks that live across services. Retries and timeouts allow you to flexibly ensure individual activity tasks are robust and complete successfully. Concurrent executions can be run in parallel at significant scale. Up to a maximum of 1 million executions can be run at once. Step Functions has a lot of potential as a framework that can support monolithic code that wants to act in a service-like way. It seemed like a great match for our workflow-like subscription billing process, so we launched an initial project to integrate it. Our first pass - bookkeeping on Step Functions The diagram of the subscription billing process showed three fundamental steps: bookkeeping, invoicing, and collections. For our first stab at this project, we decided to tackle moving the first step of this process behind a Step Functions workflow. This offered a relatively well-scoped amount of work and let us work on the interface of this process without scoping in the task of migrating invoicing and collections. Our very first step was to choose the workflow’s interface. The old batch code looked roughly like this: def runner(all_accounts):\n    for chunk in all_accounts:\n        dispatch_work(chunk)\n\ndef worker(chunk):\n    for account in chunk:\n        bill_account(account) Our batch parallelism framework divided the set of all accounts into sections, and dispatched each chunk of accounts to a different worker process. We wanted to take advantage of the significant parallelism available in Step Functions, so we determined that a single workflow should perform subscription billing for a single account. To avoid a regression from our legacy system’s end-to-end performance, we could simply fire off many of these workflows concurrently. Concurrency variation over time in the old framework: Concurrency staying stable over time with Step Functions: Once the interface was determined, we built an API for billing a single account which only required an account ID and a date for billing (in case we needed to re-run past days workflows). Now note the simplicity and beauty of this: we just made running subscription billing feel like talking to a service, even though we have yet to migrate the actual behavior out of our monolith. This isn’t quite as good as actual service migration, but it’s a big step in the right direction for very little effort! We had originally considered just wrapping the yelp-main function in a very small monolith-based API, but using Step Functions let us keep all API management code cleanly separated, which would prove essential as the workflow evolved. Finally we need to implement our workflow’s state machine. We started by keeping it simple: making one large activity to match our old monolithic functionality! We knew this probably wouldn’t be a permanent solution, but it made migration incredibly straightforward and let us quickly and easily establish a baseline implementation for our Step Functions workflow. Polishing for observability and performance We tested this in production and saw the pieces working together just as intended. We kept a careful eye on two historic pain points for subscription billing: making sure any issues in the pipeline were highly visible to on-call engineers, and that the pipeline stayed highly performant. If there were any transient issues while billing an account, the retries we built into our workflow would simply re-run the bill_account activity. If an account had fundamental issues that caused billing to fail repeatedly, we’d eventually exhaust our retries or timeouts and Step Functions would mark the whole execution as failed. This execution failure was so important to us that we added an explicit state in the workflow to represent success and failure. These activities were solely there to push that success/failure fact into Yelp monitoring systems (like SignalFX) along with basic identifying information like account ID and execution ARN. This set-up ensures that one-off errors are cleanly handled by retries but also ensures that on-call engineers stay aware of any issues that cause billing to fail systematically. That increased awareness also means if one account cannot be billed, we can continue billing other accounts in parallel while our on-call engineers are notified — no more blocking the whole pipeline for a single bad account! We also saw performance wins. Execution concurrency worked well, even outperforming our own previous batch-based parallelism by avoiding some previous limitations. We further increased this advantage by revisiting our workflow design and breaking bill_account into a few parallel steps for different types of subscription products. It was the work of a couple hours to design a new workflow where these steps were done in parallel, and the associated changes to the activities were quite easy. The result was a faster billing process with nearly no extra engineering effort. The role of Step Functions as a highly-scalable coordinator of these complex workflows worked very nicely. We proved our hypothesis that workflows would be easy to refactor down the line. Results and future plans We have installed this new subscription billing process alongside the old one and are rolling over all accounts to be processed by the Step Functions workflow. So far, we’ve found it to be very stable and capable of significant parallelism (after some adjustment of default API limits ). We have gained a clear API to bill a single account, and the whole process is more resilient and observable. To recap, the development process consisted of the following steps: Start with a single “bill account” function called from our monolithic batch process. Wrap a Step Functions API around the “bill account” function. Trigger concurrent executions for improved performance. Extract retries and failure handling from “bill account”. Move these into their own activities in Step Functions and build high quality metrics watching how often they are executed. Use this improved observability to make even more fundamental changes to the workflow (like breaking out parallel tasks). Functions get simpler and more decoupled while the overall workflow gets faster and easier to understand. Rinse and repeat until satisfied with the workflow’s design Looking forward, we aim to continue incorporating invoice and collection steps into this workflow. Each of these is even more complicated internally, and we aim to simplify the number of dependent systems by leveraging the retries, timeouts, and parallelism built right into Step Functions. Look for us at re:Invent 2017 , speaking in breakout session CMP319 on this project alongside the Step Functions team. Tweet Back to blog", "date": "2017-11-27"}, {"website": "Yelp", "title": "Keeping Yelp two steps ahead: How we built GSET to protect employee email", "author": ["\n        \n  Jose Martin de Vidales Biurrun, Application Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/12/keeping-yelp-two-steps-ahead-how-we-built-gset-to-protect-employee-email.html", "abstract": "Earlier this year, Gmail users across the globe were affected by one of the largest phishing attacks of its kind. Yelp emails were among the many corporate email systems that experienced this Google Docs phishing attack. Fortunately, our security engineers had already prepared for this level of security threat and were able to delete the suspicious emails before impacting employees. As phishing attacks have become more and more prevalent, the need for new tools and countermeasures to protect users has become more important than ever. According to the last IBM X-Force Threat Intelligence Index report, the amount of spam email increased 450% over the past two years (as shown by the graph below).  More worrisome is the potential annual cost to a company affected by a phishing attack, estimated at $3.7 million . Image source: IBM Most companies offering SaaS solutions are delivering products that are capable of combating these types of attacks, but more often than not, they don’t cover basic cases that are critical to keeping employee accounts safe. The Google Docs phishing attack mentioned above is a good example of how out-of-the-box SaaS solutions often fall short.  While Gmail has multiple anti-virus and phishing tools to prevent attacks from spreading, there are no options to mask or mass delete unwanted emails that have already landed in an employee’s mailbox. We built GSET (G Suite Email Terminator), an internal tool developed by our Corporate Application Engineering team, to give us the ability to mass delete thousands of emails with just two clicks from our Gmail corporate accounts. Because we were ready for it, we could quickly and easily protect our employees from vulnerabilities like this year’s earlier Google Docs phishing attack. Here is a screenshot of the GSET interface: GSET is built on multiple technologies including Python, Flask , Celery and PaaSTA . With these technologies, the tool the can perform three critical actions: Execute a search query using Gmail search operators and generate a report based on that query Delete, hide, or quarantine any emails matching a specific query Log actions taken to create an audit trail Technologies GSET uses a G Suite service account with domain-wide access for authentication purposes. This allows GSET to generate OAuth 2.0 tokens and act on behalf of users when using the Gmail API. Once GSET has a valid token, it can interact with the API and access a user’s Gmail data. Service accounts with domain-wide access are particularly useful for enterprise environments where admins interact with most of Google APIs without a user’s interaction. Another technology used by GSET is Celery, an asynchronous task queue/job queue based on distributed message passing. Celery’s potential is most evident through its interaction with Gmail’s API quota limits. In other words, every single time that a new report or action is made by our admins using GSET, it requires a large volume of transactions that can’t be processed instantly. These transactions need to be queued in order to comply with Gmail API quota limits. Using RabbitMQ, we can handle asynchronous tasks and queue processes in order to control the rate of requests the tool generates. Now let’s go more in depth in how broker, queues, and workers come into play. Here is a diagram that shows how GSET manages multiprocessing and API rate control using queues: Bringing it all together To illustrate how these different technologies work together in GSET, let’s walk through a user flow: Our security team is notified when there’s a spike in phishing emails. Our security admin logs into GSET using single sign-on which transfers their identity credentials from our identity provider (Okta) to the service provider (GSET). Our security admins can take two actions: Create a query for our entire G Suite domain to find a specific phishing email. Mass delete emails found in a previous query. In this case, they run a new query based on the pattern detected in the reported phishing emails. The query is logged into S3 and Scribe and then ingested into Splunk for audit purposes. This query task is then pushed to the broker. As the query runs, the task will be placed on the search queue. The task is placed in one of three different queues (search/deletion/global_search). The reason you need different queues (search vs. delete) is that the number of quota units consumed by each worker varies depending on the method called (i.e. messages.list vs. messages.delete). You can find more information about Gmail API quota limits here . Each worker will then consume from one of the three queues. This helps us maintain a good ratio of API calls per second and function within the Gmail API quota limits. Once the query task is complete, GSET pulls a report and emails it to the security admin. The security admin can then act on the findings of GSET. In this case, the security admin will mass delete these emails based on the query task ID that just ran. By leveraging the APIs of our existing tools, like we did with GSET, we can keep Yelp two steps ahead of the game. I hope you folks enjoyed this blog post and are as excited as we are about protecting people from phishing attacks! Tweet Back to blog", "date": "2017-12-20"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 9 Winner", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/01/yelp-dataset-challenge-round9-winner.html", "abstract": "Yelp Dataset Challenge Round 9 Winners The ninth round of the Yelp Dataset Challenge ran throughout the first half of 2017 and, as usual, we received a large number of highly impressive and interesting submissions. Needless to say, we were struck by the quality of the entries: keep up the good work! Today, we are proud to announce the grand prize winner of the $5,000 award:\n“CORALS: Who are My Potential New Customers? Tapping into the Wisdom of Customers’ Decisions” by Ruirui Li, Chelsea J-T Ju, Jyunyu Jiang, and Wei Wang (from the Department of Computer Science of the University of California in Los Angeles). These authors developed an elaborate recommendation system that matches businesses with potential customers by taking into account a number of aspects of the customer decision-making process. The winners of Round 9 visited the Yelp offices in San Francisco, where they met with a few data scientists. Their model considers the personal preferences of the users (based, for example, on their check-ins), but also geographical data, as well as the reputation of local businesses. In comparison with other recent and widely-used methods, the authors demonstrated that their model performs better. This work is fully relevant to what we do at Yelp, and should be of interest to tech companies across the globe. This entry was selected from numerous submissions for its technical and academic merit by our panel of data scientists, data mining engineers, and software engineers. For a list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Thanks to all who participated! Want to try your hand at our dataset? Head to yelp.com/dataset to download and use it for personal, educational, and academic purposes. And to see what else we’re up to with Yelp data, check out the Yelp blog’s data section. Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset. These examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Tweet Back to blog", "date": "2018-01-08"}, {"website": "Yelp", "title": "Scaling Gradient Boosted Trees for CTR Prediction - Part I", "author": ["\n        \n  Niloy Gupta, Software Engineer - Machine Learning\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/01/building-a-distributed-ml-pipeline-part1.html", "abstract": "Building a Distributed Machine Learning Pipeline As a part of Yelp’s mission to connect people with great local businesses, we help businesses reach potential customers through advertising and organic search results. The goal of Yelp’s advertising platform is to show relevant businesses to users without compromising their experience with the product. In order to do so, we’ve built a click-through-rate (CTR) prediction model that determines whether or not to serve ads from a particular advertiser. The predicted CTR determines how relevant the business is to the user’s intention and how much would need to be bid to beat a competitor in the auction . It is crucial to predict the most accurate CTR as possible to ensure consumers have the best experience. Previously, we used Spark MLlib to train a logistic regression model for CTR prediction.  However, logistic regression is a linear classifier and cannot model complex interactions between covariates unless explicitly specified or engineered. Consequently, we decided to use Gradient Boosted Trees (GBTs), a nonlinear classifier and a powerful machine learning technique that uses an ensemble of weak learners (decision trees, in this case) to train on the residuals of the previous classifiers and form a more accurate ensemble. The time complexity of serially constructing a GBT model with T trees each of depth D is O(TDFnlogn) , where F is the number of features and n is the number of training samples. Thus, training these GBTs on hundreds of millions of training samples with thousands of features is a computationally expensive task. It took almost  two days to train a GBT model on 1/10th the number of available training samples using a single large machine. This makes it challenging to iterate on the model for feature engineering and hyperparameter tuning purposes. As a result, we needed to adopt a distributed approach for training. Our machine learning training pipeline is comprised of four stages: Sampling, Feature Extraction, Model Training, and Model Evaluation. Figure 1. CTR model training pipeline is comprised of four stages: sampling, feature extraction, training, and evaluation. Each phase is run on AWS EMR in order to scale to our large datasets. We use a variety of open source tools including mrjob, Apache Spark, Zeppelin, and DMLC XGBoost to scale machine learning. In the sampling phase, we implemented the ScalableSRS algorithm to generate a train-and-test set from our web logs. ScalableSRS has better theoretical guarantees than Bernoulli Sampling and scales well to large datasets. We then convert the sampled logs into a sparse feature matrix for the feature extraction phase. After the model is trained, we evaluate it against the test set, generating offline metrics for analysis. For these data wrangling tasks, we rely on tools like mrjob , Apache Spark, and Apache Zeppelin. For model training, we use XGBoost , a popular library for training gradient boosted trees. Since our datasets are large, we used data parallelism techniques to train our models. Some of the optimizations incorporated to speed up training include: 1. Algorithmic Optimizations We use XGBoost’s “approx” method for determining the split at each node, which transforms continuous features into discrete bins for faster training. Although fast at training, XGBoost’s “hist” and LightGBM’s voting parallel algorithm did not converge to the same accuracy as “approx.” As we re-engineer our feature space, we intend to investigate this further. To prevent unnecessary corrective training iterations, we tuned our hyperparameter search space for our dataset. For example, since ad clicks are much less common than non-clicks, we tuned the max_delta_step parameter to prevent gradient explosions or large step sizes.  We also developed our own early-stopping logic for our dataset, balancing the demand of training highly accurate models that don’t overfit (i.e. number of trees) and the cost of computing them in real-time (scoring more trees adds to service side latency). Figure 2. The GBT models are trained using a data parallel approach using distributed xgboost. The training process is fault-tolerant and includes hyperparameter selection using k-fold cross validation. 2. Distributed System Optimizations Optimizing the training time and cost also involves configuring the number of machines, type of machines, number of worker threads, and memory. Adding more machines to the cluster reduces training time before it is dominated by network latencies. Deeper trees require more memory for the workers to cache the information in the nodes. After model tuning and a period of experimentation, we were able to configure 5 workers per node, running on 50 machines (each machine has 36 cores and 60 GB of RAM). In addition, using YARN’s DominantResourceCalculator (which takes CPU and memory into account) instead of the DefaultResourceCalculator (which only takes memory into account) proved vital for us to maximize our resource utilization. Figure 3. The average time for tree construction reduces with the increase in the number of workers before network latencies begin to dominate. 3. Ancillary Optimizations We also integrated feature importance plot generation and feature summary statistics into the pipeline to make analysis and iteration easier. XGBoost’s “refresh” updater provided a cheap way to retraining our models on newer datasets. We also implemented model checkpointing and automated retries and recoveries to make the training pipeline robust to intermittent failures. Conclusion With an automated distributed training pipeline in place, we were able to train and launch a GBT model in production. We observed a whopping 4% improvement in mean cross entropy (MXE) on our offline test set and a significant increase in online CTRs. Moreover, we were able to reduce training time to less than 3 hours at a third of the original cost. Much like training GBTs on large datasets, evaluating these large models with millions of nodes online is also a challenge. In the next part of this two part blog series, we will discuss how we scaled online prediction using gradient boosted tree models to meet our latency requirements. We have open sourced our Scalable SRS Spark implementation and the PySpark XGBoost wrapper . If you are interested in building state of the art machine learning systems at Yelp check out our careers page ! Acknowledgements This project had a number of key contributors we would like to acknowledge for their design, implementation, and rollout ideas: Ryan Drebin, Niloy Gupta, Adam Johnston, Woojin Kim, Eric Liu, Joseph Malicki, Inaz Alaei Novin and Jeffrey Shen. References Practical Lessons from Predicting Clicks on Ads at Facebook XGBoost: A Scalable Tree Boosting System Ad Click Prediction: a View from the Trenches Scalable Simple Random Sampling and Stratified Sampling Optimization in Parallel Learning for Gradient Boosted Trees Tweet Build Machine Learning Systems at Yelp Want to build state of the art machine learning systems at Yelp?  Apply to become a Machine Learning Engineer today. View Job Back to blog", "date": "2018-01-09"}, {"website": "Yelp", "title": "Scaling Gradient Boosted Trees for CTR Prediction - Part II", "author": ["\n        \n  Niloy Gupta, Software Engineer - Machine Learning\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/01/growing-cache-friendly-trees-part2.html", "abstract": "Growing Cache Friendly Trees In case you missed it, Part I of this blog series outlines how we built a distributed machine learning platform to train gradient boosted tree (GBT) models on large datasets. While we were able to observe significant improvements in offline metrics, the resulting models are too large for standard XGBoost prediction libraries to meet our latency requirements. As result, we were unable to launch the models in production as we needed to serve ads within 50ms (p50) and evaluating these large models caused time-out exceptions. This article will discuss how we compressed and reordered the trees to make online inference faster and announce that we have open sourced the GBT scoring library for the benefit of the machine learning community. For a bit of background, Yelp’s ad delivery service predicts click-through rate (CTR) for tens of thousands of candidates per second. Evaluating a GBT model has a time complexity of O(Td) and a space complexity of O(T2 d ) , where T is the number of trees and d is the depth of each tree. A typical model contains millions of nodes and on the order of a thousand trees, making online model inference in real-time a challenge. One basic implementation of a binary decision tree involves connecting the nodes with pointers, however nodes are not necessarily stored consecutively in memory with this implementation. However, since space complexity dominates time complexity, most of the model evaluation time is spent in memory lookups. To reduce this effect, a popular approach is to convert binary tree into an array, where the nodes are placed next to each other in contiguous memory blocks. This is often referred to as the “flattened tree” implementation in literature. An important aspect of the ad click-prediction dataset is that it heavily skews towards non-clicks causing the split points to partition the samples at each node into highly skewed sub-samples during the model training. In other words, branches that lead to non-clicks would be traversed more frequently than paths that lead to higher click probability which we use to our advantage at inference time. Using XGBoost’s model has the cover statistic associated with each node, which is loosely correlated with the number of data points seen by the node, we can reorder the tree in such a way that the child with the larger instance weight (or cover) is placed next to the parent in memory.( i.e. child nodes that are more frequently visited are placed adjacent to the parent in the array). The other child is located ahead in the array, making the trees more cache efficient. One can think of performing a pre-order traversal of the binary tree, but instead of biasing towards the left child, we bias towards the child with a higher cover. Figure 1. Each node is associated with a cover statistic that correlates with the number of instances at each node. Children with more instances are placed next to their parent in memory which makes the tree ordering cache friendly. The number in each node indicates the cover for that node. There are also several optimizations that can reduce redundant data stored in the node to further increase cache locality. The attributes of a node can be represented by an integer array. In particular, It can be observed that the split condition at each non-leaf node and the weight at the leaf node are mutually exclusive attributes. Hence they can share the same 32-bit integer block. It’s worth mentioning that we store the IEEE 754 floating point representation of split condition and leaf weight using floatToIntBits . We always store one child node immediately adjacent to its parent node, so the parent only needs to store the address of the distant child. If that address is 0, we can implicitly infer that it is a leaf node. The feature index and default path flag (the path to take when the feature is missing from the feature vector) can share a single integer block. We have compressed the contents of a node into an integer array of size 3 (12 bytes) rather than a basic implementation which would have taken 16 bytes just to store the left and right child pointers. Thus, if we traversed through the cache “hot” path we should get the children in the same cache line thereby making model inference faster. Figure 2. Each node is represented as a block of 3 ints, which makes the decision trees both memory and cache efficient. If the feature value is missing, we use 1-bit to indicate which child to traverse (0 for adjacent, 1 for distant). Likewise we have 1-bit to decide which child to traverse when the split condition is true (0 for adjacent, 1 for distant) To traverse the compressed array trees during inference time, we also use bit-wise operations and masks to make evaluation even faster. The limitations of this implementation include: The depth of the tree cannot exceed 31 The number of features cannot exceed 2 31 Samples in the offline dataset must skew towards one class, and the same distribution must hold online to take advantage of cache hot paths. That said, it’s rare to train GBTs on more than 2 billion features or to build models with trees deeper than 31 levels so these limitations are not constraining in practice. Figure 3. For the benchmark results, we measured prediction latency against a random sample of production data with and without our described optimizations. Our implementation using compressed nodes with a cover based pre-order layout offered a 3.1x speedup compared to a flat implementation and a 2.6x speedup compared to an implementation using a compressed representation alone. This implementation gives us tremendous performance gains. We observed a phenomenal 120% improvement in our p50 latencies, exceeding the benchmarks on the xgboost-predictor library which we forked to implement the above model compression strategies. We have open sourced the GBT prediction library for the benefit of the machine learning community. Check it out here . If you are interested in building state of the art machine learning systems at Yelp check out our careers page ! Acknowledgements This project had a number of key contributors we would like to acknowledge for their design, implementation, and rollout ideas: Ryan Drebin, Niloy Gupta, Adam Johnston, Eric Liu, Joseph Malicki and Hossein Rahimi. References The Performance of Decision Tree Evaluation Strategies Evaluating boosted decision trees for billions of users Optimal Hierarchical Layouts for Cache-Oblivious Search Trees Fast Machine Learning Predictions Tweet Build Machine Learning Systems at Yelp Want to build state of the art machine learning systems at Yelp?  Apply to become a Machine Learning Engineer today. View Job Back to blog", "date": "2018-01-17"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 11 Announcement and Kaggle Weekly Kernel Award", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/01/dataset-challenge-round-11.html", "abstract": "Yelp Dataset Challenge Round 11 Is On! The eleventh round of the Yelp Dataset Challenge has opened. It will run until June 30, 2018. As in the past, the Yelp Dataset Challenge gives college students access to reviews and businesses from 11 metropolitan areas scattered over 4 different countries. This time around, there are a staggering 5.26 million reviews written by 1.3 million users about 175,000 businesses, as well as 146,350 check-ins and 1.1 million tips left by these users. Moreover, we have added photos about these businesses in a separate file, for convenience. With such a trove of data, the sky (or the processing power you have access to) is the limit. Remember, if you are a student, you’ll have the opportunity to win a $5,000 award if your submission is selected as the winner. The most recent winners of our challenge, Ruirui Li, Chelsea J-T Ju, Jyunyu Jiang, and Wei Wang (from the Department of Computer Science of the University of California in Los Angeles) developed an elaborate recommendation system that matches businesses with potential customers by taking into account a number of aspects of the customer decision-making process. Their paper, titled “CORALS: Who are My Potential New Customers? Tapping into the Wisdom of Customers’ Decisions”, made good use of Yelp data. Kaggle Weekly Kernel Award Also, for the first time, the full review dataset (except photos) is available on Kaggle . Kaggle has an ongoing program called the Weekly Kernel Award , in which they award a different kernel author $500 for quality analyses. In the past they have set specific themes for this award that has been quite popular with the community and driven activity on those datasets. The Kernels Award now runs on the Yelp dataset, and Kaggle sponsors the weekly $500 prize. Even more reason to use the Yelp dataset! Tweet Back to blog", "date": "2018-01-29"}, {"website": "Yelp", "title": "Making 30x performance improvements on Yelp’s MySQLStreamer", "author": ["\n        \n  Abrar Sheikh, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2018/02/making-30x-performance-improvements-on-yelps-mysqlstreamer.html", "abstract": "Introduction MySQLStreamer is an important application in Yelp’s Data Pipeline infrastructure. It’s responsible for streaming high-volume, business-critical data from our MySQL clusters into our Kafka-powered Data Pipeline. When we rolled out the first test version of MySQLStreamer, the system operated at under 100 messages/sec. But for it to keep up with our production traffic, the system needed to process upwards of thousands of messages/sec (MySQL databases at Yelp on an average receive over hundreds of millions of data manipulation requests per day, and tens of thousands of queries per second). In order to make that happen, we used a variety of techniques, including smart deployments, running processes on PyPy, and optimizing logs. While some of these delivered a huge bang for our buck, others didn’t. This post will outline a step-by-step approach to optimizing performance of a Real-Time Python Application, outlining the key lessons we learned in the process of getting the MySQLStreamer to process tens of thousands of messages per second. Step 1: Identify your key performance metrics Before optimizing our system, we needed to understand what we are optimizing for. Two key metrics to track the performance of any Real-Time System are throughput and latency. For the MySQLStreamer, we defined throughput and latency as follows: Throughput: number of messages processed each second. Latency: processing time minus event time, where (borrowing definitions from the Apache Flink documentation ), “processing time” refers to the system time of the machine that is executing the respective operation, and “event time” refers to the time that each individual event occurred on its producing device. In this case, event time is when the MySQL Database commits a data event, and processing time is when MySQL Streamer actually processes that change. Our goal was to optimize for increasing the throughput and reducing the application latency. During ideal operations, MySQLStreamer is expected to operate at close to zero latency in real time. Step 2: Have a way to accurately track these metrics over time To accurately track these metrics, we use SignalFX . SignalFX is a cloud platform for storing and visualizing time series data and it includes features for building alerts on detectors that help identify system anomalies. Most systems at Yelp use SignalFX as a canonical way of application monitoring and alerting. Once you configure your system to emit your key performance metrics, you can get easy-to-follow graphs like these: Throughput measured as a time-series Latency measured as a time-series Step 3: Have a system that can detect potential performance bottlenecks Now that we have a way to visualize our key metrics we can work towards optimizing the code for performance. However, how does one know which part of the codebase to improve? Scanning the entire repository for inefficient patterns could work but quickly becomes overwhelming for large code bases. A much more efficient approach would be to use some kind of code profiler that can highlight inefficient functions . In the context of this post I would define inefficient functions as ones that take longer to run in proportion to other functions in your code’s stack trace. We leveraged vmprof for this purpose. VmProf is a statistical code profiler for Python. In principle, it samples Python stack traces for a certain time duration and then aggregates the results from those samples to generate a flame graph visualization for the stack trace in terms of percentage time each function takes relative to other function. import vmprof\n\ndef _enable_profiler(self):\n    self._profiler_fd = os.open(\n        PROFILER_FILE_NAME,\n        os.O_RDWR | os.O_CREAT | os.O_TRUNC\n    )\n    vmprof.enable(self._profiler_fd)\n\ndef _disable_profiler(self):\n    vmprof.disable()\n    os.close(self._profiler_fd) For example: Step 4: Ensure that you are running your profiler “correctly” There are two important points about code profiling: Choose to generate an online code profile on your production application as often as possible. Make sure the application is completely “saturated” before running the profiler. Generating an online code profile on your production application Generating an online code profile on your production application is important because your system’s performance patterns may vary significantly between your staging environment and your production environment. For example, you may have a memory leak on production which may not manifest in your staging environment. However, running profiles on production seems like a bad idea since profiling is costly and can slow down the very application that you’re trying to make faster. To solve this problem, in the context of MySQLStreamer, we used  a special instance of our production application, called “Canary” (borrowing from the “Canary in a coal mine” analogy). Builds for MySQLStreamer application are run and deployed using Jenkins . We configured our deployment pipeline to comprise of three stages in the following order: Canary : These are MySQLStreamer instances running in a production environment. Canary subscribes to data from production MySQL tables and publishes data to production Kafka topics. Dev/stage : These are MySQLStreamer instances running in developer & staging environments. These instances subscribe to data from our dev/stage MySQL tables and publish data to our dev/stage Kafka topics. Main : These are MySQLStreamer instances running in production. These instances subscribe to data from production MySQL tables and publish data to production Kafka topics (which are used by downstream consumers). The main distinction between Canary and Main is that: New versions of MySQLStreamer are automatically deployed to Canary without human intervention. This facilitates faster iteration on code changes. Developers can now make fearless changes to the code, have it tested against production data without potentially disrupting production usage. Deployment to Main, which is the real production application, requires human intervention on jenkins. Canary instance of MySQLStreamer is subscribed to the same set of MySQL tables as Main instance but is writing to a different set of kafka topic so that it does not affect production usage of our Data Pipeline. In addition to providing code profiles on production, the Canary instance also lets you experiment and iterate quickly on code changes to improve performance. Make sure the application is completely “saturated” before running the profiler Let me explain this concept with two hypothetical scenarios: Your system is processing messages at a rate equal to or higher than the rate of incoming messages. This is an “unsaturated” state - the system hasn’t been tested at its peak performance Your system is processing messages at a rate slower than the rate of incoming messages. This is a “saturated” state - under the current implementation, the system cannot go any faster. Code profiles taken during (A) are less interesting because all you see in the flame graphs are some functions calls that are blocked on IO. In the case of the MySQLStreamer, these manifest as binary log reads from MySQL. On the other hand code profiles taken during (B) are a significantly more accurate depiction of time spent in various function calls during the application’s lifecycle. We need the system to be in a saturated state (Scenario B) to get valuable profiles, but the system may not be in this state all the time. As a result, we need to “force” our application to get to this state. The way we did this in the MySQLStreamer was by: Shutting down the canary instance for 30 mins and then turning it back on. Once the application is up and running, we immediately start the profiler and let it run for the amount of time the application is completely saturated. We stop profiling just before the application is about to catch up to real time. The picture below captures the saturated state of the application. Notice how the Latency is decreasing while the Throughput is mostly staying constant. Step 5: Use the results of your profiler to isolate performance bottlenecks Let’s revisit the MySQLStreamer flame graph from earlier. As you can see above, the application is spending most of its time in publish() and _get_events() functions. These correspond to reading from MySQL binary log and publishing to Kafka respectively. This is not surprising given that these operations are IO bound. MySQLStreamer also spends a significant amount of time serializing data into AVRO format. Cost of this serialization operation is paid by the build_message() function. Because the logic for AVRO serialization is completely inside python there is definitely some scope for future improvement here. Step 6: Rinse and repeat Once you have good profiles and are able to isolate performance bottlenecks, the next step is to make changes and observe your key performance metrics. After trying a number of different  approaches to improve the code performance of MySQLStreamer, we learned the following lessons: Logging can be Expensive Remove unnecessary and redundant logs. Don’t have it if you don’t need it. After all, the fastest way to do something is to not do it at all. More often than not, DEBUG level logs do not actually materialize into persistent logs. Hence the cost of object serialization or string manipulation involved at the logger.debug call site can amount to significant chunk of runtime which is completely unwarranted. This can be explained by the following code example import logging\nimport timeit\nimport time\n\nlogging.basicConfig(filename='example.log',level=logging.INFO)\n\nlogger = logging.getLogger(__name__)\n\nclass SerializableClass:\n    def __str__(self):\n        # proxy for indicating complex object serialization\n        time.sleep(0.0001)\n        return \"Some Log\"\n\nobj1 = SerializableClass()\n\ndef f1():\n    logger.debug(str(obj1))\n\ndef debug_log(line_lambda):\n    \"\"\"This avoids unnecessary formatting of debug log string.\n    \"\"\"\n    if logger.isEnabledFor(logging.DEBUG):\n        logger.debug(line_lambda())\n\ndef f2():\n    debug_log(lambda: str(obj1))\n\nprint timeit.timeit(stmt=f1, number=1000) * 1000\nprint timeit.timeit(stmt=f2, number=1000) * 1000\n\nResult\n147.380113602 ms\n1.13797187805 ms In the above example, the global log level is set to INFO which means anything below INFO logging precedence does not make it into the log file. Hence the price paid for object serialization is unnecessary and can be avoided altogether. This small little trick provided a huge win in our application performance. Focus on the code that runs often I will try to explain this point using an example. The high level algorithm for MySQLStreamer would look something like: def process_event(self, event):\n    # avro serialize the event\n    avro_schema = self.get_avro_schema(event.table)\n    self.dp_producer.publish(avro_schema, event.data)\n\ndef run(self):\n    while True:\n        # read from MySQL binary log\n        event = self.get_event()\n        if event is DataEvent:\n            # handle DataEvent\n            self.process_event(event)\n        else:\n            # handle SchemaEvent Notice that process_event() gets called exactly every data event in MySQL binary log, so any code optimization inside process_event() will result in significant boost in the application performance. Batch IO operations In principle, MySQLStreamer subscribes to Data Events from MySQL and publishes them to Kafka. We observed that by increasing the batch size of requests to Kafka from 500 to 3,000 we were able to extract a large amount of performance from the system. One thing to note here is that we cannot simply keep pushing the batch size up with the hope to get linear growth in performance. If we do that, the application starts to defy the principles of a real-time system. As a result, it’s important to strike a balance between performance and real-time-ness. Run your application on PyPy PyPy performs extremely fast due to its Just In Time compilation and uses far less memory than CPython. We have seen 2X-3X improvement in application performance after upgrading from CPython to PyPy. In total, our team successfully runs 3 PyPy applications in production and has been running them for more than a year and a half without any issues. “If you want your code to run faster, you should probably just use PyPy.” — Guido van Rossum (creator of Python) Conclusion To conclude, here are the steps to keep in mind while trying to performance-optimize your real-time application: Identify your key performance metrics - typically, they are “Throughput” and “Latency.” Enable your application to emit these key metrics to some kind of time-series charting software like SignalFX. Equip your application with some kind of code profiler like VmProf. Make sure you have a production-like canary environment in a “saturated” state where you can profile your application’s performance. Understand where your application should be spending the most amount of time. Generate and inspect the VmProf flamegraph to check if it matches with your understanding. If yes, then quit. If not, document the problem and attempt to fix it. Validate by looking at SignalFX graphs. Goto (5) This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Back to blog", "date": "2018-02-05"}, {"website": "Yelp", "title": "Progressive Enhancement with Brotli", "author": ["\n        \n  Stephen Arthur, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/07/progressive-enhancement-with-brotli.html", "abstract": "Brotli is a compression algorithm from Google that has been making waves recently. You may have heard about its average 10-20% filesize savings over gzip, at comparable decompression speeds . This makes it a very irresistable format for serving static content, but it still has immature support from CDNs, which puts it out of reach for general widespread availability. A search run over HTTP Archive data indicates that there are around 25 web properties serving Brotli Content-Encoding URLs from Fastly, the CDN we use to host our static content, as of July 25th. While Fastly does not automatically convert content to Brotli for users, they removed a critical roadblock in serving Brotli by exposing the client’s Accept-Encoding header back in June 2016 . This is all we need in order to know if a user will understand a Brotli response. In this article, we explain how we implemented serving responses with Brotli compression for our existing URLs, progressively enhancing the performance of our website for the clients that support this new format! With a clear solution to upgrade responses to Brotli, the number of websites supporting it should only increase! Behind the scenes To send Brotli responses to our users, we must have a Brotli version of our files! At Yelp, when a developer makes a change to a Javascript, CSS, or SVG file, we build a new version of that package and upload it to Amazon S3. That’s the backing datastore and source of truth for all requests through Fastly to yelpcdn.com. So, if the CDN hasn’t received a request for the content with that URL, including version hash, it will be fetched from S3. Since this file doesn’t change once it’s been uploaded, we gzip it before uploading and set the Content-Encoding header in S3 itself. In order to serve Brotli content, we will now upload a second version of every file to S3 with the same path and suffixed with .br : if file_ext in TEXT_EXTENSIONS:\n    gzip_file(srcfilename, destfilename)\n    brotlify_file(srcfilename, destfilename + '.br') Since this is an article about Brotli, here is that helper’s definition: def brotlify_file(srcfilename, destfilename):\n    with open(srcfilename, 'rb') as srcfile:\n        with open(destfilename, 'wb') as destfile:\n            destfile.write(brotli.compress(srcfile.read())) The cache key Fastly uses Varnish behind the scenes, and this is where we will add logic to cache a Brotli version of a file separately from the gzip version of the same file. Other headers that are part of the response’s Vary header are part of that cache key along with the URL. The Accept-Encoding header is almost always present in the Vary response header, and normalization means you can get a much better cache hit ratio since browsers may have a lot of different combinations of values. In the following sections, you will see how this header is used to determine which backend asset to request, and why it isn’t quite enough on its own. The approach We aren’t uploading Brotli compressed versions of 100% of our assets yet, so for a client that accepts Brotli and gzip, we’d like to first try for the Brotli version. If that version does not exist, we’ll fall back to the gzip version. However, if we get a cache miss on the Brotli version for every request, we will be dramatically slowing down the response time instead of improving it! And we will rack up a larger AWS bill with so many GET requests. But we’re aware of this issue, and in the next section we’ll learn the basics of Varnish needed to avoid making that mistake. The flow of control In order to understand the next section, I’d highly recommend referring to this wonderful diagram to understand what logic is executed during each step of a request being processed by Varnish. We will add a bit of logic in three methods to give us the desired flow of control. Here is the relevant excerpt of the logic that we’re adding: The relevant Varnish methods we're interested in analyzing further. The special sauce Fastly has a boilerplate varnish file posted in their support documentation and provides extensive instructions (and, err… warnings…) on how to use it. They’ve also recently released a new online VCL editor , which might be able to complete the required modifications. We will start from the boilerplate, and will include a custom file as the first line in each of vcl_miss , vcl_fetch , and vcl_recv . The three custom files contain: custom_recv.vcl # Add br to the choices for normalized Accept-Encodings\n# Doesn't require a brotli response, but more encodings means multiple cached objects per key\nif (req.http.Fastly-Orig-Accept-Encoding) {\n    if (req.http.User-Agent ~ \"MSIE 6\") {\n        # For that 0.3% of stubborn users out there\n        unset req.http.Accept-Encoding;\n    } elsif (req.http.Fastly-Orig-Accept-Encoding ~ \"br\") {\n        set req.http.Accept-Encoding = \"br\";\n    } elsif (req.http.Fastly-Orig-Accept-Encoding ~ \"gzip\") {\n        set req.http.Accept-Encoding = \"gzip\";\n    } else {\n        unset req.http.Accept-Encoding;\n    }\n} custom_miss.vcl # Brotli compression: user must support it, and must be text file type\n# If so, look for it the first time\nif (req.http.Accept-Encoding ~ \"br\" && req.url ~ \"\\.(js|css|svg)($|\\?)\" && req.restarts == 0) {\n    set bereq.url = regsub(req.url, \"\\.(js|css|svg)($|\\?)\", \"\\.\\1\\.br\\2\");\n} custom_fetch.vcl # if we looked for a brotli file and didn't find it, restart\nif (bereq.url ~ \"\\.br($|\\?)\" && beresp.status != 200) {\n    restart;\n} Understanding the Changes We recognize the contents of custom_recv , that’s the Accept-Encoding normalization logic to add Brotli support that was provided by Fastly in the community thread about Brotli support. From the diagram above, we know that the second step in flow of control that we want to customize is vcl_miss . If there has been a cache miss, and the client can understand Brotli, and there might be a Brotli version of this Content-Type , and we haven’t restarted yet (read on), then we will change the backend URL to a version that appends .br to the end of the file (but before query string parameters if present), and try fetching that. That is, given a request from the user for: /assets/srv0/svg_icons/asset_version_hash/assets/svg_sprite.js?params We will change the backend request URL to be: /assets/srv0/svg_icons/asset_version_hash/assets/svg_sprite.js.br?params Waiting to rewrite the URL until after we have a miss is the trick to avoiding superfluous requests to the backend. If we only change the backend URL, the asset will be cached keyed against the original URL, so we don’t have to know which one was successfully fetched. If any of the criteria for Brotli are not met, we won’t do anything special at all, continuing on with Varnish’s normal logic flow. Finally, in custom_fetch , we have gotten a response from the backend. We determine if we tried to find a Brotli file and we failed. If so, restart ! That directive tells Varnish to start from vcl_recv all over again. Except the value of req.restarts will be 1 and not 0. We’ll notice that the next time through when we get to vcl_miss , we’ll continue along Varnish’s normal logic flow, and act casual like we didn’t just try to look for a non-existent file. This does mean the cache will contain a gzip version of the file cached under the Brotli normalized Accept-Encoding . So the next time the same URL is requested, that will yield a cache hit of the gzip version, even if the client does accept Brotli. This is a fine compromise for us because not every file has both versions available, we assume that if there is ever a backend miss, it will always happen. By the time we are fully upgraded to generate Brotli versions of every file, we will let these age out by the version hash in the URL (or purge the assets from cache). The results We’ve been running these 6 lines of custom Varnish in production with Fastly since January with no issues, and our users who support Brotli can rest easy knowing that we are continually trying our hardest to keep our site’s static content loading as fast as possible even through multiple asset hash changes each day. Here is the distribution of encoded body sizes for one of our Brotli-available files as seen by Android browser mobile user agents requesting m.yelp.com: The relative number of requests by filesize for a file with both gzip and Brotli versions available. About 80% of them receive the Brotli version of the file, and 20% use gzip. (And though it varies by file type and contents, this file is about 17% smaller). Well now you know the secret! Please go out and implement this, and help us implement cool tech like Brotli to keep the web modern and fast. Tweet Become an Engineer at Yelp Working on the performance team at Yelp means working on high impact projects like this one. If you're interested apply below! View Job Back to blog", "date": "2017-07-26"}, {"website": "Yelp", "title": "Auto-suppressing Tests for More Reliable Code in our Android App", "author": ["\n        \n  Sanae Rosen, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/08/auto-suppressing-tests-for-more-reliable-code-in-our-android-app.html", "abstract": "Automated testing is really important for ensuring your code is bug-free and works as expected but UI tests are particularly challenging as they tend to be flaky and unreliable. For an app like Yelp, a lot of tests are needed so developers will often see tests flake. You can look at test history to try and figure out if a test failure is your fault or not - and building tools to do so has been something we’ve invested time into - but you might be wrong and it takes time and effort to figure this out. This leads to another problem. When broken code gets into the master branch, someone else will check out that code and tests will start failing for them for reasons that have nothing to do with the feature they were working on. This makes life harder for developers and this makes them lose faith in the idea that tests reflect the quality of the code they’ve written. When we first started addressing this problem our master branch passed all the tests only about 60% of the time and it had become clear this was a problem we needed to address. The key idea behind our solution is that a flaky test is worse than no test at all. Not only does it give no useful information but it adds unnecessary work for those who have to deal with these tests and decreases trust in the tests overall. We concluded it would be better to get rid of any flaky test altogether, at least temporarily. When we auto-suppress flaky or broken tests on the master branch we know that whenever a test fails on a new branch something on that branch is broken. If all our tests give meaningful results we can enforce that no code gets into master if even one test fails and then we have fewer and fewer tests to suppress. Suppressing tests doesn’t have to mean that there are fewer tests, at least in the long run. We alert on every suppressed test and auto-generate a ticket for the relevant team. Our goal is for auto-suppression to lead to broken tests being fixed faster than they otherwise would. Presumably, flaky tests were intended to tell us something important about the app. The initial testing system The figure below shows how our testing infrastructure worked before we implemented auto-suppression. First, a new branch would be checked out from the master branch. As you work on this branch you’d run what we call a buildbot - a series of unit tests and UI tests - which would post the results to the relevant code review. These tests were run remotely. On merging back to master, a script runs the unit tests remotely and only merges if they succeed. UI tests weren’t run because we didn’t want to block a merge due to flaky tests. Finally, a buildbot was periodically run on the master branch. The solution Step 1: No more flaky tests It turns out that, in many cases, flaky tests are avoidable. Sometimes we get failures due to issues with our testing infrastructure and there isn’t much we can do about these (we don’t suppress tests in this case). In many cases flakes can be avoided by writing more robust tests. Our first step was to make sure that only non-flaky tests enter the codebase. We now auto-detect in the buildbot whenever someone commits code with a new or changed test and then launch a job that runs the test 20 times. We picked this number semi-arbitrarily based on how often we felt we were willing to tolerate a test flaking. Normally we only count a test as a failure if it fails three times, as an anti-flake measure, but here if it fails even once it’s out. This was a good first step at preventing the growth of flaky tests and hasn’t stopped the number of tests in our codebase from steadily growing. Step 2: Auto-suppress flaky tests The was a big one, and a lot more controversial too. The tests are there for a reason! What if we suppress a test and break the app? The key here is to make sure that suppressing tests mean tests are dealt with faster, not slower. We first had a bunch of our developers sit down together and figure out what team should own each test. We annotated our tests with the team that owns that test and added a lint rule to enforce that any new tests must be given owners: @Override\npublic boolean visitClassDeclaration(ClassDeclaration node) {\n\n    // Currently, we're only interested in the tests in the androidTest directory.\n    if (!mContext.file.getAbsolutePath().contains(\"androidTest\")) {\n        return super.visitClassDeclaration(node);\n    }\n\n    // Then, we rule out any class that already has an ownership annotation.\n    for (Annotation child : node.astModifiers().astAnnotations()) {\n        if (child.getDescription().equals(\"Ownership\")) {\n            return super.visitClassDeclaration(node);\n        }\n    }\n\n    boolean madeReport = false;\n\n    // Then, we check if the class contains a test.\n    // To traverse the AST to find the method annotations, you go class -> class body ->\n    // methods -> method annotations and other such features.\n    for (Node child : node.astBody().getChildren()) {\n        if (child instanceof MethodDeclaration) {\n            for (Node grandchild : child.getChildren()) {\n\n                if (grandchild.toString().contains(\"@Test\")\n                        || grandchild.toString().contains(\"@SmallTest\")\n                        || grandchild.toString().contains(\"@MediumTest\")) {\n                    mContext.report(\n                            ISSUE,\n                            Location.create(mContext.file),\n                            ISSUE.getBriefDescription(TextFormat.TEXT));\n                    madeReport = true;\n                    break;\n                }\n            }\n        }\n        if (madeReport) {\n            break;\n        }\n    }\n    return super.visitClassDeclaration(node);\n} Now when a test breaks we automatically create a ticket on that team and depending on the team’s preferences we may also send an email. An important part of this process was involving developers and managers to understand how to best integrate alerting into each team’s workflow and be sure that everyone is invested in having this system succeed. Just in case, we also threw in some alerting in case the number of suppressed tests got too high. About 3% of tests were suppressed for other reasons before we started so we set an alert to go off if we ever hit 5%. Once an infrastructure failure meant that all the tests failed and promptly got suppressed. Thanks to the alerting we were able to immediately revert the change and add some sanity checks to make sure that doesn’t happen again. Generally, test failures due to infrastructure problems cause all or most tests to fail so we can avoid auto-suppressing on these failures by having a cutoff of 10 for the number of tests we’ll auto-suppress at a time. We have alerts for this case too in case we legitimately needed to suppress this many tests. Step 3: Be strict about what we allow in the codebase Before when flakes and tests broken in master were a problem it was up to the judgement of the developer (and code reviewers) to determine if a broken test is their responsibility or not. It would happen then that new problems would be introduced to our master branch because someone incorrectly thought a test failure was a flake. Now that we are dealing with flaky or broken tests automatically we can block pushes to master without a perfect set of tests. Where are we now Here’s what our master branch stability looked like before and after these changes: We have a screenshot from a tool we’ve built that tracks the results of every set of tests we run, filtered to show only tests run on the master branch. Green runs are successes, blue are in progress, and orange runs are those where some test - either a UI or unit test - failed, or the set of tests otherwise failed to run. Our master branch isn’t perfectly stable but it’s pretty close. Our master branch tests succeeded 87% of the time in the past two weeks and the UI tests succeeded 93% of the time. Before, the success rate for the master branch was around 60%. Out of the thousands of tests we have only a handful were suppressed. It turns out that most tests are good and only a few were causing problems. And suppressed tests were dealt with promptly enough that we never ran into any problems with broken code getting into master because an important test was suppressed. Other developers have told us they are now much more confident that test failures give them meaningful information and that dealing with failed tests is now a much more pleasant experience as a result. Overall, auto-suppressing tests has led to a better quality master branch and happier developers. Tweet Back to blog", "date": "2017-08-24"}, {"website": "Yelp", "title": "Yelp Open Dataset and Dataset Challenge Round 10", "author": ["\n        \n  Tomer Elmalem, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/08/yelp-open-dataset-and-dataset-challenge-round-10.html", "abstract": "Introducing The Yelp Open Dataset About five years ago, we announced the Yelp Dataset Challenge : a competition that lets students explore and research with the help of  our large corpus of data. Each participant can also formally submit their projects for the chance to win prizes. Over the years we’ve seen incredible interest and usage of our dataset for educational purposes. We’ve had teachers use it to teach their classes about databases, engineers use it learn graph databases, and students use it to understand machine learning. We’re very proud of this type of usage and are continuing to encourage more people to do so with the announcement of the Yelp Open Dataset . The Yelp Open Dataset allows you to use our dataset for personal or educational purposes so that you can can learn from a realistic dataset. The dataset itself is well-structured and highly relational. We’re providing it as both JSON files and a SQL dump for easy use in any scenario. The dataset itself contains almost 5 million reviews from over 1.1 million users on over 150,000 businesses from 12 metropolitan areas. We’re also making 200,000 photos, their captions, and photo classification labels available for people looking to explore deep learning techniques around photo classification or search . We’re very excited to make this available to everyone and hope that it’s both fun and educational. Dataset Round 10 Today also marks the start of round 10 of the dataset challenge ! Our nine previous rounds have lead to a number of very exciting projects and dozens of winners . Round 10 kicks off today (August 30, 2017) and will run through the end of the year (December 31st, 2017). The challenge will run using the same data as the Yelp Open Dataset. Tweet Back to blog", "date": "2017-08-30"}, {"website": "Yelp", "title": "Introducing Yelp Events Data to Our Developer APIs", "author": ["\n        \n  Tomer Elmalem, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/09/introducing-yelp-events-data-to-our-developer-apis.html", "abstract": "Starting today, Yelp Events data will be available through our Fusion API and GraphQL API to provide rich, local events data in our API as part of our developer beta program. Since launching Yelp Fusion last September, we’ve seen developers use our business data in unique and interesting ways and are excited to see what our developer community will do with the addition of events data. If you’re not already familiar, Yelp Events is a place where you can create, share and discover upcoming events happening worldwide . Events range from everything from beer festivals to art shows to Shakespeare in the Park. The Events API will give you access to all of our event listings (current and historical) in the US and internationally (where available) with information on costs, locations, descriptions, associated businesses, venues, and more. You’ll be able to pull events in three different ways: events search featured event by city event lookup Searching the Yelp Events database allows you to access a number of different parameters (category, date, time, location, etc.) to see what’s happening near you. This is the easiest way to explore and discover what’s happening in specific locations. GET https://api.yelp.com/v3/events { \"total\" : 1 , \"events\" : [ { \"attending_count\" : 4 , \"category\" : \"food-and-drink\" , \"description\" : \"Saucy is throwing up a pop-up restaurant party over at Anfilo Coffee! Give the menu a little peruse and then register to reserve your spot! Prices are shown...\" , \"event_site_url\" : \"https://www.yelp.com/events/oakland-saucy-oakland-restaurant-pop-up\" , \"id\" : \"oakland-saucy-oakland-restaurant-pop-up\" , \"interested_count\" : 9 , \"latitude\" : 37.8114102 , \"longitude\" : -122.2665892 , \"name\" : \"Saucy Oakland | Restaurant Pop-Up\" , \"tickets_url\" : \"...\" , \"time_end\" : \"2017-08-19 04:00\" , \"time_start\" : \"2017-08-19 01:00\" , \"location\" : { ... }, \"business_id\" : \"anfilo-oakland-2\" , ... , } ] } We also have a group of Community Managers who organize local Yelp events in their areas that we often feature on the Yelp Events page. We’ll be exposing an endpoint to retrieve this data so that you’ll be able to pull the featured event data for any given location. GET https://api.yelp.com/v3/events/featured { \"attending_count\" : 4 , \"category\" : \"food-and-drink\" , \"description\" : \"Saucy is throwing up a pop-up restaurant party over at Anfilo Coffee! Give the menu a little peruse and then register to reserve your spot! Prices are shown...\" , \"event_site_url\" : \"https://www.yelp.com/events/oakland-saucy-oakland-restaurant-pop-up\" , \"id\" : \"oakland-saucy-oakland-restaurant-pop-up\" , \"interested_count\" : 9 , \"latitude\" : 37.8114102 , \"longitude\" : -122.2665892 , \"name\" : \"Saucy Oakland | Restaurant Pop-Up\" , \"tickets_url\" : \"...\" , \"time_end\" : \"2017-08-19 04:00\" , \"time_start\" : \"2017-08-19 01:00\" , \"location\" : { ... }, \"business_id\" : \"anfilo-oakland-2\" , ... , } Lastly, for cases where you might save event ids for your users, event lookup will allow you to pull information on a specific event to give them quick and easy access to that information. GET https://api.yelp.com/v3/events/{id} { \"attending_count\" : 4 , \"category\" : \"food-and-drink\" , \"description\" : \"Saucy is throwing up a pop-up restaurant party over at Anfilo Coffee! Give the menu a little peruse and then register to reserve your spot! Prices are shown...\" , \"event_site_url\" : \"https://www.yelp.com/events/oakland-saucy-oakland-restaurant-pop-up\" , \"id\" : \"oakland-saucy-oakland-restaurant-pop-up\" , \"interested_count\" : 9 , \"latitude\" : 37.8114102 , \"longitude\" : -122.2665892 , \"name\" : \"Saucy Oakland | Restaurant Pop-Up\" , \"tickets_url\" : \"...\" , \"time_end\" : \"2017-08-19 04:00\" , \"time_start\" : \"2017-08-19 01:00\" , \"location\" : { ... }, \"business_id\" : \"anfilo-oakland-2\" , ... , } Along with the regular endpoints, this will also be rolling out for GraphQL so that you will also have the ability to load the business data associated with each event, where possible: {\n  events(location: \"San Francisco\") {\n    name\n    location\n    business {\n      name\n      url\n    }\n  }\n} To get started, register for an API key on our developer site and make sure to join our beta program ! Tweet Back to blog", "date": "2017-09-06"}, {"website": "Yelp", "title": "SignalForm: Charts as Code with SignalFx and Terraform", "author": ["\n        \n  Antonio Verardi, Yashashree Kokje, Software Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/10/signalform-charts-as-code-with-signalfx-and-terraform.html", "abstract": "Monitoring the health of our systems is a critical part of maintaining Yelp’s infrastructure. We collect millions of data points that help us observe the performance and status of our services. This data powers visualization and monitoring systems so that we can alert on anomalies and derive actionable insights, especially during on-call procedures. SignalFx is our preferred vendor for metrics visualization and monitoring. They provide a rich UI with many robust analytics capabilities. At Yelp’s scale, we use SignalFx to create hundreds of  detectors, charts and dashboards. Managing and finding these resources quickly is a challenge. Our engineering teams need to be able to discover the graphs and detectors they own, quickly create and update them and be informed about changes to them. In an effort to programmatically improve how we manage dashboards, we developed SignalForm , a tool to codify SignalFx resources and version control them. At Yelp we believe in “infrastructure as code”. We extensively use Terraform to programmatically build, change, and version infrastructure safely and efficiently. Terraform manages most of our AWS infrastructure, allowing engineers to code review infrastructure changes before they’re applied. We decided to apply the same methodology to our SignalFx artifacts. SignalForm is a Terraform provider which leverages the new SignalFx API to create, update and delete SignalFx resources. We now keep all critical SignalFx artifacts versioned in git and engineers are able to programmatically manage them. Artifacts are namespaced under each team and project to enable quick discovery. With the ability to review the SignalFlow code powering a detector, teams have started to collectively devise and validate the alerting logic before anyone gets paged by an inefficiently configured detector. What would creating detectors as code look like? The snippet below creates a detector monitoring the maximum delay seen by our application for every region it’s been deployed to. Terraform’s interpolation syntax allows us to create one detector resource for each value declared in the list of regions. This list can be shared across multiple files and folders to create additional resources. variable \"regions\" { default = [ \"regionA\" , \"regionB\" , \"regionC\" , \"regionD\" ] } resource \"signalform_detector\" \"application_delay\" { count = \"${length(var.regions)}\" name = \"max delay - ${var.regions[count.index]}\" description = \"delay in region - ${var.regions[count.index]}\" program_text = <<- EOF filters = filter(\"region\",\"${var.regions[count.index]}\")\n        signal = data(\"app.delay\", filter=filters).max()\n        detect(\"Processing old messages 5m\", when(signal > 60, \"5m\")) EOF rule { description = \"Max delay > 60s for 5m\" severity = \"Critical\" detect_label = \"Processing old messages since 5m\" notifications = [ \"Email,foo-alerts@bar.com\" ] } } Similarly, you can create a chart with SignalForm to visualize a metric: resource \"signalform_dashboard\" \"queue_length_dashboard\" { name = \"Queue Length Dashboard\" time_range = \"-1h\" variable { property = \"region\" alias = \"region\" values = [ \"regionA\" ] values_suggested = \"${var.regions}\" value_required = true restricted_suggestions = true } chart { chart_id = \"${signalform_list_chart.queue_length.id}\" width = 6 row = 1 } } resource \"signalform_list_chart\" \"queue_length\" { name = \"queue length\" program_text = <<- EOF filters = filter(\"device\", \"dm-0\")\n        data(\"iostat.queue_length\", filter=filters).mean().publish() EOF color_by = \"Dimension\" refresh_interval = 60 sort_by = \"-value\" } To provide validation and easier introspection of all SignalFx artifacts created through Terraform, we built a set of developers tools . Additionally, these tools allow us to test whether a detector would have fired by replaying data from the past using SignalFx preflight . SignalForm has ended up being more than just another Terraform provider for us. It has enabled us to improve Yelp’s monitoring culture. Today we are able to evolve our charts, dashboards and detectors together with the rest of our infrastructure. All the code for SignalForm and the developers tools is available on GitHub. Get the latest release to start using it or clone the repo and start contributing! Tweet Back to blog", "date": "2017-10-06"}, {"website": "Yelp", "title": "The Road To HSTS", "author": ["\n        \n  Martin Georgiev, Application Security Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/09/the-road-to-hsts.html", "abstract": "What is HTTP Strict Transport Security? HTTP Strict Transport Security, commonly referred to as HSTS, is a Web standard\nthat aims to ensure all web resources off a domain are fetched over a secure\ntransport layer. The core objective of HSTS is to\nprotect users against passive and active network attacks. To this end, it\nprevents protocol downgrade attacks and blocks insecure click throughs . From a configuration perspective, HSTS is an easy to deploy HTTP header. Its\nformat is: Strict-Transport-Security: max-age=31536000; includeSubDomains; preload Unfortunately, many companies who have tried to deploy HSTS have experienced\nvarious challenges ,\nsome of which resulted in service outages. We recently deployed HSTS\nsuccessfully at Yelp and would like to share what we’ve learned so that others\ncan enjoy a quiet deployment. Before we dive into how we deployed HSTS, let’s first explain the different\ncomponents that go into making HSTS possible and how they work. What is Transport Layer Security? Most of the web today is built on top of HTTPS. The security of HTTPS depends\non that of the Transport Layer Security (TLS) protocol. TLS is composed of two\nparts, the TLS Handshake protocol and the TLS Record protocol. The objective of\nTLS is to provide a secure channel between two communicating parties.\nSpecifically, it provides confidentiality, integrity and authentication. For the purpose of this blog post, we will focus only on the authentication\npart of TLS — the TLS Handshake. Authentication in TLS is done by\nvalidating X509 certificates. The process of authenticating a party is twofold:\n1) validate that the party’s certificate is properly signed and has a valid\nchain of trust; 2) ensure that the hostname of the requested server matches one\nof the entities on the certificate. Here’s a closer look at these two steps: Chain of trust validation: Certificate Authorities (CAs) \"stamp\"\n    all X509 certificates\n    they issue by recording their unique identifier under an issuer\n    field . For security purposes, root CAs do not normally issue leaf\n    certificates (e.g., a certificate for www.yelp.com, api.yelp.com, etc.)\n    directly. Rather, they delegate that responsibility to \"intermediate\" CAs.\n    As a consequence, leaf certificates often have certificate chains of depth\n    3 or higher. TLS clients (e.g., browsers) either ship with a pre-configured certificate\n    store or use the one provided by the underlying operating system. During\n    the TLS handshake, TLS clients try to validate that the certificate the\n    server provided has a valid chain of trust: it was issued by a valid\n    intermediate CA, and is anchored at a trusted root CA. Several other checks\n    are also performed as per RFC 5280 . Hostname validation: Identity checks in TLS are done by performing\n    hostname validation. X509 certificates may contain both fully qualified\n    domain names (FQDNs) and wildcard domains. User Agents follow RFC 2818 when trying to\n    match FQDNs. Wildcard domain names are matched by following the rules\n    outlined in RFC 5280 and RFC 6125 . If either chain of trust validation or hostname validation fails, client-side\nsoftware is expected to terminate the connection. Browsers often display a\nwarning message to the user notifying her about the specific issue encountered:\ne.g., expired certificate, hostname mismatch, self-signed certificate, etc. The\nuser is then given an option to “click through”, if the risk is acceptable. Unfortunately, users often don’t quite understand the technical aspects of\n“[website’s] certificate is not trusted by your computer’s operating systems.”\nAs a result, users have become accustomed to clicking through security warnings and potentially exposing themselves to malicious third parties sharing the\nnetwork. HSTS closes this gap by informing complying User Agents to terminate insecure\nconnections. To this end, security warnings become hard failures. Deployment Plan It doesn’t hurt to be cautious when deploying a major change on your platform\nthat can block all of its users or hurt your platform’s SEO. Here’s the roadmap\nwe followed to deploy HSTS at Yelp. Support HTTPS platform wide Platforms interested in enforcing HSTS must first start supporting HTTPS\nthroughout their web app. Here at Yelp, we fully migrated our platform from\nHTTP to HTTPS in 2016. If you haven’t done it yet, go over that first. Towards Strict Transport Security Step 1: Set but do not enforce HSTS HSTS policies can have a variable time to live (TTL) period, expressed in\nseconds via the max-age directive. When complying User-Agents observe an HSTS\nheader, they store and enforce the policy until it expires. We found that deploying HSTS with max-age=0 can help us evaluate the potential\nimpact on the platform without causing an outage. In fact, max-age=0 is used to clear problematic HSTS policies (in case you need to revert a previously set large TTL). Once we set the header, we tailed our server logs to ensure we have full\nunderstanding of where it pops up. This information was very valuable and\ninformed us what relative distribution of traffic across domains may\npotentially be impacted by the change. Yelp uses a complex routing infrastructure. As a result, setting the HSTS\nheader in our monolith caused the header to pop up in responses to hostnames outside of yelp.<tld>’s\nrange. For instance, we observed several internal apps that were sending\nsingle-part hostnames (e.g., “localhost”, “internal-api”, etc.) in the host\nheader and failing to properly process the HSTS header altogether. Step 2: Enforce HSTS using a small TTL Sending HSTS with max-age=0 does not help identify whether or not any of the\napps served off the platform’s subdomains will become inaccessible once the TTL\nis increased. To this end, it’s\nimportant that the first rollout of the header with a non-zero TTL uses a\nsufficiently small time window. In our case, we tried it out with max-age=60\nfor a week. No new problems were identified so we were able to move to the next\nstage. Step 3: Clear the HSTS TTL (max-age=0) Clearing the HSTS header between steps 2 and 4 is necessary to prevent\npotential outages. These steps help determine whether or not  apps mapped to\nsubdomains off the main domain fail to operate correctly over HTTPS and\nprevents app misconfigurations (e.g., invalid certificates) from turning into a\nfirestorm. Step 4: Add includeSubDomains The HSTS includeSubDomains directive informs complying User Agents to apply the policy to all subdomains\nof the domain where the policy was observed. For instance, if the browser sees\nthe HSTS header with the includeSubDomains directive when visiting yelp.com, it\nwill assume that all subdomains of yelp.com must also be accessed over HTTPS\n(e.g., app1.yelp.com, app2.yelp.com, etc.). Unfortunately, most web apps nowadays are accessed from hosts such as\nwww.<platform>.<tld>. Users often sign up on signup.<platform>.<tld>\nand login from login.<platform>.<tld> and never land on the\nplatform’s base domain (<platform>.<tld>) in their daily usage of the web\napp. Consequently, the includeSubDomains directive cannot live up to its full\npotential without a little bit of extra help. The extra help comes in the form of a tracking pixel. Tracking pixels are often\nused by third parties to track users on the Internet. In the context of HSTS, a\ntracking pixel can be used to bootstrap the HSTS policy. When strategically\nplaced, a tracking pixel can ensure that all visitors of the platform make a\nrequest to the platform’s base domain (on the same TLD as the visited domain)\nand thus get the HSTS policy for all subdomains of the platform. Step 5: Enforce HSTS with includeSubDomains using a small TTL Similar to step 2, we deployed HSTS using a small TTL. The difference between\nthis step and step 2 is the presence of includeSubDomains. The objective here\nis the same: if there is an issue with our HSTS deployment, we can easily\nrevert. This configuration helped us uncover a few issues we\ndiscuss later on in this blog post. Fixing these issues was the most\ntime-consuming part of this project. Step 6: Enforce HSTS with includeSubDomains using a large TTL Once we ensured that HSTS with includeSubDomains and a small max-age wouldn’t\nbreak our apps, we increased the max-age: first to one week, then to one month.\nWe then followed up with a six-month policy and ultimately set the expiration\ntime to one year. Step 7: Express intention to ‘preload’ Most modern browsers ship with a preconfigured list of known HSTS hosts .\nWebsites request inclusion in the list by sending a preload directive with\ntheir HSTS policy. Setting up the preload directive in the HSTS header took the least amount of\ntime. We appended the directive to our header and confirmed in our logs that\nall domains we wanted to cover were indeed covered. No breakages were observed. Step 8: Submit to the preload list The last step in fully deploying HSTS platform-wide was to submit all our TLDs\nto the HSTS preload list . Unfortunately, we were in\nfor an unpleasant surprise: base domains cannot be submitted to the HSTS\npreload list if they have more than 3 redirects. We had a few international\ndomains that needed a touchup to qualify for the preload list: we improved\ntheir redirect chains by optimizing our geocoding methodology. Major Speed Bumps Many deployments come with a few gotchas and the deployment of HSTS is no\nexception. Just ask the companies who were too quick to deploy it and realized\nthat they had to revert. Some got even as far as being included in the HSTS preload list ,\nbefore they realized that they had a problem. Next, we’ll discuss a few major problems we had to overcome at different stages\nof the HSTS deployment process. Some of these issues were due to bugs left\nunnoticed for years; others, required new development. Vanity subdomains: [<lang>.]vanity.yelp.tld Yelpers can reserve their very own, custom subdomain on the Yelp platform.\nThese subdomains can then be used as shortcuts to users’ profile pages. Vanity subdomains follow the pattern: http://<vanity>.yelp.<tld>, where\n<vanity> is the user-chosen prefix and <tld> is determined by the country\nYelp is browsed from. Some countries have multiple official languages. Yelp provides multi-language\nsupport in these countries by prepending the language short code to the\nhostname. For instance, fr.yelp.be is used to server our French-speaking users\nin Belgium, whereas en.yelp.be is used for our English-speaking userbase there. When a user navigates to a vanity subdomain, Yelp determines user’s language\npreference based on a preset language cookie. The language short code is then\nprepended to the vanity subdomain, and a redirect is issued. That is,\n<vanity>.yelp.<tld> is redirected to <lang>.<vanity>.yelp.<tld>.\nThe latter then redirects to the user’s profile page:\n[<lang>|www].yelp.<tld>/user_details?userid=<user_id>. Vanity subdomains were introduced on the Yelp platform in 2012. Back then, all\npages, other than the login, signup and password reset ones were served over\nHTTP. The design of this feature seemed reasonable and was left untouched for\nyears. Once we deployed HSTS, we saw a slight drop in traffic from international\nvanity subdomains. Concurrently, we got a report via our bug bounty program notifying us that some international vanity subdomains were non-navigable. We\nquickly dug into the problem and determined that international vanity\nsubdomains with language short code prepended to the hostname could not pass\ncertificate validation. Recall how hostname validation works :\nhostnames must either exactly match a whitelisted entry or loosely match a\nwildcard one. For example, if the user is trying to access <vanity>.yelp.com,\nthen either <vanity>.yelp.com or *.yelp.com needs to be on the X509\ncertificate for the certificate validation to be successful . Note that neither RFC-2818 , RFC 5280 , RFC 6125 , nor browsers allow\ndouble-nested wildcards (e.g., *.*.yelp.com is not supported). Consequently, the redirect chains for international vanity subdomains had to\nchange. To this end, we could no longer prepend the language short code to the\nhostname and redirects had to go from <vanity>.yelp.<tld> straight to\n[<lang>|www].yelp.<tld>/user_details?userid=<user_id>. We fixed the redirect chains by reordering the redirects as described above.\nThe change was completely transparent to the users. After the change, all\nvanity subdomains could be served over HTTPS. Data centers hostnames We have a set of reserved subdomains on which we serve content directly from a\ngiven datacenter. These domains aren’t meant for public access but are used for\ndebugging. We also change our datacenters’ hostnames as our infrastructure\nevolves. Consequently, we do not list our datacenters’ hostnames on our prod\nTLS certificates. Prior to enabling HSTS, devs used to either navigate to our datacenters’\nhostnames over HTTP or accept the certificate warning and access the hostnames\nover HTTPS. HSTS removed the former and made the latter impossible. To solve this problem, we could either add all our datacenters’ hostnames to\nYelp’s TLS certificates or serve a completely different certificate on these\ndomains. All of these domains are only used internally and never meant to serve\nexternal traffic so we opted for serving a certificate signed by our internal\nCA. Dev Playgrounds Engineers build new features in local playgrounds — these are dev\n“copies” of Yelp that run on subdomains based off the main yelp domain. Dev playgrounds are not routable over the Internet, nor is there any real\nusers’ data in dev playgrounds. As a result, we used to serve dev playground\ntraffic over HTTP. The HSTS deployment made that impossible. We had to make all\nplayground traffic fully HTTPS compatible. To resolve this issue, we started signing dev playground certificates using our\ninternal CA. Certificates were then issued each time a developer ran the “make”\ncommand to prep their playground. 1-Offs Historically, Yelp supported several custom subdomains such as\nmail.app.yelp.com and calendar.app.yelp.com. Yelp IT used those to create and\npreconfigure bookmarks to commonly used apps, e.g., Mail, Calendar, etc.\nProviders could then be changed transparently. These bookmarks were part of our\nlegacy infrastructure. They were preconfigured to use HTTP. When users click on any of the custom bookmarks, Yelp issues a 301 redirect to\nthe service provider; the browser follows the redirect and the user lands on\nthe desired page. HSTS changed this perspective. The core objective of HSTS is to prevent protocol downgrade attacks. That is,\nif the HSTS header with the includeSubDomains directive is served on the base\ndomain (e.g., yelp.com), then content from all subdomains must be fetched over\nthe HTTPS protocol. Furthermore, certificate validation must be successful for the\nconnection to go through. Browsers validate the certificate they receive against the hostname the user\ntries to access. In the example here, the browser tries to match\nmail.app.yelp.com against either the common name or one of the subject\nalternative names listed on the X509 certificate it receives at connection\nestablishment time. Yelp does not have {mail|calendar}.app.yelp.com on the X509\ncertificate, and thus no match can be found. The browser rejects the connection\nand blocks the user from accessing the mail/calendar/etc. app. The possible solutions to the problem here were: (1) add the troubled hostnames\nto our X509 certificate, (2) serve a certificate signed by our internal CA for\nall *.app.yelp.com, or (3) remove all legacy bookmarks. We went with the last option: we removed all legacy bookmarks from employees’\nmachines. For those who wanted to have a bookmark they can click on and go to\ncommon apps, such as Gmail and Google Calendar, we set up direct links to those\napps. Browser Quirks: While testing our HSTS deployment, we observed a quirk in Firefox: If the\nbrowser is set to “Never remember history”, the configuration is auto updated\nto: Surprisingly, the “Clear history when Firefox closes” is not checked on this\nscreen. Yet, HSTS pins are cleared when the browser is closed. Here is the sequence of steps we followed (last tested on Firefox 54.0.1): User sets “Never remember history” in Firefox. HSTS is deployed on yelp.com. includeSubDomains is set. User navigates to yelp.com (browser now knows that HSTS should be enforced on any subdomain of yelp.com). User tries to visit http://mail.app.yelp.com. 4a. Browser rewrites the URL to https://mail.app.yelp.com (due to HSTS). 4b. Browser fails to validate the certificate and issues an HSTS error. 4c. User cannot access mail.app.yelp.com. User closes the browser. 5a. Browser clears the HSTS directive. Later on: User opens the browser and tries to access http://mail.app.yelp.com. 1a. Browser doesn’t remember the HSTS directive; access is successful. While this behavior is not necessarily a bug, it’s a bit unexpected. So keep\nthis in mind while testing HSTS deployments. Catch22: Certificate Exceptions Browsers issue TLS errors whenever they encounter a certificate that cannot be\nvalidated for the website the user is trying to access. Without HSTS, users can\nbypass these errors temporarily or permanently. Temporary exceptions are\nhonored for the duration of the session. Permanent exceptions are honored until\nthey are manually unset. TLS error bypasses make users vulnerable to MITM attacks. To prevent such\nattacks, HSTS removes the TLS error bypass option: certificate errors are\ntreated as hard failures. HSTS does respect pre-approved certificate exceptions. For instance, if a user\nnavigates to a website that serves a bad certificate, and the user accepts the\nTLS warning before HSTS is deployed, then the browser will validate the\ncertificate with or without HSTS; users who are under constant MITM cannot be\nprotected by rolling out HSTS. This is known as the bootstrap MITM\nvulnerability. Conclusion Deploying HSTS at Yelp was a fun exercise. We got to learn a lot about what’s\nunder the hood, we identified and fixed several bugs, and removed code debt in\nseveral features along the way. We hope that by sharing our knowledge we will\nmake everyone else’s HSTS deployment go smoothly. Here’s to a more secure Yelp! Tweet Back to blog", "date": "2017-09-11"}, {"website": "Yelp", "title": "Day in the Life of a Product Manager", "author": ["\n        \n  Erin Rusonis\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/10/day-in-the-life-of-a-product-manager.html", "abstract": "Meet Brittany Cheng, a 5-star Product Manager! Learn what she loves about product management and how she has grown over her career here at Yelp. Are you interested in building great products? Do you love working with different teams? Are you motivated by the user experience? Mentorship, ownership, and great people — we’ve got it all. Bring your ideas to life and join our all-star Product Management team! Head to https://www.yelp.com/careers to learn more. Tweet Back to blog", "date": "2017-10-19"}, {"website": "Yelp", "title": "Yelp @ Grace Hopper - Orlando Edition!", "author": ["\n        \n  Rachel Zhao, Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/11/yelp-returned-to-grace-hopper-once-more-in-orlando.html", "abstract": "Yelp returned to Grace Hopper Celebration once more, this time in Orlando!\nLet’s take a look at what GHC2017 is all about from the insights of our\nattendees. Who Neha H.: Android engineer @ Search User Experience team Grace J.: Recruiting manager @ University Recruiting team Tiffany K.: Product manager @ Contributions team Lauren C: Product designer @ Messaging team Xun T.: Software engineer @ Ad Creative team, first time speaker at GHC\nthis year. Favorite sessions during GHC Tiffany: Women Who Build The Product Management Journey . I really\nappreciated hearing from women leaders who have been successful in the\nproduct role. They gave some great advice on how growth often happens\nwhen you take on opportunities that make you uncomfortable and how\nimportant it is to have a manager that believes in you. Neha: Keynote by Dr. Fei-Fei\nLi. Being a foreign national, Dr. Fei-Fei Li\nstruggled her way to the top by breaking many stereotypes. She rekindled\nthe image recognition field by adopting machine learning and AI in\nrevolutionary ways. In addition to her exceptional scientific\nachievements in the field of AI, she is also a mother and a good\nteacher. Hearing her story has truly inspired me and empowered me to\nrediscover myself. So you want to be an\nentrepreneur? Dr. Laura Mather, CEO and Founder of\nTalent Sonar, shared an example of how she gave the same pitch to\nmultiple VCs in a single day. The pitches didn’t always go well but she\nwould stay focused on the task at hand, despite incremental setbacks and\nmove on to the next VC. My takeaway from this was that there may be\nsetbacks in life and in your career, possibly one after another, but\nthat should never slow you down. Presentations: Career\nSuccess. This group of panelists talked about\nasking the right questions and developing relationships with your\nmanagers and mentors early on at a company. They suggested making them\nstrategic career partners so that you can …. One of the presenters\nalso talked about finding a sponsor, who will vouch for you to other\nseniors and executives and give you actionable feedback to help grow\nyour career. Lauren: My favorite session was HCI: Anthropology, Accessibility, and\nUser Confidence. I spent the early stage of my career feeling a bit like\nan odd duck on the team as a designer with a foundation in Anthropology.\nIt was really inspiring to hear Emily Grace from Capital One talk about\nanthropology as it is, the study of people and how the foundations of\nanthropology make her a better researcher. She discussed her journey\nfrom field researcher, forensic anthropologist to working on a\ncutting-edge UX Research team at Capital One. Most memorable moment @ GHC Grace: My favorite part about GHC is seeing so many students and\nprofessionals come together with the same desire and goal to improve\nopportunities for women in tech. There is a sense of camaraderie across\nall companies and organizations in attendance that I find unique to\nGrace Hopper. I particularly appreciate that companies share best\npractices for recruiting and new tools or workflows that they’re trying\nout to make the candidate recruiting experience better. I also enjoy\ngetting to know and work closely with so many of our own amazing female\nengineers and allies during Grace Hopper! It’s really rewarding to hear\nour engineers share what they love about working at Yelp with students\nand professionals. Neha: I was completely in awe at keynote on first day, listening to so\nmany powerful women personalities and being surrounded by 18,000 women\nin tech. In my over ten years of working in the industry, I have usually\nseen only one or two women engineers on a team so this was a new\nexperience for me. Lauren: The realization that this was the first time in my life that I\nwas completely surrounded by thousands of women in technology. Takeaways to apply at work Xun: Dr. Ya Xu, Principal Staff Engineer & Statistician at LinkedIn,\ngave a talk in which she questioned the typical A/B testing ramp-up process where we gradually increase traffic to the new\ntreatment (1%, 5%, 20%, etc.). Instead, she proposed ramping traffic\nquickly to Maximum Power Ramp (MPR)  as soon as the risk is determined\nsmall. MPR is calculated as the ramp that gives the most statistical\npower to detect differences. At Yelp we run A/B testing on almost everything we launch. We’ve\ndesigned guidelines on documentation and how to properly run an A/B\ntesting experiment. It’s refreshing to hear how other experts in the\nindustry speed up the A/B testing  process while maintaining accuracy of\nexperimental results. Grace: Grace Hopper attendees are often interested in hearing about the\nwork environment at your company. They want to hear stories and\nanecdotes about what it’s like to be a female in specific organizations.\nIn these conversations, it’s important to share what you like best about\nworking at your company. People want to hear why you do things, not what\nyou do. I’ve learned this over many recruiting seasons and have seen it\nto be really effective at Grace Hopper. I hope to share more of our Yelp\nvalues and stories of our employees as I continue to talk with\ncandidates. Neha: In Career Success Presentation, three engineers gave talks about\nhow to develop a professional relationship with your manager / mentor /\nother people in the company to build allies. They talked about how a new\nhire should understand manager and business priorities. They also talked\nabout how to effectively communicate to earn recognition. I feel this\nadvice is very useful for fresh graduates starting their careers, who\ndon’t really get this coaching in their schools and learn this on their\nown. Lauren: I loved the discussions on accessibility in tech. This is so important and it’s on us as gatekeepers for the technological world to\nmake sure we are inclusive of everyone trying to access our platforms.\nI’m currently talking with a couple of coworkers on creating an\naccessibility workshop at our workplace to educate coworkers on the\nreality of these disabilities and the current experience it provides for\nour users facing this issue. Pro tips for future speakers Xun: Be prepared for technical challenges at the event venue. For\nexample, the venue where GHC was hosted this year adopts a special\ninternal system where everything to be projected to the audience has to\nbe included on the slides (pptx format). It was not possible to switch\nto the browser, to the editor, or to the command line to run live demo.\nHence we enriched the slides with screenshots of each step and detailed\ninstructions. WIFI connectivity at the scene could be unpredictable as\nwell. Since we were running a workshop, we prepared USB drives with the\ninstallations and dataset to pass around in case of poor internet\nconnection. The USB drives have become quite handy and helped the\naudience set up the environment quickly. Almost 200 people attended our workshop. We labeled the workshop as\n“beginner/ intermediate” level, which could be a wide range for a\nworkshop under “data science” track. The diversity of the group amazed\nme. We got folks who don’t program regularly to folks who predicted the\ncontent on our next slide. I’d definitely recommend running the workshop\nbefore GHC with a smaller (but hopefully as diversified) audience. If\npossible, structure the workshop so that it can be scaled to large\ngroups of people by focusing on individual practices and small group\ndiscussions. It could be also beneficial to include some complimentary\nmaterials via printed booklets or a Github link. Pro tips for future attendees Neha: Popular sessions get filled up very quickly (hundreds of people\nlining up ahead of time). I highly recommend doing some homework to\nidentify the talks / presentations you want to attend and get there an\nhour early to secure a spot. Another tip is if you upload your resume on the GHC website, you will\nget invites to all the after hours events by various companies. For\nstudents and people who are looking for a change in their career, this\ncould be great way to make new connections and reach out to recruiters\nor engineers in addition to talking to them at the career expo (which is\nusually rushed). Grace: Keep your resume to one page and wash your hands often so you\ndon’t get sick from shaking so many hands ;) Lauren: Plan Those Sessions! Honestly, there is so much going on and\nit’s really hard to figure it out on the fly with so much grabbing your\nattention even when simply walking down the aisles of the main room. I’m\nreally glad I made an effort the first night to pick what the must-sees\nand would-likes were in case I needed to be available at the booth or\nfound myself in a captivating conversation. Women @ Yelp Engineering This is Yelp’s 5th year attending GHC and this year is particularly\nspecial as we were recently named one of AnitaB.org’s Top Companies for\nWomen\nTechnologists . A large impact has been made through forming AWE\n(Awesome Women in Engineering) chapters across all engineering offices\nand having an Executive Sponsor (Sam Eaton). Through AWE, we’ve been\nable to host programming (both professional development and social) for\nmembers, trainings for allies and managers, outreach and partnerships\nwith many organizations and recruiting efforts. Check out our website if\nyou want to learn more about how Yelp supports women to grow in the\nengineering department! Tweet Back to blog", "date": "2017-11-03"}, {"website": "Yelp", "title": "Moving the Rest of the Monolith to PaaSTA", "author": ["\n        \n  Kyle Anderson, Site Reliability Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/06/monolith-on-paasta.html", "abstract": "This past April (2017) we finally migrated our monolith to PaaSTA (our open source PaaS based on Apache Mesos). Yes, although Yelp does subscribe\nto the Service-Oriented-Architecture theory and we constantly try to reduce the scope of the monolith ,\nrealistically it still looms over us as a large towering codebase that pays the\nbills . But that doesn’t mean we can’t try to constantly improve it. This blog\npost is about our latest improvement to the monolith: treating it just like any\nother service at Yelp and running it on PaaSTA. Background: What is Yelp’s Monolith Made Of? Yelp’s monolith is composed of perfectly proportioned parts of Puppet, Apache, mod_wsgi, and Python + virtualenv. Before PaaSTA, it was\ndeployed directly to servers (a hybrid of on-premise datacenter and Amazon EC2)\nusing a bespoke rsync-based deployment system. For the purposes of this blog\npost I’m going to call this “Classic” infrastructure. (“Legacy” has such a\nnegative connotation, I rarely use it. I like “Classic” because it comes with a\nsense of respect for the past. Think “Classic Cars.”) This in itself isn’t very exciting but it comes with some challenges that are\nbaked into the system: The application is tightly coupled to the operating system it runs on (hard to upgrade) The bespoke deployment system means it is different than all our other applications, improvements to it only apply to the monolith Our Puppet configuration for the host is tightly coupled to the application, some changes require Ops help and are hard to coordinate (a developer can’t just try a new version of mod_wsgi on stage) Servers for the monolith are different than servers that run services (in many ways…) Yeah Well, How Do You Make the Monolith “Just Like Any Other Service”? The direction is obvious: the monolith should be run on the PaaS like any other\nservice.  The roadmap is unclear: how do we go from Puppet to PaaSTA without\nbreaking the website? Step 1: Dark launch Dark launching the monolith on new infrastructure The same best-practices around launching large experimental features apply\nequally to infrastructure and applications at Yelp. To “dark launch” our\nmonolith on PaaSTA, first we deploy it as a new and different SmartStack endpoint. SmartStack is the service-discovery tool we use, created by Airbnb,\nthat allows us to decouple the deployment of a service from the discovery of that service. By launching under a new endpoint (haproxy frontend) in\nSmartStack, our normal traffic to the monolith won’t be able to discover it,\nbut we can still test it through a special HAProxy access control list (ACL)\nthat will send you to the PaaSTA deployment if you have a special cookie. Doing it this way gives us some good benefits: We exercise the PaaSTA components every time we deploy, allowing us to find breakages before it’s live In these very early days there is no risk for normal users to find PaaSTA-powered webservers Core team members can opt in with the cookie to eat our own dogfood :) Once the glaring issues are found and fixed, we could increase the scope. Step 2: Canary Sending canary traffic to the new infrastructure We already used canary deployments with our\nclassic rsync-based system. For this step we have two canaries: one on PaaSTA\nand one on Classic. There is a critical difference between this canary step and\nthe “dark launch” step: the canary gets live traffic! We need the live traffic to hit this thing so we can find more bugs and\nevaluate the performance. At this stage we made critical decisions about\ncontainer sizes, hardware / instance classes, etc. Here would be a good time to start fixing things like monitoring dashboards,\nalerting tools, orchestration scripts, etc. All of these little odds and ends\nwill need to handle the hybrid mode.  For example, this is where we want to\nmake sure the tooling we have for rolling back code is solid and fast on both\nplatforms. It is “ok” to have them broken during this canary, but these small\nbreakages are blockers for the next step. Step 3: Migrate! (Rampup) Sending 50% traffic to the new infrastructure Once the canary has proven itself, it’s time to crank up the traffic onto more\nand more servers. You might consider a Blue/Green deployment from here, but for\na change this large and fundamental we decided not to do this for a business\nreason: it would cost too much. Remember from the introduction that we run a\nhybrid infrastructure, servers in datacenter and using AWS. We can’t just rack\ntwice as many servers and flip everything. No, for a change this large we took\nit slow and re-imaged our physical and virtual servers over the course of a\nmonth. During this phase you’ll want to to exercise your load balancing tier, making\nsure it can handle things like traffic shifting, dynamic backend discovery,\nsane timeouts, etc. A concrete example of an issue we found at this stage was the classic “running\nout of ephemeral ports” problem .\nWe knew we would encounter issues like this as we exercised the new stack.\nLuckily we have the classic infrastructure still in place to hold us over while\nwe fix these types of bugs. Step 4: Cleanup All PaaSTA, all the time Although the cleanup step is not that interesting, it is kinda fun. I hear that\nin some shops engineering time is not always allocated to this step, but for\noperations and infrastructure teams that would be crazy; you have to clean up\nor you will drown. For Yelp we were able to clean up a custom AMI baking pipeline, tons of puppet\ncode, and of course, the classic rsync-based deployment mechanism. Step 5: … Profit? Once on a new platform, some things that were very difficult to do become easy\nto do! Here are some examples: Using PyPy instead of CPython. With a\nDocker-based deployment system, this is “just” a change to the Dockerfile\n(after blacklisting some packages that have PyPy-incompatible C-extensions).\nSome teams at Yelp can now easily use this alternative interpreter and get\nmassive speedups. Upgrade the base linux distro with a code push. Again, a\ncontainer-based approach gives good isolation between the host OS and the\napplication’s base image. This is no longer a large multi-team effort across\nmultiple months. Can take advantage of the built-in goodies of a PaaS like\nautomatic monitoring, error reporting, autoscaling ,\netc. Perhaps an underrated gain with this migration is the massive reduction in\ncognitive load between the two systems. Now Yelp developers and operations\nengineers have a unified experience when deploying services, even if the\nmonolith is a very big service . An unanticipated bonus side-effect for Yelp is that our deploys are faster! The\nspeed of the new system is really a function of how quickly we can launch new\nDocker containers and how much spare capacity there is on the cluster, and we\ncan tune these knobs to hit our desired speed/cost balance. And of course, literally profit. Running on PaaSTA means that we can declare how many resources a service actually needs (cpu/ram/instance count) based on\nreal data, and Mesos can pack the cluster as best as it can. This means that\nspare resources on a machine no longer need to be wasted, new smaller tasks can\nbe scheduled in. On top of that we can autoscale the entire cluster to make our\ncompute spend match our actual compute demand, on an hour-by-hour basis! And then there is our true “secret” weapon for saving money by running on\nPaaSTA: Using Amazon Spot Fleet .\nThe nitty gritty details on how we do this sanely without sacrificing\navailability of the website are reserved for another blog post. Current State of The Art The monolith and almost all other services at Yelp run on PaaSTA. We don’t run\nour stateful (Kafka, Cassandra, Memcache) things on it, yet. While the\nmigration was rough and slow, the payout makes it worth it. There is still\nplenty of work to do! There are still many use-cases for running code at Yelp\nthat PaaSTA can’t do, like large analytic (EMR) jobs, realtime streaming\nworkloads (Apache Flink), and even just random one-off tasks (xargs!). Now that\nthe biggest use-case (web serving) is migrated, I look forward to extending\nPaaSTA to do even more new and exciting things! Tweet Become an Engineer at Yelp Backend application teams at Yelp work on a lot of incredible infrastructure projects like this. If you're interested apply below! View Job Back to blog", "date": "2017-06-19"}, {"website": "Yelp", "title": "Taking Zero-Downtime Load Balancing even Further", "author": ["\n        \n  Joseph Lynch, Lawrence Matthews\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/05/taking-zero-downtime-load-balancing-even-further.html", "abstract": "Ever since we rolled out our zero-downtime HAProxy reload system a few years ago, we have been disappointed that it required additional\ninvestment to work well for our external load balancing on our edge. We did\ngenerate a prototype that used an intermediary qdisc so we could apply the\napproach, but after evaluating the prototype, and finding that Linux wasn’t\ngoing to fix the upstream Kernel issue , we decided to go another\nway. Our edge is different than our internal load balancing tier because we have\ntypically terminated TLS with another great proxy: NGINX . NGINX is useful because it does support\nhitless HTTP (and recently TCP) listeners through file descriptor passing, so\nwe naturally started thinking about how we could replace or combine NGINX with\nHAProxy to solve the problem of global, dynamic, highly available external and\ninternal load balancing. This article sketches out the answer we came up with\nas the next iteration of our load balancing infrastructure. Load Balancing Gremlins Yelp, like many internet companies, faces two distinct load balancing\nchallenges: getting requests to our edge (external load balancing) and routing\nservice calls within our Service Oriented Architecture (internal load\nbalancing). In late 2016 our systems for both of these problems were not\nkeeping up with our rapidly evolving application architecture. Traditionally, our edge was fairly static and did not need the rapid\nreconfiguration that SmartStack provides. Web servers came and went infrequently and our HAProxy configuration\nonly needed to reload when we bought new hardware or manually launched new\ninstances. A relatively static NGINX terminating TLS and forwarding to a\nHAProxy load balancer was sufficient. This assumption stopped holding true,\nhowever, as we moved fully onto AWS, used autoscaling more heavily, and shifted\nour monolith into our PaaS . Our old, mostly\nstatic, configuration had to become dynamic, which meant we needed SmartStack.\nWe also hit scalability issues because our original edge design had NGINX\nconnect back to HAProxy over a loopback IP, which eventually led to exhaustion\nof ephemeral TCP ports. Our internal load balancing solution was also showing signs of deterioration\nunder changing requirements. Hundreds of services appeared on our PaaS and we\nstarted running into the edge cases of Marathon where containers constantly\nappear and disappear for brief periods. For example, Marathon needs to move\ntasks around when a container briefly lives but dies due to an application bug,\nor because container autoscaling scales the service up or down. This can cause issues for dynamic load balancing\nbecause the routing data plane must quickly converge across your entire\ninfrastructure or else slow down application deployments, or accidentally route\ntraffic meant for one service to a different service’s container. Marathon’s\ndistributed control plane and inability to keep tasks in the same “place” makes\nit very difficult for highly available dynamic load balancing tiers to achieve\nthis. Synapse and Nerve could handle the churn, but as we\nscaled we started seeing some performance issues on the internal load balancing\nsystem with our previous reload approach .\nIn particular, we started observing performance issues with HAProxy reloads,\nwhich now introduced 50-100ms of latency due to large HAProxy configuration\nsize and were triggered extremely frequently by changes in Marathon. Reloads\nwould also, very rarely, drop new connections because of various low\nprobability race conditions .\nIn addition, we ran into a kernel bug when we upgraded to Linux 4.4 where\nlocalhost queuing disciplines arbitrarily drop TCP packets and introduce\n200ms of latency. Linux 4.2 did fix the three-way handshake bug that our qdisc\nwas primarily defending against, but other races (e.g. accept before close )\nremained and the 4.4 qdisc performance regression was a serious issue. We wanted a solution that could solve our entire problem. In particular, we\nwanted to be able to: Add, remove, change, and modify load balancing configuration without any\npossibility of downtime or added latency Propagate load balancing updates across a global infrastructure in a few\nseconds in the typical case. Accept connections for both TCP and HTTP services Access comprehensive metrics and monitoring Access advanced load balancing features, especially tooling for automated\nfailover Scale TLS termination easily to multiple cores Make the system easy to understand for a typical operator Our Solution Our infrastructure team decided to solve this general problem by combining these\ntwo great pieces of software, and by using some relatively new features of\nHAProxy and NGINX. HAProxy 1.5+ (Jun 2014) supports listening on Unix domain sockets, and NGINX\n1.9.0+ (Apr 2015) supports both TCP and HTTP listeners (previously just HTTP).\nWhile Linux’s limitations meant that HAProxy couldn’t support hitless reloads\nusing SO_REUSEPORT , HAProxy Unix domain socket listeners have always been\nzero-downtime because the new listen sockets are atomically moved into place\nwith rename before the\nold process calls close . We believe there is technically still a very low\nprobability race that the old HAProxy calls close with connections on its accept() queue. In practice, we have not observed this race as the new HAProxy\nbinds the sockets first and only then signals the old HAProxy. Since the old\nHAProxy stops receiving new connections the moment the new one binds, it\ntypically has more than enough time to accept() its entire\nremaining queue before getting the shutdown signal. With this understanding, we can create the design shown in Figure 1, where\nNGINX terminates TCP (or TLS) and proxies back to an instance of HAProxy\nlistening on local Unix sockets for each service. For our load-balancers that\nterminate TLS, we run NGINX with multiple workers. For our internal\nload-balancers that don’t terminate TLS we explicitly choose to use only\nstream sections in NGINX to avoid any risk of NGINX messing with our low\nlatency internal traffic (e.g.  stripping headers, adding headers, buffering\nrequests to disk etc …). Figure 1: Architecture (source) Results To show this really works, we’ve created a test setup on a Linux 4.4 (also\nreproduced on 3.13) Ubuntu Trusty VM running on AWS (4 core, 7 gig ram). We\nhave: NGINX as the listening proxy HAProxy as the load balancing proxy Local NGINX serving a canned response to port 80 All details of the setup can be found in this gist .\nOur NGINX config is setup with two TCP listeners proxying back to Unix sockets: 400: Invalid request And our HAProxy config listens on those Unix sockets and operates one backend in\nHTTP mode and one in TCP mode: 400: Invalid request We run three interesting stress tests : restarting HAProxy with -sf , reloading NGINX workers, and upgrading NGINX masters. We run a control where nothing is restarting to get a feel of the\ntypical latency of the system, which you can see in Figure 2. In all of these\nlatency graphs the x-axis shows the progression of the benchmark\n(left to right) and the y-axis shows the response time as measured by apache\nbenchmark. This is not a heatmap, the vast majority of data is in the 1-2ms\nrange, but the outliers are what we care about in this analysis. We observe the control latency to be better than the qdisc approach because\nqdiscs add a few milliseconds of latency under high concurrency workloads. Under HAProxy reloads as seen in Figure 3, there are a few minor latency spikes\n( < 5ms ), which are most easily observed in the overlay Figure 4. 400: Invalid request We can also reload NGINX configuration and check that as well: 400: Invalid request Finally we can check upgrading the NGINX binary: 400: Invalid request If we look at NGINX reload/upgrade latency overlaid on the control, we observe\nin Figure 7 a greater impact on latency when reloading NGINX. This added latency\nis is still extremely small ( < 10ms ) and in this design NGINX is reloaded so\nrarely that in practice this is perfectly acceptable. The results clearly show that by combining both pieces of software we can\nreload our load balancing proxy (HAproxy) as much as we want, and SmartStack\nwill ensure going forward that everything is perfectly automated. Design Tradeoffs and Alternatives While designing our solution, we considered a number of alternative designs,\nand made a set of tradeoffs based on our engineering organization’s preferences\nand the technologies available at the time. We explored a number of options,\nbut in particular four main options: Fix the Linux kernel To be honest, we were hoping that the Linux kernel would fix the long standing\nissues with gracefully switching listen ports during the Linux 4.2 re-write of\nLinux’s TCP listen subsystem, and they did fix the SO_REUSEPORT three-way\nhandshake bug as far as we are aware. Unfortunately, to the best of our\nknowledge there is still the accept-close race as described in this netdev thread . We could have invested engineering effort in fixing the kernel, but we run a\nnumber of versions of the Linux kernel, and the time it would take to engineer\nthis solution and get it integrated simply didn’t make business sense. Just Use HAProxy The closest contender is a design that uses two HAProxy instances, where the\nfront HAProxy terminates TLS with multiple processes (as NGINX does in our\ndesign), and the back HAProxy listens on Unix sockets and does the actual load\nbalancing. This is possible because HAProxy added pretty good TLS support in\nversion in 1.5/1.6. We decided against it, however, because of the connection\nissues mentioned above and our SREs have significant operational\nexperience with NGINX as a TLS termination layer. At the time we made this decision, we would have had to accept connection\nissues if we chose to use just HAProxy, but this is no longer true !\nRecently, (April 2017) patches have been submitted to HAProxy which should\nallow perfectly graceful reloads by passing TCP sockets over a local Unix socket and reserving server slots that can be dynamically updated ,\npreventing the need for restarting. These patches are hopefully going to land\nin version 1.8 in a few months. Given these awesome changes, our next iteration\nwill likely change back to using just HAProxy and improving Synapse and our automation on top of Synapse to fully take advantage of these new dynamic features. Just Use NGINX Another option would be to just use NGINX, and for simple use cases this is a\ngood option, which is why Synapse now supports NGINX as a first class\nload balancer. For complex load balancing applications such as ours, however,\nthe open source version of NGINX lacks a number of important load balancing\nfeatures. For one, it is very difficult to configure NGINX correctly for transparent\nreverse proxying as most HTTP proxy defaults are setup for replacing something\nlike Apache rather than HAProxy. For example NGINX by default manipulates a\nnumber of headers and buffers requests and responses (potentially to disk!).\nThis concern can be solved relatively easily with the right incantations of\noptions, but it’s worth noting that in our experience HAProxy is designed first\nand foremost to be a load balancer, NGINX is designed first and foremost to be\na web server, and these are actually different challenges. Another major problem is that open source NGINX has no online control\ninterface, so you need to restart the proxy for it to pick up any configuration\nchanges - including downing servers in an emergency. In a PaaS environment, you\nend up with constant NGINX worker reloads. Combine those constant reloads with\nlong lived TCP connections, and you can quickly waste gigabytes of memory on\nevery machine in your fleet due to lingering NGINX processes which must remain\nrunning as long as active TCP sessions exist. You can rate limit restarts to save your boxes (as we do with HAProxy ),\nbut this then increases the latency it takes SmartStack to respond to failed\nmachines. With HAProxy stats socket updates, SmartStack can down a server\nglobally in a few seconds, but with NGINX it may take minutes. NGINX also lacks crucial load balancing tools like statistics, monitoring\ndashboards, healthchecks (to an extent) and support for complex routing ACLs.\nThese are not theoretical issues, we actively rely on the HAProxy stats socket\nto quickly up and down servers in an emergency, and monitor application replica health .\nWe also use a number of HAProxy ACLs and routing rules for transparent service instance failover (between AWS\navailability zones to regions and from one region to another) and for universal\nservice caching (acl routes to universal caching instance before routing to\nactual instance). NGINX+, the paid version, does solve some of these problems, but it is\nexpensive ( > free) and would require re-tooling. Use Both At the time we wrote this solution both proxies had unique benefits, and while\nit may have been simpler to use only one, using both gave us the ability to get\nthe best of both worlds. This solution has worked perfectly for our external load balancing for many\nmonths, but for our internal load balancing we had to make the main tradeoff\nthat our system is now significantly more complex, and required a significant\nre-architecture of SmartStack ,\nhow we configure SmartStack ,\nand a new implementation of SmartStack’s new config_generator API that\nsupports NGINX .\nThe qdisc solution we have used for years only took about one week of\nengineering time, but in contrast supporting multiple proxies has taken months\nof engineering time. We chose to do this because it gave us flexibility in\nother regards, especially in the face of an ever changing proxy landscape. The other major tradeoff is that listening on Unix sockets is not an especially\ncommon practice with HAProxy and it’s also not as widely supported in other\nsoftware. For example, curl only began supporting HTTP over Unix sockets in\nversion 7.40.0+ (Jan 2015). This makes debugging harder, and potentially\nexposes us to uncommon bugs, for example this load related bug in HAProxy (we have not observed this bug in production). Conclusion Highly available, dynamic load balancing is a constantly changing\ninfrastructure area. In addition to stalwarts like HAProxy and NGINX, we are\nseeing new players like envoy , linkerd , and vulcand come onto the scene. In\nthis iteration we decided to go with the simplest, most proven technology we\ncould, but with the infrastructure we’ve built around SmartStack it will be\nvery easy to continue iterating and making the best choices for our platform\ngoing forwards. Acknowledgements This project had a number of key contributors we would like to acknowledge for\ntheir design, implementation, and rollout ideas: Josh Snyder for coming up with the idea of using Unix sockets in the first place Evan Krall and John Billings for design feedback and review Tweet Back to blog", "date": "2017-05-15"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 8 Winner", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/06/dataset-round-8-winners.html", "abstract": "Yelp Dataset Challenge Round 8 Winners The eighth round of the Yelp Dataset Challenge ran throughout the first half of 2017 and, as usual, we received a large number of very impressive and interesting submissions. Today, we are proud to announce the grand prize winner of the $5,000 award:\n“Clustered Model Adaption for Personalized Sentiment Analysis” by Lin Gong, Benjamin Haines, and Hongnin Wang (from the Department of Computer Science of the University of Virginia). The authors built a personalized sentiment classification model at the group level. Their model is based on social theories about group psychology and how human beings tend to associate with similar individuals. These groups they form lead to more homogeneous opinions. Most text-based sentiment analysis models work at a global level, ignoring more “localized” group psychology and thus failing to capture the wide ranging opinions amongst users. In comparison, and partly based on their results with the Yelp dataset, the authors demonstrated that their model performs better than most. This work is fully relevant to many tech companies. This entry was selected from many submissions for its technical and academic merit. For a full list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Thanks to all who participated! Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset. These examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Tweet Back to blog", "date": "2017-06-23"}, {"website": "Yelp", "title": "Making Photos Smaller Without Quality Loss", "author": ["\n        \n  Stephen Arthur, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/06/making-photos-smaller.html", "abstract": "Yelp has over 100 million user-generated photos ranging from pictures of dinners or haircuts, to one of our newest features, #yelfies . These images account for a majority of the bandwidth for users of the app and website, and represent a significant cost to store and transfer. In our quest to give our users the best experience, we worked hard to optimize our photos and were able to achieve a 30% average size reduction. This saves our users time and bandwidth and reduces our cost to serve those images. Oh, and we did it all without reducing the quality of these images! Background Yelp has been storing user-uploaded photos for over 12 years. We save lossless formats (PNG, GIF) as PNGs and all other formats as JPEG. We use Python and Pillow for saving images, and start our story of photo uploads with a snippet like this: 400: Invalid request With this as a starting point, we began to investigate potential optimizations on file size that we could apply without a loss in quality. Optimizations First, we had to decide whether to handle this ourselves or let a CDN provider magically change our photos. With the priority we place on high quality content, it made sense to evaluate options and make potential size vs quality tradeoffs ourselves. We moved ahead with research on the current state of photo file size reduction – what changes could be made and how much size / quality reduction was associated with each. With this research completed, we decided to work on three primary categories. The rest of this post explains what we did and how much benefit we realized from each optimization. Changes in Pillow Optimize flag Progressive JPEG Changes to application photo logic Large PNG detection Dynamic JPEG quality Changes to JPEG encoder Mozjpeg (trellis quantization, custom quantization matrix) Changes in Pillow Optimize Flag This is one of the easiest changes we made: enabling the setting in Pillow responsible for additional file size savings at the cost of CPU time ( optimize=True ). Due to the nature of the tradeoff being made, this does not impact image quality at all. For JPEG, this flag instructs the encoder to find the optimal Huffman coding by making an additional pass over each image scan. Each first pass, instead of writing to file, calculates the occurrence statistics of each value, required information to compute the ideal coding. PNG internally uses zlib, so the optimize flag in that case effectively instructs the encoder to use gzip -9 instead of gzip -6 . This is an easy change to make but it turns out that it is not a silver bullet, reducing file size by just a few percent. Progressive JPEG When saving an image as a JPEG, there are a few different types you can choose from: Baseline JPEG images load from top to bottom. Progressive JPEG images load from more blurry to less blurry. The progressive option can easily be enabled in Pillow ( progressive=True ). As a result, there is a perceived performance increase (that is, it’s easier to notice when an image is partially absent than it is to tell it’s not fully sharp). Additionally, the way progressive files are packed generally results in a small reduction to file size. As more fully explained by the Wikipedia article , JPEG format uses a zigzag pattern over the 8x8 blocks of pixels to do entropy coding. When the values of those blocks of pixels are unpacked and laid out in order, you generally have non-zero numbers first and then sequences of 0s, with that pattern repeating and interleaved for each 8x8 block in the image. With progressive encoding, the order of the unwound pixel blocks changes. The higher value numbers for each block come first in the file, (which gives the earliest scans of a progressive image its distinct blockiness), and the longer spans of small numbers, including more 0s, that add the finer details are towards the end. This reordering of the image data doesn’t change the image itself, but does increase the number of 0s that might be in a row (which can be more easily compressed). Comparison with a delicious user-contributed image of a donut (click for larger): (left) A mock of how a baseline JPEG renders. (right) A mock of how a progressive JPEG renders. Changes to Application Photo Logic Large PNG Detection Yelp targets two image formats for serving user-generated content - JPEG and PNG. JPEG is a great format for photos but generally struggles with high-contrast design content (like logos). By contrast, PNG is fully-lossless, so great for graphics but too large for photos where small distortions are not visible. In the cases where users upload PNGs that are actually photographs, we can save a lot of space if we identify these files and save them as JPEG instead. Some common sources of PNG photos on Yelp are screenshots taken by mobile devices and apps that modify photos to add effects or borders. (left) A typical composited PNG upload with logo and border. (right) A typical PNG upload from a screenshot. We wanted to reduce the number of these unnecessary PNGs, but it was important to avoid overreaching and changing format or degrading quality of logos, graphics, etc. How can we tell if something is a photo? From the pixels? Using an experimental sample of 2,500 images, we found that a combination of file size and unique pixels worked well to detect photos. We generate a candidate thumbnail image at our largest resolution and see if the output PNG file is larger than 300KiB. If it is, we’ll also check the image contents to see if there are over 2^16 unique colors (Yelp converts RGBA image uploads to RGB, but if we didn’t, we would check that too). In the experimental dataset, these hand-tuned thresholds to define “bigness” captured 88% of the possible file size savings (i.e. our expected file size savings if we were to convert all of the images) without any false-positives of graphics being converted. Dynamic JPEG Quality The first and most well-known way to reduce the size of JPEG files is a setting called quality . Many applications capable of saving to the JPEG format specify quality as a number. Quality is somewhat of an abstraction. In fact, there are separate qualities for each of the color channels of a JPEG image. Quality levels 0 - 100 map to different quantization tables for the color channels, determining how much data is lost (usually high frequency). Quantization in the signal domain is the one step in the JPEG encoding process that loses information. The simplest way to reduce file size is to reduce the quality of the image, introducing more noise. Not every image loses the same amount of information at a given quality level though. We can dynamically choose a quality setting which is optimized for each image, finding an ideal balance between quality and size. There are two ways to do this: Bottom-up: These are algorithms that generate tuned quantization tables by processing the image at the 8x8 pixel block level. They calculate both how much theoretical quality was lost and how that lost data either amplifies or cancels out to be more or less visible to the human eye. Top-down: These are algorithms that compare an entire image against an original version of itself and detect how much information was lost. By iteratively generating candidate images with different quality settings, we can choose the one that meets a minimum evaluated level by whichever evaluation algorithm we choose. We evaluated a bottom-up algorithm, which in our experience did not yield suitable results at the higher end of the quality range we wanted to use (though it seems like it may still have potential in the mid-range of image qualities, where an encoder can begin to be more adventurous with the bytes it discards). Many of the scholarly papers on this strategy were published in the early 90s when computing power was at a premium and took shortcuts that option B addresses, such as not evaluating interactions across blocks. So we took the second approach: use a bisection algorithm to generate candidate images at different quality levels, and evaluate each candidate image’s drop in quality by calculating its structural similarity metric ( SSIM ) using pyssim , until that value is at a configurable but static threshold. This enables us to selectively lower the average file size (and average quality) only for images which were above a perceivable decrease to begin with. In the below chart, we plot the SSIM values of 2500 images regenerated via 3 different quality approaches. The original images made by the current approach at quality = 85 are plotted as the blue line. An alternative approach to lowering file size, changing quality = 80 , is plotted as the red line. And finally, the approach we ended up using, dynamic quality, SSIM 80-85 , in orange, chooses a quality for the image in the range 80 to 85 (inclusive) based on meeting or exceeding an SSIM ratio: a pre-computed static value that made the transition occur somewhere in the middle of the images range. This lets us lower the average file size without lowering the quality of our worst-quality images. SSIMs of 2500 images with 3 different quality strategies. SSIM? There are quite a few image quality algorithms that try to mimic the human vision system.\nWe’ve evaluated many of these and think that SSIM, while older, is most suitable for this iterative optimization based on a few characteristics: Sensitive to JPEG quantization error Fast, simple algorithm Can be computed on PIL native image objects without converting images to PNG and passing them to CLI applications (see #2) Example Code for Dynamic Quality: 400: Invalid request There are a few other blog posts about this technique, here is one by Colt Mcanlis. And as we go to press, Etsy has published one here ! High five, faster internet! Changes to JPEG Encoder Mozjpeg Mozjpeg is an open-source fork of libjpeg-turbo , which trades execution time for file size. This approach meshes well with the offline batch approach to regenerating images. With the investment of about 3-5x more time than libjpeg-turbo, a few more expensive algorithms make images smaller! One of mozjpeg’s differentiators is the use of an alternative quantization table. As mentioned above, quality is an abstraction of the quantization tables used for each color channel. All signs point to the default JPEG quantization tables as being pretty easy to beat. In the words of the JPEG spec : These tables are provided as examples only and are not necessarily suitable for any particular application. So naturally, it shouldn’t surprise you to learn that these tables are the default used by most encoder implementations… 🤔🤔🤔 Mozjpeg has gone through the trouble of benchmarking alternative tables for us, and uses the best performing general-purpose alternative for images it creates. Mozjpeg + Pillow Most Linux distributions have libjpeg installed by default. So using mozjpeg under Pillow doesn’t work by default , but configuring it isn’t terribly difficult either.\nWhen you build mozjpeg, use the --with-jpeg8 flag and make sure it can be linked by Pillow will find it. If you’re using Docker, you might have a Dockerfile like: 400: Invalid request That’s it! Build it and you’ll be able to use Pillow backed by mozjpeg within your normal images workflow. Impact How much did each of those improvements matter for us? We started this research by randomly sampling 2,500 of Yelp’s business photos to put through our processing pipeline and measure the impact on file size. Changes to Pillow settings were responsible for about 4.5% of the savings Large PNG detection was responsible for about 6.2% of the savings Dynamic Quality was responsible for about 4.5% of the savings Switching to the mozjpeg encoder was responsible for about 13.8% of the savings This adds up to an average image file size reduction of around 30%, which we applied to our largest and most common image resolutions, making the website faster for users and saving terabytes a day in data transfer. As measured at the CDN: Average filesize over time, as measured from the CDN (combined with non-image static content). What we didn’t do This section is intended to introduce a few other common improvements that you might be able to make, that either weren’t relevant to Yelp due to defaults chosen by our tooling, or tradeoffs we chose not to make. Subsampling Subsampling is a major factor in determining both quality and file size for web images. Longer descriptions of subsampling can be found online, but suffice it to say for this blog post that we were already subsampling at 4:1:1 (which is Pillow’s default when nothing else is specified) so we weren’t able to realize any further savings here. Lossy PNG encoding After learning what we did about PNGs, choosing to preserve some of them as PNG but with a lossy encoder like pngmini could have made sense, but we chose to resave them as JPEG instead. This is an alternate option with reasonable results, 72-85% file size savings over unmodified PNGs according to the author. Dynamic content types Support for more modern content types like WebP or JPEG2k is certainly on our radar. Even once that hypothetical project ships, there will be a long-tail of users requesting these now-optimized JPEG/PNG images which will continue to make this effort well worth it. SVG We use SVG in many places on our website, like the static assets created by our designers that go into our styleguide . While this format and optimization tools like svgo are useful to reduce website page weight, it isn’t related to what we did here. Vendor Magic There are too many providers to list that offer image delivery / resizing / cropping / transcoding as a service. Including open-source thumbor . Maybe this is the easiest way to support responsive images, dynamic content types and remain on the cutting edge for us in the future. For now our solution remains self-contained. Further Reading Two books listed here absolutely stand on their own outside the context of the post, and are highly recommended as further reading on the subject. High Performance Images Designing for Performance Tweet Become an Engineer at Yelp Backend application teams at Yelp work on a lot of incredible infrastructure projects like this. If you're interested apply below! View Job Back to blog", "date": "2017-06-01"}, {"website": "Yelp", "title": "Upcoming Deprecation of Yelp API v2", "author": ["\n        \n  Quy L., Product Mananger\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/06/upcoming-deprecation-of-yelp-api-v2.html", "abstract": "As we continue to invest in the Yelp Fusion API , we are announcing that Yelp API v2 will be discontinued on June 30, 2018. If you are currently using v2 API endpoints, you have until June 30, 2018 to move them over to the Yelp Fusion API. We hope this gives everyone sufficient time to transition your applications from Yelp API v2 to Yelp Fusion, giving you the ability to tap into awesome new features like GraphQL , which we launched last month. Here is a summary of the Yelp API v2 sunset timeline: April 1, 2017 - Disabled Yelp API v2 signups June 28, 2017 (Today) - Announcement, no changes to Yelp API v2 keys June 30, 2018 - Yelp API v2 endpoints will no longer work A big thank you goes out to all the developers in our community, who have given us great feedback. We have made recent updates based on your input, and would love to continue to hear from you. Check out the GraphQL changelog and give us feedback on GitHub. If you have any questions or concerns, please email api@yelp.com . Tweet Back to blog", "date": "2017-06-28"}, {"website": "Yelp", "title": "Moving Yelp's Core Business Search to Elasticsearch", "author": ["\n        \n  Umesh Dangat, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/06/moving-yelps-core-business-search-to-elasticsearch.html", "abstract": "While newer search engines at Yelp typically use Elasticsearch as a backend, Yelp’s core business search used its own custom backend, built directly on top of Lucene. This system was one of the oldest systems at Yelp to still be deployed in production.\nSome features of this custom search engine were Distributed Lucene instances Master-slave architecture Custom text analysis support for various languages Custom business ranking which relied mostly on using business features (think business attributes like reviews, name, hours_open, service_areas, etc.) Derived Yelp analytics data to improve quality of search results; e.g. most popular queries for a business Problems with the Legacy System Suboptimal Support for Realtime Indexing Our legacy system used a master-slave architecture where masters were responsible for handling writes (indexing requests) and the slaves would serve live traffic. A master instance was responsible for taking a snapshot of the Lucene index and uploading it to S3 so that the slave could download it periodically and serve fresh data. Thus there was a delay in the updated index being served by the queries at search time.\nSome search features, such as reservations and transactions, could not afford this delay (a few minutes), and required the indexed data to be served instantly (within a few seconds). To solve this problem, we had to use another realtime store, Elasticsearch, and query it in parallel to the business store (legacy search store) which meant that the application service would then need to compute the final search results based on both results. As we grew, this approach did not scale well and we had to deal with performance issues arising from combining and sorting results in the application layer. Slow code pushes We have a large team of developers working constantly on improving the ranking algorithm for the search results. We end up making code pushes to the underlying search ranking algorithm multiple times a day and with the legacy system, each code push took several hours. Almost all microservices at Yelp now use PaaSTA as the deployment vehicle. At the time, the legacy system was probably the largest “microservice” at Yelp using PaaSTA.\nOur data was large enough that it had to be sharded . We used a two-tier sharding approach. Geosharding: We split the businesses into separate logical indexes based on their location. For example, a business in San Francisco would be in a separate index from a business in Chicago because the shard might split down the middle of the country. Microsharding: We further split each geographical index into multiple “microindexes” or “microshards”. For this we used a simplistic mod-based approach. For example business_id % n, where “n” is the number of microshards desired Thus our final lucene index would be <<geographical_shard>>_<<micro_shard>> Each Lucene index-backed process was its own service instance. We had to account for replication as well to ensure availability. For example each <geoshard>_<microshard> would have multiple instances called “replicas” to guard against outages of instances.\nThis meant we had a large number of service instances and each instance would take some time to start up since each instance needed to download tens of gigabytes of data from S3 warm the Lucene index to preload Lucene caches compute and load various data sets into memory force garbage collection, since startup created a lot of ephemeral objects Each code push meant we would have to cycle the workers and they’d have to go through this process each time. Inability to do certain feature work Reindexing all of the data was time consuming which meant that the cost of adding some new features would become exponentially more expensive. We wouldn’t be able to do things like Quickly iterate on our sharding algorithm. Iterate on analyzers. We developed custom language-based analyzers to tokenize text. Search engines like Lucene use specific analyzers at index time (to do things like generate tokens, filter stopwords) and generally prefer to use the same analyzer at query time so the tokenized query string is found in the inverted index. Changing analyzers means reindexing the entire corpus so we generally tried to stay away from optimizations in our analysis codebase. Indexing additional fields to improve ranking. Business attributes are one of our primary signals to rank a business in search result. As our business data got richer we could improve our ranking using this data. Sadly, we had to use other realtime stores to lookup these business attributes since a change to our legacy system was an ordeal. We were pushing the JVM heap limits with respect to amount of data we stored in auxiliary data structures. We had custom data that could not be stored on the Lucene index but this data was needed to rank businesses (for example, storing the most popular queries for each business). As this data grew it became harder to scale since this was limited by the JVM heap size. At this point, we became convinced that the legacy system had to be overhauled. So what would be the design of the new system? First let’s take a look at the existing system so we know the challenges we need to solve in the new system without introducing any regressions. The Legacy System Legacy business search stack Legacy business search stack It starts with search requests coming into the coordinator service. This service figures out the corresponding geographical shard to use (based on the physical location of a business) and forwards the request to the appropriate shard, in this simplified case either west or east. The request would be broadcast to all the microshards (2nd tier of sharding for horizontal scaling) within that geographical shard. After getting the result from one to N microshards the coordinator would combine the results. Microshard (Single legacy search node) Let’s dive into a single node and go through how we go from a query to a result. Microshard A search request is transformed into a Lucene query, which is then sent to the Lucene index. Lucene returns a stream of results as per the collector specified. One can think of the collector as the ranker , the thing deciding the order of the results. This is where the ranking logic is applied. Ranking logic at Yelp uses a number of heuristics to determine the end result ranking. These heuristics include looking into certain business related data Business Field cache : forward index for a business (things like business attributes) Top Query Info : Data derived from our user activity Misc data : includes Yelp-specific data like Yelp categories With this, we are now ready to define the design goals of the new system. Goals for the Next-Generation Business Search Based on the previous sections, we can quickly summarize some of our high-level goals as: decoupling of application logic from the backend used faster code pushes ease of storing custom data and forward indexes that powered the search results ranking (e.g. context specific data) real-time indexing linear performance scalability with respect to growth of our business data We looked at Elasticsearch and it seemed to solve some of these goals for us out of the box. Challenges Decoupling Application Logic from Backend Used The ranking code does not need to know what backend it runs on. Thus it made sense to decouple this code from the underlying search backend store. In our case this is a Java codebase which means we can deploy it as a jar. More concretely, we had to be able to run our ranking jar within a distributed search environment. Elasticsearch offers plugin support which allows us to do exactly this. We took care to isolate our ranking code from the Elasticsearch plugin implementation details. Decoupling plugin from scoring jar Interfaces We have two primary definitions that allow us to decouple the ranking code from relying on the underlying Elasticsearch libraries directly. Thus none of our ranking code has a hard dependency on Elasticsearch (or Lucene), giving us the flexibility to run this ranking code on any other backend. public interface ScorerFactory {\n       Scorer createScorer(Map<String, Object> params);\n}\n\npublic interface Scorer {\n     double score(Document document);\n}\n\npublic interface Document {\n   <T> T get(Class<T> clz, String field);\n} The Document interface is used by the module/ranking code to lookup business attributes. However the ranking code does not know about its implementation. The concrete implementation for Document is injected by the Elasticsearch plugin. The Scorer interface is implemented by the module. Again, it has no dependencies on Elasticsearch. This Scorer is loaded by a private classloader inside the elasticsearch plugin. Module The module is the ranking code where the heart of the search relevance logic lives. This is the code that is pushed to production potentially multiple times a day. This jar is deployed on our Elasticsearch cluster and is then loaded inside the Elasticsearch plugin. Plugin The Elasticsearch plugin houses the ranking code. This is mostly Elasticsearch-related wiring code that loads the module code and delegates to it for ranking documents. Faster pushes As stated earlier, we push code multiple times a day but restarting Elasticsearch for each push was not an option for us. Since we built our relevance module to be decoupled from Elasticsearch itself, we were able to reload it without restarting the entire Elasticsearch cluster. We start by uploading our ranking jar to S3. We added an Elasticsearch REST endpoint that is invoked during the deploy process so that the Elasticsearch plugin can reload the specified jar. public class YelpSearchRestAction extends BaseRestHandler {\n   @Override\n   protected RestChannelConsumer prepareRequest(RestRequest request, NodeClient client) {\n                   moduleLoader.loadModule(); //a. invoke re-loading of module.jar\n       return channel -> channel.sendResponse(new BytesRestResponse(RestStatus.OK, content));\n   }\n} Once this endpoint is invoked it triggers the loading of the module.jar through a private class loader that loads the entry point top level class from the module.jar itself public final class ModuleLoader {\n    public synchronized void loadModule(){\n        final Path modulePath = downloadModule(); //1. download the module.jar\n         createClassloaderAndLoadModule(modulePath); //2. create classloader and use that to load the jar\n    }\n\n    private void createClassloaderAndLoadModule(final Path modulePath){\n        final URLClassLoader yelpySearchClassloader = new YelpSearchPrivateClassLoader(\n             new URL[]{modulePath.toUri().toURL()},\n             this.getClass().getClassLoader()    //3. Create URLClassloader\n        );\n         scorerFactory = Class.forName(\"com.yelp.search.module.YelpSearchScorerFactoryImpl\",\n                                        true,\n                                        yelpySearchClassloader)\n              .asSubclass(ScorerFactory.class)\n              .getDeclaredConstructor(new Class[]{Environment.class})\n              .newInstance(environment);  //4. Create instance of ScorerFactory that return the Scorer\n    }\n     public Scorer createScorer(Map<String, Object> params) {\n          return scorerFactory.createScorer(params);  //5. Scorer factory returning scorer, called once per query\n     }\n} Download the module jar to a Path Create private classloader based off the Path Use the URL of the module.jar to create URLClassloader Create the instance which implements ScorerFactory. Note the use of asSubclass and passing argument Environment. Environment is another interface which provides some resources needed by the module code. ScorerFactory has a createScorer method that returns the Scorer instance Then we have the Elasticsearch plugin code that invokes the reloaded Scorer class YelpSearchNativeScriptFactory implements NativeScriptFactory {\n  public ExecutableScript newScript(@Nullable Map<String, Object> params) {\n    Scorer scorer  = moduleLoader.createScorer(params));\n    return new ExecutableScriptImpl(scorer); //1. Create elasticsearch executable script\n  }\n} Create the elasticsearch Executable script and pass it our scorer instance which was “hot loaded” previously class ExecutableScriptImpl extends AbstractDoubleSearchScript implements Document { \n  public ExecutableScriptImpl(final Scorer scorer) {\n    this.scorer = scorer;\n  }\n  public double runAsDouble() {\n    return scorer.score(this);  //1. score this document\n} Elasticsearch executable script finally uses the scorer to score and pass the document itself. Note the “ this ” passed in. That is how the document attribute lookups aka doc value lookups in elasticsearch can actually happen inside elasticsearch plugin while the module code simply uses the interface. Loading custom data One of the original problems with the legacy system was that the memory footprint of a single search node was getting larger with time; the primary reason being that we loaded a lot of auxiliary data on the JVM heap. With Elasticsearch we were able to mostly offload these in-memory data structures to doc values . We had to ensure our hosts had enough RAM available so that Elasticsearch could make use of disk cache efficiently when retrieving these doc values. ScriptDocValues worked well for most types of attributes like String, Long, Double, and Boolean, but we did need to support custom data formats as well.\nSome businesses have context-specific data stored that is computed separately from search. This signal allows our search to help boost the score of a business for things like “chance of business being associated with a search query given its most popular queries” based on past history. We represent this structure like so: Custom data format If we wanted to store this data per business as a doc value it would have to be serialized Serialized layout of custom data per business Since query strings are of arbitrary lengths and take up more space we decided to represent them using positive integers. We identified a string with a monotonically increasing long value. This allowed us to save space by using long instead of string and keep the records to fixed sizes. So, let’s say we have two strings, “restaurants” and “mexican restaurants”. Our plugin would identify “restaurants” as 1 and “mexican restaurants” as 2. The strings themselves would be replaced with the long value corresponding to the query so you would end up with “1” and “2”.  Thereby allowing us to represent strings with fixed size of Long.Bytes . This made it easier for us to serialize and deserialize our query related data. This is a simplified example, in practice we need to store analyzed form of the string based on the language e.g. “restaurants” could be tokenized to “restaur” in english. Now, we could change our data structure to store only longs and doubles as the strings got replaced by their references: Serialized layout of fixed size length entries per business The user query and its associated values per business can be represented as a list of objects. Class QueryContextInfo {\n    private long queryId;\n    private double valueOne;\n    private double valueTwo;\n} With this we can index all the records for a business as binary data type in Elasticsearch using our custom serialization. public static byte[] serialize(QueryContextInfo[] queryContextInfoRecords) {\n  byte[] bytes = new byte[Integer.BYTES + (queryContextInfoRecords.length * (Long.BYTES + 2 * (Double.BYTES)))];\n  ByteBuffer.wrap(bytes, 0, Integer.BYTES).putInt(queryContextInfoRecords.length);\n  int offset = Integer.BYTES;\n  for (QueryContextInfo queryContextInfo : queryContextInfoRecords) {\n    ByteBuffer.wrap(bytes, offset, Long.BYTES).putLong(queryContextInfo.getQueryId());\n    ByteBuffer.wrap(bytes, offset + Long.BYTES, Double.BYTES).putDouble(queryContextInfo.getValueOne());\n    ByteBuffer.wrap(bytes, offset + Long.BYTES + Double.BYTES, Double.BYTES).putDouble(queryContextInfo.getValueTwo());\n    offset += Long.BYTES + 2 * (Double.BYTES);\n  }\n  return bytes;\n} One issue that arose was looking up binary data using ScriptDocValues. We submitted a patch to Elasticsearch to support this, allowing you to do something like: List<ByteBuffer> queryContext = document.getList(ByteBuffer.class, \"query_context\"); Once we read the ByteBuffer out of Elasticsearch we could search on the query_id we wanted, for example, the query_id the user issued, inside the serialized QueryContextInfo[] . A match on query_id allowed us to retrieve the corresponding data values; i.e. QueryContextInfo for the business. Performance Learnings As part of building our new system, we spent time making sure it would outperform our legacy search system. Here are some of the lessons we learned through this process. Find Your Bottlenecks The Elasticsearch Profile API is a useful way to find bottlenecks in your query. Scoring is Linear and Scales Well With Sharding In our case, scoring was the bottleneck since we rely on many features to rank the results. We realized that we could horizontally scale by adding more shards which meant we got more parallelism out of Elasticsearch at query time since each shard had fewer businesses to score. Be warned when doing this; there is no magic number since these depend on your recall size and your scoring logic, amongst other things. The performance growth with regard to the increase in number of shards is not unbounded. We had to find our sweet spot mostly through trial and error by increasing the shards and reindexing data. Use Java Profiling Tools Using Java tools like jstack , jmap , and jprofiler gave us insights into hotspots (computationally intensive components) in our code. As an example, our first implementation of binary data lookup involved deserializing the entire byte array into a list of Java objects, namely List , and then searching for query_id linearly. We found this to be slow and also caused more garbage collection on short lived objects since we were doing this for every recalled business for every query. We reworked our algorithm by doing a binary search on the serialized data structure without deserializing it. This allowed us to quickly search for the query_id if it is inside the blob for this business. This also meant we do not need to incur costs of garbage collection for deserializing the entire blob into Java objects. Conclusion Moving Yelp’s core search to Elasticsearch was one of the more challenging projects undertaken by the Yelp search team in the recent past. Since this was a technically challenging project in regards to its feasibility, it was important we iterated on this project in fail-fast mode. In each short iteration we tackled the high risk items like hot code loading, ability to support custom data in Elasticsearch, and Elasticsearch performance, which allowed us to gain confidence in our approach while not committing for far too long to unknowns. Ultimately, the project was successful and now we are able to reindex data regularly and add new fields easily, allowing us to improve our ranking algorithm in ways we were unable to before. Our code pushes now take a few minutes instead of a few hours. Maybe most importantly, we do not need to maintain a legacy system which was hard to understand, making it is easier to find developers who know and want to learn Elasticsearch. Tweet Back to blog", "date": "2017-06-29"}, {"website": "Yelp", "title": "What Does A Yelp Associate Product Manager Do? Fred Wang Shares All", "author": ["\n        \n  Erin Rusonis\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/07/what-does-a-yelp-associate-product-manager-do.html", "abstract": "Fred Wang has already done a lot. He graduated from U Penn’s Wharton School with degrees in Finance and Operations Management and then headed to New York City to co-found a startup. No big deal. Finding himself at a crossroads a few years later, he decided to try his luck on the west coast and joined Yelp’s new (at the time) Associate Product Manager (APM) program. Since his two-year anniversary at Yelp Eat24 is right around the corner, we decided to ask him a few questions about his experiences. Here’s what he had to say. Q: As a recent grad, early in your career, why did Yelp catch your eye? A: I still wanted to tap my experiences and interest at the intersection of business, design, and engineering and directly shape a product’s roadmap. Yelp’s APM Program was just entering its second year when I applied. The recruiting team was extremely responsive and moved quickly… Within two to three weeks of applying I had a job offer, and speaking to PMs and APMs currently at Yelp confirmed I could grow and learn a ton about scaling teams and businesses. Q: Describe your job in three words. A: Getting things done. Q: What has been your greatest triumph at Yelp? A: Getting visibility into how the product is working at the individual order level through post-order feedback and implementing changes to the product to get more feedback. We’ve done a whole lot of iterations on that. We have a graph of the percent of orders we have with feedback and each improvement we’ve done has seen a jump in that percent. The idea is that this feedback will be useful to identify areas for product improvement and provide actionable insights for restaurant owners. Q: Without revealing any secrets, what are you working on right now? A: A slew of projects that aim to provide a better experience for Yelp Eat24 users. Q: It sounds like you’ve accomplished a lot so far. What’s your dream project at Yelp/Eat24? A: My dream project is making Eat24 more useful around lunchtime. This includes bringing online more restaurant choices but also giving office workers access to a “corporate” lunch product. Q: Wait, are you saying that the entire Eat24 staff could order 800 burritos at lunch? A: Not just people in our offices, but every office in major US metro areas. Burritos, sushi rolls, burgers, slices of pizza, cheesesteaks — you name it. It’s a ways off, but definitely on our radar. Q: What is one of your most memorable moments at Yelp? A: I really like our quarterly hackathons. One hackathon we built Eat24 for Apple TV. After two long days and some late-night coding, we pulled together a demo and got it to place a real order. We really re-envisioned the product from end to end and got designers, PMs, and engineers together to solve problems from the ground up. Q: You ditched New York for laid-back California. What’s one of the biggest differences between SF and NYC? A: People tend to care a lot more about work/life balance here… In NYC people didn’t really have hobbies outside of work and the social life was centered pretty heavily around going out and drinking. In SF it’s much more normal to have a boardgames night or organize a hike for the weekend. People also are very passionate about solving problems through technology. Q: Sounds like you’ve had some time to take on new hobbies. What does your ideal weekend look like? A: Grabbing dim sum with friends in the morning, working on hobbies or projects after, jogging along the embarcadero, strength training, and then swinging by a friend’s house to hang out in the evening. Q: Besides dim sum — I’m obsessed with it too btw — what’s your favorite local business in SF? A: That’s a tough one. One of the places I frequent a lot is Gott’s Roadside at the Ferry Building. Sometimes I’ll grab a late night bite after running along the Embarcadero because they’re open late. I’ve also gone on weekend mornings, sitting outside to enjoy the SF skyline while listening to live music from the street musicians around there. Q: Thanks so much for your time, Fred! Any last words of advice for recent grads or those looking to make a transition to Product Management? A: Be passionate about the product you’re looking to work on. Think about where the product can be in the 3-6 month range, and also the 2-5 year range. Ideas in the 3-6 month range are incremental improvements that compound over time and are impactful to the business. Thinking in the 2-5 year range is a strategic investment that keeps your business competitive. Tweet Become an Associate Product Manager Interested in joining Yelp's Associate Product Manager program and helping build our product? Apply to become an APM! View Job Back to blog", "date": "2017-07-20"}, {"website": "Yelp", "title": "Generating Web Pages in Parallel with Pagelets, the Building Blocks of Yelp.com", "author": ["\n        \n  Arnaud Brousseau, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/07/generating-web-pages-in-parallel-with-pagelets.html", "abstract": "At Yelp, pagelets are a server-side optimization to parallelize the rendering of\nweb pages across multiple web workers (loosely inspired by Facebook’s Big Pipe ).\nWe’ve implemented this and have been running it successfully in production for\na while now. This blog post is about our journey implementing and rolling out\n pagelets, including what we’ve learned since the initial rollout. Pagelets at Yelp: an overview Main and pagelet workers Usually a request made to Yelp is fulfilled by a single web worker. This worker\nis in charge of generating a response (in the form of an HTTP packet, with\nheaders and body) from start to finish. Pagelets parallelize server-side processing by issuing multiple asynchronous\nHTTP requests to render sections of a page. When pagelets are active,\nseveral processes are working in parallel to compute the response: Pagelets: MVCs within an HMVC Each pagelet is a small MVC ( Model-View-Controller ),\nresponsible for rendering a portion of the final page. Since the main worker is\nalso its own MVC, the main worker and pagelet workers taken together are often referred to\nas HMVC ( Hierarchical Model-View-Controller ).\nThe rendering of a page looks like the diagram below – which only includes two\npagelets for the sake of simplicity: The main worker, in charge of the rendering of the overall page, forwards the request to enable parallel processing in pagelets. In this case, mapbox and photos sections are rendered in parallel. Each pagelet is implemented as an HTTP endpoint that returns a blob of HTML and\ntakes care of data loading, marshalling, and templating. Pagelets can be thought\nof as a mini-webapp, using vanilla HTTP. They have their own URI, just like\nevery service in our infrastructure. This design has a few happy consequences: Full-stack developers, used to working with HTTP, find it easy to debug\npagelets Pagelets are understood by product managers since they are defined by\nhigh-level, natural UI boundaries (sidebar, navigation, map box, review\nhighlights, etc) The design guarantees no coupling between pagelets, which makes their code\neasy to change without worrying about the rest of the page Pagelets are self-contained, and portable: their content can be moved around\neasily The main trade-offs we are making with this design are overhead (due to issuing\nand handling separate requests) and lack of shared context (the self-contained\nnature of pagelets prevents this, which makes things like logging a bit more\nchallenging). Show me the code! Here’s a concrete example using the pagelet generating the map box section of\nour business details page. Controller (we use Pyramid ) @ pagelet_view_config ( route_name = 'biz_details_map_box' , renderer = 'templates/lib/biz_details/map_box#render.tmpl' , ) def map_box ( request ): map_box_presenter = get_map_box_presenter_for_request ( request ) return { 'mapbox' : map_box_presenter , 'tld_country' : request . tld_country , } config . add_route ( 'biz_details_map_box' , '/biz_details/map_box' ) View (we use Cheetah ) #def render(mapbox, tld_country)\n    ## Given a MapBoxPresenter, display its contact and map info\n    <div class=\"mapbox\" data-lightbox-page-title=\"$mapbox.directions_presenter.title\" data-map-library=\"$mapbox.library\">\n        $render_map($mapbox)\n        $render_biz_contact_info($mapbox, tld_country)\n    </div>\n#end def Model (we use SQLAlchemy for data loading) def get_map_box_presenter_for_request ( request ): \"\"\"Builds a map box presenter from a request.\n    More specifically, this returns a map box presenter built from things\n    that can be derived from a request\n    :param request: A request object\n    :return: A MapBoxPresenter for the given request\n    :rtype: MapBoxPresenter\n    \"\"\" # \"logic\" is a reference to our collection of SQLAlchemy models logic = auto_logic_from_request ( request ). logic business_dict = business_dict_from_biz_details_request ( request , logic ) business_id = business_dict [ 'id' ] business_attribute_dict = logic . BusinessAttribute . load_attributes ( business_id ) # [snip, you get the idea -- we load data and format it] return MapBoxPresenter (...) So, a call to /biz_details/map_box returns the markup representing the map box\non the page. Below we show how the main worker loads pagelets. Main controller # Generates a configuration which enumerates all pagelets on the current page pagelet_config = get_pagelet_config ( self . request , site = 'www' , pagelet_namespace = 'biz_details' ) # Creates a loader object capable of loading/rendering pagelets pagelet_loader = create_loader ( self . request , pagelet_config ) # Kick-off the asynchronous request to fetch content for all pagelets in the loader pagelet_loader . load () # do work.... env [ 'rendered_call_to_action' ] = pagelet_loader . render ( '/biz_details/call_to_action' ) env [ 'rendered_map_box' ] = pagelet_loader . render ( '/biz_details/map_box' ) # renders our pagelet from above\n# more renders (for all pagelets) return self . render_template ( 'biz_details' , env = env ) Main template <!DOCTYPE html>\n<html>\n    <!-- [snip] -->\n    <div class=\"biz-page-subheader\">\n        <div class=\"mapbox-container\">\n            $rendered_map_box  # that's it!\n        </div>\n        <div class=\"showcase-container\">\n        <!-- [snip] -->\n</html> Pagelet loading and rendering Pagelet requests are performed through Fido , an\nopen-source HTTP library built on top of crochet/twisted.\nIn practice, the processing of a pagelet-enabled page happens in 3 steps as\nseen from the main pagelet worker: def handle_request ( request ): # 1. Load: as early as possible, instantiate HTTP futures to send requests # out for rendering. future1 = fido . fetch ( '/pagelet/path1' ) # this is non-blocking future2 = fido . fetch ( '/pagelet/path2' ) # etc # 2. Work: fetch and transform data, orchestrate other things while pagelet # workers are busy doing their job. page_data = do_synchronous_work () # 3. Render: read the value of all pagelet HTTP futures and assemble the # final response. pagelet1 = future . wait ( timeout_in_seconds ) # this blocks pagelet2 = future . wait ( timeout_in_seconds ) # pagelet1/pagelet2 contain the fully rendered HTML of a portion of the page! return assemble_response ( page_data , pagelet1 , pagelet2 ) Note that conceptually the same HTTP request is used as input to both the\nmain and pagelet worker. This is important since it makes asynchronous and\nsynchronous rendering modes transparent to pagelet code. To make this play well\nwith routing, we modify a couple of HTTP headers: An asynchronous pagelet request is differentiated with a special header: X-Yelp-Pagelet: true The original request path (say, /biz/foo ) is moved to X-Original-Path before being forwarded downstream. The request path is replaced with the path\ncorresponding to the part of the page which we want to render\ne.g. /pagelet/map_box Once an HTTP request is received, the pagelet worker reconstitutes the original\nrequest object so that the application code is unaware of the context it’s\nrendered in (the pagelet worker when pagelet requests are made asynchronously,\nthe main worker when we fallback to synchronous rendering in the main worker). User requests are forwarded to pagelet workers with a few headers tweaks to enable routing to work correctly. Failure mode: synchronous fallback We made the decision early on to avoid partial failures: we’d rather deliver an\nerror page than a partially rendered, broken page. This decision was made with\nusers in mind both directly (it’s not cool to get a half-rendered page!) and\nindirectly (crawlers should not index incomplete content).\nThis dictated our failure mode strategy: try as hard as possible to complete\nthe rendering, otherwise error out. That’s why we introduced synchronous\nrendering. When a pagelet fails to render asynchronously, the main worker tries\nto render the failed pagelet synchronously (in the same Python process). If\nthis fails still, a user-facing error is returned (500 error page). Results Here are internal performance graphs of our server-side timings for business\ndetails pages:\nOur pagelet system is disabled at the start of these graphs and gets enabled at\n2:25pm (each graph is for one datacenter): Timings when pagelets went from disabled to enabled across our datacenters. The dropoff in server-side timings (which happens a little after the 2pm mark\nmark) is substantial in all the datacenters graphed here, for all percentiles\n(50th, 75th, 99th). Rolling out pagelets over time: improvements & lessons learned Over the past few years, we have gradually rolled out pagelets to a number of\npages. Along the way, we have hit some issues and made a number of\nimprovements. Enhanced configuration In order to improve stability, we’ve made pagelets more configurable. For\nexample: Overall kill-switch to let our SRE team turn off this system altogether as fast\nas possible if it causes operational problems. Tighter configuration around timeouts. More specifically, the ability to tune\ntimeout values per pagelet instead of a global value. Integration with our experiment systems to release pagelets gradually (to 1%\nof traffic, 5%, etc). This has proven very valuable when implementing new\npagelets or rolling out existing pagelets to new pages. Operational problem #1: self-DoS The main and pagelet workers share the same codebase and originally ran on the\nsame host. Essentially, pagelet requests were routed back to localhost. This\nwas done to ensure that the pagelet workers use the same code version than the\nmain worker even when deployments are ongoing (Yelp ships changes several times\na day). It turns out that fanning out to localhost is a good way to overload our own\nsystems. As we rolled out more and more pagelets, a user facing request turned\ninto 3, 4, 5, 10 pagelet requests. This requires each host to have enough free\nworkers at any given time to handle the original request and all subrequests.\nOtherwise it blocks on responses it cannot afford to compute. After some rough\ntimes when our traffic was at its peak, we quickly implemented a capacity check\nto make sure a host has enough free workers to fulfill the pagelet requests. If\nthat’s not the case we fallback to rendering pagelets synchronously. Operational trouble #2: resource utilization We’ve seen from the previous point that capacity planning with pagelets gets\ntricky since it requires accounting for local fanout (e.g. 1 request turning\ninto 10). That means a web server has to constantly reserve spare capacity to\nenable pagelets to render asynchronously. Since most of our requests are not\npagelet-enabled (e.g. mobile APIs responses do not use pagelets, but are served\nfrom the same web servers), we end up needing a lot of extra capacity even for\nrequests that don’t need it. Put bluntly: pagelets make resource utilization\ntank.\nFor this reason we decided to make a change to treat pagelets as just another\nuser-facing request and let these requests be distributed across our fleet of\nservers 1 Resource utilization with pagelets, and how using a load balancer made things better In practice, this seemingly small change led to much better resource utilization in our PaaSTA production cluster. This translated to considerable cost savings because we were able to significantly downsize the size (CPU, memory) of the Docker container used to run our main app: Relative change in cost in our production infrastructure While the improved resource utilization contributed directly to the cost\nsavings, the smaller container size also enabled other infrastructure\nimprovements that reduced cost even more. For example, we were able to optimize our Amazon Spot Fleet usage and auto-scaling strategy to reduce cost further. No such thing as a free lunch Performance optimizations like this one (taking advantage of parallelism)\naren’t free. They do have overhead! For each pagelet, we’re issuing an HTTP\nrequest and receiving a response from across the wire. We’ve measured the\noverhead of rendering a page through asynchronous pagelets compared to inline\n(by rendering pagelets in the same process) and found that rendering pagelets\nasynchronously adds a 20% CPU overhead. That’s expected since issuing HTTP calls,\nparsing responses, and re-entering the request context is extra work that doesn’t\nhave to be done when working within the same Python process. In other words, pagelets trade off CPU capacity for better response time for\nYelp users. Well worth it! 1 : The astute reader will note that this optimization means that we cannot\nguarantee version matching during deploys. To work around this problem we’re\nnow carrying the version information through pagelet requests with a new header\nand dropping requests that don’t match. Our synchronous fallback catches this\nand renders the requests hitting the wrong version in the main worker. Tweet Want to become one of our performance engineers? Yelp engineering is looking for engineers passionate about solving performance problems. Sounds interesting? Apply below! View Job Back to blog", "date": "2017-07-06"}, {"website": "Yelp", "title": "Announcing Docker Hook Support for Pre-Commit", "author": ["\n        \n  benp\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/01/announcing-docker-hook-support-for-pre-commit.html", "abstract": "In 2014, Yelp launched pre-commit , a framework for managing pre-commit hooks. Since then, the community has contributed many different hooks to help with development in a wide variety of languages, including Python, Ruby, Go, Node.js, and Clojure. You can see a list of many of these here . Pre-commit has been invaluable in increasing the productivity of Yelp’s code base and code-reviews by making style issues a thing of the past and allowing us to easily check for broken and deprecated code usage with a variety of code linters. While pre-commit supports a number of popular languages, the goal of pre-commit is to simplify writing pre-commit hooks in any language. Through system-level executable support this has been possible for a while, but this requires pre-commit hook users to manually go grab those programs. In addition to that, there are a number of languages it’s difficult to support multiple concurrent versions for, such as PHP. We wanted to simplify the process so that users can easily run hooks created in any language without having to install new dependencies for each. To accomplish this, pre-commit 0.10.0 adds support for Docker-based hooks. Docker’s ability to package dependencies alongside programs is perfectly suited to the task of supporting hooks across many different languages and even operating systems. While Docker is used by many for distributing long-running web applications, it’s also very capable of running command-line programs. benp@yelp:~/eat24 $ docker run cogniteev/echo echo Raynor here.\nRaynor here. Creating a Docker-based hook is easy. You’ll need a git repo with two files: A Dockerfile, and a .pre-commit-hooks.yaml configuration. 400: Invalid request With that, your hook can be used in a project exactly like any other pre-commit hook. If you’re not using pre-commit in your projects yet, try it out! It’s a great way to enforce consistency before code reaches code review, whether it’s for an open- or closed- source project. Read more about pre-commit at: pre-commit.com . Tweet Back to blog", "date": "2017-01-31"}, {"website": "Yelp", "title": "Open Sourcing Yelp Love", "author": ["\n        \n  Sven S., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/02/open-sourcing-yelp-love.html", "abstract": "Encouraging positivity has been a central pillar to Yelp’s culture from the beginning. If a teammate rolls out a huge feature, helps you out in a bind, or just makes a funny joke, we believe you should be able to show them a small token of appreciation. That’s why we developed an internal tool called Yelp Love, a web application for sending notes of appreciation (aka love) to your coworkers within Yelp. Yelp Love started as a Hackathon effort back in 2013. Over the course of two hackathons, we rolled out Yelp Love to the entire company. Since its deployment, we’ve sent over 280,000 loves to Yelp employees (and counting!), averaging 2,000 loves per week. The Yelp Love App Within the engineering organization, we give out two of the most coveted honors each week at our engineering weekly meeting: the Care Bear and the Unicorn. The Care Bear goes to the person who sent the most loves the week prior, and the Unicorn to the person who received the most loves. The Care Bear and Unicorn stay with that person for the week and are passed on to the next two winners at the following all hands. The Care Bear and The Unicorn Today, we’re excited to open source this tool that’s been so pivotal to building an environment of acknowledgement and positive energy at Yelp. We hope it can add some good mojo to your organization as well. Yelp Love Overview Yelp Love is a Flask application that runs on Google App Engine. For instructions on getting up and running, checkout the readme on GitHub . Out of the box, it includes the following features: Send love to one or more recipients (publicly or privately) Email notifications when users receive love Viewing the most recent 20 loves sent or received by any user Leaderboard with the top 20 users who sent and received love API that allows external applications to send and retrieve love data Manual or automated synchronization between Yelp Love and your employee data Admin section to manage aliases and API keys Internally, development of the Yelp Love tool has been very collaborative. Since its release, developers have written scripts to incorporate it into their commandline, Alfred and Quicksilver workflows, and added support for sending love through our internal IRC bot. The application is easily extendable using the API. Build something cool with it or use it for your organization? Let us know on GitHub ! Tweet Back to blog", "date": "2017-02-14"}, {"website": "Yelp", "title": "Recent Improvements to the Fusion API (and Upcoming v2 Changes)", "author": ["\n        \n  Quy L., Product Mananger\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/02/recent-improvements-to-the-fusion-api.html", "abstract": "Since our launch of the Yelp Fusion API , we’ve been working hard over the past few months,  fixing bugs people have reported on GitHub and adding missing features. We’ve also added a few things to improve the overall experience: The search endpoint got a small update on filtering by attributes. We’ve added support for filtering businesses based on whether or not they have online waitlisting , participate in Yelp Cash Back , or offer Yelp Deals. We added a public changelog to help the community keep track of changes to the API. The changelog will contain updates such as bug fixes, feature enhancements, or other general changes that could impact or improve usage. We updated the assets for Yelp review stars and improved the functionality. The stars on the display requirements page are now pre-built based on the rating and match the style that we use on yelp.com. Disabling Signups for API v2 in the coming months With our continued focus on new and improved functionality in Yelp Fusion, we are going to start ramping down work on public API v2.  Thus, we will disable new v2 API signups on April 1, but current API v2 keys will continue to work. As soon as we’ve established a timeline for shutting down API v2, we will share it with the community and ensure that everyone has sufficient time to transition their applications. We’re excited about the Yelp Fusion API and encourage everyone to migrate from v2 to Fusion whenever possible. We have many new features in the pipeline for the Yelp Fusion API and will  share more details soon! If you have any concerns, please email api@yelp.com. Tweet Back to blog", "date": "2017-02-16"}, {"website": "Yelp", "title": "Open Sourcing Yelp Beans: Bringing People Together, One Cup of Coffee at a Time", "author": ["\n        \n  Kent W., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/02/open-sourcing-yelp-beans.html", "abstract": "We’re excited to be open sourcing a tool we built during a hackathon, that helps Yelp employees meet coworkers within the company: Yelp Beans . A screenshot of the Yelp Beans app It started a little over a year ago, when I began going to orchestrated one-on-one coffee meetups in the Bay Area sponsored by a local startup. It was a program where you could meet designers, engineers, and product managers in your area for a 30 minute coffee meeting. It helped me understanding my career direction and was an opportunity to ask questions of people at varying stages in their own careers. The coffee houses seemed to be filled to the brim with people in these meetings -  it was growing quickly. Eventually, the startup expanded its offerings to companies, so we were able to bring the concept to Yelp, matching people together from different teams to get to know one another over a cup of coffee. The program was a hit! We were getting positive feedback from the meetings and half of our engineering organization had signed up to take part. It was great while the program lasted, but unfortunately we weren’t able to keep working with them. For weeks afterwards, engineers were asking about the program and whether or not we could revive it in-house. There was so much enthusiasm to build something internally that we decided to make it a Hackathon project, and it ended up being pretty popular! One of the great things about our engineering culture is that if we see a need, we will work to solve it quickly. Thus, Yelp Beans was born. The way it works is users will be initially asked to sign up for the program via an email link hosted on the Yelp Beans website. From there, you set your preference on when you would like to meet. It will send you a request each Monday to ask if you want to opt-in for a meeting based on your preferences. On Wednesday, you get matched and on Thursday and/or Friday you will meet that match! We ensure that you don’t meet the same person in the last n weeks and we also ensure that you don’t meet someone on the same team as you. If you decide not to opt-in that week, you can always sign up for the following week. Since launching Yelp Beans internally, we have facilitated hundreds of meetings and we are exploring pilots with satellite offices. We constantly hear feedback about how happy engineers are to have this program and we are continuing to look at different ways in which Yelp Beans can continue to increase engagement at the company. What problem does it solve? Encouraging communication across the organization. How do you meet a person outside of your org? When did we make it? During Hackathon! How is it integrated into our culture? It is part of onboarding and passes via word of mouth. Once you opt-in to the program, you receive participation emails every week. Tech Specs Google AppEngine NDB data layer Python Flask API layer React/Redux Frontend Features Expandable to use with any datasource, provided you adhere to some basic constraints Multiple types of meetings 1:1 weighted matching Matching is optimal based on constraints (Bloom Algorithm) Configurable way to ensure no rematching Tweet Back to blog", "date": "2017-02-21"}, {"website": "Yelp", "title": "Returning to Grace Hopper: Bringing our Workshop Back for Round Two", "author": ["\n        \n  Brittany C., Jen W., Susanne L., Wei W.\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/03/returning-to-grace-hopper-bringing-our-workshop-back-for-round-two.html", "abstract": "Jen W. is a backend engineer at Yelp in San Francisco and, when not running after her toddler, loves nothing more than to knit while watching superhero TV. Susanne L. is a database engineer at Yelp in San Francisco and loves to [b/h]ike the hilly city and its surroundings. Wei W. is an iOS engineer at Yelp in Hamburg and an avid (trashy) TV connoisseur in her free time. This past year at Grace Hopper, Jen, Susanne, and Wei teamed up to present “Crucial Conversations in Your Career: Increasing Your Impact,” a workshop featuring interactive skits and group discussion. This was their second time presenting this topic, having previously presented at GHC 2015. Learn more about their experiences as second-time speakers, coordinating across time zones, and advice for others who want to get into public speaking! How did you choose the “Crucial Conversations” topic for a workshop? Where did this idea come from and how did you know there would be interest in this topic? Wei: Jen, Susanne, and I had all attended GHC together in 2014. Even though we all worked at Yelp, the conference was where we really got to know each other. We were all individually inspired by the talks that we went to, so when we got back, we all had a desire to give back and contribute as speakers in 2015. When I saw that Susanne had decided to brainstorm and develop a GHC talk as a hackathon project, I just glommed on! Susanne: When I came back from GHC 2014, I had this idea of giving a workshop with the topic of “how to successfully approach a booth at a career fair” because I observed that a lot of women there were not representing themselves to their full potential. During one of Yelp’s internal hackathons, I started a project to work on this topic, then Jen and Wei joined me, and the rest is history! We brainstormed around this topic and other themes we could cover. Jen came up with the idea of performing skits as an engaging format for a workshop, so the whole idea moved towards the more general concept of “crucial conversations” in the workplace. The three of you also presented a “Crucial Conversations” workshop at Grace Hopper in 2015. Why did you decide to submit a return proposal last year? Susanne: Because it was so much fun to do the first one! I loved working with my co-presenters. The process was so much fun and so constructive at the same time. Jen: What Susanne said! We also had an overwhelmingly positive response — people approached us at the career fair, in the conference halls, even on the airplane, to tell us how much they enjoyed the talk. In fact, someone came up to us at GHC this year to tell us how much she loved our talk last year! Speaking at GHC was such a positive experience for us, plus there were additional topics we wanted to explore, so we said, “Let’s do it!” What changes did you make from your workshop last year compared to this year, in terms of content, presentation structure, or delivery? Jen: The format stayed the same, but the topics were completely different. Our workshop last year was focused on people starting out in their career — how to effectively ask questions, how to deal with a difficult coworker. This time around, we wanted to focus on mid-career topics such as asking for a promotion and seizing opportunities. The skits were different too — in the first workshop, we drew from our personal experiences (at a previous job, someone literally threw a binder at Susanne!). The situations in the second workshop were much more nuanced and complex — it took a lot longer to develop the skits because it was hard to compress the scenarios into a 2-minute scene. What was your experience like as return speakers to Grace Hopper? Was there anything surprising about your experience running a workshop for the second time? Susanne: During my part of the presentation, the system started malfunctioning and randomly running through our slides very fast. So while I tried to control this behavior on our laptop and present at the same time, I got really nervous. I told the audience that we were having trouble with the display, but I think if this were to happen in the future, I would fully stop and wait until the problems are fixed instead of trying to do everything at once. Lesson learned: no matter how much you prepare there can be always things out of your control. Don’t let these things throw you off. Jen: The room layout was tables instead of rows of seats, so we had to navigate around a bit differently. This also affected the group dynamic — the tables were too big for everyone to hear each other and it took a bit longer for the audience to really get into the workshop. Your team includes engineers across Yelp teams and even across continents (Jen and Susanne work at Yelp HQ in SF, while Wei works in Hamburg, Germany)! How did you prepare for your workshop working across teams and time zones? Wei: We met via video conference bi-weekly for months to prepare our proposal, then eventually weekly as the conference drew closer. It was a bit grueling for me since the meetings were always in the evenings, but the end result was totally worth it! Jen: We deliberately scheduled meetings for Wednesdays when I work from home. That made us all remote, relative to each other, so Susanne and I couldn’t have side conversations excluding Wei and we all had to cope with the connection lag. Susanne: Wei also came to SF one week before GHC so we had in-person facetime during the final week, which was crucial to giving the whole presentation its polished look. Are you planning for a round three of “Crucial Conversations”? What’s next for you as speakers and presenters? Jen: Although my day-to-day life is full of awkward interactions, I think we’ve pretty much covered all of the crucial ones I’ve experienced. I’d love to find more opportunities to present the two workshops we’ve already developed — they’re fun to run and impactful for the participants. What advice and/or resources do you have for people who are interested in public speaking? Susanne: Do as many dry runs as you can in a safe space, such as with your team. Ask them and other people you trust for constructive feedback. Jen: Start small. The first version of our talk was a five minute lightning talk in front of a small group of Women Who Code members. It was a super low key and casual event (helps with the nerves!) and the audience was extremely attentive, which isgreat for feedback. We loved the experience so much, we decided to expand the talk into a full workshop. :) Wei: Practice, practice, practice! I’m a bundle of nerves before public speaking, and I find that the only cure is to practice my talk a lot so that the content comes out almost like muscle memory. Resources & Links Slides: https://yelp.engineering/talks/ghc16-cc Additional Resources: https://yelp.engineering/resources/ghc16-cc Tweet Back to blog", "date": "2017-03-08"}, {"website": "Yelp", "title": "Continuous Integration on Android", "author": ["\n        \n  Coltin Caverhill, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/04/continuous-integration-on-android.html", "abstract": "Continuous Integration (CI) is a powerful tool for the Android team at Yelp. It gives us a platform to ensure quality on all eight of our Android apps, allowing us to emphasize testing and ensure our newest features aren’t breaking our oldest masterpieces. For example, we don’t want Yelfies to crash the app while you’re writing your next great review. To achieve this, we have a dedicated cluster of AWS machines running our CI server. These machines run JUnit tests, static analysis, build APKs, merge code, and send us notifications. They also work with Firebase Test Lab to start up around 10,000 emulators each day to run our UI tests. This post will walk through how you can replicate our setup. You Miss 100% of the CI You Never Build We use Jenkins at Yelp as our CI server. Thanks to the hard work of our Release Engineering and Operations teams, we can use Terraform to describe the machines we want in our cluster by specifying the AWS instance type and number of machines. Terraform will handle starting up the machines, and shutting down old ones. We also use Puppet on top of this, which defines how each machine is configured, making it easy to have a consistent kernel version or describe how SSH will work. Snippet from our Terraform file: module \"jenkins-android-master\" { source = \"git::ssh://git@sysgit.yelpcorp.com/terraform-modules/jenkins.git?ref=ng\" master = \"true\" agent = \"false\" instance_type = \"r4.xlarge\" team = \"core_android\" asg_name = \"android-master\" clustername = \"android\" account = \" ${ var .account } \" region = \" ${ var .region } \" ephemeralsubnets = \"<redacted>\" security_group = \"<redacted>\" azs_list_all = \"<redacted>\" } While you can quickly get set up with Jenkins on AWS without Puppet or Terraform, we find they cut down significantly on our maintenance and allow us to easily scale out machines as our needs change. Regardless of how you configure your machines you’ll need to create jobs that do things. For us these jobs are building Android apps into APKs, running static analysis, tests, and aggregating statistics/information. We originally configured these jobs through the Jenkins web UI. It worked okay just to get us started, but it was very error prone and led to many issues. It was common for someone to make a change and not realize something unrelated broke. Usually the culprit was this person testing their job and seeing that it worked, but not realizing another job that relies on it would break the next time it ran. Whoever ran that parent job would now think they were at fault, and spend time debugging and trying to track down how they could have possibly broken it. This problem compounds the larger your team gets and the more changes that are in flight. The solution for us was the Jenkins Job Builder (JJB). This allowed us to define jobs as YAML or JSON files. More importantly, we could source control, and thus code review our jobs. This meant our changes had a lot more quality behind them, and code reviews encouraged knowledge sharing. At least one other person was now aware of our changes. # Useful comment about what this job does. - job-template : name : ' {app-name}-Build-Dat-APK' concurrent : true node : cpu-intensive builders : - gradle : wrapper : true executable : true switches : - \" --continue\" - \" --profile\" tasks : | clean :{app-name}:assemble{flavor} :{app-name}:assemble{flavor}AndroidTest Now that we can define jobs, let’s do something useful with them. Some very easy to set up jobs are ones that run static analysis on your code. You can configure tools like PMD , FindBugs , Checkstyle , Android Lint, or JUnit tests to run with gradle. Once you can execute these on a local development machine, it’s very easy to get them to run on CI with Jenkins without any special setup. Using the JJB it might look like this: builders : - gradle : wrapper : true executable : true switches : - \" --continue\" - \" --profile\" tasks : | clean :{app-name}:assemble{flavor} :{app-name}:androidPmd :{app-name}:findBugs :{app-name}:lint{flavor} :{app-name}:test{flavor}UnitTest You could of course do this through the web UI if you’re not inclined to set up the JJB just yet. This job will run each of the gradle tasks, and publish artifacts (like the Lint report) making it very easy to figure out what you need to fix when something goes wrong. You can now have this job run before people merge code, giving a lot more quality to new changes. One important thing CI does for us is building android apps (APKs), and we’ve found a really great way of doing that with Docker . Docker allows us to configure an environment that can build Android apps and then drop that environment on arbitrary machines, like our Jenkins AWS slaves. A good starting point is this Dockerfile , which we used as inspiration for our own. Building APKs on CI allows us to verify that the changes we make can still compile and build working apps. It also allows us to run automated UI tests. UI tests on Android require emulators or physical devices. Running emulators on AWS machines, or putting together a device farm, are no easy tasks. Thankfully “Emulators as a Service” are a growing trend, and we’ve had great success with Firebase Test Lab. To use Test Lab you need to install the Google Cloud SDK and sign up for Firebase Test Lab . Once you’ve done those two things it’s very easy to run tests on these emulators. You can find details here but your invocation will look something like this: gcloud beta test android run --app = app.apk --test = test.apk --device-ids Nexus4 --os-version-ids 22 This will start up a Nexus4 emulator running Android Lolipop 5.1 (API 22) and run all of your UI tests against your app. When the tests are done running you can get video, logcat, and xml test results. There is a lot of flexibility with how you run this command allowing you to run only specific tests or test classes. For Yelp, it’s very important that we get results back very quickly, so we spin up hundreds of emulators every time we want to run tests. Each emulator is responsible for a single test class, allowing us to get fast results even if we add a lot more tests. In order to start up emulators on our Jenkins machines, we created another Docker container that knows about the Google Cloud SDK. Luckily, one is already provided by Google , making it trivial to start up emulators. Conclusion Continuous integration is a powerful tool that requires a lot of love to get working and maintain, but if you’re willing to put in the work, you’ll get a lot out of it. Having a tool that automatically builds your commit, runs static analysis and tests on it, and then merges it into your master branch… do you feel that? Yeah, goosebumps. Tweet Back to blog", "date": "2017-04-20"}, {"website": "Yelp", "title": "How Yelp Runs Millions of Tests Every Day", "author": ["\n        \n  Chunky G., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/04/how-yelp-runs-millions-of-tests-every-day.html", "abstract": "Fast feature development is critical to a company’s success. We all strive to\nincrease developer productivity by decreasing the time to test, deploy, and\nmonitor changes. To enable developers to push code safely, we run more than\n20 million tests every day using our in-house distributed system called Seagull. 'Seagull' by Fil.Al is licensed under CC BY 2.0 What is Seagull? Seagull is a fault tolerant and resilient distributed system which we use to\nparallelize our test suite execution. Seagull is built using the following: Apache Mesos (manages the resources of our Seagull cluster) AWS EC2 (provides the instances that make up the Seagull and Jenkins cluster) AWS DynamoDB (stores scheduler metadata) Docker (provides isolation for services required by the tests) Elasticsearch (tracks test run times and cluster usage data) Jenkins (builds code artifacts and runs the Seagull schedulers) Kibana and SignalFx (provide monitoring and alerting) AWS S3 (serves as the source-of-truth for test logs) Challenge Before deploying any new code to production for our monolithic web application, yelp-main , Yelp developers run the entire test suite against a specific version\nof yelp-main. To run the tests, the developer triggers a seagull-run job, which\nschedules the tests on our cluster. There are two important things to consider: Performance : Each seagull-run has nearly 100,000 tests and running them sequentially\ntakes approximately 2 days to finish . Scale : More than 300 seagull-runs are triggered on a typical day with 30-40 simultaneous\nruns in peak hours. The challenge is to execute each seagull-run in minutes rather than days, while\nstill being cost-effective, at our scale. How Does Seagull Work? First, the developer triggers a seagull-run from the console. This starts a\nJenkins job to build code artifact and generate a test list. Tests are then\ngrouped together and passed to a scheduler to execute the tests on the Seagull\ncluster. Finally, test results are stored in Elasticsearch and S3. (1) A developer triggers a seagull-run for a specific version of code\n(based on the git SHAs from their branch). Let’s say the git branch name is test_branch . (2) A code artifact and a test list is generated for test_branch and uploaded to S3. (3) Bin Packer fetches the test list along with test’s historic timing metadata\nto create multiple bundles containing tests. Efficient bundling is a bin packing problem and we use the following two algorithms\nto solve this problem, depending on the parameters passed to Seagull by the developer: Greedy Algorithm : Tests are first sorted based on their\nhistoric test durations. Then we start filling bundles with 10 minutes of work (tests). Linear Programming (LP): In case there is a test dependency,\na test needs to run with another test in a same bundle. For this, we use LP for bundling.\nObjective function and constraints of the LP equation are defined as: Objective Function : Minimize the total number of bundles generated Main Constraints : A single bundle’s work should be less than 10 minutes A test is put in only one bundle Dependent tests are put in same bundle We use Pulp LP solver to solve this equation # Objective function: problem = LpProblem ( 'Minimize bundles' , LpMinimize ) problem += lpSum ([ bundle [ i ] for i in range ( max_bundles )]), 'Objective: Minimize bundles' # One of the constraint, constraint (1), looks like: for i in range ( max_bundles ): sum_of_test_durations = 0 for test in all_tests : sum_of_test_durations += test_bundle [ test , i ] * test_durations [ test ] problem += ( sum_of_test_durations ) <= bundle_max_duration * bundle [ i ], '' Where, bundle and test_bundle are LpVariable , max_bundles and bundle_max_duration are integers. Normally we consider the testcases’ setup teardown durations in our LP constraints\nbut for simplicity’s sake we ignore them here. (4) A scheduler process is started on a Jenkins host which fetches the bundles\nand then starts a mesos framework. We create a new scheduler for each seagull-run . Each run generates more than 300 bundles, grouping each bundle to be around 10\nminutes worth of tests. For each bundle the scheduler creates one mesos executor and schedules it on the Seagull cluster whenever sufficient resources are offered\nby the Mesos master. (5) Once an executor is scheduled on the cluster the following steps occur inside the executor: Each executor starts a sandbox and downloads the build artifact from S3 (uploaded in step 2).\nDocker images corresponding to test service dependencies are then downloaded to\nstart the docker containers(services). When all the containers are up and running,\nthe tests start executing. Finally, test results and metadata are stored in\nElasticsearch (ES) and S3. To write to ES we use our in-house proxy service Apollo . If you are living in a distributed systems world one thing you cannot avoid is\nhost failure. Seagull is fault tolerant towards any instance failure. For example, suppose a scheduler has two bundles to run. Mesos will offer the\nresources of an agent ( A1 ) to the scheduler. Assuming the scheduler deems the\nresources sufficient, the two bundles will be scheduled on A1 . Suppose for\nsome reason A1 goes down, then Mesos will let the scheduler know that A1 is\ngone. The scheduler’s task manager makes a decision to retry those bundles or abort.\nIf the bundles are retried, they will be re-scheduled when Mesos provides the\nnext sufficient resource offer (in this case, from  agent A2 ). In case bundles\nare aborted, scheduler will mark those bundle’s tests as not executed. (6) Seagull UI fetches the results from ES using Apollo and loads it\ninto a UI for developers to see their results. If they all pass, they’re ready to deploy! What scale are we talking about? There are around 300 seagull-runs every day with 30-40 per hour at peak time.\nThey launch more than 2 million Docker containers in a day. To handle this,\nwe need to have around 10,000 CPU cores in our seagull cluster during peak hours. Challenges at this scale To maintain the timeliness of our test suite, especially at peak hours, we need\nto have hundreds of instances always available in Seagull Cluster. For a while\nwe were using AWS ASGs with AWS On-Demand Instances but fulfilling this capacity was very expensive for us. To reduce costs, we started using an internal tool, called FleetMiser, to maintain\nthe Seagull Cluster. FleetMiser is an auto-scaling engine which we built to\nscale a cluster based on different signals such as current cluster utilization,\nnumber of runs in pipeline, etc. It has 2 main components: AWS Spot Fleet : AWS has Spot Instances which can be consumed at much lower prices than On-Demand instances and Spot Fleet\nprovides an easier interface for using Spot Instances. Auto Scaling : Our cluster usage is volatile, with major utilization\nbetween 10:00 to 19:00 PST when developers do most of the work. To automatically\nscale up and down, FleetMiser uses the cluster’s current and historic utilization\ndata with different priorities. Every day the seagull cluster scales up and down\nbetween approximately 1,500 cores to 10,000 cores. Auto Scaling: Cluster capacity for a few weeks ago FleerMiser saved us ~80% in cluster cost. Before FleetMiser, the cluster was\ncompletely on AWS On-Demand Instances with no auto scaling. What have we achieved? Seagull has achieved a test result response time improvement from 2 days to\n30 minutes and has also delivered a large reduction in execution costs. Our\ndevelopers are able to confidently push their code without needing to wait hours\nor days to verify their changes haven’t broken anything. If you are interested in learning more, check out the AWS re:invent talks on Seagull and FleetMiser . Tweet Want to help build such distributed systems tools? Like building this sort of thing? At Yelp we love building systems we can be proud of, and we are proud of Seagull and FleetMiser. Check out the Software Engineer - Distributed Systems positions on our careers page View Job Back to blog", "date": "2017-04-26"}, {"website": "Yelp", "title": "Introducing Yelp's Local Graph", "author": ["\n        \n  Tomer Elmalem, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/05/introducing-yelps-local-graph.html", "abstract": "We’re continuously adding new features to our API to make it easier for developers to integrate with our data and share great local businesses through their apps. Today, we’re releasing access to query our data via GraphQL , a graph query language. This is available immediately through our developer beta program. What is GraphQL? GraphQL is a query language for APIs that places emphasis on being able to query for exactly the data you want. I’m sure most of you at some point have thought, “I really wish this endpoint had more data,” or, “I only need one or two pieces of data from this huge API response,” or, “I wish I could make one API call instead of 2 or 3 to get the data I want.” GraphQL gives you the flexibility to control exactly what data you get back from our API! For example, if you want some data, the query could be: { business ( id : \"garaje-san-francisco\" ) { name\n    url\n    photos } } And we’ll return to you { \"data\" : { \"business\" : { \"name\" : \"Garaje\" , \"url\" : \"https://yelp.com/biz/garaje-san-francisco\" , \"photos\" : [ \"https://s3-media2.fl.yelpcdn.com/bphoto/_nN_DhLXkfwEkwPNxne9hw/o.jpg\" ] } } } If you want more data about a business, then just add those attributes to your query: { business ( id : \"garaje-san-francisco\" ) { name\n    url\n    photos\n    rating\n    review_count\n    categories { title } } } Which will give you { \"data\" : { \"business\" : { \"name\" : \"Garaje\" , \"url\" : \"https://yelp.com/biz/garaje-san-francisco\" , \"photos\" : [ \"https://s3-media2.fl.yelpcdn.com/bphoto/_nN_DhLXkfwEkwPNxne9hw/o.jpg\" ], \"rating\" : 4.5 , \"review_count\" : 1132 , \"categories\" : [ { \"title\" : \"Mexican\" }, ... // trimmed for the blog post ] } } } As the name implies, GraphQL also makes traversing graphs (and therefore relational data) very easy. Unlike most REST APIs, you don’t need to make multiple requests to pull relational data. Based on the schema, you can retrieve data based on the relations they have. For our API users, this means easy access to all the great business information we have available: { business ( id : \"garaje-san-francisco\" ) { reviews { user { name } } } } This sample query would let you pull the name of users who’ve written reviews on a business, something that normally takes multiple calls on a REST API. This is only scratching the surface. Read our intro to GraphQL and our usage guide to dive more into what you could do with GraphQL and our data. We’ve setup GraphiQL , an in-browser IDE for GraphQL, and have put many examples throughout all of the docs. What’s this about a beta program? Today we’re also rolling out a developer beta program for our API. Right now the only component to it will be GraphQL, but we’ll be adding new content and features over the next few months and we want to give our developers the ability to try them out early and give us feedback . You can sign up now through the manage app page . Tweet Back to blog", "date": "2017-05-03"}, {"website": "Yelp", "title": "Embedded Reviews at Yelp", "author": ["\n        \n  David W., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/10/embedded-reviews-at-yelp.html", "abstract": "Yelp is known for useful, funny, and cool reviews of all kinds of businesses, so it’s no surprise that they often get shared in other places. While we love seeing screenshots of these reviews shared on other websites, they don’t make for a great user experience. Their image formats make it easy to create and share but they become outdated, load slower than text, and make it hard for users to find out more about the business. We wanted to build something that retains screenshots’ ease of sharing while solving some their biggest shortcomings, so we created embedded reviews. Now, content creators and business owners can directly embed Yelp reviews on their websites. Embedded reviews are always up-to-date, load as HTML, and link back to the business. Read Krsna V. ’s review of Badlands National Park on Yelp What makes a good embed widget? The HTML snippet is what content creators and business owners copy and paste onto their websites, so it should have the minimal amount of HTML code necessary to achieve an embedded review. It should also be consistent so that it doesn’t need to be changed every time the review widget is updated with new contextual information. The HTML snippet renders and displays fully styled embedded review widgets which should be responsive , so that it looks good on any website or any device. The content should also stay up-to-date with contextual information like the latest rating of the business and the reviewer’s most recent edit of their review. To better understand how embedding is being used, and to inform future improvements, we should also record accurate analytics on the widgets to see where reviews are being embedded and which reviews are being embedded. Performance is a major consideration for the embed widget. Embedded reviews need to load fast alongside the content on the embedding page. Embedded reviews also need to gracefully handle traffic spikes from popular websites. Minimizing HTML and Maximizing Functionality With our embedded content, we took the approach of minimizing the HTML snippets that content creators and business owners embed on the web page, and dynamically loading in a fully styled widget when users view the page the widget is embedded on. This lets us keep the content up to date and roll out changes to our embed widget without requiring updates to the HTML snippets. The embed HTML we provide uses a script tag that loads JavaScript called widgets.js. All of the functionality of styling and dynamically loading in a full embedded review resides in widgets.js. Here’s what our HTML snippet looks like: This is what users see before widgets.js runs: Read Jr P. ’s review of Kelly’s Cove on Yelp Good candidates for the HTML snippet are static, unstyled, or empty elements such as the script tag. For our HTML snippet, we included some static links in case the JavaScript couldn’t load. All of the functionality of loading in the fully styled embedded content is contained in widgets.js. Widgets.js consists of pure JavaScript with no dependencies on libraries so it can run on any website. Dynamically Creating the Fully Styled Embedded Review Once the page loads, widgets.js executes. Widgets.js does a bit of magic to create a fully styled embedded review. We decided to have widgets.js create the fully styled embedded review widgets within iframes, rather than directly in the containing page. Review widgets are served as Yelp pages within iframes, adhere to the Yelp Styleguide , and show the most up-to-date review. iframe embeds are preferable to direct embeds in these ways: iframe embeds allow for a simpler widgets.js . widgets.js only needs to construct the iframe and it defers rendering and styling the review widget to the iframe. In order to display the same review widget with direct embeds, widgets.js needs to retrieve review data from Yelp, construct the markup for the content, and style the markup. iframe embeds make development easier . The review widget can be developed and tested as its own Yelp page instead of as part of widgets.js, which also contains the embedding functionality. Yelp pages also come with Yelp Styleguide resources built in. Direct embeds would have to duplicate these resources inside widgets.js. iframe embeds can take advantage of HTML caching . When served as a Yelp page inside an iframe, review widgets are static and therefore the entire page can be cached. Directly embedding the review widget requires JavaScript code to generate and insert the markup, losing caching ability. We’ll go over this in a later section. The review widget is built so that it displays nicely inside the iframe. Initially, the iframe has 0 height and width. After the review widget is loaded, the iframe height and width changes to fully display the review widget. This requires the review widget hosted on Yelp to pass sizing information to widgets.js running on the embedding page. Communication between scripts hosted on different domains are restricted due to the same-origin policy . window.postMessage() enables us to safely do cross-origin communication. Read Jr P. ’s review of Kelly’s Cove on Yelp Accurate Analytics in Embedded iframes So far, our responsive embedded review widget consists of the widgets.js controller, iframes for review widgets, and postMessage to communicate between the two. We realized that we could overcount page impressions for embedded reviews if a user embedded multiple reviews on the same site. To handle this we architected our analytics to serve Google Analytics (GA) on a Yelp domain in a separate, standalone iframe on the embedding page. We call this the GA iframe. We create one instance of the GA iframe per embedding page to handle sending events: one ‘pageview’ event when the user loads the embedding page and a separate ‘load’ event for every review widget iframe on the embedding page. These events are all sent through the single GA iframe. Review widget iframes and the GA iframe communicate with postMessage with widgets.js as the hub. Once a review widget loads inside the iframe, it notifies widgets.js via postMessage. widgets.js then resizes the review widget’s iframe and sends a GA event. Putting it all together, this is what the architecture looks like: Speedy and Scalable Now that we have all the functionality of a good embedded review widget, we need to look at performance. Embedded reviews were designed with speed and scalability in mind. We wanted our embedded review widgets to load fast to be as seamless as possible with the web page content. We also wanted our embedded reviews to be scalable to handle traffic spikes so that viral web pages loading multiple embedded reviews wouldn’t overload our servers. We identified two parts of the embed flow that could be optimized to be speedy and scalable: widgets.js and the iframes served by Yelp. We wanted to ensure that we could update widgets.js in the future without content creators and business owners having to push new embed snippets. If we had hosted widgets.js on a static CDN site, we could not have fulfilled this requirement. Instead, we serve it through our web app, much like how a regular Yelp page is served, allowing us to serve different versions of widgets.js. The problem is that a centralized web app serving JavaScript represents a single point of failure and slower load times for users far away from the web app. We use our web app to redirect to a static JavaScript file hosted on a CDN to serve widgets.js faster to users around the world. Since the review widget iframe and GA iframes are static pages served by Yelp, we use full page HTML caching. This way, most of the user traffic sees the cached page and our servers won’t even be contacted from traffic spikes from particularly viral web pages. In summary, here is how we designed a good embed widget: The embed HTML snippet consists of unstyled and empty elements so that the HTML snippet is minimal and durable. We use a controller loaded via script tag to create and load iframes.  The controller consists of pure JavaScript and doesn’t use any libraries. The controller communicates with the iframes using postMessage. We use a Google Analytics iframe served by Yelp to handle sending events for multiple review widget iframes on a single page. Resources served by Yelp, such as the controller and the iframes are either cached or served via CDN. Check out Embedded Reviews on Yelp Business Pages! Tweet Back to blog", "date": "2016-10-04"}, {"website": "Yelp", "title": "Comparing searches at the DNC and RNC", "author": ["\n        \n  David K., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/11/Comparing-searches-at-the-DNC-and-RNC.html", "abstract": "Ah - politics. It’s frustrating, it’s funny, it’s serious business. It is perhaps one of the best windows we have into the nature of the human condition. So we think it’s fitting that we combine politics with another window into human nature - search data. In particular, Yelp’s PR team was interested in looking at how the Republican and Democratic National Conventions affected what people looked for on Yelp . Could we confirm certain political stereotypes? Would there be surprises? Methodology From a data analysis perspective, we first need to precisely state the question before we can get to tackling it. We want to look at how the DNC and RNC affected people’s search behavior - but what specifically does that mean in terms of the data? Well, we should obviously look at the searches made at the relevant time and place: during each convention, in the area surrounding the corresponding cities (Cleveland for RNC, Philadelphia for DNC). Now, Yelp stores a lot of information on these searches, including which business category (restaurants, bars, home services, etc.) the search was for. Looking at these business categories for these searches seems like a natural thing to do. But can we merely count the number of searches made in a category? Can we just say, “There were 120 searches for ‘Guns and Ammo’ during the DNC, and 116 during the RNC”? # of searches during DNC # of searches during RNC guns_and_ammo 120 116 Of course not. The conventions were held in two different cities, with different populations and levels of Yelp usage. One convention may simply have had more searches due to these factors. Okay, then - can we look at what percent of all searches were made in a given category? Can we just state, “0.955% of the searches during the DNC were for ‘Cheese Steaks’, and it was 0.083% during the RNC”? # of searches during DNC # of searches during RNC % of all searches during DNC % of all searches during RNC guns_and_ammo 120 116 0.0436% 0.0690% cheesesteaks 2626 140 0.9548% 0.0832% Well, no - the two cities have different cultures and historical searches, and we need to take that into account. The DNC had a much greater search percentage for ‘Cheese Steaks’, but that was only because it was held in Philadelphia. If we stopped our comparison here, we’d be mostly characterizing the cities, and not the conventions. How about comparing each city with itself to mitigate those effects? We can compare the searches during the convention with the searches from the week before, and look at the changes. We would then get statements like “Searches for ‘Comfort Food’ increased during the RNC, from making up 0.091% of all searches to 0.112%”. Now that’s getting somewhere. That is an increase that we can actually attribute, at least in part, to the RNC. % of all searches before RNC % of all searches during RNC change in % of searches, RNC guns_and_ammo 0.0400% 0.0690% 0.0290% cheesesteaks 0.0788% 0.0832% 0.0045% comfortfood 0.0912% 0.1118% 0.0207% But we’re still not quite there yet. You can see this with something like ‘Churches’. During the RNC, searches for ‘Churches’ increased from making up 0.117% of all searches to 0.121% - this means that Republicans like churches, right? % of all searches before RNC % of all searches during RNC change in % of searches, RNC churches 0.1172% 0.1213% 0.0041% Well, maybe. Or this may just be the effect of a bunch of out-of-towners coming into the city over a weekend, which is not actually particular to the RNC. In fact, the DNC saw a similar effect, except the difference was even larger: ‘Churches’ increased from 0.085% of all searches to 0.100% during the DNC. Similar effects, where both conventions increase the fractional searches for a category, can also be seen for ‘Gay Bars’ and ‘Pizza’. change in % of searches, RNC change in % of searches, DNC churches 0.0041% 0.0148% gaybars 0.0435% 0.0210% pizza 0.5622% 0.1840% To improve the comparison further, we need to then look at the differences in these changes. We look at the increase in the percentage of searches in a given category, and take the difference of these values across the conventions. So at the DNC, ‘Vegan’ food searches increased from 0.39% of all searches to 0.53%. But can we attribute that increase specifically to the DNC, and not to just an influx of out-of-towners or the effect of the searches being made later in the summer? % of all searches before DNC % of all searches during DNC change in % of searches, DNC vegan 0.3925% 0.5301% 0.1376% We have good reason to believe we can - because in the RNC, ‘Vegan’ searches did not really move at all. In fact it decreased slightly compared to the week before, as a percentage of all searches. This gives us some confidence when we say that the increase in the ‘Vegan’ category is specifically attributable to the DNC. % of all searches before DNC % of all searches during DNC change in % of searches, DNC change in % of searches, RNC vegan 0.3925% 0.5301% 0.1376% -0.0086% So, after considering all these things, we can now make a precise statement of the question. For each category, we want to find the percentage of all searches that are taken up by that category. We then calculate the change in this value compared to the week before the convention. Lastly, we take the difference in these changes, between the DNC to the RNC. The categories that have the most extreme values of differences are the ones that can be said to be most disproportionately affected by the national conventions. change in % of searches, DNC change in % of searches, RNC difference in changes in %, DNC-RNC pizza 0.1840% 0.5622% -0.3782% guns_and_ammo -0.0061% 0.0290% -0.0350% gaybars 0.0210% 0.0435% -0.0225% comfortfood 0.0002% 0.0207% -0.0205% churches 0.0148% 0.0041% 0.0107% cheesesteaks 0.0846% 0.0045% 0.0802% vegan 0.1376% -0.0086% 0.1462% But this gives us a final value that’s difficult to explain and interpret. So, when we report the results, we just give the percentage increase in the number of searches in these heavily affected categories, for each convention. This methodology can certainly be refined a great deal, but it’s enough to give us a decent first look. Getting to the final story Obviously, we can do more to improve our list. We can put error bars on the final values, and look for statistical significance in these differences. We can consider only searches made by non-local users. We can consider a different and more comprehensive “baseline” search set, instead of just looking at one week prior. We can use different cutoff points for throwing out small categories, or use different metrics to measure the changes. But before we dive into more sophisticated data analysis techniques, we should consider the original goal: we wanted a story we can tell about the national conventions, using Yelp data. So then, what is the easiest thing we can do to make an interesting story about the DNC and RNC? We already have a list, which we believe reflects the effects of the national conventions, at least in part. Additional data analysis techniques will slightly rearrange the order of the categories. This will make the final list more “accurate”, but that may not be necessary - it’s not like we’re trying to derive each party’s platforms from Yelp’s data. And for that purpose, the low-hanging fruit is to make the story more interesting by focusing on interesting categories. Consider ‘Adult Entertainment’, for example. Seeing how the conventions impacted that category is far more interesting than determining exactly where the ‘Restaurants’ category falls on the overall list. In fact, the ‘Adult Entertainment’ category would be more interesting even if it turned out not to have been influenced by the conventions at all. So, as the last part of this project, we curated the above list for interesting categories which also showed strong signs of being impacted by the national conventions. The results are presented on the Yelp blog . For you statistics geeks out there, we also did a Bayesian analysis on whether the categories we presented really leaned the way we said. It turned out that for over 90% of the categories, we are at least 90% sure, and very frequently more than 99% sure, of the direction of the relative difference in searches due to the national conventions. Even for the least certain category, we’re more than 75% sure. So our conclusions are quite defensible statistically, even though “interesting” was our main goal. The final product is a unique mix - yes, it does confirm some political stereotypes. But there are also surprises that go against these stereotypes, and still other elements that don’t fit neatly into a typical political narrative yet humanize the people gathered at these conventions. It’s a small, slightly unexpected glimpse into human nature, that serves as both window and mirror. Future work Our methodology is incredibly flexible. It can be used to generate many other comparisons similar to the DNC/RNC story. Do you want to know how California is different from Texas? Simply compare the searches from the two states, in the same way we compared the searches from a convention city to the week before. Do you want to know how California and Texas celebrate the 4th of July differently? Compare California searches during the 4th of July to the week before, and do the same for Texas. Then compare them against each other. Other questions of this kind include: How are red states and blue states different from one another? What are the most distinctive categories in each city in relation to the rest of the country? How do the people in Japan and Australia celebrate Christmas differently? How did the searches in Rio de Janeiro change from 2015 to 2016, as it prepared for the Olympics? What do people search for in the evenings vs. during the day? How does the day-to-evening search changes compare in New York vs. in Hawaii? If we’re willing to expand the methodology just a bit, we can include things like user information and slicing around things other than business categories. Then we can address questions like: Which categories are more popular among the Yelp Elites vs. regular users? What time of the day are Elites more active compared to regular users? Which state has the most, or least, gender disparity in what each gender searches for? As you can see, there’s a lot we can do with this. You can expect more upcoming stories - in fact, the red state/blue state analysis is now up on the Yelp blog ! Tweet Back to blog", "date": "2016-11-04"}, {"website": "Yelp", "title": "Streaming Messages from Kafka into Redshift in near Real-Time", "author": ["\n        \n  Shahid C., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/10/redshift-connector.html", "abstract": "This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 The Yelp Data Pipeline gives developers a suite of tools to easily move data around the company. We have outlined three main components of the core Data Pipeline infrastructure so far. First, the MySQLStreamer replicates MySQL statements and publishes them into a stream of schema-backed Kafka topics. Second, the Schematizer provides a centralized source of truth about each of our Kafka topics. It persists the Avro schema used to encode the data in a particular topic, the owners of this data, and documentation about various fields. Finally, our stream processor, PaaStorm , makes it simple to consume from the Data Pipeline, perform transformations, and publish back into the Pipeline. Together, these tools can be used to get data to the people who care about it most. Let’s focus on the “Targets” box in our diagram of the Data Pipeline. We can see that transformed data has to make its way into target data stores before services can use it. After abstracting much of the data transfer and transformation into the Data Pipeline infrastructure, we still have to connect data inside Kafka topics to the final data targets that services use. Each data target has its own idiosyncrasies and therefore requires a separate connector. One of the most popular tools for doing analytics processing at Yelp is Redshift . It makes computation and aggregation easy with its column-based storage, and also scales well for performing complicated joins across tables with billions of rows. This makes Redshift a great data warehouse that analysts, data scientists, and engineers can use to interactively get answers to their complex data questions. This post will focus on a Redshift connector: a service that uses PaaStorm to read from Kafka and load data into a Redshift cluster. The Data Warehouse is dead. Long live the Data Warehouse. Our legacy ETL system served us for many years in moving data from our production application databases into our data warehouse. The diagram below outlines the high-level infrastructure of this system. We had triggers on various tables in our MySQL databases listening for row changes. On each row change, we would write to a table changelog. A worker tailing this changelog would then create ETL tasks and put them on our job queue. Next, a pool of workers would perform transformations on each row according to custom logic written for each type of ETL. The output of these transformations would be written to MySQL, where they would sit temporarily before being dumped to S3 and finally COPY ‘d into Redshift. This system served us well for getting data from our MySQL databases into Redshift and, despite the complexity, it was made robust through various alerts and tooling we had built over the years. However, this system suffered from major drawbacks which made it hard to scale. One of the most pressing concerns was that it would take precious developer time to write custom ETLs, their associated unit tests, get through code review, and push it into our monolithic code base. Furthermore, these code changes would often require performing schema migrations in both MySQL and Redshift across development and production clusters every time a field we cared about in the underlying source table changed. Worst of all was that this system was designed to support a single Redshift cluster (i.e. the data warehouse) which meant that each team that managed a Redshift cluster would have to build their own systems for ingesting the data they cared about. We had reached a tipping point and desperately needed a new system that would better scale with the needs of our growing company. The Redshift Connector: A New Hope To address our issues with our legacy Redshift ingestion systems, the new system needed the following features: Ingestion of new tables without writing custom ETL Automated schema migrations Low latency ingestion Graceful recovery from failures Idempotent writes Support for multiple Redshift clusters From experience with our legacy ETL system, we knew that Redshift was great for performing bulk COPYs from S3: doing row-by-row inserts would not scale for performing millions of upserts per day. Therefore, we needed the new system to accomplish two main tasks: one to write data into S3, and another to take the data from S3 and upsert it into Redshift. Fortunately, PaaStorm already provided the necessary abstraction, the Spolt, to perform these two tasks. A Spolt reads in a message from a specified Kafka topic, processes that message in some way, and optionally publishes a new message downstream. An S3 Spolt could read in messages from an upstream Kafka topic, batch them together into small chunks, and write them to S3. After each write to S3, the S3 Spolt could then publish a downstream message to Kafka about the data that written to S3. Thus, the downstream topic would serve as state, recording which messages had been written to S3 successfully. Next, a Redshift Spolt could read the Kafka messages published by the S3 Spolt and use that to figure out how to write the S3 data into Redshift. This way, the system that moves data into S3 and the system that moves data into Redshift could operate independently, using Kafka as the common protocol for communication. The S3 Spolt An S3 Spolt is responsible for reading data from an upstream Kafka topic in the Data Pipeline and writing that data into files on S3. The diagram below outlines the S3 Spolt at a high level. The upstream Kafka topics might contain raw row events coming from a table in MySQL, raw messages written to the Data Pipeline by a developer’s batch, or any extension of these raw data sources via a series of transformations performed by intermediate Spolts. Writing to S3 in batches Redshift is great for loading multiple files from S3 into Redshift in parallel. To take advantage of this, the S3 Spolt batches messages together and writes them into a key on S3 as a CSV file. The key is determined by the topic, partition, and starting offset of the messages in the batch. The S3 Spolt publishes a new batch to S3 whenever either the message limit or a time limit (or a new schema_id in the upstream messages) is reached. Both the number of messages to batch and the time limit for each batch are tunable parameters. The S3 Spolt performs some transformations on the data it writes to S3. It annotates each message with an additional field, the message type, to describe what sort of data event this message represents (create, update, delete, or refresh). It also constructs an additional field for each message containing its Kafka offset from the upstream topic. Each batch of messages is then flattened into a CSV file using a carefully constructed csv.Dialect object – which enables easy parsing of our input CSVs while offering great robustness around parsing diverse formats. This data is then written to the appropriate key on S3. Signaling to the Redshift Spolt Once the S3 Spolt has written a batch of messages to a key on S3, it needs a way to signal this event to the Redshift Spolt. The S3 Spolt constructs a Kafka message containing the following information about data written to S3: name of the upstream topic start and end offsets of the messages in the upstream topic dialect of CSV used to write data path to the data on S3 schema id corresponding to the data written to S3 This message is published to a topic from which the Redshift Spolt will eventually read. Checkpointing and recovery Because we can experience occasional outages, or network flakes, the S3 Spolt needs to be resilient against restarts. We don’t want the S3 Spolt to have to start from the earliest offset for a topic every time it restarts. We also can’t afford to lose any data, so we need to know for sure where we left off. The S3 Spolt solves this problem in an elegant way. Upon restart, the S3 Spolt starts a lightweight consumer and reads the tail message for the downstream topic for that spolt. The tail message contains information about the upstream topic and end offset, which the S3 Spolt uses to restore its position in a Kafka topic if necessary. Furthermore, the S3 file writes are idempotent, so if the Kafka write fails, the S3 Spolt will replace the file upon restart. This provides a safeguard against duplicating work and dropping messages. The Redshift Spolt A Redshift Spolt reads in messages from topics coming out of S3 Spolts, determines where the data lives on S3, and upserts this data into a Redshift cluster. Each Redshift cluster has a dedicated Redshift spolt. The diagram below outlines the Redshift Spolt at a high level. Creating S3 manifest files in batches We take advantage of Redshift’s ability to COPY in multiple files from S3 by using a batching strategy similar to the one used by the S3 Spolt. Each Kafka message that the Redshift Spolt reads in represents a batched S3 file–in turn, we can batch up some number of those messages and COPY them all via an S3 manifest. For example, 1,000 messages in Kafka, representing 10,000 rows each on S3, gives us 10,000,000 rows at a time to be upserted with a COPY command. As long as we are batching by the schema id, all the rows across all the files in the batch are in the exact same format. We buffer messages by their schema id and flush them once we hit the desired number of messages, or if we hit the timeout for accumulating messages in a buffer. When we flush a buffer, we first extract the S3 paths in each of the messages, and write them into a unique manifest file on S3. Because S3 lacks list after write consistency, this manifest file is critical in ensuring strong consistency in fetching the files we need to COPY into Redshift. The Redshift Spolt then passes along the location of this manifest file to our Redshift upserter. Performing schema migrations The Schematizer provides a set of utilities for converting Avro schemas to Redshift table CREATE statements. Given a new set of files to COPY into Redshift, the Spolt has to check for three cases: if the target table doesn’t exist, if it does exist but has a different schema, and finally if it both exists and has the proper schema already. When the Redshift spolt starts upserting data, it checks a state table that maps each table to its schema id to see if we have already created that target table in Redshift. If not, we’ll use the schema id from the task to ask the Schematizer for a CREATE TABLE statement. If that target table already exists, but the schema id we used to create it differs from the one in the current upsert task, this indicates the schema has changed, so we generate a sequence of statements to migrate our data from the old table into the new desired table. Finally, if the schema id in the state table matches the desired schema id in our current upsert task, we do nothing–the target table already has the schema we want. Staging and merging data into the target table Now that our target table exists and has the desired schema, we need a way to upsert rows into it. We create a temporary staging table using the schema id for the current task. Then, we use the S3 manifest file to tell redshift to bulk COPY files into this temporary table. We now have a table containing (possibly) millions of rows, which we need to merge into our target table. Each row in the staging table represents a row event, that is, a create , update , delete , or refresh event in the upstream source. We can’t naively upsert each row into the target table, because we have to deal with special events like updates and deletes. Instead, we perform a merging strategy, similar to the one recommended in the Redshift documentation with some slight modifications. This has the added benefit of enforcing unique key constraints, which is not natively supported by Redshift. First, we only keep the highest offset row per primary key. This way, if a series of changes on the same upstream row occurred, we will only retain the most recent representation of that row. For the remaining rows in our staging table, we delete all the corresponding rows in the target table with the same primary key. Intuitively, we’re removing the rows from the target table that we’re about to replace with new content. Note that this handles delete messages as well, because those rows are simply removed with nothing to replace them. Next, we delete all rows in the staging table which correspond to delete events, since they’ve just been removed from the target table. All the remaining rows represent data that does not exist in the target table already, so we simply insert them all into the target table and drop the staging table. Now our data has been fully merged from the staging table into the target table. Checkpointing and recovery Each upsert task can take minutes and often represents millions of underlying row changes. Like the S3 Spolt, the Redshift Spolt needs to be robust against restarts due to events like network flakiness or losing connection to Redshift so that we don’t have to duplicate any work. We use a state table in Redshift that maps every topic and partition to an offset. When the Redshift Spolt completes an upsert task, it updates the state table in Redshift with the largest offset in its batch of messages for the corresponding topic and partition. This entire upsert and checkpointing process happens within a transaction so that each task is all-or-nothing and it never leaves Redshift in a bad state. Any time the Spolt restarts, it will first check this state table and restore its position for its Kafka topics to the last known checkpoint. Looking Forward Yelp’s Data Pipeline has dramatically changed the way we think about data. We are shifting away from moving data via batches and scheduled jobs to a future where we connect streams like building blocks in order to build more real-time systems. The abstractions the Data Pipeline infrastructure provides have proved to be useful in building connectors to various data targets such as Redshift and Salesforce. The potential unlocked so far by the data pipeline is encouraging and we are excited to see how we can further leverage these tools to make data at Yelp even more easily accessible in the future. Acknowledgements Special thanks to the co-author of the Redshift Connector, Matt K. and to Chia-Chi L. and Justin C. for providing feedback and direction throughout the entire project. Many thanks also to the entire Business Analytics and Metrics team whose work on the entire Data Pipeline infrastructure has made this project possible. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-10-17"}, {"website": "Yelp", "title": "Open-Sourcing Yelp's Data Pipeline", "author": ["\n        \n  Matt K., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/11/open-sourcing-yelps-data-pipeline.html", "abstract": "For the past few months we’ve been spreading the word about our shiny new Data Pipeline :  a Python-based tool that streams and transforms real-time data to services that need it. We wrote a series of blog posts covering how we replicate messages from our MySQL tables , how we track schemas and compute schema migrations , and finally how we connect our data to different types of data targets like Redshift and Salesforce . With all of this talk about the Data Pipeline, you might think that we here at Yelp are like a kid with a new toy, wanting to keep it all to ourselves. But unlike most kids with new toys, we like to share–which is why we’ve decided to open-source the main components of our pipeline, just in time for the holiday season. Without further ado, here are some stocking-stuffers for the upcoming holidays: The MySQL Streamer tails MySQL binlogs for table changes. The Streamer is responsible for capturing individual database changes, enveloping those changes into Kafka messages (with an updated schema if necessary), and publishing to Kafka topics. The Schematizer service tracks the schemas associated with each message. When a new schema is encountered, the Schematizer handles registration and generates migration plans for downstream tables. The Data Pipeline clientlib provides an easy-to-use interface for producing and consuming Kafka messages. With the clientlib, you don’t have to worry about Kafka topic names, encryption, or partitioning your consumer. You can think at the level of tables and data sources, and forget the rest. The Data Pipeline Avro utility package provides a Pythonic interface for reading and writing Avro schemas. It also provides an enum class for metadata like primary key info that we’ve found useful to include in our schemas. The Yelp Kafka library extends the kafka-python package to provide features such as a multiprocessing consumer group. This helps us to interact with Kafka in a high-performance way. The library also allows our users to discover information about the multi-regional Kafka deployment at Yelp. A diagram of different components in the Data Pipeline. Individual services are shown with square edges, while shared packages have rounded edges. Each of these projects is a Dockerized service that can easily be adapted to fit your infrastructure. We hope that they’ll prove useful to anyone building a real-time streaming application with Python. Happy hacking, and may your holidays be filled with real-time data! This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-11-17"}, {"website": "Yelp", "title": "Finding Beautiful Yelp Photos Using Deep Learning", "author": ["\n        \n  Alex M., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/11/finding-beautiful-yelp-photos-using-deep-learning.html", "abstract": "Yelp users upload around 100,000 photos a day to a collection of tens of millions, and that rate continues to grow. In fact, we’re seeing a growth rate for photos that is outpacing the rate of reviews. These photos provide a rich tapestry of information about the content and quality of local businesses. One important aspect of photos is the type of content being displayed. In August of 2015 we introduced a system that classified restaurant photos as food, drink, outside, inside, or menu. Since then, we have trained and put into production similar systems for coffee shops and bars, thus helping users more quickly find the photos they are looking for. More recently, we began to investigate how to make users happier by showing them more beautiful photos and improving our photo ranking system. Understanding Photo Qualities Comparing the quality of photos can seem like a very subjective task. What makes one photo preferable to another can depend on many factors, and may be different depending on the user who is performing a search. In order to provide a great experience for Yelp users, the Photo Understanding team had the challenging task of  determining what qualities make photos appealing, and developing an algorithm that can reliably assess photos using these characteristics. We first attempted to build a click rate predictor for photos using click data mined from our logs. Our hypothesis here was that those photos which were clicked a greater proportion of times would be noticeably better. However, this idea did not work that well, for a couple of reasons. For one, people will often click on blurry or text-heavy photos in order to enlarge them and understand what the image shows. In addition, because of the many ways photos are displayed on Yelp it was difficult to meaningfully compare metrics for particular photos. Following this, we turned to various computer vision techniques, trying to discover intrinsic features of a given image that could be associated with a quality score. For example, one important feature for photographers is depth of field, which measures how much of the image is in focus. Using a “shallow” depth of field can be an excellent way to distinguish the subject of an image from its background, and photos uploaded to Yelp are no exception. In many cases, the most beautiful images of a given restaurant were very sharply focused on a specific entrée. Depth of Field Alexander's Steakhouse Art of The Table Another important feature of how people perceive photos is contrast. Contrast measures the difference in brightness and color between an object in an image and other nearby objects. There are several formulas for contrast, but most involve comparing the luminance, or light intensity of neighboring regions of an image. Contrast Anthony's Cookies Tac N Roll Finally, the location of objects in an image with respect to one another can be a significant aesthetic consideration. Studies have shown, for example, that people have an innate predisposition towards symmetry in art. In addition, some photographers also promote what is called the “rule of thirds,” a method of aligning important elements of an image along certain axes to create a sense of motion or energy. Alignment Traif Augie Chang Photography Using Deep Learning to Build A Photo Scoring Model All of these considerations relied on understanding the relationship between regions of photos. So when it came time to implement a photo scoring algorithm, we wanted a method where this relationship was paramount. As a result, we were very keen on using a model called convolutional neural networks , or CNNs . Over the past decade, CNNs have been hugely successful in image classification and processing tasks, such as facial recognition and molecular disease detection. Similar to ordinary neural networks, they apply a series of transformations to an input vector and use the output errors to dynamically improve future predictions. However, CNNs have a few additional layers that help to take advantage of the spatial features discussed above. Specifically, convolutional layers tile a set of filters across an image, and pooling layers downscale the output of previous layers in order to reduce computation. To develop this model, we first needed to collect training data. One method of doing so would have been to manually label hundreds of thousands of photos as beautiful or not. However, this approach would have been costly, time-consuming, and highly dependent on the preferences of our graders. Instead, we were able to take advantage of the fact that when photos are uploaded to Yelp, they often contain additional information known as EXIF data . In particular, we found that a good proxy for quality is whether a photo was taken by a digital single-lens reflex camera, or DSLR . These cameras give the photographer more control over which parts of the image are in focus, by adjusting the lens type and aperture size. Further, DSLR sensors are larger and more sensitive to light, allowing great photos to be taken in even very dim situations. Finally, people who regularly use DSLR cameras may have more experience and skill in capturing higher quality images. Training our model on such photos allows it to learn important photo features and recognize great photos even when they are not taken by a DSLR camera. Even though this photo was taken by an iPhone, our model gives it a very high score. We tried several methods of training this model. Initially, we collected 100,000 DSLR and non-DSLR images to use as positive and negative labels, respectively, and fed these into a model known as AlexNet , which was created by researchers at the University of Toronto in 2012. To improve the accuracy of this model, we trained an additional model with more than ten times the previous amount of training data. Finally, we tested a model called GoogLeNet , which was developed by researchers at Google in 2014 and achieved state of the art performance by having significantly deeper layers than previous top-of-the-line models. In each of these cases, we further evaluated the model against a dataset of thousands of images manually evaluated by Yelp engineers, which consisted of only those images which we could confidently say were very good or very bad. We saw that with each iteration, our ability to correctly identify good and bad photos improved. Finally, to convert the results of our model to quality scores, we used the probability output by the final layer of the model that a given photo would generate a positive label. In other words, if our model predicts an 80% chance that the label should be “high quality,” we give that photo a score of 0.8. This gives us a straightforward way of converting the results of a binary classifier into easily ranked results. The Bigger Picture Our preliminary analysis showed that the photos algorithm promoted were indeed more focused, bright, and aesthetically pleasing. However, there were cases that prompted us to find ways of re-weighting and reordering certain images. As a result, we have put in place a system to combine multiple pieces of information to show users the best possible photos for a business. Photo Scoring Algorithm Business Photo Ranking Pipeline In our current pipeline, we first retrieve all the quality scores for a business generated by the model described above. These scores are then adjusted based on characteristics such as the following: Logo filtering : We noticed that photos of logos often earned a high score from our model even though users have very little interest in seeing or clicking on them. This might include, for example, a poster that simply consists of the restaurant’s name. As a result, we trained a separate classification model based on the entropy of the image’s intensity histogram to lower the score of these images. Resolution : In order to standardize input to our neural network and speed up computation, we shrink each image to 227 by 227 pixels before feeding it in. However, this means that the model does not recognize whether a photo is too small to provide users with good content about a business. To deal with this, we downgrade photos which fall below a certain size threshold. Finally, we use the labels determined by our classification algorithm to ensure that different kinds of photos are displayed in the top results for a business. Application: Cover Photo Sorting At Yelp, each business’s page showcases a few of its best photos, which we call cover photos. For many years we have chosen these photos purely by calculating a function based on likes, votes, upload date, and image caption. However, this approach suffered from a few drawbacks. First, this system was highly subject to selection bias. Cover photos are viewed and clicked significantly more often than average. As a result, once a photo ends up on the business page, it is highly likely to remain there, even if more attractive and useful photos are uploaded at a later date. Additionally, relying solely on likes to determine prominent photos can end up promoting “clickbait” photos- that is, those that may have low relevance and quality but are upvoted due to their provocative nature. Now, as a result of our scoring algorithm, we believe that the quality of cover photos for restaurants has significantly improved. See for yourself! Country Way, Fremont Old Version High Quality Version Octavia, San Francisco Old Version High Quality Version Kunjip, Santa Clara Old Version High Quality Version Gary Danko, San Francisco Old Version High Quality Version What’s Next While the feedback we have received on this change has been very positive, there is more work we can do to improve the usefulness and relevance of our photos. The Photo Understanding team is working on a more comprehensive system that takes into account the type of business and photographer identity, as well as the user feedback and quality signals we discussed above, to provide an even better experience for Yelp users. Stay tuned for another update in the future! Acknowledgements : The photo scoring system was designed and implemented by Wei-Hong C., Alex M., Colin P., Prasanna S., Joel O., and Frances H. Tweet Back to blog", "date": "2016-11-29"}, {"website": "Yelp", "title": "First 100 Days of Yelp's Public Bug Bounty Program", "author": ["\n        \n  Martin Georgiev, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/12/100-days-public-bug-bounty-program.html", "abstract": "One hundred days ago we launched Yelp’s public bug bounty program on HackerOne.\nSince launching the program, we received over 564 reports from 512 reporters.\nThe distribution of the reports was as follows: Resolved: ~ 7% Informative: ~ 36% Duplicate: ~ 31% Not Applicable: ~ 26% Looking back on the first 100 days of our program, we fixed 39 vulnerabilities\nand paid out $13,850 in rewards. We maintained less than 24 hours response time\nand less than 1 month resolution time. The distribution of bug-bounty payouts\nover time is shown in Chart 1. Chart 1: Distribution of bug-bounty payouts over time Chart 2 shows the distribution of “New” reports over time. The large volume of\n“New” reports in the first few days after the public launch is (anecdotally)\nconsistent with what other companies experience with their public bug bounty\nprograms. For instance, in the first week after the launch we received 239\nout of the 361 reports received in September. Chart 2: Distribution of new reports over time Below, we list the top three (by impact and category) bug reports from the past\n100 days. All issues reported here have been fixed. The fixes were verified\nboth internally and by the respective bug reporter. The 0x02 List 0x00 Background: Users on Yelp can write reviews, upload images, engage in\ndiscussions, etc. Further, they can receive compliments in one of several\ncategories (e.g., “Good Writer”, “Great Photo”, “Like Your Profile”, etc).\nInternally, each compliment has an id and a type. Vulnerability: @mlitchfield analyzed this functionality of Yelp and reported a Denial of Service\nvulnerability at the application layer. The vulnerability stemmed from lack of\nproper error handling when checking compliment ids. Effectively, an attacker\ncould crawl Yelp, collect user ids and then issue compliments to those users\nreferencing a malformed compliment id. When the recipient of a compliment logs\nin, the server tries to process and notify the user of all newly received\ncompliments since the user’s last login. However, if the server encounters a\nmalformed compliment id, it throws an error. The error was not handled properly\nand hence bubbled up as an HTTP 500. 0x01 Background: Yelpers can check in and broadcast their location whenever they\nvisit a local business. Users can subscribe to receive check-in notifications\nfrom their friends. Two users are considered friends on Yelp when they both\nconfirm their friendship via the app. Vulnerability: @vinesh1989 analyzed the Friend check-in notification feature and found that the friendship\nwas not properly verified. A malicious user could subscribe for and track\ncheck-in notifications from anyone, not just his friends. 0x02 Background: Users can link multiple email addresses to their Yelp accounts.\nOnce linked and verified, all emails are considered equally valid w.r.t. login,\npassword reset, etc. functionality. Vulnerability: @insomniac thoroughly tested this feature and discovered that logged-in sessions and\npassword reset tokens do not expire even after the email used for the login is\nremoved from the list of authorized emails. An attacker, who could obtain\naccess to a victim’s account via some means (e.g., managed to guess the\nvictim’s password), could issue herself a password reset token. Even if the\nvictim were to change his password, the attacker could use the previously\ngenerate password reset token to take over the victim’s account. Private vs. Public Bug Bounty Program Prior to launching Yelp’s public bug bounty program, we ran a private bug\nbounty program with a small number of invite-only bug reporters for\napproximately 2 years. The private program helped us both handle rate of “New”\nreports (e.g., by controlling the number of hackers in our program) and\nallocate enough time to systematically fix common vulnerabilities (e.g., XSS). Table 1 shows a comparison of our private bug bounty program to the first 100\ndays of our public bug bounty program. Private Program Public Program Reports Received 399 564 Reports Resolved 125 39 Bounties Paid $65,160 $13,850 Duration Approx. 2 years 100 days Table 1: Private vs. Public bug bounty program stats Chart 3 shows the distribution of reports by type in our private vs. public bug\nbounty programs. Chart 3: Private vs. Public bug bounty program reports by type The public bug bounty program has resulted in a significant increase in bug\nreports. Yet, our stats show that bugs have gotten harder to find and exploit. Recommendations Based on our experience running bug bounty programs, we recommend that\ncompanies leverage private bug bounty programs before launching any public\nones. Private programs are ideal for “testing the waters”. They help bring the\nEngineering and Security teams together and get them used to validating\nand systematically fixing new issues. The most common classes of security\nvulnerabilities are often found and fixed through private bug bounty programs. Public programs (when pipelined after private programs) are great for finding\nremaining security loopholes. We suggest that companies open their\nprivate programs to the public only when they can devote sufficient engineering\nresources to resolving issues found. They should also be prepared for the high\nvolume of reports in the first few days of the public program launch. Conclusion The first 100 days of Yelp’s public bug bounty program have been a great\nsuccess. We collaborated with hundreds of bug hunters on HackerOne and as a\nresult have made significant improvements in our bot detection, API-abuse\nprevention, spam identification, and suspicious user-activity detection.\nWhen compared to our private program, the public bug bounty program shows that\nhigh-severity security and privacy vulnerabilities have become harder to find.\nWe attribute these favorable results to the issues addressed during our private\nprogram. We’d like to thank all bug reporters for their participation in our program and\nhelping us improve the security of our sites, apps and infrastructure. We look\nforward to continuing our collaboration with the security community and\nensuring the security of Yelp’s users. Tweet Back to blog", "date": "2016-12-20"}, {"website": "Yelp", "title": "What it’s Like To Be a First-Time Speaker at Grace Hopper", "author": ["\n        \n  Jenni S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/01/what-its-like-to-be-a-first-time-speaker-at-grace-hopper.html", "abstract": "Alexa H. has been a Product Designer at Yelp for two years, having previously graduated from California College of the Arts studying Graphic Design. This year at Grace Hopper, she co-presented “Ask questions, lots of questions: A workshop for practicing building beautiful presentations in Google Slides and giving design critique.” It was her first public speaking experience, and after watching her fantastic performance, I wanted to know more! Jenni: So Alexa, I’ll just dive right in: how did you feel about the prospect of public speaking before you submitted this proposal? Had you considered it in the past? Alexa: Well, I actually felt fine about it at first. When I submitted the proposal with my partner, I had a very different expectation around what the event was going to be. We had submitted a proposal for a workshop which, in my mind, would be a small, intimate event. It wasn’t until much later, once our proposal was accepted, that I found out the audience could consist of hundreds of people. Had I known how many people were going to be there, I probably wouldn’t have gone through with it.  I had considered getting into public speaking in the past… but not in front of hundreds of people! – Jenni: How did you go about choosing your topic? What experiences have you had that made it clear that there was interest? Alexa: The topic was presented to me by my partner and it was a topic I knew I could get behind. Since attending design school, critique has become an extremely valuable part of my design process. We had design critiques just about every class period in one way or another and that process directly translates to your work professionally. The design community is already pretty well versed with this process, but there are other folks I work with, from engineering to product management to marketing and beyond, that I know have been wanting to learn more about this practice. Grace Hopper seemed like the right audience. – Jenni: What was your overall process for developing your slides and presentation, and how did you practice? Alexa: My process was to first gather all of the lessons I wanted to teach before starting to design the presentation. Once I did start to put the slides together, I began meeting with a handful of my colleagues to do dry runs and gather feedback. After every dry run I would go back and tweak parts of the presentation that needed refinement. One of the most memorable pieces of feedback I got was that I hadn’t told much of my story and that information could help my audience trust my expertise in the subject. At work, I gave about two small group practice presentations as well as one larger group one; but I did most of my practicing on my own at home. I practiced a lot. I knew that if I didn’t memorize the content that I would be worried and potentially freeze in front of such a large group. I knew I was knowledgeable about the content, so I wasn’t worried about that; but rather, I really wanted people to take something special away from the talk and I think that kind of message really needs to be thoughtful. – Jenni: I was particularly impressed at how you handled the AV failures. You went for 5 minutes without your slides, and the transition back to slides was very smooth! Alexa: Thanks Jenni! Yeah, it was pretty wild. Even if you think you’ve prepared for everything to go perfectly well, something will still go wrong. The reason I was able to transition back out of that so smoothly was because of how much I practiced. – Jenni: I see a lot of similarities between design critique and code reviews. What do you think non-designers in the software side of the house can learn from your slides? Alexa: Well, my slides are all about building stronger communication and that’s endlessly relevant, no matter what community you’re talking to. Just like most of the lessons I learned from design school, I think the design critique process goes way beyond how to create good work. Being able to have an objective conversation around what is working and not working is going to benefit you and your company, but being able to objectively critique your solutions to any problem you may face in life – now that will help you in all aspects of work and play. Jenni: I am really looking forward to what you have to teach us next. What will you do differently next time? Alexa: Next time I will probably involve some of my close friends in the process earlier on. I am really grateful to have had the support and feedback from them. I am also going to make sure that I check the technology as soon as I have access to it since we had problems with our slides working once we started the presentation. We had time before we presented where we could have double checked to see it was working. – Jenni: And are you already thinking of the next session you’d like to present, at GHC or otherwise? Alexa: I have actually already given this presentation a second time for a class of engineering students at Colgate University! Besides that, I haven’t thought about it quite yet but would like to keep engaging. – Jenni: Final thought: what advice and/or resources do you have for someone reading this post, who’s thinking of diving into public speaking? Alexa: Gather a group of people you trust and make sure to involve them in your process. My presentation would not have been the same if I tried to do it on my own - it was a group effort. Other advice: use lots of photos, tell your story, practice, practice, practice and make it clear why people should care about what you’re talking about. And if you’re looking to get into public speaking and found any of this information helpful, you’re always welcome to reach out to me to continue the conversation: alexah@yelp.com. – Resources Ask Questions Lots of Questions: Giving Good Design Critique Original slides from Alexa H., Product Designer at Yelp Four Things Working at Facebook Has Taught Me About Design Critique Tanner Christensen, Product Designer at Facebook How to give and receive a good design critique AIGA.org Stop dreading feedback conversations SYPartners How to do a Product Critique Julie Zhuo, VP of Product Design at Facebook Tweet Become a Product Designer at Yelp Design products that get used by millions of people each day View Job Back to blog", "date": "2017-01-19"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 7 Winner and Announcing Round 9", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2017/01/dataset-round-7-winners-and-announcing-round-9.html", "abstract": "Yelp Dataset Challenge Round 7 Winners The seventh round of the Yelp Dataset Challenge ran throughout the first half of 2016 and, as usual, we were impressed with the projects and ideas that came out of the challenge. Today, we are proud to announce the grand prize winner of the $5,000 award: “Semantic Scan: Detecting Subtle, Spatially Localized Events in Text Streams” by Abhinav Maurya, Kenton Murray, Yandong Liu, Chris Dyer, William W. Cohen, and Daniel B. Neill (from Carnegie Mellon University, University of Notre Dame in Indiana). The authors created a model to detect and characterize emerging topics in text streams. Their new model is called Semantic Scan, and unlike traditional methods like Latent Dirichlet Allocation (LDA) it accounts for drift in concepts over time. Semantic Scan combines contrastive topic modelling with online document assignment and spatial scan statistics to quickly identify emerging topics. Compared to methods like Topics over Time, Online LDA, and Labeled LDA, their algorithm is both faster and results in lower Hellinger distances. Needless to say, this work is highly relevant to many technology companies. This entry was selected from tons of submissions for its technical and academic merit. For a full list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Thanks to all who participated! Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset. These examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Next Yelp Dataset Challenge: Round 9 The ninth round of the Yelp Dataset Challenge opened on January 24, 2017 (and will close on June 30, 2017), giving students access to reviews and businesses from 11 metropolitan areas scattered over 4 different countries. Compared to the previous round, we added a new metropolitan area: Cleveland, the largest in the Buckeye State! This new dataset and the photo auxiliary file are available for immediate download, containing over 4.1 million reviews. Note that the new dataset JSON files have a slightly different format compared to past rounds. Tweet Back to blog", "date": "2017-01-25"}, {"website": "Yelp", "title": "How We Scaled Our Ad Analytics with Apache Cassandra", "author": ["\n        \n  Qui N. and Shafi B., Software Engineers\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/how-we-scaled-our-ad-analytics-with-cassandra.html", "abstract": "On the Ad Backend team, we recently moved our ad analytics data from MySQL to Apache Cassandra . Here’s why we thought Cassandra was a good fit for our application, and some lessons we learned that you might find useful if you’re thinking about using Cassandra! Why Cassandra? First, a little bit about our application. We have over 100,000 paying advertisers. Every day, we calculate the numbers of views and clicks each ad campaign received the previous day and the amount of money spent by each campaign. With these analytics, we generate bills and many different types of reports. Back in the early days of Yelp, we chose to store this data in MySQL. As we scaled and our advertising product evolved, we realized that Cassandra was a better fit. We didn’t need the benefits of traditional relational databases because we don’t have a transactional workload or a complex data model that would require joining tables. For us, the main advantages of Cassandra were simple scaling of storage, flexible schemas, and high write performance. Simple scaling of storage Growing our ad business means more campaigns, which means more analytics data. Because Cassandra is designed to be distributed, it’s easy to increase storage space - all you have to do is add more nodes. Flexible schema Our data requirements change over time. For example, we recently decided to start breaking down analytics information by ad type like mobile search or desktop review. With Cassandra’s flexible schemas, we can easily add new columns to our data tables. High write performance Cassandra is well-known for its impressive write capacity. In one benchmark test , Netflix achieved over 1 million writes per second! As we increase the amount of analytics data we write each day, we need to be sure that our system can handle it quickly, especially because we need the latest data available for daily reports and billing. Lessons learned Here are a few of the most interesting things we learned about developing with Cassandra. 1. Rethink your queries along with your schema Cassandra is essentially a structured key-value store and the choice of columns used for the primary key dramatically impacts the performance of queries. As a result, the cardinal rule in Cassandra data modeling is to design your schema for your queries. Something less obvious, though, is that you may also need to redesign your queries. In Cassandra, only the columns in the primary key can be used to query data - if you select data with conditions on other columns, Cassandra will need to do a full table scan. Each column in the primary key can either be part of the partition key or a clustering column. The partition key determines the node that data is stored on. Then, within each partition with a particular set of partition key values, clustering columns determine the sort order. We noticed that all analytics queries in our existing system looked something like this: analytics = get_stats_for_campaign(campaign_id, time_period) In other words, our clients were requesting analytics for a particular campaign over some time period. Given that, we chose campaign_id as our partition key and used day as a clustering column. As expected, this schema worked well for cases where the client needed analytics for a single campaign, such as a report for one campaign over time. However, it was not the ideal schema for clients that needed analytics for all campaigns, e.g. our monthly overall reports. We found that a query for one primary key took slightly longer in Cassandra than in MySQL. This was imperceptible for one query, but in a report where thousands of queries were made, the extra time added up. With Cassandra, you want to minimize the number of partitions read. Those reports ultimately needed data on all campaigns for the same time period, even though the code queried a single campaign at a time. In this case, it would have been more efficient to use day as the partition key and query analytics by day. When using Cassandra, it can make sense to denormalize or duplicate your data to improve query performance because of Cassandra’s good write performance and storage scalability. If you find yourself reading many partitions to satisfy some requests, consider redesigning your queries and using another schema for those requests. 2. Definition of rows and how that affects compaction Compaction is the process of merging updated rows in Cassandra. Cassandra provides a few different compaction strategies, and the main factor in choosing one is how and when your application updates rows. While we were making this decision, we found existing documentation on the definition of row updates unclear, so we’d like to share what we learned. We initially thought that rows would be infrequently updated in our application, because we simply write new analytics data for the previous day every day. We don’t change data for other days except in special circumstances. Remember, though, that we chose campaign_id as the partition key and day as a clustering column. We assumed that the primary key, including both partition and clustering columns, defined separate rows: In reality, however, Cassandra stores the data for each partition key in wide rows. The clustering columns just determine the ordering of cells within that row: As a result, each of our writes for a new day for an existing campaign actually counted as a row update that could be compacted. Once data is flushed to disk, Cassandra uses compaction to maintain clustering within partitions. With the default compaction strategy, size-tiered compaction, the data for a single campaign ended up spread out all over disk, because the updates each day were not frequently merged. Switching to leveled compaction, which provides guarantees on the number of unmerged updates for each row, vastly improved our overall read performance. Leveled compaction does require more I/O at write time, but if you frequently update rows, it may be worth the tradeoff. 3. Migration to Cassandra will increase your write needs Keeping data normalized is considered a best practice in MySQL. As we mentioned before, however, when using a NoSQL database like Cassandra, denormalizing data often improves query performance. One row in a normalized table could become tens of rows in many denormalized tables, meaning migrating to Cassandra can really increase your write load. Cassandra does have better write performance, but you should still keep that consideration in mind. We didn’t fully appreciate the magnitude of this increase at the beginning, when we were deciding whether or not to use a object relational mapper (ORM). ORMs abstract out the complex underlying relational data models, making code easier to reason about. At Yelp, we already use the SQLAlchemy ORM for almost all of our MySQL operations. Given the benefits we’ve observed from SQLAlchemy, we decided to use cqlengine , the ORM packaged with the Cassandra python driver. In our application, all writes happen in a nightly batch, which aggregates statistics for the previous day and writes them to a number of denormalized tables. During the pilot stage of the project, we were experimenting with one Cassandra table. At this stage, we were satisfied with the write performance we achieved with cqlengine. When we wrote to multiple tables, though, the nightly batch started taking too long, holding up other things in our pipeline. Unfortunately, cqlengine does not support asynchronous writes. You can only send one write at a time, and it’s a blocking call. This really limits how quickly you can process writes. One of the reasons Cassandra can have such great write performance is that all nodes can handle writes, but you can’t take advantage of that if you’re only sending one write at a time. As a result, if you have a large volume of data to write, cqlengine may not be the best choice. Instead, you might prefer to write asynchronously by using the execute_async operation from the Cassandra driver directly. We made this realization in a later stage of our project, so we ended up sticking with cqlengine and using some tricks to speed up writes, which we describe in the next section. 4. Cassandra batch statements can be useful A batch operation in Cassandra combines multiple data modification operations (such as insert, update, and delete) into a single logical operation. Unlike MySQL batch operations, Cassandra batch operations are rarely used for performance optimization. Instead, batch operations are mostly used to provide atomicity, and using them to perform bulk writes is considered an anti-pattern . In our project, though, we found that using batch operations was an effective way to improve our write performance. Our initial method of writing each row synchronously using the cqlengine save operation turned out to be really slow. For each row, we were sending a write request from our service to Cassandra and then waiting for it to complete. That is, each row written required a complete network round trip from our service to the Cassandra cluster. Furthermore, we had to wait for each round trip sequentially. This is where the batch operation came in handy. By combining writes for several rows together into a single cqlengine batch query, we were able to reduce the number of round trips needed by almost tenfold, and the Cassandra nodes could coordinate the writes in each batch in parallel. This technique cut our nightly batch run time in half. 5. TTL may not be the right choice Cassandra supports an optional time to live (TTL) expiration period for each column of a data row. If a column has a TTL value set, it will expire automatically after that period, so you don’t have do anything extra to delete old data later. During the initial stages of our project, the requirement was to keep analytics data in Cassandra for one year. Accordingly, in our first backfill of data from MySQL, we set the TTL on all the data to one year in the future. Then, as the project progressed, the requirement increased to two years. Turns out, to change the TTL on data, you need to re-insert that data with the new TTL value, so we had to rerun the entire backfill again! Whether or not TTL is the right choice depends on your application. When using Cassandra as a cache, for example, TTL can be very useful because it clears old data automatically, and it’s not necessary to update the TTL of existing data. In an analytics use case like ours, however, the data in Cassandra is very critical, and automatically expiring data can become problematic if  data requirements ever change. In that case, running a periodic batch to delete old data may be a better choice. Final thoughts Moving our ad analytics data from MySQL to Cassandra has produced benefits across the organization. It increased the flexibility of the ad analytics system in terms of data growth and schema changes while also removing the load of one of the largest tables from our shared MySQL clusters. If you’re looking to build or migrate a system with a high volume of writes, Cassandra could be a good choice as long as you consider the lessons we learned. Tweet Back to blog", "date": "2016-08-08"}, {"website": "Yelp", "title": "AMIRA: Automated Malware Incident Response and Analysis", "author": ["\n        \n  Kuba S., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/amira-automated-malware-incident-response-and-analysis.html", "abstract": "Brave malware analysts at Yelp have spent a lot of time looking at the digital forensics from potentially infected macOS systems, gathered using our open source project, OSXCollector . Early on, we automated parts of the analysis process, augmenting the initial set of digital forensics collected from the machines with the information gathered from the threat intelligence APIs and internal blacklists. This involved identifying potentially suspicious domains, URLs and file hashes but our approach to the analysis still required a certain degree of configuration and manual maintenance which was tedious for the malware response team. In this blog post I will explain how we identified and automated the manual effort needed to analyze digital forensics collected from potentially infected macOS machines. I will also introduce AMIRA: Automated Malware Incident Response and Analysis, which we have open sourced on GitHub . AMIRA logo (Don’t) Repeat Yourself Our traditional approach to malware incident response process used to start with taking a machine we suspected to be infected off the network and collecting digital forensics by running OSXCollector. This task was performed by our incredible HelpDesk ninjas as they had the best outreach to the users in terms of different office locations. Once the OSXCollector output was obtained the HelpDesk engineer had to attach it to the malware case in our incident response platform so that the analysts from the security team could take a look and asses the risk, e.g. confirm that the machine is infected. The analysts would execute the Analyze Filter to get the full overview of what happened on the machine in order to determine how the malware possibly got there. The Analyze Filter checks various threat intel sources, like VirusTotal and OpenDNS Investigate , as well as internal blacklists and whitelists for known bad domains and file hashes (see our previous blog post for more information). The time it takes to run the whole suite of the Output Filters depends on the size of the original OSXCollector output file and can take anywhere from a few minutes to several hours. During this time our engineers usually switched to some other tasks and then periodically checked the status of the analysis. If there were several malware cases to investigate, each analysis had to be run sequentially, in order to not exhaust the quota limits put on the threat intel APIs. Worse still, it was easy to interrupt the analysis if the machine that the analyst used to run the Output Filters on went to sleep or was otherwise disconnected from the network. This was not an ideal way of analyzing malware alerts efficiently. Automating repetitive tasks Enter automation: we enabled faster forensic collection and analysis by scripting repetitive tasks which also means fewer possibilities to make a mistake. We went ahead and turned OSXCollector and its awesome Output Filters into AMIRA: Automated Malware Incident Response and Analysis. AMIRA is a service that turns the forensic information gathered by OSXCollector into an actionable response plan, suggesting the infection source as well as suspicious files and domains that require a closer look. Furthermore, we integrated AMIRA with our incident response platform, making sure that as little interaction as necessary is required from the analyst to follow the investigation. Our malware responders do not need to spend any more time configuring and running the analysis filters on their own. AMIRA takes care of it and publishes a neat report summarizing the findings, so they can simply grab a cup of coffee and read it like a morning newspaper. AMIRA inner workings The service uses the S3 event notifications to trigger the analysis. We have configured a bucket for the OSXCollector output files so that when a file is added there the notification is sent to an SQS queue ( AmiraS3EventNotifications in the picture below). AMIRA periodically checks the queue for new messages, and upon receiving one it will fetch the file from the S3 bucket. Then, it will run the Analyze Filter on the OSXCollector output file. The Analyze Filter runs all the filters contained in the OSXCollector Output Filters package sequentially. Some of them communicate with the external resources, like domain and hashes blacklists (or whitelists) and threat intel APIs, e.g. VirusTotal , OpenDNS Investigate or ShadowServer . The original OSXCollector output is extended with all of this information and the very last filter run by the Analyze Filter summarizes all of the findings into a human-readable form. After the filter finishes running, the results of the analysis will be uploaded to the Analysis Results S3 bucket. The overview of the whole process and the system components involved in it are depicted below: Running AMIRA AMIRA is written in Python, so the only prerequisites you will need to run it on your system are Python 2.7 and pip . You will also need an S3 bucket and an SQS queue configured to receive notifications about new objects created in the bucket. Another S3 bucket will be also necessary to store the results of the analysis. Check out AMIRA GitHub repository to read about the necessary configuration and how to run the automated analysis in more depth. Integrating AMIRA with remote forensics collection AMIRA takes all of the responsibility for configuring the Output Filters from analysts, but there is still some room for improvement to streamline the whole malware response process. The biggest delays and the most manual effort is wasted during the initial part of the response process, when we need to physically collect the machine to run the OSXCollector and then upload the results to the S3 bucket, to trigger AMIRA via the S3 event notifications mechanism. To cut back on that time we have written a small shell script that executes OSXCollector and uploads the analysis results to the S3 bucket: #!/bin/bash # based on http://tmont.com/blargh/2014/1/uploading-to-s3-in-bash # set \"bash strict mode\" from: # http://redsymbol.net/articles/unofficial-bash-strict-mode/ set -euo pipefail file = \" $1 \" bucket = \" $2 \" echo \"Uploading file $file to bucket $bucket \" resource = \"/ ${ bucket } / ${ file } \" contentType = \"application/x-compressed-tar\" dateValue = ` date -u + \"%a, %d %b %Y %T GMT\" ` stringToSign = \"PUT \\n\\n ${ contentType } \\n ${ dateValue } \\n ${ resource } \" s3Key = \" $3 \" s3Secret = \" $4 \" signature = ` echo -en ${ stringToSign } | openssl sha1 -hmac ${ s3Secret } -binary | base64 ` curl -X PUT -T \" ${ file } \" \\ -H \"Host: ${ bucket } .s3.amazonaws.com\" \\ -H \"Date: ${ dateValue } \" \\ -H \"Content-Type: ${ contentType } \" \\ -H \"Authorization: AWS ${ s3Key } : ${ signature } \" \\ \"https:// ${ bucket } .s3.amazonaws.com/ ${ file } \" | cat We deploy this script via our enterprise asset management software on any individual machine that is under investigation. This saves malware responders and HelpDesk engineers a lot of hassle when compared to the previous process which required chasing down the employees and acquiring the machine to launch OSXCollector (or, for remote employees, even shipping the laptop in from overseas!). Integrating with the incident response platform On the other hand, we didn’t want to require our analysts to fetch the analysis results from the S3 bucket on their own each time the analysis was completed. First of all, they would have to monitor the bucket for the new files themselves, or set up the S3 event notifications for the Analysis Results bucket as well. Second, they would need to get the actual results summary file using something like AWS Command Line Interface , which meant they will need to manage the AWS credentials on their machines - an additional set up step we wanted to avoid by introducing AMIRA. We integrated AMIRA with our incident response platform. Each time the analysis is finished, the original OSXCollector output as well as the analysis results are attached to the related malware incident response case. This keeps things neat and clean, as the malware analysts do not have to leave the incident response environment to fetch forensics analysis results. Another few minutes saved and another win for the automation. Adding up all these precious seconds saved AMIRA helped us to save a lot of time during our malware incident response. In some cases it cut our time in responding to an incident from several hours to just minutes. It also offloaded much of the work from our HelpDesk team related to physical acquisition of the machine for forensics collection. Additionally, we can proactively run OSXCollector on the machines we suspect to be infected more often, as the cost of collecting and analyzing digital forensics decreased so dramatically. Thanks to all this automation, the incident response team members can focus on what they excel at: finding unusual patterns and novel ways that malware sneaks onto our corporate infrastructure. Tweet Become an Information Security Engineer at Yelp If you're interested in building tools like AMIRA that help us secure Yelp and its users, apply to become an Information Security Engineer. View Job Back to blog", "date": "2016-08-18"}, {"website": "Yelp", "title": "Undebt: How We Refactored 3 Million Lines of Code", "author": ["\n        \n  Evan H., Software Engineering Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/undebt-how-we-refactored-3-million-lines-of-code.html", "abstract": "Peter Seibel wrote that to maximize engineering effectiveness, “Let a thousand flowers bloom. Then rip 999 of them out by the roots.” Flowers, in how the metaphor applies to us, are code patterns — the myriad different functions, classes, styles, and idioms that developers use when writing code. At first, new flowers are welcome — maybe the new pattern seems easier to use, more scalable, more efficient, or more suited to some particular task than the old. As a code base grows, and the flowers proliferate, however, it becomes clear which patterns work and which don’t. Suddenly, code patterns that were once beautiful new flowers become technical debt in need of removal. When that happens, it’s time to start ripping. Otherwise, since developers learn by reading (and occasionally copy-and-pasting) from existing code, the bad flowers and the technical debt that comes with them will continue to grow unchecked. Ripping out flowers with Undebt Removing a bad code pattern by hand, especially in a massive code base like ours, is a Herculean task that puts a massive drain on developer time; time that could be better spent working on new features and shipping new code. That’s why other members of the Core Backend team and I built Undebt, an elegant, fast, reliable tool for performing massive, automated code refactoring. Undebt works on any language and lets you define complex find-and-replace rules using standard, straightforward Python that can be applied quickly to an entire code base with a simple command. Since its inception at our most recent hackathon, Undebt has become a key tool for performing en masse code refactoring. Used along with our open-source debt tracker , we can now efficiently monitor and remove technical debt before it becomes a serious problem. Usage of a deprecated method going to zero with the help of Undebt The graph above, generated using our open-source debt tracker, shows the usage of a particular deprecated method across the 3 million lines of Python that make up Yelp’s codebase. The effect of Undebt can be seen near the end. We were able to completely remove two years of accumulated debt using a variant of the included method_to_function example . We have also made heavy use of the sqla_count example in refactoring our SQL alchemy code to remove inefficient sub-queries. How it works To use Undebt, you define a custom pattern that specifies what code pattern to look for and how to replace it. Undebt leverages the popular open source tool pyparsing to make writing a pattern file as easy as plain Python. Undebt also comes loaded with tools and examples to make this process as painless as possible. Ready to start gardening with Undebt? Check out Undebt’s GitHub repository for more information on how to get started. Tweet Back to blog", "date": "2016-08-23"}, {"website": "Yelp", "title": "More Than Just a Schema Store", "author": ["\n        \n  Chia-Chi Lin, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/more-than-just-a-schema-store.html", "abstract": "This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 When you have a system that streams billions of messages a day , in real-time, from MySQL into Kafka , how do you effectively manage all of the schemas stored\nacross all the databases? When you are building to connect hundreds of services,\nyou will encounter thousands of different schemas and manually managing those\nschemas is unsustainable. An automated solution became necessary to handle schema\nchanges from upstream data sources to all downstream consumers. Confluent’s Schema Registry and Kafka Connect are excellent choices to begin with, except they did not exist at the time\nwhen we started building the Yelp Data Pipeline. This is how the Schematizer was born. Schematizer… who? One of the key design decisions of Yelp’s Data Pipeline is schematizing all data.\nIn other words, the pipeline forces all the data flowing through it to conform to\npredefined schemas, instead of existing in a free form. Standardizing the data\nformat is crucial to the Yelp Data Pipeline because we want the data consumers to\nhave an expectation about what type of data they are getting, as well as being\nable to avoid immediate impact when the upstream data producers decide to change\nthe data they publish. Having a uniform schema representation also gives the Data\nPipeline a really easy way to integrate and support various systems that use different\ndata formats. The Schematizer is a schema store service that tracks and manages all the schemas\nused in the Data Pipeline and provides features like automatic documentation support.\nWe use Apache Avro to represent our schemas. Avro has\nseveral very attractive features we need in the Data Pipeline, particularly schema\nevolution, which is one of the key ingredients that make decoupling data producing\nand consuming possible. Each message that flows through the Data Pipeline is serialized\nwith an Avro schema. To reduce the message size, rather than embedding the entire\nschema into the message, the message only contains a schema ID. Data consumers then\ncan retrieve the schema from the Schematizer and deserialize the messages at run time.\nThe Schematizer is the single source of truth for all the predefined schemas. Look at all these schemas. We can organize them in various ways. The Schematizer associates and organizes schemas in two ways: one from the data\nproducer’s perspective and the other from the data consumer’s perspective. The first method groups the schemas based on the data origin. Each group is\ndefined by a namespace and a source. The data producers must specify a namespace and\na source when they register the schemas with the Schematizer. For instance, a service\nthat wishes to publish its database data into the Data Pipeline can choose its service\nname as the namespace, and each table as a source. Group schemas based on namespace and source The second method is based on the data destination. Each data destination,\nsuch as a Redshift cluster or a MySQL database, is a data target, and can\nhave one or more data origins. Each data origin consists of one or more schemas,\nthat is, one or more namespace and source pairs as defined in the first method. Group schemas based on data origins of single data target These two different approaches allow us to search and query related schemas\nbased on different needs. For instance, an application may want to know all the\ntopics to which it is publishing, while another service may want to know the\nsources of all the data in its Redshift cluster. Let’s register schemas, shall we? The Data Pipeline requires all the data to be schematized and serialized with\npredefined Avro schemas. Therefore, when a data producer would like to publish\ndata into the Data Pipeline, the first thing the producer does is register the\nschema with the Schematizer. The most common way to do this is to register\nan Avro schema directly. For the data producers that do not have or cannot create Avro schemas, schema\nconverters can be easily added into the Schematizer to convert a non-Avro schema to\nan Avro schema. A good example is the MySQLStreamer ,\na service that pumps data from MySQL databases into the Data Pipeline, which only\nknows about MySQL table schemas. The Schematizer can take a MySQL table schema\ndefinition and create the corresponding Avro schema. The data producer must also\nre-register the new schema whenever there is a schema change. OMG, the upstream schema has changed! Will my service break? A common pain point that every data pipeline must address is how to deal with\nupstream schema changes. Often times such an event requires a lot of communication and\ncoordination between upstream data producers and downstream data consumers.\nYelp is not immune to this problem. We have batch jobs and systems that ingest data\ncoming from other batch jobs and systems. It has become a painful problem every time\nthe upstream data changes because it may cause the downstream batch jobs or systems to\ncrash or necessitate a backfill. The entire process is pretty labor-intensive. We tackle this problem with schema compatibility. During schema registration,\nthe Schematizer determines which topic should be assigned to the new schema based on\nschema compatibility. Only compatible schemas can use the same topic. When an\nincompatible schema is registered, the Schematizer creates a new topic in the same\nnamespace and source for the new schema. How does the Schematizer determine the compatibility? Avro resolution rules .\nThe resolution rules ensure that, in the same topic, the messages serialized with\nlater versions of the schemas can always be deserialized with older version of\nthe schemas, and vice versa. Different topics are assigned to incompatible schemas Right now, Yelp’s main database data flows through the Data Pipeline via\nthe MySQLStreamer. Let’s say that at some point we decide to add a column with\na default value to the Business table. The MySQLStreamer will re-register the\nupdated Business table schema with the Schematizer. Since such a schema change\nis a compatible change based on the resolution rules, the Schematizer will create\na new Avro schema and assign the latest existing topic of the same namespace and\nsource to it. Later, if someone decides to change one column type of the Business\ntable from int to varchar , this will cause an incompatible\nschema change, and the Schematizer will create a new topic for the updated\nBusiness table schema. The guarantee of schema compatibility within each topic makes sure when the\nupstream schema is changed, the downstream data consumers can continue to\nconsume the data from the same topic using their current schema, without worrying\nthat the change may break the downstream systems. They also have the flexibility to\ntransition to the newer topics based on their own timeline. It provides the pipeline\nfurther automation and less human intervention in schema change events. Besides resolution rules, we also build in custom rules into the Schematizer to\nsupport some Data Pipeline features. The primary key fields of a schema are used\nfor log compaction in the Data Pipeline. Because the key for log compaction must remain the same for\na single topic, any change to the primary key fields is considered an incompatible\nchange, and the Schematizer will create a new topic for the new schema. Also, when\na non-PII (Personally Identifiable Information) schema starts to include a PII field,\nthat change is qualified as an incompatible change as well. The PII data and non-PII\ndata will be stored in separate topics, which simplifies the security implementation\nfor the PII data, preventing downstream consumers from accidentally accessing the data\nthey do not have permission to. Logic flow to decide whether a new topic is required One thing worth noting is the schema registration operation is idempotent.\nIf identical schema registration calls are made multiple times, only the\nfirst one creates a new schema. The subsequent calls will simply return the\nregistered schema. This also provides a simpler way for an application or\na service to initially set up its Avro schemas. Most applications and services\nhave Avro schemas defined in the files or in the code, but they do not hardcode\nschema IDs, since schema IDs depend on the Schematizer. Instead of first querying\nthe Schematizer to get the schema information back and registering it if it doesn’t\nexist yet, the application or the service can use the schema registration endpoint to\nachieve these two operations at the same time. Streamline all the way. To fully streamline the pipeline for schema change events, the Schematizer can\nfurther generate the schema migration plan for the downstream systems to be\napplied based on the existing schema and the new schema. Currently the Schematizer is\nonly able to create the schema migration plan for Redshift tables. A downstream\nsystem that loads data into a Redshift cluster from the Data Pipeline can simply\nquery and apply the schema migration plan when a schema change event occurs, and\nthen automatically pick up the new table schema without any manual process. This\nfeature is designed to be easily extensible, and each schema migration plan generator is\nan exchangeable component, so later we can add more generators to support different\nschema types, or switch to the ones that use more sophisticated algorithms to\ngenerate the migration plans. Who’s the data producer? Who consumes these data? The Schematizer knows it all. In addition to registered schemas, the Schematizer also tracks the data producers and\ndata consumers in the Data Pipeline, including which team and which service is\nresponsible for producing or consuming the data, and how often they expect to\npublish the data. We use this data to effectively contact and communicate with\nthe corresponding teams when events that require human intervention occur. This\ninformation also allows us to monitor and decide which schemas and topics may be\nout of service and can be deprecated or removed. As a result, it simplifies the\ncompatibility validation during schema registration. The Schematizer can now skip\ndeprecated schemas and check the schema compatibility only against active ones\ninstead of every single schema for the same topic. The data producers and consumers are required to provide this information\nwhen they start up. Initially, we planned to only store this information in\nthe Schematizer. Since this data is also very valuable for exploratory analysis\nand alerting, we instead decided to write the data into separate Kafka topics\noutside the Data Pipeline. The data then can be ingested into Redshift and Splunk,\nas well as loaded into the Schematizer and displayed through the front-end web\ninterface. We choose to use the async-Kafka producer, a non-blocking Kafka producer\ndeveloped at Yelp which writes data through Clog ,\nbecause it will not interfere with the normal producer publishing messages.\nIn addition, it can avoid the circular dependency situation in which the normal\nproducer tries to register itself by using another copy of same producer, which\nalso tries to register itself. Wait, which Kafka topic should I use? The Schematizer takes care of it for you. Unlike regular Kafka producers, the data producers of the Data Pipeline do not\nneed to know which Kafka topics they should publish the data to in advance. Since\nthe Schematizer dictates the topic which each registered schema uses, the producers\nonly need to tell the pipeline the schema they use to serialize the messages.\nThe pipeline then asks the Schematizer for the topic information and publishes\nthe messages to the correct topic. Abstracting away the topic awareness makes\nthe interface simpler and easier to use. The same mechanism exists for data consumers. Although they may define a specific\nKafka topic to consume from, the more common use-case is to allow the Schematizer\nto provide the correct topics to consume from based on the group the data consumer\nis interested in. We introduced various grouping mechanisms in the Schematizer earlier.\nThe data consumer can specify either a namespace and a source, or a data target, and\nthe Schematizer will figure out the right topics in that group. This is especially\nuseful when the data consumer is interested in a group of topics that may change\nover time due to incompatible schema changes. It relieves the data consumers from\nthe burden of keeping track of every single topic in the group. Schemas are good. Documentation is even better! Schemas standardize the data format but may not provide enough information for\npeople who want to understand the meaning of the data. We noticed that the people\nwho use the data are usually not the same people who produce the data, and they\noften don’t know where to find the information about the data they try to use.\nSince the Schematizer already has the knowledge about all the schemas in the\nData Pipeline, it becomes an excellent candidate to store information about the data. Meet our knowledge explorer, Watson. The Schematizer requires schema registrars to include documentation along with\ntheir schemas. The documentation then is extracted and stored in the Schematizer.\nTo make the schema information and data documentation in the Schematizer accessible to\nall the teams at Yelp, we created Watson, a webapp that users across the company\ncan use to explore this data. Watson is a visual frontend for the Schematizer and\nretrieves its information through a set of RESTful APIs exposed by the Schematizer. Watson provides valuable information about the state of the Data Pipeline: existing\nnamespaces, sources, and the Avro schema information within those sources.\nMost importantly, Watson provides an easy interface to view documentation on\nevery source and schema the Schematizer is aware of. Those docs aren’t going to write themselves. The majority of the data flowing through the Data Pipeline right now come from databases.\nTo document the sources and the schemas of these data, we leverage SQLAlchemy models.\nAt Yelp, SQLAlchemy is used to describe all models in our databases. Besides the docstring,\nSQLAlchemy also allows users to include additional information for the columns of\nthe model. Therefore, it becomes a natural location for us to put documentation on\nthe purpose and usage of both the model and its fields. A new ownership field is also introduced to the SQLAlchemy model to capture\nthe maintainers and the experts for each model. We think the people who generate\nthe data are the best source to provide documentation. Also, this approach encourages\nus to keep the actual data models and their descriptions in sync all the time. class BizModel(Base): __yelp_owner__ = Ownership(\n        teams=[TEAM_OWNERS['biz_team']],\n        members=[],\n        contacts=[]\n    ) __table_name__ = 'my_biz_table' __doc__ = 'Business information.' id = Column(Integer, primary_key=True, doc=r\"\"\"ID of the business.\"\"\" )\n    name = Column(String(64), doc=r\"\"\"Name of the business.\"\"\" ) Simple SQLAlchemy model with documentation and ownership information Developers may not always remember to include the documentation when they\nwork on the SQLAlchemy models. To prevent that, we set up automated tests\nto enforce that the model is attributed and documented. Hard checks are also\nput in place to ensure that we never regress. Whenever a new model is added,\nthe test suite will fail if the model is not properly documented or is missing\nownership information. These automated tests and checks have moved us much\ncloser to our goal of 100% documentation coverage. Extract delicious documentation to feed Watson. Once the documentation is available in the data models, we are ready to get it\ninto the Schematizer and eventually present it in Watson. Before diving into\nthe extraction process, we first introduce another component that plays\nan important role in this process: the Application Specific Transformer,\nor AST for short. The AST, as its name suggests, is a framework that takes in\na stream of messages from one or more Data Pipeline topics, applies transformation\nlogic to the messages’ schema and data payloads, and then outputs the transformed\nmessages to a new set of Data Pipeline topics. Transformation components that provide\nspecific transformation logic are chainable, and therefore multiple components\ncan be combined to perform more sophisticated transformation logic. We use a set of transformation components in the AST to generate more understandable\ndata using SQLAlchemy model reflection. Since the components are chainable, we now\nsimply create a transformation component which extracts the documentation and\nownership information from the SQLAlchemy models and add it into the existing\ntransformation chain. The documentation and ownership information of the models\nthen are automatically extracted and loaded into the Schematizer through the\nexisting pipeline. The implementation is surprisingly simple and seamlessly\nintegrates into the entire pipeline, so it’s a very effective way to produce\ngood quality documentation. Transformation components in AST As mentioned above, some transformation components already exist in the AST\nto generate more meaningful data to end users. The bit flag field transformation\ncomponent flattens a single integer flags field into multiple boolean fields,\neach of which represents what each bit of the integer value means. Similarly\nthe enum field transformation component converts the numeric enum value into\na readable string representation. A nice bonus from these transformation components\nis they also produce self-explanatory and self-documented schemas at the same time,\nand consequently create better documentation. Collaborate, contribute, and find. The story doesn’t end with developer documentation. Watson also provides mechanisms\nfor end users to collaborate and contribute toward making all of Yelp’s data easily\nunderstandable. The first mechanism is tagging. Watson allows users to tag any source with a\nrelevant category. A source may be a MySQL database table or a data model.\nFor example, a Business source can be tagged with the “Business Information” category,\nand a User source may be tagged with the “User Information” category. End users\ncan tag related sources with the same category and organize them in a way that\nmakes the most sense to them. The effect of tagging is a richer understanding of\nhow our own data sources relate and connect to each other. Business source tagged with “Business Info” Adding notes is the second mechanism Watson provides. This enables users, especially\nnon-technical users, to contribute their own documentation to a source or a field.\nUsers such as business analysts often have valuable experience on using data, and notes\nhave been a great way for them to share gotchas, edge-cases, and time-sensitive information. The number one feature requested by end users for Watson was a more nuanced search.\nTo support that, we have implemented a simple search engine in Watson that allows\nusers to search on various aspects of the data such as schemas, topics, and\ndata model descriptions. We chose Whoosh python package to back the search functionality as opposed to something like\nElasticsearch because it allowed us to get the development moving quickly.\nWhoosh also provides decent performance for the volume of search data we have so far.\nAs the volume of data increases, we will consider switching to more scalable engine later. Conclusion The Schematizer is a key component of Yelp’s Data Pipeline. Its schema registration\noperation enables the major features of the pipeline, including mitigating\nthe upstream schema change impact on downstream consumer applications and services.\nThe Schematizer also takes care of topic assignment for data publishing, removing\nthe need for users to determine which topic to use. Finally, it requires and\nprovides the documentation for every piece of data flowing through the pipeline\nto facilitate knowledge-sharing across the entire organization. Combined with Watson,\nall the employees at Yelp now have a powerful tool to access up-to-date information. We now have dug into the Schematizer and its front-end documentation system, Watson.\nNext, we are going to explore our stream processor: Paastorm. Stay tuned! Acknowledgements Many thanks to Josh Szepietowski, Will Cheng, and Bo Wu for bringing Watson to life.\nSpecial thanks to Josh, the author of the Application Specific Transformer,\nfor providing invaluable inputs on the AST and Watson sections in this blog post. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-08-11"}, {"website": "Yelp", "title": "Yelp's Bug-Bounty Map", "author": ["\n        \n  Martin Georgiev, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/09/yelp-public-bug-bounty-map.html", "abstract": "For the past two years we’ve been running a private bug-bounty program. We\nworked with academic researchers and bug hunters from all over the world and,\nas a result, we have fixed over a hundred potential vulnerabilities, and have\npaid bug bounties to dozens of security experts. Today we’re launching our public bug-bounty program as our next step towards\nimproving the security of Yelp’s systems and services. Our vulnerability reward\npayouts will go up to $15,000 USD for the most impactful exploits. Since getting familiar with our infrastructure may be a bit intimidating,\nwe’ve put together some information below to help you through the bootstrap\nprocess. Consumer Site Location: www.yelp.com , m.yelp.com . Purpose: With millions of people using Yelp every day both on their desktops\nand mobile devices, our consumer site is one of our major assets. Users come to\nour consumer site to search for and message local businesses, order food, review\nlocal establishments, engage with other local users, etc. Under the hood: Python, Java, C++. What to look for: We are interested in any vulnerabilities that allow the\nattacker to map user profiles to their respective email addresses. Other\ncritical vulnerabilities in our consumer site would involve the ability of a\nmalicious user to modify other users’ reviews, order food for free or gain\naccess to another user’s payment details: e.g., reveal PANs. Look also for web\nvulnerabilities that result in sensitive data disclosure, data\ninjection/exfiltration, insecure session management, etc. Business Owner’s Site Location: biz.yelp.com . Purpose: Our biz site allows business owners to manage their Yelp\npresence, track visitor engagement, respond to customer inquiries and messages,\nreply to reviews with a private message or a public comment, subscribe to\nadvertising programs and track ad spending. Under the hood: Python, Java, C++. What to look for: Similar to the consumer site, look for any web\nvulnerabilities that result in authentication or authorization bypass,\nsensitive data exfiltration, data injection, or request forgery. We are\nespecially interested in vulnerabilities that allow an attacker to impersonate\na business owner, escalate account privileges within a business page\n(e.g., upgrade an employee account to an admin account), modify ad spending,\nobtain non-public or bulk data sets that ought to be restricted to the business\nowners, or obtain non-public or bulk information about Yelp users’ interactions\nwith a particular business. Mobile Apps Android: Yelp , Yelp for Business Owners . iOS: Yelp , Yelp for Business Owners . Backend: auto-api.yelp.com, mobile-api.yelp.com, biz-app.yelp.com. Purpose: Our consumer apps help users find great local businesses while\non the go. The biz apps offer a bundle of free tools that enable business\nowners to advertise their businesses and connect with the Yelp community. In the most recent quarter content (reviews and photos) on Yelp was predominantly\ngenerated on our mobile apps; searches on Yelp, by and large, came from mobile\ndevices. Thus, we’re dedicated to ensuring the security of our iOS and Android\napps. Under the hood: The backend API is written in Python. Our iOS apps are\nwritten in Objective-C and Swift, and integrate a number of libraries via\nCocoaPods. Our Android apps are written in Java and integrate libraries via\nMaven, including Glide for image loading, Apache’s HTTP client for web requests,\nand Android Priority Job Queue for high priority jobs. Several components of our\napps use WebViews. Always test against the latest mobile app that is currently\navailable on Google Play, for Android, or the App Store for iOS. What to look for: In this category, we are most interested in\nmobile-specific vulnerabilities. Look for insecure storage of data, insecure\nWebView configs, insecure network connections, sensitive data disclosure via\nlogs/errors, privilege separation, etc. Vulnerabilities that allow tracking\nlarge number of users in real time are also considered high-severity issues. Yelp Reservations Location: www.yelpreservations.com , Restaurant Manager iOS app . Purpose: Yelp Reservations is an online management system for\nrestaurants, bars and nightlife venues that provides floor management and\nonline reservations. Restaurant managers use Yelp Reservations via a dedicated\nmobile app or via a web dashboard. Consumers use Yelp Reservations indirectly whenever they reserve a table at local businesses via our Yelp platform or\none of our partners. Under the hood: Python, Django (on the Web), Objective-C (mobile app). What to look for: Web vulnerabilities such as XSS, CSRF, SQLi, etc. are\nall in scope. We are also interested in any mobile vulnerability you find in\nour mobile app. Engineering Blog, The Yelp Blog Location: engineeringblog.yelp.com , yelpblog.com . Purpose: We use our engineering blog to notify the general public about\nall the cool technology we are developing here at Yelp. The Yelp Blog is the\nofficial voice of Yelp HQ. We use it to talk about news, product, community,\nbusiness, etc. Under the hood: Jekyll, Ruby, PaaSTA (Engineering Blog); PHP, WordPress\n(The Yelp Blog). What to look for: Vulnerabilities that enable attackers to add, delete or\nmodify any of the content on the engineering blog. We are also interested in\ndisclosure of sensitive information via path traversal and vulnerabilities in\nthe authentication component of the system. Public API Location: api.yelp.com . Purpose: We recently released our Public API v3 in developer preview mode. This API aims to enable third-party developers to\nbuild great mobile and web apps on top of our data. With API v3, developers can\nprogrammatically search for great local businesses, retrieve review excerpts,\nobtain business specific data such as address, phone number and photos. While most of our effort going forward will be focused on the Public API v3,\nits predecessor - Public API v2 - will continue to exist. Our API v2 supports\ngeographically-oriented search, searching for businesses offering a Yelp Deal,\nidentifying businesses that have been claimed on Yelp, etc. Under the hood: Python, Pyramid, uWSGI. What to look for: Focus on authentication bypasses, rate limiting issues\nand the ability to obtain large number of full-length reviews. We are also\ninterested in data injection attacks that may alter the internal state of our\ndata stores or leak sensitive information to malicious users. Yelp Support Location: www.yelp-support.com . Purpose: We use the Support Center to provide answers to frequently asked\nquestions in categories such as searching on Yelp, managing your user profile,\nmanaging your business presence, acquiring and maintaining an Elite status, etc. Under the hood: Salesforce’s Service Cloud Platform. What to look for: We are interested in any vulnerability that allows an\nunauthorized modification of content. The security team at Yelp is committed to keeping our users, our data, and our\nplatform and services safe and sound. If you find a security issue in any of our\nsystems, let us know immediately . We are ready to\nwork with you and make every effort to address the identified vulnerability in\na timely manner. Happy hacking! Tweet Back to blog", "date": "2016-09-06"}, {"website": "Yelp", "title": "PaaStorm: A Streaming Processor", "author": ["\n        \n  Matt K., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/paastorm-a-streaming-processor.html", "abstract": "This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Trouble in Paradise Back in 2010, Yelp open-sourced MRJob , a framework to run big MapReduce jobs on top of AWS infrastructure. Engineers at Yelp used MRJob to support everything from ad delivery to translation. MRJob proved to be a powerful tool for computation and aggregation on our existing rich dataset. Unfortunately, as the number of services using MRJob expanded, running and scheduling jobs became more complex. Since most jobs depended on other upstream jobs, the whole system had to be arranged in a topology. MapReduce jobs were never designed to be real-time, so the topology had to be scheduled daily. What’s worse, if an upstream job failed, downstream systems would break, and backfilling bad data required an intrepid detective to determine exactly which jobs had to be rerun, and in what order, before data could be updated. Inquiring minds started to inquire: how can we perform computations and transformations in a more efficient way? In particular, we’d like to support dependencies between different data transformations in a complex flow, where schema changes and upstream issues are handled gracefully. We’d also like to have the system run at (or close to) real-time. That way, the system can be used for business analytics and metrics. In other words, we need a streaming processor. An off-the-shelf solution like Storm , the computation system, would have been ideal. But since Python isn’t well-supported by many stream-processing frameworks, integrating the rest of our backend code with Storm or other off-the-shelf systems would have been painful. We built Pyleus first–an open-source framework that aimed to let developers process and transform data in Python. Pyleus still relied on Storm under the hood. Build times were long; the topology was slow. Twitter Heron had been announced, and we’d been seeing many of the same issues that they cited. We didn’t want to run dedicated Storm clusters if we could rely instead on PaaSTA , Yelp’s Platform-as-a-Service to deploy services, which is much more versatile. In July of 2015, a group of engineers began work on a new version of our Data Warehouse, which was facing the typical issues of scaling and performance. At first they wanted to use Pyleus to transform data in preparation for copying it to Redshift. Then they realized that deploying a full Storm cluster to run simple Python logic was unnecessary–a Python-based stream processor, deployed using Yelp’s in-house platform for launching services, would suffice. The design for the processor was based on Samza , aiming to provide a simple interface for transformations with a “process message” method. The engineers used Hackathon 17 to build a prototype for the stream processor running on PyPy, and thus PaaStorm was born. What’s in a Name? PaaStorm’s name is a mashup of PaaSTA and Storm. So what does PaaStorm actually do? To answer that, let’s go back to the basic architecture of the Data Pipeline: Focusing on the “Transformer” step, we can see that most messages stored in Kafka aren’t ready to be loaded into a target yet. Consider a target Redshift cluster storing data for ad delivery. The ad delivery cluster might want only a single field from an upstream source (say, the average rating of a business), or it might need to perform some aggregation or computation on the data. It would waste storage space and decrease performance if the ad delivery Redshift cluster had to store the entire upstream data source. In the past, service owners wrote complex MRJobs to transform their data before inserting it into their target datastore. However, those MRJobs faced the performance and scaling issues explained above. One of the benefits provided by the Data Pipeline is that a consumer can have data in exactly the form it needs, regardless of how the upstream data source used to look. Reducing Boilerplate In principle, we could let each individual consumer figure out their own transformations to get data in the form they want. For example, the ad delivery team could write a transformer that strips out review averages from business data in Kafka, and then they could own that transformation service as well. That strategy works fine at first, but eventually we hit problems of scale. We wanted to provide a transformation framework for a few reasons: Many transformations are common and can be used by multiple teams (e.g., expanding flags into meaningful columns). There is a lot of boilerplate code surrounding transformations (e.g., connecting to and from data sources and targets, saving state, monitoring throughput, recovering from failure) that need not be copied in each service. Transforming data needs to happen as fast as data is coming in (i.e., on a stream basis), in order to maintain a real-time system for data processing. The natural way to reduce boilerplate is to provide an interface for transformations. The interface defines the minimum amount of logic needed to specify a transformation. Then, the rest of the work is taken care of by our streaming framework. Kafka as a Message Bus PaaStorm started off specifically as a Kafka-to-Kafka transforming framework, although it has since grown to allow other endpoints. Using Kafka as the endpoints for PaaStorm makes things simpler: each interested service can register as a consumer to any transformed or raw topic, without regard to who “created” a transformed topic, reading new messages as they arrive. The transformed data is persisted according to Kafka’s retention policy. Since Kafka is a pub-sub system, downstream systems can consume data whenever they’re ready. Taking the World by Storm So how do we visualize our set of Kafka topics, once PaaStorm has entered the picture? Since some topics feed into other ones in a source-to-target way, we can think of our topology as a directed acyclic graph: Each node represents a Kafka topic; arrows represent data transformations provided by PaaStorm. The name ‘PaaStorm’ should make more sense now: similar to Storm, PaaStorm provides real-time transformation from stream sources (like Spouts) via transformation components (like Bolts). The Eye of the Storm (or, PaaStorm Internals) PaaStorm’s core abstraction is called a Spolt (a cross between Spout and Bolt). As the name suggests, the Spolt interface provides two important things: a source for incoming messages, and a way to process the messages in that source in some way. At its most basic level, a Spolt looks like this: class UppercaseNameSpolt(Spolt):\n        “““Pseudocode implementation of a simple Spolt.”””\n\n        spolt_source = FixedTopics(“refresh_primary.business.abc123efg456”)\n\n        def process_message(message):\n            new_message_data = message.payload_data\n            new_message_data[‘uppercase_name’] = new_message_data[‘name’].upper()\n            yield CreateMessage(\n                schema_id=2,\n                payload_data=new_message_data\n            ) This Spolt takes each message in the topic “refresh_primary.business.abc123efg456” and adds an extra field, an uppercased version of the ‘name’ column in the original message. The Spolt then passes on the new version of the message. It’s worth noting that messages in the Data Pipeline are immutable. In order to yield a modified message, a new object has to be created. Also, because we’re adding a new field to the payload (i.e., the message is gaining an ‘uppercase_name’ column), the schema for the new message is different. In production, a message’s schema_id would never be hardcoded. Instead, we rely on the Schematizer service to register and provide the proper schema for a modified message. One last note: the Data Pipeline clientlib provides several nuanced ways of setting a ‘spolt_source’ with combinations of namespaces, topic names, source names, and schema_ids. That way it’s easy for a given Spolt to group together exactly the sources it needs and read from only those. For more detail, check out the Schematizer blog post . But Wait, What Happened to Kafka? You’ll notice that there is no code that actually interacts with Kafka topics in the Spolt above. That’s because in PaaStorm, all of the actual Kafka interfacing is handled by a runner instance (which is just called PaaStorm ). The PaaStorm instance takes a Spolt and hooks it up to the proper sources and targets, takes care of feeding messages to the Spolt, and then publishes the messages that the Spolt yields to the correct topic. Each PaaStorm instance is initialized with a Spolt. For example, the following command begins a process using the UppercaseNameSpolt from above: PaaStorm(UppercaseNameSpolt()).start() That means that someone interested in writing a new transformer can simply write a new subclass of Spolt, without having to modify the PaaStorm runner at all. Internally, the PaaStorm runner’s main method is surprisingly simple. It looks like this (pseudocode): with self.setup_counters(), Producer()\nas producer, Consumer()\nas consumer:\n    while self.running:\n        message = consumer.get_message()\n        if message:\n            self.increment_consumer_counter()\n            for downstream_message in spolt.process_message(message):\n                producer.publish(downstream_message)\n                self.increment_producer_count() The runner starts off with some set-up: it initializes its producer and consumer, along with message counters. Next, it polls for a new message in the upstream topic. If it finds one, that message is processed through the Spolt. The Spolt in turn yields one or more transformed messages, which the producer publishes to the downstream topic. A quick side-note: the PaaStorm runner also provides some utilities like consumer registration, and a heartbeat called “tick”–that way, if the Spolt needs to do something like flush its contents every so often, the tick can trigger it. On State Saving PaaStorm guarantees reliable recovery from failure. If there’s a crash, we want to restart by consuming from the correct offset. But, unfortunately, that offset is not necessarily the last message we consumed from the upstream topic. That is, we might have consumed some messages, but not actually published the transformed version of those messages. The right place to restart is actually the place in the upstream topic corresponding to the last successful downstream message. Given the last message downstream, we have to figure out which upstream message it came from so we can restart from there. To facilitate this, whenever PaaStorm’s Spolt processes a raw message, PaaStorm injects the Kafka offset of that raw message into the last transformed message. The transformed message, in turn, passes that offset to a callback in the producer. That way, we know which upstream offset corresponds to the furthest-along message in the downstream topic. Since the callback is only triggered when the producer successfully publishes the messages, the consumer can reliably commit the offset in the callback, knowing that the raw message has been successfully processed. If we crash, we can then restart from the upstream messages that have not been fully processed. As seen in the above pseudocode, PaaStorm also counts the number of messages consumed and the number published. That way, interested users can check throughput on the upstream and downstream topics. This gives us free monitoring and performance checks for any given transformation. At Yelp, we send our stats to SignalFX : A SignalFX graph showing throughput for a producer and consumer in a Paastorm instance. In this case, incoming and outgoing messages are not matched. One benefit of having separate counters for both the producer and consumer in PaaStorm is that we can overlay their throughputs to see where bottlenecks are occurring. Without this granularity, it can be hard to understand performance in the pipeline. The Future of PaaStorm PaaStorm provides two things: an interface and an implementation of a framework to support that interface. Although we don’t expect the interface of PaaStorm to change any time soon, there are some exciting incubating projects that aim to solve the transformation-and-connection problem. In the future, we’d love to replace the internals of PaaStorm with Kafka Streams or Apache Beam –the main blockers are the extent of Python support and support for the endpoints we care about most. Overall, we think of Paastorm as a bridge for Yelp to use until open-source Python stream-process projects mature. Next Time on our Show We’ve discussed how PaaStorm provides real-time transformation of data from sources into targets. PaaStorm was originally written as a Kafka-to-Kafka system. However, many internal services are not set up to directly consume from Kafka–instead, they load data into a datastore like Redshift or MySQL and perform service magic from there. Even after a transformation to get data in the right format, there’s a further connection step needed: the data has to actually be uploaded to a target data store. As noted above, PaaStorm’s Spolt interface contains no reference to Kafka. In fact, with only a bit of modification, the Spolt can directly publish messages to targets other than Kafka. In a subsequent post, we’ll talk about Yelp’s Salesforce Connector–a service that uses PaaStorm to load data from Kafka into Salesforce in a high-volume, high-performance way. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-08-25"}, {"website": "Yelp", "title": "Data Pipeline: Salesforce Connector", "author": ["\n        \n  Ian F., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/09/data-pipeline-salesforce-connector.html", "abstract": "This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Yelp uses Salesforce, a customer relationship management (CRM) platform, for our 2000+ person sales team. Salesforce provides a range of out of the box features that make it easy to allow our sales leadership team to customize their business processes. What does our sales team do? They sell advertising packages! Who do they sell them to? Businesses on Yelp! So how do we get that business information from Yelp’s databases  to Salesforce’s? You’re about to find out. Previous approach Our existing one-way sync infrastructure, dubbed “Bulk Workers” was designed back in 2010 in order to dramatically improve the time it took to send data end-to-end. That design was able to improve sync times from 3 weeks down to 24 hours. Impressive! So what was this approach? These “Bulk Workers” were cron’d Gearman workers that would crawl through each row in our Business table and transform the data to conform with our Salesforce schemas. These workers would then pass data off to Salesforce using a Salesforce client based on Beatbox that was modified to add support for the Salesforce Bulk API . Legacy integration diagram And all was well… until it wasn’t. Over the next five years, this infrastructure saw the number of rows synced grow from around 30MM to >100MM across 12 other tables in 2015. The impact on data freshness was significant. Eventually the sync times started to get worse and worse. We knew we had to change our systems to support faster updates. Enter: Data Pipeline So we began to gather requirements. We determined we needed a solution with: Real time processing An at least once guarantee Built-in monitoring and alerting Configuration driven transformations between schemas The ability to easily add new fields and transformations Around the same time, we had already begun working on our new Data Pipeline leveraging Kafka , a distributed pub/sub messaging system. This pipeline would provide us the first three requirements “out of the box.” All we had left to build was a transform framework that would fulfil the last two requirements and a connector to Salesforce.com. Salesforce Pipeline integration diagram Transformers We used a former Yelp Hackathon project that was productionised as the basis for our Kafka-to-Kafka processor, named PaaStorm due to its similarities to Storm and being deployed using Yelp PaaSTA . Keeping with the paradigm of Storm, we built a generic transformer and then spun up a multiple instances instances to consume from each topic that had source data we wanted to get into Salesforce. Using the source topic, each instance would look up the transformation steps from a YAML file and then perform copying, moving, and/or mapping values. This was important because the Salesforce schema predated our new infrastructure and could not be easily changed. This also meant there was no automatic way to map many of the fields. Having a config-driven mappings allowed us quick iterations on our transforms without the need for code deployments. This was critical to the project’s viability. Each transformer would then publish a serialized Salesforce object into a new Kafka topic for our uploader to consume and then send that object to Salesforce. Uploader Having the uploader live as its own instance allows us to isolate how much of our service needs to be informed about communication to Salesforce.com. The uploader instance consumes transformed messages from each transformer and batches them up before sending requests to Salesforce. Since requests to Salesforce require reaching out into the Internet, this is one of the slowest parts of our pipeline. Batching efficiently is critical to performance. It is also important to use the ideal API. This can sometimes be hard when working with the various APIs that Salesforce provides. In order to make it easy for us to switch between APIs without extra work, we wrote a unified client that wraps existing Python clients for the SOAP, REST, and Bulk APIs. We also wrote an Object-relational Mapping (ORM) client and defined models for each of the tables we wish to write to. This allows us to validate data before it’s sent to Salesforce.com as well as identify which Salesforce External ID to use when writing. Evaluation The first table that we targeted was our Advertiser table. This was chosen since it’s smaller subset of our total businesses on Yelp and it is critical for our sales team to operate effectively. It had been taking 16 hours to sync changes from Yelp to Salesforce. When we launched our new infrastructure we saw that drop to an average of 10 seconds with spikes sometimes only as high as a few minutes! This meant that the data in Salesforce was reliable enough that our sales team doesn’t have to constantly request updated data whenever they view a record. Average Sync Latency in Seconds Challenges Wasn’t that easy? Well, not quite. During the process of designing and building our connector we had to be sure to solve for several problems. Since going live, we noticed that a good portion of our failed updates were due to requests that timed out in Salesforce or were rejected due to failing to get a lock for a required row. Both of these can be attributed to the heavy usage of triggers and rollups in our Salesforce instance. Nearly every table has very complicated logic that is executed with every write to ensure consistency across various records or to automate certain business processes. These features are great until you start hitting these kinds of problems. We are working now to reduce the amount of processing we do during write operations. Moving as much as we can to asynchronous processes will reduce the amount of time we need to lock a row and reduce the amount of processing done for every write. Another problem that we have had to solve is dependency resolution. While our original data sources (MySQL) has constrained dependencies, Kafka does not. While messages written to individual Kafka topics are guaranteed to be in order, we cannot guarantee that the topics will be read at a specific or consistent rate. In the case of tables with dependencies on each other this is problematic because a one table may be read and updated before the other, causing data to go out of sync for periods of time. A common example is with our Advertiser records coming in shortly before a User. Since the Advertiser record contains a Lookup Field (aka. Foreign Key) to the User table, the write will fail. This requires us to keep track of records that fail due to missing dependencies and then retry them when their dependency is seen by the uploader. Serializing our uploads in dependency orders and handling retries covers most of our use cases, though it means we cannot achieve a high degree of parallelism. Our other problem is that not all data exists in a single database row, yet the data that is readily available to us is of single data rows. To resolve this we are building new functionality to read two topics, join them, and republish the joined data. Conclusion We’ve seen huge improvements using our Kafka-backed data pipeline in getting data in front of our sales teams. Our next steps are building up our infrastructure so we can perform additional transformations, simple aggregations, and higher reliability when writing to Salesforce. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-09-27"}, {"website": "Yelp", "title": "Autoscaling PaaSTA Services", "author": ["\n        \n  Matt S., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/05/autoscaling-paasta-services.html", "abstract": "If you haven’t heard about PaaSTA before, feel free to check out the blog post introducing\nit . One step in creating a service is to decide how many compute resources it\nneeds. From the inception of PaaSTA, changing a service’s resource allocation\nhas required manually editing and pushing new configs, and service authors had\nto pore over graphs and alerts to determine the proper resource allocation for\na service whenever load requirements changed. This changed earlier this month\nwhen autoscaling was introduced into PaaSTA. Why did we do this? Autoscaling was introduced into PaaSTA to make sure services are allocated an\nappropriate amount of resources as their load changes. Many services at Yelp\nhave large differences in how many resources they need throughout the day.\nPreviously we’ve dealt with this by overprovisioning – giving services the\nresources they need to operate at peak load, even if they’re only using a\nfraction of that most of the time. Autoscaling allows PaaSTA to free up\nresources from services that aren’t using them and either give those resources\nto services that need them or scale down our clusters and cut our\ninfrastructure bill. Alternatively, if usage increases we can prevent\nperformance degradation by automatically allocating more resources to a\nservice. How did we do this? One of our goals when developing autoscaling was to make it as easy as possible\nfor service authors to migrate to autoscaling. When a service author enables\nautoscaling, they only need to specify the minimum and maximum number of\ninstances for their service. Here’s an example of a PaaSTA service’s config file with autoscaling enabled: ---\nmain:\n  cpus: 1\n  mem: 1024\n\n  # All that's required to enable autoscaling\n  min_instances: 3\n  max_instances: 12 That’s it! PaaSTA then chooses sane defaults for the other autoscaling parameters: the metrics_provider and the decision_policy . The metrics_provider tells the\nautoscaler how much a service is under load. The decision_policy code takes\nload information from a metrics_provider and determines how many instances a\nservice should be running. If a service author wants more control, they can\noverride PaaSTA’s defaults by specifying a metrics_provider and decision_policy in their service configs. They can also inject additional\nautoscaling\nparameters by adding them to their service configs. Here’s an example of a customized service config: ---\nmain:\n  cpus: 1\n  mem: 1024\n  min_instances: 3\n  max_instances: 12\n\n  # Advanced configuration\n  autoscaling:\n    metrics_provider: http\n    endpoint: metrics.json\n    decision_policy: threshold\n    setpoint: 0.5 If the autoscaler decides to scale a service, the new instance count is written\nto Zookeeper and an event is logged describing why the service was scaled. The\nnext time PaaSTA updates Marathon , it\nreads the updated instance count and scales the corresponding Marathon app. As\nthe app scales, Smartstack handles service discovery by registering or\nde-registering instances. Our Sensu replication alerts and other tools are also\nautomatically updated to use the new instance counts specified in Zookeeper. Above is an example of PaaSTA dealing with a spike in CPU usage for a service.\nThis service uses a metrics_provider that emits the average cpu utilization for a service and a decision_policy that uses a PID controller to control the instance count. Bespoke autoscaling methods But what happens if a service’s utilization isn’t accurately represented by any metrics_provider ? For example: a queue worker might want to scale based on\nthe change in its queue length over time. Since the instance count for a\nservice is just a number written in Zookeeper, PaaSTA can get autoscaling\nsignals from custom external sources. Service authors can specify that they\nwant to use the 'bespoke' decision_policy and PaaSTA will skip its\ninternal autoscaling code and instead respond to external autoscaling signals\nin Zookeeper. Why can’t you just use AWS autoscaling (or something similar)? Let’s compare PaaSTA autoscaling to Amazon’s ECS\nautoscaling .\nBoth of these technologies can scale Docker containers, and AWS has CloudWatch\nas a very powerful monitoring and metrics solution. Why didn’t we just use ECS?\nThe biggest difference between the two is that PaaSTA autoscaling is\ninfrastructure-agnostic (just like PaaSTA itself) while ECS autoscaling\noperates only on Amazon ASGs. ECS autoscaling also creates and destroys EC2\nservers, while PaaSTA autoscaling only changes the number of services that are\nrunning on existing servers. Finally, ECS autoscaling uses fairly simple logic for determining to scale up\nor down, checking to see if a metric passes above or below a static threshold\nfor a certain amount of time. PaaSTA autoscaling has multiple decision_policy that provide different methods of control: some can do simple threshold-based\nlogic, while other decision_policy use more complicated control systems such\nas PID controllers to achieve\nbetter control at the cost of increased complexity and reduced transparency. Conclusion PaaSTA autoscaling allows us to ensure services stay performant and cost-efficient even as new code is shipped or their utilization changes. It’s\nflexible enough to support many of the hundreds of services we have at Yelp;\nwidespread adoption across our services will greatly increase the elasticity of\nour infrastructure. If you want to learn more about PaaSTA, or see the source\nfor autoscaling, check out our Github . Stay\ntuned for a future blog post where we use the same concepts to autoscale\nclusters of virtual machines. Tweet Want to help cook PaaSTA? Like building this sort of thing? At Yelp we love building systems we can be proud of, and we are proud of PaaSTA. Check out the Site Reliability Engineer positions on our careers page if you like building systems you can be proud of too! View Job Back to blog", "date": "2016-05-25"}, {"website": "Yelp", "title": "Monitoring Cassandra at Scale", "author": ["\n        \n  Joseph Lynch, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/06/monitoring-cassandra-at-scale.html", "abstract": "At Yelp we leverage Cassandra to\nfulfill a diverse workload that seems to combine every consistency and\navailability tradeoff imaginable. It is a fantastically versatile datastore,\nand a great complement for our developers to our MySQL and Elasticsearch\nofferings. However, our infrastructure is not done until it ships and is\nmonitored . When we started deploying Cassandra we immediately started looking\nfor ways to properly monitor the datastore so that we could alert developers\nand operators of issues with their clusters before cluster issues became\nsite issues. Distributed datastores like Cassandra are built to deal with\nfailure, but our monitoring solution had to be robust enough to differentiate\nbetween routine failure and potentially catastrophic failure. Monitoring Challenges We access Cassandra through our standard NoSQL proxy layer Apollo ,\nwhich gives us great application level performance and availability metrics for\nfree via our uwsgi metrics framework. This immediately gives us the capability to alert developers when\ntheir Cassandra based application is slow or unavailable. Couple these pageable\nevents with reporting the out of the box JMX metrics on query timing to customers and the monitoring story is starting\nto look pretty good for consumers of Cassandra. The main difficulty we faced was monitoring the state of the entire database\nfor operators of Cassandra. Some of the metrics exposed over JMX are useful to operators. There are a lot of good resources online to learn which JMX metrics are most relevant to operators, so I won’t\ncover them here. Unfortunately, most of the advanced cluster state monitoring\nis built into nodetool which is useful for an operator tending to a specific cluster but does not\nscale well to multiple clusters with a distributed DevOps ownership model where\nteams are responsible for their own clusters. For example, how would one\nrobustly integrate the output of nodetool with Nagios or Sensu ? OpsCenter is\ncloser to what we need, especially if you pay for the enterprise edition, but\nthe reality is that this option is expensive, does not monitor ring health in\nthe way we want and does not (yet) support 2.2 clusters. We need to be able to determine a datastore is going to fail before it\nfails . Good datastores have warning signs, but it’s a matter of identifying\nthem. In our experience, the JMX metrics monitoring technique works well when\nyou’re having a performance or availability problem isolated to one or two\nnodes. The technique falls flat, however, when trying to differentiate between\nan innocuous single node failure impacting a keyspace with a replication factor\nof five and a potentially critical single node failure impacting a keyspace\nwith a replication factor of two. Finding Cassandra’s Warning Signs Cassandra uses a ring topology to store data. This topology divides the\ndatabase into contiguous ranges and assigns each of the ranges to a set of\nnodes, called replicas. Consumers query the datastore with an associated consistency\nlevel which indicates to Cassandra how many of these replicas must participate when\nanswering a query. For example, a keyspace might have a replication factor of\n3, which means that every piece of data is replicated to three nodes. When a\nquery is issued at LOCAL_QUORUM , we need to contact at least ⅔ of the\nreplicas. If a single host in the cluster is down, the cluster can still\nsatisfy operations, but if two nodes fail some ranges of the ring will become\nunavailable. Figure 1 is a basic visualization of how data is mapped onto a single Cassandra\nring with virtual nodes. The figure elides details of datacenter and rack\nawareness, and does not illustrate the typically much higher number of tokens\nthan nodes. However, it is sufficient to explain our monitoring approach. Figure 1: A Healthy Ring In this case we have four nodes, each with three “virtual” nodes (a.k.a. vnodes ) and the keyspace has a replication factor of three. For the sake of\nexplanation, we assume that we only have twelve ranges of data and data\nreplicates to the three closest virtual nodes in a clockwise fashion. For\nexample, if a key falls in token range 9, it is stored on Node A , Node B and Node C . When all physical hosts are healthy, all token ranges have all\nthree replicas available, indicated by the threes on the inside of the ring.\nWhen a single node fails, say Node A , we lose a replica of nine\ntoken ranges because any token range that would replicate to Node A is\nimpacted. For example, a key that would map to token range 8 would typically\nreplicate to Node D , Node A and Node B but it cannot replicate to to Node A because Node A is down. This is illustrated in Figure 2. Figure 2: Single Node Failure At this point we can still execute operations at LOCAL_QUORUM because ⅔ of\nreplicas are still available for all token ranges, but if we were to lose\nanother node, say Node C , we would lose a second replica of six token ranges\nas shown in Figure 3. Figure 3: Two Node Failures This means that any key which exists on those six token ranges is unavailable\nat LOCAL_QUORUM , while any keys not in those ranges are still available. This understanding allows us to check if a cluster is unavailable for a\nparticular consistency level of operations by inspecting the ring and verifying\nthat all ranges have enough nodes in the “UP” state. It is important to note\nthat client side metrics are not sufficient to tell if a single additional node\nfailure will prevent queries from completing, because the client operations are\nbinary: they either succeed or not. We can tell they are failing, but can’t\nsee the warning signs before failure. Monitoring Cassandra’s Warning Signs Under the hood, nodetool uses a JMX interface to retrieve information like\nring topology, so with some sleuthing in the nodetool and Cassandra source\ncode we can find the following useful mbeans : type=StorageService/getRangeToEndpointMap/{keyspace} type=EndpointSnitchInfo/getDatacenter/{node} type=StorageService/Keyspaces type=StorageService/LiveNodes In order to programmatically access these mbeans we install jolokia on all of our Cassandra clusters. An HTTP\ninterface to Cassandra’s mbeans is extremely powerful for allowing quick\niteration on automation and monitoring. For instance, our monitoring script can\nbe as simple as (pseudocode): 400: Invalid request Often our applications using Cassandra read and write at LOCAL_ONE , which\nmeans that we can get robust monitoring by deploying the above check twice: the\nfirst monitoring LOCAL_ONE that pages operators, and the second monitoring LOCAL_QUORUM that cuts a ticket on operators. When running with a replication\nfactor of three, this allows us to lose one node without alerting anyone,\nticket after losing two (imminently unavailable), and page upon losing all\nreplicas (unavailable). This approach is very flexible because we can find the highest consistency\nlevel of any operation against a given cluster and then tailor our monitoring\nto check the cluster at the appropriate consistency level. For example, if the\napplication does reads and writes at quorum we would ticket after LOCAL_ALL and page on LOCAL_QUORUM . At Yelp, any cluster owner can control these\nalerting thresholds individually. A Working Example The real solution has to be slightly more complicated because of\ncross-datacenter replication and flexible consistency levels. To demo this\napproach really works, we can setup a three node Cassandra cluster with two\nkeyspaces, one with a replication factor of one (blog_1) and one with a\nreplication factor of three (blog_3). In this configuration each node has the\ndefault 256 vnodes. 400: Invalid request We like to write our monitoring scripts in Python because we can integrate\nseamlessly with our pysensu-yelp library for emitting alerts to Sensu , but for this\ndemo I’ve created a simplified monitoring script that inspects the ring and exits with a status code that conforms to the Nagios Plugin API .\nAs we remove nodes we can use this script to see how we gradually lose the\nability to operate at certain consistency levels. We can also check that the\nnumber of under-replicated ranges matches our understanding of vnodes and\nreplication: 400: Invalid request Now it’s just a matter of tuning the monitoring to look for one level of\nconsistency higher than what we actually query at, and we have achieved robust\nmonitoring! An important thing to understand is that this script probably won’t “just work”\nin your infrastructure, especially if you are not using jolokia, but it may be\nuseful as a template for writing your own robust monitoring. Take it to 11, What’s Next? Our SREs can sleep better at night knowing that our Cassandra clusters will\ngive us warning before they bring down the website, but at Yelp we always ask,\n“what’s next?” Once we can reliably monitor ring health, we can can use this capability to\nfurther automate our Cassandra clusters. For example, we’ve already used\nthis monitoring strategy to enable robust rolling restarts that ensures ring\nhealth at every step of the restart. A project we’re currently working on is\ncombining this information with autoscaling events to be able to intelligently react to hardware failure in an automated fashion.\nAnother logical next step is to automatically deduce the consistency levels to\nmonitor by hooking into our Apollo proxy layer and updating our monitoring from\nthe live stream of queries against keyspaces. This way if we change the queries\nto a different consistency level, the monitoring follows along. Furthermore, if this approach proves useful long term, it is fairly easy to\nintegrate it into nodetool directly, e.g. nodetool health <keyspace> <consistency_level> . Tweet Back to blog", "date": "2016-06-01"}, {"website": "Yelp", "title": "Yelp Hackathon 19: Color Code", "author": ["\n        \n  Srivatsan S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/06/hackathon-19-color-code.html", "abstract": "One of the values that we cherish at Yelp is to “Be Unboring” . It’s the quality of never accepting “standard” as okay and the guiding principle for creating new and remarkable things. One of the ways we foster that creative spirit at Yelp Engineering is through our internal hackathons. Many remarkable projects have come out of them - some of them open sourced , some of them revealing interesting demographic insights and some of them pushing the boundaries of science & technology . The 19th edition of our internal hackathon wasn’t any different. Close to 80 fantastic projects across our engineering offices in San Francisco and Hamburg came out of the event. Here are some highlights: Clockwise from top:  Hackathon 19 t-shirts with an encrypted “color-code” design (can you decode it? :) ); folks expressing their artistic skills at a collaborative coloring area set up during hackathon; set-up for the hackathon kickoff mimosa toast; taking a break from the hacking to do some relaxing coloring Food Coma Yelp hosts tens of millions of photos uploaded by Yelpers from around the world. A lot of fun, useful and cool projects have been built on top of our rich photo data.  A team comprising of Shahid C., Keegan P. and Piyush K. decided to take it one step further - what if we took some of those photos and created cool, psychedelic dream sequences out of them? They put together an algorithm called a “dream filter” and applied it on photos of dishes from local SF restaurants to create sequences like this : To build the “dream filter”, they used the GoogleNet Caffe model from BVLC (Berkeley Vision and Learning Center) that was pre-trained on the ImageNet dataset. Their filter consisted of a forward-pass through a convolutional neural network up to a specified layer. They then set the gradients equal to the activations at that layer. That way, they could backpropagate through the network and then transform the original image accordingly. They passed in numerous photos through this filter and stitched up the individual frames together to create a single dream sequence. Because this process requires hundreds of frames, they used an AWS EC2 GPU (g2.2xlarge) instance to improve performance in Caffe. MrKafkaJob We run a lot of mapreduce jobs at Yelp, some on local Hadoop clusters and some on Amazon’s EMR framework. We also heavily rely on technologies like Kafka and Docker, with many of our systems using Kafka to transport data , and Docker for containerization of services (as part of PaaSTA , our open-sourced SOA platform). Would it be possible to leverage both of these technologies to replace Hadoop? Ryan I. set out to answer that question. He put together a workflow that pieces together Docker and Kafka, to create a new way of running mapreduce jobs. Docker compose creates an ephemeral Kafka cluster of desired scale, enabling mappers and reducers to communicate in parallel via partitioned Kafka topics. Turns out that it worked pretty well! Top row : Hacking - with code and with plants :)\nBottom row : Our engineers demo-ing their hackathon projects at our science-fair style presentation. Frontend Services on Python 3 We value our ability to quickly ship code. One key factor that has helped us maintain our iteration speed has been our move to a service-oriented infrastructure. Over the past couple of years, we’ve been breaking apart our monolith to create new frontend services which serve individual pages of our website. Many of these services use Python2. As the community moves towards adoption of Python3, there may be a growing need for us to switch our services to use Python3 instead. Driven by the curiosity to find how complex that switch would be, Anthony S. spent his hackathon porting a handful of modules of varying complexity from Python2 to Python3 with backward compatibility support. Some of those modules included pgctl (our playground controller to manage developer playgrounds), Cheetah (a templating engine), yelp_clog (Python package for logging to scribe), bravado (Python client library for Swagger ) and fido (an asynchronous HTTP client). Mechanical Unix Clock As engineers, we all love hardware hacks. Especially the ones that involve building something from scratch. And especially, the ones that involve Unix. A team comprising of Evan K., Joseph L. and Matthew S. came up with a clever hardware hack to keep track of unix time - they built a clock! Using our resident 3D printer at the office, the team printed out tiny gears that could interlock and spin each other. Each of these gears represented a digit in unix time. They then hooked up these gears to a motor calibrated to move the first gear to increment by a second. The subsequent gears would then move in lockstep to try to keep the unix time consistent. What happens when we hit the next leap second ? Hmm…only time will tell. Tweet Ready for Hackathon 20? Are those creative juices flowing through your head right now? Why don’t you come join forces as we embark on our next awesome Hackathon this summer! View Job Back to blog", "date": "2016-06-09"}, {"website": "Yelp", "title": "The Great HTTPS Migration", "author": ["\n        \n  Andrew E., Technical Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/09/great-https-migration.html", "abstract": "Yelp is now entirely on HTTPS! While several pages have been secured for quite some time (we began securing pages with sensitive information like passwords, credit card numbers, and even the reviews you submit, in 2008), we’ve finally made the transition to using TLS across the entirety of our website. To some, this will sound like quite the accomplishment while others may wonder why it took until mid-2016 to complete this migration. Why Now? Brief History of HTTPS Netscape created SSL in 1994 and by 2000, TLS became the default encryption protocol and the modern HTTPS spec was created. But it wasn’t until around 2010 that HTTPS began to gain traction on more than just login and payment pages. Facebook, for example, implemented HTTPS as an option in 2011 and by mid-2013 it was the default setting . Google added HTTPS to search as early as 2010 and continued adding it to their other services over the following years. By 2014 Google announced that HTTPS would be a minor factor in their search ranking algorithm. Advantages to Securing all Pages Yelp has always cared deeply about its users’ security. A core value at Yelp, encompassing more than just web security and borrowed from journalism, is “protect the source.” By serving all content over HTTPS, we can ensure that wherever and whenever action is taken on Yelp, it’s being done in the context of a TLS-secured environment. Additionally, with all pages on HTTPS, users would be able to login or submit reviews from any page, not just dedicated and secured pages for those specific actions. And since HTTPS sites don’t send referrers to HTTP pages by default, we’d actually see more of our referrers by transitioning. Even with the advantages there were a few things that prevented us from rolling out HTTPS across the entirety of the site. Unknowns While we knew the number of requests would increase, we didn’t know to what extent or for how long. Because we wanted to monitor any changes caused by the migration and because of the way our rollout toggle was set up (more on that below), we actually redirected at the server level where we could have more detailed logging, instead of at the load balancer. As a result, even increasing 301s could have a significant impact on operations load. The impact on traffic from search engines when suddenly issuing 301s for almost every URL in their indexes was also unclear. While Google now has official guidance that supports full site or per-URL transitions, it was unclear at the time what rollout strategies were supported by search engines. Blocking Issues Despite the fact that we already supported TLS on a technical level, rolling it out globally would still take a tremendous amount of developer effort. We had no tests written for the transition and we would need to ensure all XHR requests, internal links, assets, metadata and more were all fully transitioned at the same time as well. Complicating matters further, issuing permanent redirects made it virtually impossible to roll back the changes if something went wrong. The only real technical blocker, however, was our use of display advertisements, which didn’t universally support HTTPS and as a result securing all pages would have had a meaningful impact on revenue. In late 2015 however, we ended our display advertising business, removing this barrier entirely, and so it became time to take the leap and make the transition. What It Took Rollout Strategy We decided to roll out HTTPS to logged-in users first, since doing so wouldn’t affect search engine crawling. Ensuring we hadn’t overlooked any mixed content or HTTP endpoints, we’d continue on a per-TLD basis to avoid any potential search engine issues with having a domain sharded across the two protocols. We chose to start in Canada (a good proxy to the US market, but on a smaller scale) and continued from there once we had more information on the impact. Development Strategy In order to be sure we had converted everything correctly, we decided to take a test driven development approach. We wrote tests to catch, among other things, any mixed content warnings or HTTP links on the page. These tests then allowed us to analyze failures to determine what still needed to be converted. We then migrated page by page, shipping the new code to production behind a toggle that served the HTTPS pages only to logged in users. It was important to make sure that changes were roll forward safe. When the changes were deployed, several users still had stale pages (with HTTP forms and links) and so the new form post and XHR endpoints needed to support HTTP urls for a period of time after the flip. Our testing strategy allowed us to catch the few mixed content endpoints that our tests missed before beginning the bigger roll out to all our users. Flipping Canada in March At Yelp, we have a variety of tools to monitor both user traffic and search engine crawl in real time. Within moments of enabling for logged out users, we saw many of the changes we expected. Overall requests to the site increased 50% (and more than doubled for non-XHR requests). Some of that increase was from our users coming into the site with 301s, and a lot of it was Googlebot and Applebot immediately increasing their crawl rates. Crawl on HTTPS began only a minute after the flip, and for weeks we saw meaningful crawl to both HTTP and HTTPS urls (graph 2 below shows this for the US, which had similar crawl patterns). But the most interesting thing we learned was how long these changes lasted. It actually took about two weeks for bot traffic to peak. Google also updated its index slowly, with about half of our incoming traffic from Google still being directed to HTTP a month after the flip. Still, the transition had been a success. We saw no hit to our traffic from search engines, no mixed content warnings, and everything seemed to be pointing in a positive direction. The only real change in our initial plan was the amount of time between country rollouts, as the increased traffic took a little longer to reach equilibrium than we expected. It took us four weeks before we felt confident in migrating the next set of countries. Flipping the US in August By June we had completed the rollout to all countries except the US and the results had been more or less the same as with Canada. Because the US constitutes the majority of our traffic, we wanted to know how long the increased server load would last since search engines would be crawling every link resulting in millions of 301 redirects. As a result, we waited a couple months until bot crawl stabilized and users from Google were largely being directed to HTTPS pages receiving 200s before launching. When we did make the change, we were pleasantly surprised by the results. While Google was crawling us a bit more than they had when we rolled out the other TLDs, they were also updating their index far faster. Within only a few days, the majority of users were being referred straight to HTTPS (graph 1 below), and we had only a fraction of the 301s we’d seen in the previous stages. Graph 1: US requests referred by Google and split by status code. Graph 2: Googlebot crawl rates in the US and split by scheme. Lessons Learned Most importantly, we saw almost no change in search engine traffic whatsoever. It’s still early, and we will be monitoring this closely in the future, but search engines did not seem to either penalize or reward us at all for this mass migration. This is obviously good news for anyone considering migrating that receives lots of traffic from search engines. This also means that if you’re tempted to make the change solely because Google announced that they take it into account in their search algorithm, it would probably be wise to temper your expectations a bit at this point in time. Armed with this knowledge, Google’s guidance on migrating per-URL, and with overall requests increasing only about 150%, we probably could have skipped the toggle and tempered rollout and instead migrated the entire site either all at once, or as we completed work. We did have some fallout from the migration as well. Due to the way we had set up our page cache headers, most of our HTTP pages were being cached by browsers whereas our HTTPS pages were not. We overlooked this at first, and saw some discrepancies in some internal metrics as a result. Still, outside of a few alerts being set off, there wasn’t really anything wrong. Additionally, since we were now HTTPS, we were no longer sending referrers to HTTP pages by default, and needed to implement a new way to send information to some of our businesses and partners to let them know what traffic was referred to them by Yelp (this can be done easily with a meta referrer tag ). Nevertheless, we’re happy with the result, and proud to say that we are fully HTTPS at Yelp! Tweet Back to blog", "date": "2016-09-15"}, {"website": "Yelp", "title": "Yelp API v3 Developer Preview", "author": ["\n        \n  Tomer Elmalem, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/07/announcing-yelp-api-v3-developer-preview.html", "abstract": "For the past few months we’ve been working on revamping our API based off your feedback of wanting more Yelp data and functionality. Today, we’re excited to announce that the newest version of our API is entering developer preview . What’s new? We’re exposing two new features as part of the developer preview: autocomplete and transaction search. As a user performs a search, autocomplete will help them find what they want (some might even say we have the ability to read their minds ). With autocomplete, a user’s search experience will feel much more intuitive. The API now exposes a search endpoint for businesses that let you order online, specifically food delivery. If you’re interested in working with businesses which deliver food to your users, then look no further! Any food delivery business available on Yelp (such as Eat24, ChowNow, Delivery.com and EatStreet) will be returned through this endpoint. What’s changed? By default, we’re also providing more business data. The business endpoint will now return hours, pricing, and 3 hi-res photos. A reviews endpoint has been added that’ll provide you with 3 review excerpts for a business from Yelp users. Last but not least, we’ve updated the authentication for the API. The new API will use OAuth 2.0 to for all requests as opposed to OAuth 1.0 (we’ve seen all the emails and issues around OAuth 1.0). We’ve exposed a client credentials flow which will allow for simpler access to the API (no more invalid signature exceptions!). We hope that the new design of the API has made it more powerful yet simpler for developers to use. The endpoint names, parameters, and responses should be easier to understand and overall much more consistent. Under the hood Our goal was to make building functionality easier for developers. We’ve rebuilt the API from the ground up - enabling us to take it out of the monolith - so we could make use of our platform as a service and distributed tracing systems . The result is faster deployment, iterations, and automatic scaling. We’ll be sharing more on the work we put into the API in the coming months. Get started now Sign up here for access to the new Yelp API! All documentation can be found on GitHub . If you have any feedback (bugs, feature requests, or random thoughts), please create a new issue on the repo or message me directly on Twitter ( @zehtomer ) or on Yelp’s engineering account ( @YelpEngineering ). Check back to our blog and the Yelp Engineering Twitter account for additional updates coming out in the next few weeks! Tweet Back to blog", "date": "2016-07-18"}, {"website": "Yelp", "title": "Billions of Messages a Day - Yelp's Real-time Data Pipeline", "author": ["\n        \n  Justin C., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html", "abstract": "This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Faced with the challenges of scaling out its engineering organization, Yelp transitioned to a service oriented architecture (SOA).  Services improved developer productivity but introduced new\ncommunications challenges.  To solve these problems, Yelp built a real-time\nstreaming data platform. We built a unified system for producer and consumer applications to stream\ninformation between each other efficiently and scalably. It does this by\nconnecting applications via a common message bus and a standardized message\nformat. This allows us to stream database changes and log events into any\nservice or system that needs them, for example:  Amazon Redshift, Salesforce,\nand Marketo. The Challenges with Scaling Out In 2011, Yelp had more than a million lines of code in a single monolithic\nrepo, “yelp-main”.  We decided to break the monolith apart into a service\noriented architecture (SOA), and by 2014 had more than 150 production services,\nwith over 100 services owning data.  Breaking apart “yelp-main” allowed Yelp\nto scale both development and the application, especially when coupled with our\nplatform-as-a-service, PaaSTA . Services don’t solve everything.  Particularly when dealing with communication\nand data, services introduce new challenges. Service to Service Communication Service-to-service Communication Scales Poorly Metcalfe’s Law says that the\nvalue of a communications network is proportional to the square of the number\nof connected compatible communications devices.  Translating to a SOA, the\nvalue of a network of services is proportional to the square of the number of\nconnected services.  The trouble is, the way that service-to-service\ncommunication is typically implemented isn’t very developer-efficient. Implementing RESTful HTTP connections between every pair of services scales\npoorly.  HTTP connections are usually implemented in an ad hoc way, and\nthey’re also almost exclusively omni-directional.  22,350 service-to-service\nomni-directional HTTP connections would be necessary to fully connect Yelp’s\n150 production services. If we were to make a communications analogy, this\nwould mean every time you wanted to visit a new website, you’d have to first\nhave a direct link installed between your computer and the site. That’s\nwoefully inefficient. Failing at Failure Aside from complexity, consistency is problematic.  Consider this database\ntransaction and service notification: session.begin()\nbusiness = Business()\nsession.add(business)\nsession.commit()\nmy_service_client.notify_business_changed(business.id) If the service call fails, the service may never be notified about the\nbusiness creation.  This could be refactored like: session.begin()\nbusiness = Business()\nsession.add(business)\nmy_service_client.notify_business_changed(business.id)\nsession.commit() Then the commit could fail, in which case the service would be notified that a\nbusiness was created that doesn’t exist. Workarounds exist.  The service could poll for new businesses, or use a\nmessaging queue and call back to make sure the business was actually added.\nNone of this is as easy as it initially appears.  In a large SOA, it wouldn’t\nbe strange to find multiple notification implementations, with varying degrees\nof correctness. Working with Data Across Services is Hard ~86 million is a magic number Yelp passed 100 million reviews in March 2016.  Imagine asking two questions.\nFirst, “Can I pull the review information from your service every day?”  Now\nrephrase it, “I want to make more than 1,000 requests per second to your\nservice, every second, forever.  Can I do that?”  At scale, with more than 86\nmillion objects, these are the same thing.  At scale the reasonable becomes\nunreasonable.  Bulk data applications become service scalability problems. Joins get pretty ugly.  The N+1 Query Problem tends to turn into the N Service Calls problem.  Instead of code making N\nextra queries, it makes N service calls instead.  The N+1 Query Problem is\nalready well understood.  Most ORMs implement the eager loading solution\nout-of-the-box.  There isn’t a ready solution for service joins. Without a ready solution, developers tend to design adhoc bulk data APIs.  These\nAPIs tend to be inconsistent because developers are distributed across teams and\nservices.  Pagination is particularly prone to inconsistency issues, with no\nclear standard.  Popular public APIs use everything from custom response\nmetadata to HTTP Link headers . To join across services scalably you need to upgrade your service stack.  Every\ndata-owning service and client library will need work. Possible Solutions? The first solution that developers usually come to is implementing bulk data APIs.\nOf course, implementing a bulk data API for every data type stored by every\nservice can be very time consuming.  Somewhat naturally, a generalized bulk data\nAPI comes up, where the API can take arbitrary SQL, execute it, and return the\nresults.  Unfortunately, this is a pretty major violation of service boundaries.\nIt’s equivalent to connecting to a service’s database to create new data,\nresulting in a distributed monolith.  And it’s brittle.  Every caller needs to\nknow a lot about the internal representation of data inside the services that it\nneeds data from, and needs to respond in lockstep to data changes in the\nservice, tightly coupling the caller and service. A potential solution for bulk data sharing is periodically snapshotting the\nservice database, and sharing the snapshots.  This approach shares the\nbrittleness of the bulk data API, with the added challenge that differential\nupdates can be difficult to implement correctly, and full updates can be very\nexpensive.  Snapshots are further complicated by having some data that is\nmeaningless without the underlying code.  Concretely, boolean flags stored in\nbitfields or categorical data stored as integer enums are common examples of\ndata that isn’t inherently meaningful without context. A Generalized Solution Now that you have the context of the problem, we’ll explore how it can be solved\nat a high level using a message bus and standardized data formatting.  We’ll\nalso discuss the system architecture when those two components are integrated,\nand what can be accomplished with that architecture. The Message Bus Architecturally, a message bus seemed like a good starting point for addressing\nthese issues. A bus would reduce the connection complexity from n^2 to n, and in our case from\nmore than 22,000 connections to just 150. Apache Kafka , a distributed, partitioned, replicated\ncommit log service, is ideal for this application.  Aside from being both fast\nand reliable, it has a feature called log compaction that’s very useful in this\ncontext.  Log compaction prunes topics with a simple guarantee – the most\nrecent message for a given key is guaranteed to remain in the topic.  This\nyields an interesting property, if you were to write every change that happens\nin a database table into a topic, keyed by the primary key in the database,\nreplaying the topic would yield the current state of the database table. Log compaction retains at least the most recent message for every key. Stream-table duality is well-covered by Jay Kreps in The Log: What every\nsoftware engineer should know about real-time data’s unifying\nabstraction ,\nand in the Kafka Streams docs .\nExploiting this duality using log compaction allows us to solve many of our bulk\ndata problems.  We can provide streaming differential updates and with them\nguarantee that a new consumer, replaying a topic from the beginning, will\neventually reconstruct the current state of a database table.  In Yelp’s data\npipeline, this property enables engineering teams to initially populate and\nstream data changes to Redshift clusters. Decoupled Data Formats Selecting how data will be transported is only part of the solution.  Equally\nimportant is determining how the transported data will be formatted.  All\nmessages are “just bytes” to Kafka, so the message format can really be\nanything.  The obvious answer to this is JSON, since it has performant parsing\nimplementations in most languages, is very broadly supported, and is easy to\nwork with. However, JSON has one core issue: it’s brittle.  Developers can\nchange the contents, type, or layout of their JSON data at any time, and in a\ndistributed application it’s hard to know the impact of data changes.\nUnfortunately, JSON data changes often are first detected as production errors,\nnecessitating either a hotfix or rollback, and causing all kinds of problems\ndownstream. Yelp’s data processing infrastructure is tree-like.  Our core data processing\ntends to produce intermediate outputs that are consumed, reprocessed, and\nrefined by multiple layers and branches.  Upstream data problems can cause lots\nof downstream problems and backfilling, across many different teams, especially\nif they’re not caught early.  This problem is one we wanted to address, when we\nmoved to a streaming architecture. Apache Avro , a data serialization system, has\nsome really nice properties, and is ultimately what we selected.  Avro is a\nspace-efficient binary serialization format that integrates nicely with dynamic\nlanguages like Python, without requiring code generation.  The killer feature of\nAvro, for our system, is that it supports schema evolution.  That means that a\nreader application and a writer application can use different schema versions to\nconsume and produce data, as long as the two are compatible.  This decouples\nconsumers and producers nicely - producers can iterate on their data format,\nwithout requiring changes in consumer applications. We built an HTTP schema store called the “Schematizer,” that catalogs all of the\nschemas in Yelp’s data pipeline.  This enables us to transport data without\nschemas.  Instead, all of our avro-encoded data payloads are packed in an\nenvelope with some metadata, including a message uuid, encryption details, a\ntimestamp, and the identifier for the schema the payload was encoded with.  This\nallows applications to dynamically retrieve schemas to decode data at runtime. High Level Architecture If we standardize the transport and formatting of data,\nwe can build universal applications that don’t care about the data itself. Messages generated by our logging system are treated exactly the same as\nmessages generated from database replication or from a service event.  Circling\nback to Metcalfe’s Law, this architecture increases the value of Yelp’s\nstreaming data infrastructure so that it scales quadratically with the number of\nuniversal consumer or producer applications that we build, yielding strong\nnetwork effects.  Concretely, as a service author, it means that if you publish\nan event today, you can ingest that event into Amazon Redshift and our data\nlake , index it for search,\ncache it in Cassandra, or send it to Salesforce or Marketo without writing any\ncode.  That same event can be consumed by any other service, or by any future\napplication we build, without modification. Yelp’s Real-Time Data Pipeline The data pipeline’s high level architecture gives us a framework in which to\nbuild streaming applications.  The remaining sections will discuss the core of\nYelp’s real-time data pipeline, focusing on the invariants that the system\nprovides, and the system-level properties that result.  Following posts in the\nseries will discuss specific applications in depth. A Protocol for Communication Yelp’s Real-Time Data Pipeline is, at its core, a communications protocol with\nsome guarantees.  In practice, it’s a set of Kafka topics, whose contents are\nregulated by our Schematizer service.  The Schematizer service is responsible\nfor registering and validating schemas, and assigning Kafka topics to those\nschemas.  With these simple functions, we’re able to provide a set of powerful\nguarantees. Guaranteed Format All messages are guaranteed to be published with a pre-defined schema, and the\nschemas are guaranteed to be registered with the schema store.  Data Pipeline\nproducers and consumers deal with data at the schema level, and topics are\nabstracted away.  Schema registration is idempotent, and registered schema are\nimmutable. Any consumer, when first encountering data written with any arbitrary schema,\ncan fetch that schema exactly once, and decode any data written with it. Guaranteed Compatibility One of the Schematizer’s core functions is assigning topics to schemas.  In\ndoing so, the Schematizer guarantees that if a consumer starts reading messages\nfrom a topic with an active schema from that topic, it will be able to continue\ndoing so forever, despite upstream schema changes.  In other words, every active\nschema assigned to a topic is guaranteed to be compatible with every other\nactive schema assigned to the same topic.  Applications won’t break because of\nschema changes. At runtime applications will fetch schemas used to write data messages\ndynamically, as messages encoded with previously unseen schemas appear in the\ntopic.  A producer can change the data format it’s producing without any action\nfrom any downstream consumers.  The consumers will automatically retrieve the\nnew writer schemas, and continue decoding the data with the reader schema\nthey’ve been using.  Producer and consumer data evolution is decoupled. Guaranteed Registration Data producers and consumers are required to register whenever they produce or\nconsume data with a schema.  We know what teams and applications are producing\nand consuming data across Yelp, which schemas they’re using, and with what\nfrequency. This allows producers to coordinate breaking data changes with their consumers,\nand allows for automated alerting of consumers in the event of a data fault.\nProducers are given the tools that they need to coordinate incompatible schema\nchanges in advance.  Registration further enables the deprecation and\ninactivation of outdated schemas.  We can detect when a schema no longer has\nproducers, and can coordinate the migration of consumers to more recent active\nschema versions out-of-band.  Registration simplifies the compatibility story,\nsince we can artificially constrain the number of active schemas in a topic -\ncompatible schema changes typically need to be compatible with only a single\nexisting schema. Guaranteed Documentation and Data Ownership The Schematizer requires documentation on all schema fields, and requires that\nall schemas assign a team that owns the data.  Any schemas without this\ninformation will fail validation.  That documentation and ownership information\nis then exposed through a web interface called Watson, where additional\nwiki-like documentation and comments can be added. In many cases, we’ve extended this capability to systems that generate messages\nand schemas automatically.  For example, schemas derived from database tables\nare documented by extracting docstring and ownership information from the\ncorresponding models in our codebase.  Automated tests prevent adding new data\nmodels without documentation and owners, or modifying existing data models\nwithout adding documentation. Watson enables users to publicly ask the data owners questions, and to browse\nand contact data producers and consumers.  The Schematizer has the concept of\ndata sources and data targets, where it can track, for example, that a schema\noriginates from a MySQL database table, and the data is streamed into a Redshift\ntable.  It’s able to produce documentation views dynamically for these data\nsources and targets.  Effectively, adding documentation to code automatically\ndocuments Redshift tables, MySQL tables, and Kafka topics. Guaranteed Data Availability As mentioned above, one of the major issues with data transfer\nbetween services is dealing efficiently with bulk data.  Using Kafka log\ncompaction and keyed messages, we’re able guarantee that the most recent message\nfor each key is retained. This guarantee is particularly useful in the context of database change capture.\nWhen materializing a table from a topic containing captured database changes,\nthis guarantees that if a consumer replays the topic from the beginning, and\ncatches up to real time, it will have a complete view of the current state of\nthe data in that table.  The same system that provides differential updates, can\nthus be used to reconstruct a complete snapshot. All Aboard the Databus! describes the utility of streaming database change capture, which is effectively\na single universal producer application in our unified infrastructure. The Next Stage In this post, we’ve stepped through the communications and bulk data challenges\nassociated with SOAs, and explored the core of the real-time streaming data\nplatform that Yelp built to solve them.  We showed how using a message bus and\nstandardized message format, along with some simple guarantees, enables the\nconstruction of powerful, data-agnostic producer and consumer applications. Join us in the coming weeks, where we will dive deeply into some of the\napplications we’ve built inside and on top of the platform.  We will cover the\nSchematizer and its documentation front-end Watson, our exactly-once MySQL\nchange data capture system, our stream processor, and our Salesforce and\nRedshift connectors. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-07-14"}, {"website": "Yelp", "title": "Yelpicons: A Cross-Platform Icon Build System", "author": ["\n        \n  Kent W., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/05/yelpicons.html", "abstract": "It’s 10:00 AM and a designer just created a new icon for Yelp.  At 10:05 AM iOS has received  a newly versioned CocoaPod containing 1x, 2x, and 3x PNGs, Android has received an AAR Library containing mdpi, hdpi, xhdpi, and xxhdpi WebPs , and the Web CDN has a new SVG and PNG spritesheet, code to manage the PNG fallback for our IE8 users, and a new Python package to access the icon in a template. The workflow wasn’t always this streamlined and was only made possible through a cross-team collaboration with Android, iOS, Web, and Design at Yelp.  With a product as large as ours, (with around 120 metaphors ) making sure that new icons with varying colors and sizes are up-to-date and look correct in all resolutions across all platforms is no easy feat.    Yelp is divided into several different product teams with a mix of developers from each platform, so updating icons requires collaboration with all teams.  This story is about taking the human process of developing, sharing, and requesting new icons for different platforms and automating it.  We went from passing images through emails, Dropbox, and even Jira to automatically delivering the icons to the developers right when they needed them. The process started with our design team.  Their effort redefined the icon design standard, creating a unified set that looks great across all platforms.  A few key choices in the beginning set the project up for success: using the SVG format, designing for pixel density, and using a bounding box . Using SVGs as the base image format allows us to scale an icon without losing quality and effortlessly colorize the icons via code.  Since the Yelp product is supported on a variety of devices, each image had to define icon size in device independent pixels or in our case defining 2x, 3x, etc. densities for images.  Finally, we needed to ensure that icon spacing was consistent so that developers could easily align the icons in whatever context they needed to use them.  By creating a box that bounds each icon we were able to balance the visual weights individually and have the confidence that icons would look great on all platforms. Once the design specifications were finalized, we turned to creating a build/delivery system.  The system had to support conversion and scaling of icons and publish packages that could be consumed by all platforms. Multi-Platform Build System In defining an SVG-first icon system, the first task was to convert icons to different image formats in a fluid, programmatic way. Icon Conversion and Scaling Each platform has different image requirements.  The Web needs to support SVGs and PNGs, Android requires WebPs, and iOS requires PNGs.  Additionally, both Android and iOS require icons at different pixel densities. To automate creating these assets, we used build systems and CI.  At Yelp, our traditional build systems are supported using make (this does not include platform specific build system such as gradle) with Jenkins running on Linux and OSX as our CI service. All image manipulation is done in Python and one node package (SVGO).  For image conversion from SVG to PNG and scaling we used CairoSVG .  PNG stitching and WebP conversion is handled with PIL .  For SVGs, we optimize with SVGO and clean and manipulate with lxml . Pipeline When a designer commits a new SVG icon to our git repository, we kick off the jenkins build pipeline.  The first stage of the pipeline is to run unit tests for the repository and some checks on the icons themselves.  We ensure all SVGs are valid and and that the naming of the icon follows our pre-defined naming scheme. Finally, we kick off the builds for each platform: Web There is only one build stage for Web.  We optimize SVGs for the SVG spritesheet while we simultaneously convert the SVGs to PNGs.  After we optimize the SVGs, we remove the fill attributes and namespace tags using lxml.  These generated images are then used to create both the SVG and PNG spritesheets.   Any JS or CSS needed for the PNG spritesheet is generated at the time the spritesheet is built with mako .  Finally, we upload all of our assets to Amazon S3 (our backing store for our CDN) and upload a new python package to our internal PYPI server that contains all of the asset metadata needed for grabbing the assets from the template. Android and iOS We have increasingly started to use Docker as a encapsulation method for tools that require system dependencies.  Specifically, to convert an image to the WebP format using PIL, we need to have libwebp installed on the system.  We could install this on all of our systems that we would run this build, but this dependency is likely only going to be specific to our build, so it made sense to use Docker. FROM    our-base-image\n\nRUN apt-get update \\ && DEBIAN_FRONTEND = noninteractive apt-get -y --no-install-recommends install \\ build-essential \\ libwebp-dev \\ libjpeg-dev \\ libffi6 \\ npm \\ python3-dev \\ python3-pip \\ zlib1g \\ # keep the image small; clean up APT when done && apt-get clean \\ && rm -rf /var/lib/apt/lists/ * /tmp/ * /var/tmp/ * RUN pip3 install pillow\nRUN mkdir -p code\nADD . code\nWORKDIR code\nENTRYPOINT [ \"python3.4\" , \"convert_png_to_webp.py\" ] After generating the icons in specific formats and sizes we use templates to generate both the Cocoa Pod and the AAR file. Web Our Front-End web developer defined our SVG-first spriting system with a minimal PNG fallback for any browsers that don’t support SVG.  We developed two major components: the template helper and the fallback. Template Helper Creating this new infrastructure for developers doesn’t do us any good if it isn’t easy to use.  We use a template helper to abstract the details away, so that they can focus on what’s important, the product.  At Yelp we use the cheetah templating language, but the general concept applies to anyone.  We defined a simple helper function that gives complete control over the icon. For example $icon(‘14x14_arrow’) creates the following markup: <span class= \"icon icon--14-arrow\" style= \"width: 14px; height: 14px;\" aria-hidden= \"true\" > <!-- :before pseudo-element for the fallback generated with CSS --> <svg class= \"icon_svg\" > <use xlink:href= \"#14x14_arrow\" /> </svg> </span> This allows the size to be inlined and the icon is hidden for screen readers when no title parameter is passed in. Template Helper: colors Yelp has a standard colors palette where color, hover (+ focus) and active states have an alias. When the color is omitted we use the default.  We use currentColor , an additional color in our toolkit, to fill the SVG icon with the CSS color value of the .icon element itself (or its parent’s) so when you hover over a link both the link and the icon color will change accordingly.  The template supports colors in the following ways: $ icon ( '14x14_foo' , color = 'inactive' ) ## .icon .icon--inactive $ icon ( '14x14_foo' , color = 'inactive' , color_hover = 'error' ) ## .icon .icon--inactive .icon--hover-error $ icon ( '14x14_foo' , color = 'inactive' , color_active = 'error' ) ## .icon .icon--inactive .icon--active-error $ icon ( '14x14_foo' , color = 'currentColor' ) ## .icon .icon--currentColor One important thing to note is the color (classname or inline) is set on the span element instead of the SVG element.  This is so that we can also style the fallback (the :before pseudo element).\nThis works well because the actual SVG fill property is set to inherit and the element inherits the fill color from the parent.  For example: <span style= \"icon--neutralgrey\" > <!-- .icon--neutralgrey { fill: #999; }--> :before <!--.icon--neutralgrey:before { left: -50%; }--> <svg> <!--fill: inherit;--> ... </svg> </span> Template Helper: state As seen above, the icon color can be changed on hover (focus) and be marked as active via the color_hover and color_active params .  When those options are defined a modifier class with the –state- prefix is added to the icon component i.e .icon--hover-neutralgrey or .icon--active-neutralgrey .  These are in fact like promises that get resolved when the element is hovered, focused or the icon is marked as active by adding the .is-active class , i.e. .icon--hover-neutralgrey:hover , .icon--hover-neutralgrey:focus { fill: #999; } , .icon--active-neutralgrey.is-active { fill: #999; } .  Sometimes though the icon is part of other components and its color has to change only when the the parent component or element state changes. <a href= \"https://yelp.com\" > <Icon /> Yelp </a> We made our icons system so that state classes can be manually set on any ancestor to delegate the control of the icon state. <a href= \"https://yelp.com\" class= \"icon--hover-neutralgrey icon--active-neutralgrey\" > <Icon /> Yelp </a> Will change the Icon color when the link is hovered or has the is-active class. [ class *= \"icon--active-\" ] .is-active .icon , [ class *= \"icon--hover-\" ] :hover .icon , [ class *= \"icon--hover-\" ] :focus .icon { fill : inherit ; } .icon--active-neutralgrey.is-active { fill : #999 ; } Finally since we are working with SVG and CSS the icon can inherit the parent’s color by using the currentColor color. <a href= \"https://yelp.com\" > <Icon class= ”icon--currentColor” /> Yelp </a> The icon inherits the color from the link. If the link color changes (on hover for example) the icon color will change accordingly. Fallback We decided to support black and white PNG icons as a fallback.  This fallback needed to be simple but functional because it provides support to a small portion of our users.  Furthermore we wanted to be able to eventually move to an SVG only framework.  Mark Hinchliffe’s awesome article SVG icons are easy but the fallbacks aren’t provided some great inspiration.  We started there and built a simplified version of his solution that doesn’t care about: many colors – remember we have only two: black and white icon sizes and scaling – we just have a spritesheet with icon of “any” size. Here the .icon element is our canvas and the :before pseudo-element is an element with the PNG spritesheet as background-image. The size of the pseudo-element reflects the PNG one. The Y-axis is for the symbol (icon) and X-axis is for the color. In our case we have two columns of icons, one for the black and another for the white so the left position of the pseudo-element will be either 0 or -($png_width / 2) .icon--14-arrow :before { top : -64px ; } // symbol .icon :before { left : 0 ; } // default color .icon--fallback-inverted { left : - ( $png_width / 2 ) px ; } // inverted color iOS and Android iOS and Android were a little more straightforward than Web.  Once we had the images converted to their respective formats at different pixel densities, we just needed to figure out how we were going to color them.  Due to improvements on native devices, with Android releasing tinting capability as of api version 21 and iOS tinting as of iOS 7,  we decided that we could tint icons at runtime on both mobile platforms.  By tinting them at runtime, we reduce the number of icons in each bundle and avoid duplication of icons (a common error we faced).  Furthermore, we leverage the use of resource shrinking on Android to further shrink the asset bundle size.  Finally, we package them up into consumable items for both platforms.  For iOS this meant that we published a CocoaPod and for Android this meant that we would generate an AAR resource . And that is all there is to it!  Want to see the Finished Product? Check out yelp.com/styleguide/icons . Tweet Want to Join Yelp as a Full-Stack Engineer? Do you love engineering processes?  Do you understand the web ecosystem?  Can you tackle full-stack problems on a larger scale with ease?  If so, our core web team is looking for someone like you! View Job Back to blog", "date": "2016-05-18"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 6 Winner", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/yelp-dataset-challenge-round6-winner.html", "abstract": "Yelp Dataset Challenge Round 6 Winners The sixth round of the Yelp Dataset Challenge ran throughout the second half of 2015 and we were really impressed with the projects and ideas that came out of the challenge. Today, we are proud to announce the grand prize winner of the $5,000 award:\n“Topic Regularized Matrix Factorization for Review Based Rating Prediction” by Jiachen Li, Yan Wang, Xiangyu Sun, Chengliang Lian, and Ming Yao (from the Language Technologies Institute, School of Computer Science, at Carnegie Mellon University). The authors created a recommender system to inform Yelpers about which business they might be interested in, by predicting the star rating they would give it. To achieve this, the authors propose combining topic modeling with a Latent Dirichlet Allocation and matrix factorization through a new model called Topic Regularized Matrix Factorization (TRMF). Their model incorporates topic modeling as a constraint for regulating the learning process of matrix factorization. TRMF is an original method that performs better than other ones proposed in the past, and is a clever way to solve a problem that is both difficult and highly relevant to many tech companies. This entry was selected from tons of submissions for its technical and academic merit. For a full list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Thanks to all who participated! Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset. These examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They recently wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Next Yelp Dataset Challenge Round Submissions for Round 7 closed on June 30, 2016, but Round 8 is just around the corner so stay connected. We are excited to see what you will come up with! Tweet Back to blog", "date": "2016-08-04"}, {"website": "Yelp", "title": "Streaming MySQL tables in real-time to Kafka", "author": ["\n        \n  Prem Santosh Udaya Shankar, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html", "abstract": "This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 As our engineering team grew, we realized we had to move away from a single monolithic application towards services that were easier to use and could scale with us. While there are many upsides to switching to this architecture, there are downsides as well, one of the bigger ones being: how do we provide services with the data they require as soon as possible? In an ecosystem with hundreds of services, each managing millions and millions of rows in MySQL, communicating these data changes across services becomes a hard technical challenge. As mentioned in our previous blog post , you will quickly hit the N+1 query problem . For us, this meant building a system, called MySQLStreamer, to monitor against data changes and alert all subscribing services about them. MySQLStreamer is a database change data capture and publish system. It’s responsible for capturing each individual database change, enveloping them into messages and publishing to Kafka. The notion of replaying every changeset from a table to reconstruct the entire table at a particular snapshot in time and reproducing a stream by iterating over every change in a table is referred to as a stream-table duality . In order to understand how we capture and publish database changes, it’s essential to know a bit about our database infrastructure. At Yelp, we store most of our data in MySQL clusters. To handle a high volume of visits, Yelp must operate a hierarchy of dozens of geographically distributed read replicas to manage the read load. How does replication work? In order for replication to work, events on the master database cluster are written to a special log called the binary log. When a replica connects to its master, this binary log is read by the replica to complete or continue the process of replication, depending on the replication hierarchy. This process is bolstered by two threads running on the replica, an IO thread and a SQL thread. This is visualized in the figure below. The IO thread is primarily responsible for reading the binary log events from master, as they arrive, and copying them over to a local relay log in the replica. The SQL thread then reads these events and replays them in the same order that they arrived in. Replication in MySQL It should be noted here that an event played by the replica’s SQL thread need not necessarily be the latest event logged in the master’s binary log. This is shown in the figure above and is called replication lag. Yelp’s MySQLStreamer acts as a read replica, persisting updates into Apache Kafka , instead of materializing them into a table. Type of MySQL replication There are two ways of replicating a MySQL database: Statement-based replication (SBR) Row-based replication (RBR) In statement-based replication, SQL statements are written to the binary log by master and the slave’s SQL thread then replays these statements on the slave. There are a few disadvantages of using statement-based replication. One important disadvantage is the possibility of data inconsistency between master and the slave. This is because the SQL thread in the slave is simply responsible for replaying log statements copied over from the master, but there could be instances where the statements generate non-deterministic outputs. Consider the following query: INSERT INTO places (name, location)\n(SELECT name, location FROM business) This is a scenario where you want to SELECT certain rows and INSERT them into another table. In this case, selecting multiple rows without an ORDER BY clause would result in rows returned–but the order of those rows may not be the same if the statement was to be replayed multiple times. Also, if a column had an AUTO_INCREMENT associated with it, the rows might end up with different ranks each time the statement is executed. Another example would be using the RAND() or the NOW() methods which would end up generating different results when played on different hosts in the replication topology. Due to these limitations, we use row-based replication in the database required by the MySQLStreamer. In row-based replication, each event shows how the individual rows of a table have been modified. UPDATE and DELETE statements contain the original state of the row before it was modified. Hence, replaying these row changes will keep the data consistent. Now we know what replication is, but why do we need the MySQLStreamer? One of the main uses of Yelp’s real time streaming platform is to stream data changes  and process them, in order to keep downstream systems up to date. There are two kinds of SQL change events that we have to be aware of: DDL (Data Definition Language) statements, which define or alter a database structure or schema. DML (Data Manipulation Language) statements, which manipulate data within schema objects. The MySQLStreamer is responsible for: Tailing the MySQL binary log consuming both these types of events Handling the events depending on their type, and publishing the DML events to Kafka topics. The MySQLStreamer publishes four distinct event types: Insert , Update , Delete and Refresh . The former three events correspond to DML statements of the same type. Refresh events are generated by our bootstrap process, described in detail later. For each event type, we include the complete row contents. Update events include the full row, both before and after the update. This is particularly important for dealing with cycles. Imagine implementing a geocoding service that consumes Business updates, and triggers a latitude and longitude update on that same Business row if the address changes. Without the before row content, the service would have to store a significant amount of state to determine if a row’s address had actually changed, and to ignore latitude and longitude updates. With both before and after content, it’s trivial to generate a diff and break these cycles without keeping any state. Event Type Message Contents Insert Full Row Update Full Row Before Update and Full Row After Update Delete Full Row Before Delete Refresh Full Row MySQLStreamer Event Types Database Topology The MySQLStreamer is powered by three databases as shown below: Database Topology The Source Database This database stores change events in the upstream data. It is tracked by the MySQLStreamer in order to stream those events to downstream consumers. A binary log stream reader in the MySQLStreamer is responsible for parsing the binary log for new events. Our stream reader is an abstraction over the BinLogStreamReader from the python-mysql-replication package. This api provides three main functionalities: to peek at the next event, to pop the next event, and to resume reading from the stream at a specific position. The Schema Tracker Database The schema tracker database is analogous to a schema-only slave database. It is initially populated with schemas from the source database, and kept up to date by replaying DDL statements against it. This means it skips the data and stores the skeleton of all the tables. We lazily retrieve CREATE TABLE statements from this database to generate Avro schemas using the Schematizer service. Schema information is also necessary to map column name information to rows in the binary log. Because of replication lag, the database schema for the current replication position of the MySQLStreamer doesn’t necessarily match the current schema on the master. Hence, the schema used by the MySQLStreamer cannot be retrieved from the master. We chose to use a database for this to avoid re-implementing MySQL’s DDL engine. Should the system encounter a failure during the execution of the SQL statement, the database might end up in a corrupted state, as DDL statements are not transactional. To circumvent this issue, we treat the entire database transactionally. Before applying any DDL event, we checkpoint, take a schema dump of the entire schema tracker database, and store it in the state database. The DDL event is then played. If it succeeds, the stored schema dump is deleted and another checkpoint is taken. A checkpoint basically consists of saving the binary log file name and the position along with the Kafka offset information. In case of failure, after the MySQLStreamer restarts it checks to see if a schema dump exists. If it does, it replays the schema dump before handling the DDL event it failed on. Once the schema dump is replayed, the MySQLStreamer restarts the tailing of events from the checkpointed position, eventually catching up to real time. The State Database The state database stores the MySQLStreamer’s internal state. It consists of three tables that store various pieces of state information: DATA_EVENT_CHECKPOINT Stores information about each topic and the corresponding last known published offset. GLOBAL_EVENT_STATE The most important piece of information stored in this table is the position. The position looks like this: heartbeat_signal heartbeat_timestamp log_file log_position offset Position Information One of the prerequisites for  fail-safe replication is to have an unique identifier associated with every transaction. It not only provides benefit in the process of recovery but also in hierarchical replication. The global transaction identifier (GTID) in one such identifier. It’s identical across all the servers in a given replication setup. Though our code supports GTID, the version of MySQL we use does not. Hence, we needed an alternative approach to store the state that could be easily translated across the entire replication setup, which motivated us to piggyback on Yelp’s heartbeat daemon. This python daemon is responsible for sending periodic heartbeat database updates that consists of a serial number and a timestamp. This is then replicated to all other replicas. The MySQLStreamer takes the heartbeat serial number and timestamp and attaches the log file name and log position it is currently working with and stores that in the global_event_state table. If the current master fails for some reason, a batch finds the log file and the log position from the new master using the heartbeat serial number and heartbeat timestamp. MYSQL_DUMPS Stores the schema dump of the schema tracker database for restoring the database to a stable state after a failure. How does the MySQLStreamer work? Working of MySQLStreamer As the MySQLStreamer starts, it acquires a Zookeeper lock before it initiates processing on any of the incoming events. This step is necessary to prevent multiple instances of the MySQLStreamer from running on the same cluster. The problem with multiple instances running on the same cluster is that replication is inherently serial. In some applications we want to maintain order within and between tables, so we prevent multiple instances from running, preserving order and preventing message duplication. As we have previously discussed, the MySQLStreamer receives events from the source database (Yelp database as seen in the figure above) via the binlog parser. If the event is a data event then the table schema for that event is extracted and sent to the Schematizer service. This service then returns the corresponding Avro schema and a Kafka topic. The Schematizer service is idempotent. It will return the exact same Avro schema and topic if it’s called with the same create table statement multiple times. Data events are encoded with the received Avro schema and published to Kafka topics.\nThe data pipeline’s Kafka Producer maintains an internal queue of events to be published to Kafka. If a schema event is received from the binlog parser, the MySQLStreamer first flushes all the events already present in the internal queue and then takes a checkpoint for the purposes of recovery in case of a failure. It then applies the schema event on the schema tracker database. Failure handling of data events is slightly different compared to schema events. We checkpoint before processing any data event, and continue checkpointing as we publish batches of messages successfully. We trust successes, but never failures. If we encounter a failure we recover from it by inspecting the last checkpoint and Kafka high watermarks, and publishing only those messages that were not successfully published previously. On the Kafka side, we require acks from all in-sync replicas, and run with a high min.isr setting, trading availability for consistency.  By methodically verifying and recovering from failures, we’re able to ensure that messages are published exactly once. Bootstrapping a Topic Yelp was founded in 2004. Many of the most interesting tables have existed for nearly as long as Yelp. We needed to find a way to bootstrap a Kafka topic with existing table contents. We engineered a procedure that can perform a consistent topic bootstrap while still processing live replication events. Before we talk about bootstrapping, let us see what the actual database replication topology looks like. Referencing the figure above, we notice there exists a master and a replica of master called the intermediate master. There are other replicas of intermediate master called local masters. The MySQLStreamer is connected to a replica called refresh primary which in turn is a replica of one of the local master. The refresh primary is setup with row-based replication whereas all other replicas run statement-based replication. Bootstrapping is initiated by creating a table like the original table we want to bootstrap on the MySQLStreamer’s refresh primary, using MySQL’s blackhole engine. blackhole_table = create_blackhole_table(original_table)\nwhile remaining_data(original_table):\n    lock_table(original_table)\n    copy_batch(original_table, blackhole_table, batch_size)\n    unlock_table(original_table)\n    wait_for_replication()\ndrop_table(blackhole_table)\n\nPseudo-code Bootstrap Process The blackhole engine is like the /dev/null of database engines. The primary reason we chose to use the blackhole engine is because writes to blackhole tables aren’t persisted, but are logged and replicated. This way we’re recreating the binary logs of the original table and not worry about storing the duplicate table. Once we’ve created the blackhole table, we lock the original table to prevent any data changes during the bootstrap process. We then copy rows from the original table to the blackhole table in batches. As shown in the figure, the MySQLStreamer is connected to one of the leaf clusters. This is because, we do not want any change triggered by the bootstrap logic to trickle down to every child cluster. But we do want the original table to be updated with the latest changes during bootstrapping hence, between batches, we unlock the original table and wait for replication to catch up. Locking the original table can cause replication from a local master to the replica ( refresh primary ) to stall , but it guarantees that the data we’re copying into the blackhole table is consistent at that point in replication. By unlocking and allowing replication to catch up, the bootstrap process naturally throttles itself. The process is very fast, since the data never leaves the MySQL server. The replication delay we’re causing is measured in milliseconds per batch. All of the complexity of this process happens in the database. Inside the MySQLStreamer, our code simply treats inserts into specially named blackhole tables as Refresh events on the original table. Refresh events are interleaved in topics with normal replication events, as regular Insert , Update , and Delete events are published during the bootstrap. Semantically, many consumers treat Refresh events like upserts. The Takeaway Engineering time is valuable. A good principle for engineers to follow is “ Any repetitive task should try to be automated ”. For Yelp to scale we had to engineer a single, flexible piece of infrastructure that would help with a multitude of applications. With the data pipeline we can index data for search, warehouse data and share transformed data with other internal services. The data pipeline proved to be immensely valuable and was a positive step towards achieving the required automation. The MySQLStreamer is a cardinal part of the data pipeline that scrapes MySQL binary logs for all the change events and publishes those changes to Kafka. Once the changes are in the Kafka topics, it’s easy for the downstream consumers to utilize them based on their individual use case. Acknowledgements Thanks to the entire Business Analytics and Metrics (B.A.M) team, the Database Engineering team and everyone that helped build the MySQLStreamer: Abrar Sheikh, Cheng Chen, Jenni Snyder and Justin Cunningham. This post is part of a series covering Yelp's\n    real-time streaming data infrastructure.  Our series explores in-depth\n    how we stream MySQL and Cassandra data at real-time, how we automatically\n    track & migrate schemas, how we process and transform streams,\n    and finally how we connect all of this into data stores like\n    Redshift, Salesforce, and Elasticsearch. Read the posts in the series: Billions of Messages a Day - Yelp's Real-time Data Pipeline Streaming MySQL tables in real-time to Kafka More Than Just a Schema Store PaaStorm: A Streaming Processor Data Pipeline: Salesforce Connector Streaming Messages from Kafka into Redshift in near Real-Time Open-Sourcing Yelp's Data Pipeline Making 30x Performance Improvements on Yelp’s MySQLStreamer Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift Fast Order Search Using Yelp’s Data Pipeline and Elasticsearch Joinery: A Tale of Un-Windowed Joins Streaming Cassandra into Kafka in (Near) Real-Time: Part 1 Streaming Cassandra into Kafka in (Near) Real-Time: Part 2 Tweet Build Real-time Data Infrastructure at Yelp Want to build next-generation streaming data infrastructure?  Apply to become an Infrastructure Engineer today. View Job Back to blog", "date": "2016-08-01"}, {"website": "Yelp", "title": "Introducing venv-update", "author": ["\n        \n  Buck E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/03/venv-update.html", "abstract": "venv-update is an MIT-Licensed tool to quickly and exactly synchronize a Python\nproject’s virtualenv with its requirements . This project ships as two separable components: pip-faster and venv-update.\nBoth are designed for use on large Python projects with hundreds of\nrequirements and are used daily by Yelp engineers. For complete documentation, please see http://venv-update.rtfd.org Making large Python projects painless The majority of yelp.com is implemented in a single Python project, dubbed\n“yelp-main”. Initially, yelp-main installed all of its dependencies at the\nsystem level. We’ve done the work to transition this to using virtualenv, and\nmanaging yelp-main’s (Python) requirements via pip and requirements.txt.\nImmediately we saw some problems with simply running virtualenv && pip install ;\nany removed requirements would persist, creating potential mismatches between\ndevelopers’ environments and production. The simplest solution was to\ncompletely remove the virtualenv when any requirements changed. It’s too slow! This led to a second problem: rebuilding a virtualenv from scratch was slow.\nSome of our requirements had a long build process (e.g. lxml, numpy) which\nwould repeat each time. The obvious solution was wheels, which essentially are\na post-build zip of a Python package. Given our own pypi server, we could\nupload Linux wheels for all of our requirements. Then a full rebuild would only\nconsist of some hits to pypi and unzip. It’s still slow! Even at this point, re-installing yelp-main would take 3-5 minutes. This was\ntrue even if we didn’t remove the virtualenv beforehand. Profiling revealed\nthat all of the time was taken in http requests to our PyPI server. Even though\nwe had pinned all our dependencies (with requirements like package-x==1.2.3 ),\npip would still reach out to PyPI before before deciding which version to pick.\nIt would do this even when the pinned version was available in the local wheel\ncache, and even when the pinned version was installed. Enter pip-faster I tried for over a month to untangle the hairball that was pip 1.5, but at last\ngave up. A workable solution was to invent a wrapper on pip that would insert\nthe necessary monkey-patches to do what we needed: don’t hit PyPI when a\nrequirement is pinned and there’s already a wheel available in the local cache.\nWhile monkey-patching is a despicable code smell, it worked quite well. What\nused to take minutes now takes seconds! We gave pip-faster an extensive test\nsuite and called it Good Enough . What’s more, we could add a bit of logic to simply uninstall any packages that\nwere no longer required, eliminating the need to delete the virtualenv when our\nrequirements changed. You can use this via pip-faster install --prune . It’s notable that pip has since fixed or improved several of these issues, and\nI should try again to push these improvements upstream someday. How much faster is pip-faster? If we install plone (a large python application with more than 250\ndependencies) we get these numbers: testcase pip v8.0.2 pip-faster improvement cold 4:39s 4:16s 8% noop 7.11s 2.40s 196% warm 44.6s 21.3s 109% In the “cold” case, all caches are completely empty. In the “noop” case nothing\nneeds to be done in order to update the virtualenv. In the “warm” case caches\nare fully populated, but the virtualenv has been completely deleted. The Benchmarks page has more detail. The bootstrap problem Now we can smoothly and quickly update our virtualenv, but how exactly does the\nvirtualenv come to exist in the first place? After all, pip-faster needs to be\ninstalled somewhere. We initially implemented our virtualenv bootstrap as a\nbash script, but it quickly got out of hand. There are many edge cases to be\nhandled. For example, we’ve made a couple major transitions in our virtualenv:\nwe stopped using --system-site-packages , and we moved from python2.6 to python2.7 . During these transitions, we needed developers to be able to switch\nbranches across these changes without things exploding in inexplicable ways.\nEnter venv-update venv-update solves all of the above problems, from start to finish. It\nidempotently gives you a correct virtualenv with the correct packages\ninstalled. That includes creating a virtualenv as necessary, or possibly\nremoving a virtualenv that’s become invalid due to the above edge cases (and\nmore). It installs pip-faster to that virtualenv (as necessary), and uses it to\nquickly and exactly update your installed Python packages to match your\nchecked-in requirements.txt. This is quite useful in a Makefile (a developer\ncan always just git pull && make to be up-to-date), but it’s sufficiently\nconfigurable to be a general solution to these problems in your Python\nprojects. For the full documentation of venv-update, please visit http://venv-update.rtfd.org Tweet Back to blog", "date": "2016-03-04"}, {"website": "Yelp", "title": "ElastAlert: Alerting At Scale With Elasticsearch, Part 2", "author": ["\n        \n  Quentin L., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/03/elastalert-part-two.html", "abstract": "It’s 10:51 PM on a Friday, and someone on the internet has decided to try to break into your network. They are guessing passwords and generating failed login events. Your security team is paged, the attacker is blocked, and everyone can go back to bed. This is one example of the power of ElastAlert. Now we’ll give you background on how it works and how to set it up yourself. In part one of this blog post, we introduced an open source alerting framework for Elasticsearch which allows you to match and take action on a wide variety of patterns. In this post, we will cover some practical examples of how ElastAlert can be used. Note that some of these examples may not be useful in all environments, but rather they are to show some of the capabilities of ElastAlert. We’ll give a quick overview of our Security Information and Event Management system to provide some context to these examples. It starts off with a multitude of data sources, web requests, syslog from all of our machines, and pretty much anything any developer wants to log. We use a combination of Logstash, Kafka and our existing log infrastructure to parse all of this data and index it into Elasticsearch. Once it’s in Elasticsearch, we can search through, analyze, and monitor it with ElastAlert. In many of these examples, we will omit the alerting configuration and focus on the pattern and rule type configuration. The alert itself can be customized a number of ways to suite your needs. In fact, several new alert types have been added since the last post: AWS SNS, HipChat, Slack, PagerDuty, Telegram, and VictorOps. For information on how the alert section is set up, please consult the documentation. SSH and Other Logins Let’s start off with a simple rule to alert us whenever a failed SSH authentication occurs. We will assume that the log _type is ssh and that it also contains an outcome, success or failure, and a username. # Alert when any SSH failures occur\nfilter:\n  - term:\n      _type: ssh\n  - term:\n      outcome: failure\ntype: any The any type will alert on any document which matches the filter. Maybe this would generate too many alerts. We can change the type to frequency to only alert if multiple SSH failures occur. We can refine it further by using query_key to group the events by username. This will allow us to alert only if a certain number of failures have occurred for a given user. # Alert when 10 SSH failures occur for a single user\nfilter:\n  - term:\n      _type: ssh\n  - term:\n      outcome: failure\ntype: frequency\nnum_events: 10\ntimeframe:\n  hours: 1\nquery_key: username This will alert you if someone is trying to brute force an SSH login, but what if an attacker has already taken your credentials? There are a few other things we could look at such as has the same user connected from multiple IP addresses? # Alert when a single user has connections from 2 IPs\nfilter:\n  - term:\n      _type: ssh\n  - term:\n      outcome: success\ntype: cardinality\nmax_cardinality: 1\ncardinality_field: ip_address\ntimeframe:\n  days: 1\nquery_key: username In this example, we are alerting if the cardinality of the ip_address field, grouped by username, is greater than one within a day. This specific alert could also be accomplished using the change rule type, but cardinality gives you more flexibility and could be used for a variety of other purposes. Error Logs ElastAlert can be very helpful for trying to make sense of things like error logs. We’ll assume that error messages are parsed in such a way that they contain an error_type . You could use type: any and alert on every single error message. However, this might not scale well. You might only care about “has the number of errors spiked?” Often, setting an explicit threshold is hard, as baseline normal may vary over time. To handle this, we can use the spike type. # Alert if number of errors triples in an hour\nfilter:\n  - term:\n      _type: error\ntype: spike\nspike_height: 3\nspike_type: up\nthreshold_ref: 10\ntimeframe:\n  hours: 1\ntop_count_keys:\n  - error_type With this rule, we are comparing the number of errors in the last hour with the hour before that. If the current hour contains more than 3x the previous hour, and the previous hour contains at least 10 events, it will alert. If this is too sensitive, increasing the timeframe would effectively smooth out the average error rate. By setting top_count_keys , the alert will contain a breakdown of the most common types which occurred within that spike. If our errors also had a URL associated with them, you could group by that, and alert only if a large number of errors occurred for a single URL. This is all well and good if we don’t care about common errors sending us alerts, but critical error messages could still sneak by. Another approach we could take is to send an alert only when a new, never before seen, error message occurs. # A new error type occurred\nfilter:\n  - term:\n      _type: error\nquery_key: error_type\ntype: new_term\ninclude:\n  - traceback Service Monitoring ElastAlert can be very useful for monitoring the health of hosts and services. In these examples, we will assume there is some service, hosted on multiple machines, which logs requests and response codes. Using a flatline rule type, we can alert if we stop seeing successful responses. By grouping the hostname, we can get an alert if just a single host stops. # Alert if a host stops serving 200 responses for an hour\nfilter:\n  - not:\n      range:\n        response_code:\n          from: 500\n          to: 599\ntype: flatline\nthreshold: 1\ntimeframe:\n  hours: 1\nquery_key: hostname With this rule, we allow new hosts to be added to the server pool and they will be automatically monitored. You could also modify the filters such that a “healthy” response is a range of status codes. Another thing we could look at is timing. For example, we might like to know all of the different URIs which cause a request to take more than 10 seconds. We can create an alert which will send a report of all requests which occur during a week which take this long. # All requests in a week which took over 10 seconds\nfilter:\n  - range:\n      response_time:\n         from: 10\ntype: any\naggregation:\n  weeks: 1\ninclude:\n  - ip_address\n  - hostname\n  - uri\n  - response_time These are just a few possible use cases. With custom rule types or alerts, anything is possible. If you can get it into Elasticsearch, you can monitor and alert on it. For a full list of features, as well as a tutorial for getting started, check out the documentation . Source can be found on Github . Pull requests and bug reports are always welcome! If you have any questions, jump into our Gitter channel . Tweet Securing The Yelps If you're interested in building tools like ElastAlert that help us secure Yelp and its users, apply to become a Software Security Engineer View Job Back to blog", "date": "2016-03-23"}, {"website": "Yelp", "title": "Distributed tracing at Yelp", "author": ["\n        \n  Prateek A., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/04/distributed-tracing-at-yelp.html", "abstract": "Yelp is powered by more than 250 services , from user authentication\nto ad delivery. As the ecosystem of services has grown since 2011, performance\nintrospection has become critical. The tool we chose to gain visibility was Zipkin , an open source distributed tracing framework. Since most of our\nservices are built with Pyramid , we built instrumentation for Zipkin called pyramid_zipkin and a Swagger client decorator, swagger_zipkin .\nBelow, we walk you through how we use these powerful tools at Yelp and how you\ncan leverage them for your organization. Why Distributed Tracing is Critical in an SOA Environment If you get an alert after a deployment that the average timing for a particular\nendpoint has increased by 2-3x, how would you start debugging this? For us, we\nrely on distributed tracing. Distributed tracing is a process to gather timing\ninformation from all the various services that get invoked when an endpoint is\ncalled. In the diagram below, we can see that out of a total ~1.8s, 0.65s were\nconsumed by one service, allowing us to investigate further. As more services\nget called, this becomes immensely helpful in debugging performance regressions. Another benefit of distributed tracing is that it helps service owners know\nabout their consumers. Using aggregation and dependency graph tools, the\nservice owner can check which upstream services rely on them. When a particular\nservice wants to deprecate their API, they can notify all their consumers about\nit. With tracing, they can track when all traffic to the deprecated API stops,\nknowing it’s safe to remove the API. Background To understand distributed tracing we first need to cover some terminology.\nThere are two key terms that need to be explained: span - is information about a single client/server request and response trace - is a tree of spans which represent all service calls triggered\nduring the lifecycle of a request As shown in the diagram above, a span lifetime starts with the request invocation\nfrom the client (Service A) and ends when the client receives back the response\nfrom the server (Service B). To create a full featured trace, the system needs to ingest proper span\ninformation from each service. The four necessary points to build a span, as seen\nin the diagram above, (in chronological order) are: Client Send (CS): timestamp when upstream service initiated the request. Server Receive (SR): timestamp when downstream service receives the request. Server Send (SS): timestamp when downstream service sends back the response. Client Receive (CR): timestamp when upstream service receives back the response. CS and CR as client side span information and SR and SS as server side span information. Introducing Zipkin Zipkin provides a nice UI to visualize the traces and have them sorted by conditions\nlike longest query, etc. At Yelp, we use Kafka as the primary transport for Zipkin\nspans and Cassandra as its datastore. Zipkin consists of three basic parts: collector : reads from a transport (like Kafka) and inserts the traces to a datastore (like Cassandra) query : reads the traces from the datastore and provides an API to return the data in JSON format web UI : exposes a nice UI for the user to query from. It makes API calls\nto the query and renders the traces accordingly These components are explained in more detail in the Zipkin docs . One critical component to the system is the tracer . A tracer gets hooked into a\nservice (which is to be traced) and its responsibility is to send the service\ntrace information to the Zipkin collector while making sure it does not interfere\nwith the service logic and does not degrade service performance. Openzipkin/zipkin provides tracer support for various languages like Scala , Java and Ruby , but currently not Python, which we\nuse extensively. There is a third party tracer available for\nDjango but we wanted a solution for Pyramid so we implemented a Pyramid\nPython tracer which integrates with our services and sends data to Zipkin.\nThe tracer is called pyramid_zipkin . Server side span: Introducing pyramid_zipkin Most of our services run on Pyramid with uWSGI on top for load balancing\nand monitoring. Pyramid has a feature called tweens , which are\nsimilar to decorators and act like a hook for request and response processing. pyramid_zipkin is a tween which hooks into the service and records SR and SS\ntimestamps to create a span. A span will be sent to the collector based on two conditions: If the upstream request header contains X-B3-Sampled , it is accepted. This\nis only sent if the  value is “1”, otherwise it’s discarded. If no such header is present, pyramid_zipkin assumes the current service to\nbe the root of the call stack. It rolls a die to decide whether to record the\nspan or not. The probability is based on a configuration option tracing_percent which can be different for each service. pyramid_zipkin provides other features like recording custom annotations and\nbinary annotations. Complete documentation is available here . Connecting spans: Introducing swagger_zipkin Before getting into how we record client side spans, let’s explain how we connect\nZipkin spans. This is done by sending span_id and trace_id information in each\nrequest’s header. We do this with the help of our open source package swagger_zipkin . Most of our services have a corresponding Swagger schema attached to\nthem. Services talk to each other using our in-house Swagger clients, swaggerpy (supports schema v1.2) and bravado (supports schema v2.0). swagger_zipkin provides\na decorator to wrap these clients which enables sending zipkin-specific headers\nwith requests. Behind the scenes it calls pyramid_zipkin ’s API to get the necessary\nheader information. More details are available in the documentation . A sample service call looks like: from bravado.client import SwaggerClient from swagger_zipkin.zipkin_decorator import ZipkinClientDecorator client_b = SwaggerClient . from_url ( \"http://service_B/swagger.json\" ) zipkin_wrapped_client = ZipkinClientDecorator ( client_b ) response = zipkin_wrapped_client . resource . operation ( query = \"foo\" ). result () Client side span: Leveraging Smartstack It might seem obvious that the client spans ( CS and CR ) can easily be recorded\nby swagger_zipkin since it is already used to decorate the client calls. However,\nusing swagger_zipkin can lead to incorrect numbers when the request is made\nasynchronously, where the response is stored as a non-blocking future. The client\nmay ask for the result from this future at any point which could be five or even\nten seconds after the response was actually returned back by the server. The\ndecorator will include this waiting time in the CR resulting in misleading numbers. We need a way that’s not dependant on whether the call was made by a synchronous\nor an asynchronous client. We leverage our SmartStack setup to get\nthe correct CS and CR numbers. All of Yelp’s services are on SmartStack which means an upstream service connects to localhost with the port number assigned\nto the downstream service. A local HAProxy listens on this port, which\nthen forwards the request to the actual machine hosting the service. Conveniently, HAProxy can be configured to log requests and include relevant headers, timestamps,\nand durations. Thus by parsing HAProxy logs, we find CS (origin timestamp) and CR (origin timestamp + time taken for response) and send client span directly to the collector . Zipkin components: Powered by PaaSTA As discussed earlier, to complete the infrastructure we need to deploy the three\nZipkin components ( collector , query and web ). These are deployed as PaaSTA services ( more ). This part was relatively simple as Zipkin already provides Dockerfiles for each of these components. PaaSTA can read these Dockerfiles and launch the containers. Wrapping things up This blog post gave an overview about Zipkin as a distributed tracing tool and\nhow Python services can be integrated with this system. Zipkin is still under\nheavy active development thanks to its healthy open source community. We are\ntrying to improve its Python support to be more powerful and friendly to work\nwith. We built pyramid_zipkin and swagger_zipkin for this purpose. Go\ncheck them out! Tweet Want to help build such distributed systems tools? Like building this sort of thing? At Yelp we love building systems we can be proud of, and we are proud of our robust infrastructure. Check out the Software Engineer - Infrastructure positions on our careers page if you like building systems you can be proud of too! View Job Back to blog", "date": "2016-04-01"}, {"website": "Yelp", "title": "Dropdown Like It's Hot", "author": ["\n        \n  Maulik S., Front-end Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/04/dropdown-like-its-hot.html", "abstract": "As our number of mobile web visitors continues to increase, we want to make sure that the user experience is great on small screens. This quarter, we took some time to assess our reusable UI components and look for ways to improve their behavior across devices. We found a lot of room for improvement in our dropdown component - it looked great on desktop, but it was hard (i.e., in some cases it’d go offscreen or touch/tap being not very smooth) to use on mobile devices. Story We built our dropdown component to be generic enough to handle any use case our developers might need. Some dropdowns contains simple redirects, some are hooked up to make AJAX requests or sort elements on the page. We wanted to keep our changes to the dropdown component backwards compatible with all these behaviors. Our first thought was if we could improve it by just adding some media queries or JavaScript but that would still make cross-platform testing difficult. We also thought to replace dropdown with native select component (using the HTML <select/> tag). But, that would force us to deal with problems inherent to native select elements. We would need to write JS fallbacks and include other graceful degradations in order to style our native select elements to look like custom dropdowns. Solution We decided to show native selects on touch devices instead of a custom dropdown menu – a hybrid solution of our options. Our custom dropdown template and CSS files were left untouched; all the magic happens in the JavaScript component.\nWhen someone clicks or taps on the dropdown toggle, we detect whether the screen supports touch events. If it does, we create a native select instead of showing custom dropdown menu. dropdownToggle.on('click', function (event) {\n    if (shouldEnableNative()) {\n        showNativeSelect();\n    } else {\n        toggleCustomDropdownMenu();\n    }\n}); function shouldEnableNative() {\n// Checks if screen support touch events\n    return (('ontouchstart' in window) ||\n        window.DocumentTouch && document instanceof window.DocumentTouch\n    ) && window.matchMedia(dropdownOptions.mediaQuery || ‘all’).matches;\n} If shouldEnableNative() returns true , then we execute the showNativeSelect() method which creates a native select in the DOM. function showNativeSelect() {\n\t// Creates a new <select/> var select = getSelect();\n\tvar links = this.children.menu.find('.dropdown_link');\n\n\t// Prepares a list of options and wraps in select\n\tvar selectOptions = links.map(function () {\n\t\treturn $(' <option> ').text(this.textContent);\n\t});\n\tselect.setHTML(selectOptions.get());\n\n\t// Registers change event of select to trigger respective\n\t// custom dropdown option\n\tselect.one('change', function () {\n\t\t$(links.get(select[0].selectedIndex)).trigger('click');\n\t\tselect.hide();\n\t}.bind(this));\n\n\t// Appends it to the container\n\tselect.appendTo(this.container).show();\n\n\t// Opens native select on click of existing custom dropdown’s toggle\n\tvar e = document.createEvent('MouseEvents');\n\te.initMouseEvent('mousedown', true, true, window);\n\tselect[0].dispatchEvent(e);\n}; getSelect() creates a native select and returns it back to the showNativeSelect() . function getSelect() {\n\tvar styles = {\n\t\t'position': 'absolute',\n\t\t'bottom': '0',\n\t\t'width': '0',\n\t\t'appearance': 'none',\n\t\t'border': '0'\n\t};\n\n// Creates a new select in DOM and make it invisible\n\treturn $(' <select> ', { 'class': 'js-dropdown-mobile-select' })\n\t\t.css(styles)\n\t\t.hide();\n}; This solution not only saved  our time but allowed us to maximize mobile accessibility while retaining custom dropdown styles for desktop users. And now, Gifshots! Dropdowns on non-touch devices stays custom. And, on touch supported devices becomes native. Tweet Join Yelp as Front-end Engineer Interested in building reusable, responsive components? Join Yelp! If you care about good code, good design, and an outstanding user experience, we want to hear from you! View Job Back to blog", "date": "2016-04-13"}, {"website": "Yelp", "title": "Yelp Restaurant Photo Classification, Winner's Interview: 1st Place, Dmitrii Tsybulevskii", "author": ["\n        \n  Fang-Chieh C., Data Mining Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/04/yelp-kaggle-photo-challenge-interview-1.html", "abstract": "A few months ago, Yelp partnered with Kaggle to run an image classification competition , which ran from December 2015 to April 2016.  355 Kagglers accepted Yelp’s challenge to predict restaurant attributes using nothing but user-submitted photos.  We’d like to thank all the participants who made this an exciting competition! Dmitrii Tsybulevskii took the cake by finishing in 1st place with his winning solution. In this blog post, Dmitrii dishes on the details of his approach including how he tackled the multi-label and multi-instance aspects of this problem which made this problem a unique challenge. This interview blog post is also published on Kaggle’s blog . The Basics What was your background prior to entering this challenge? I hold a degree in Applied Mathematics, and I’m currently working as a software engineer on computer vision, information retrieval and machine learning projects. Dmitrii on Kaggle Do you have any prior experience or domain knowledge that helped you succeed in this competition? Yes, since I work as a computer vision engineer, I have image classification experience, deep learning knowledge, and so on. How did you get started competing on Kaggle? At first I came to Kaggle through the MNIST competition, because I’ve had interest in image classification and then I was attracted to other kinds of ML problems and data science just blew up my mind. What made you decide to enter this competition? There are several reasons behind it: I like competitions with raw data, without any anonymized features, and where you can apply a lot of feature engineering. Quite large dataset with a rare type of problem (multi-label, multi-instance). It was a good reason to get new knowledge. Let’s get technical: What preprocessing and supervised learning methods did you use? Outline of my approach depicted below: Photo-level feature extraction One of the most important things you need for training deep neural networks is a clean dataset. So, after viewing the data, I decided not to train a neural network from scratch and not to do fine-tuning. I’ve tried several state-of-the-art neural networks and several layers from which features were obtained. Best performing (in decreasing order) nets were: Full ImageNet trained Inception-BN Inception-V3 ResNet The best features were obtained from the antepenultimate layer, because the last layer of pretrained nets are too “overfitted” to the ImageNet classes, and more low-level features can give you a better result. But in this case, dimensions of the features are much higher (50176 for the antepenultimate layer of “Full ImageNet trained Inception-BN”), so I used PCA compression with ARPACK solver, in order to find only few principal components. In most cases feature normalization was used. How did you deal with the multi-instance aspect of this problem? In this problem we only needed in the bag-level predictions, which makes it much simpler compared to the instance-level multi-instance learning. I used a paradigm which is called “Embedded Space”, according to the paper: Multiple Instance Classification: review, taxonomy and comparative study . In the Embedded Space paradigm, each bag X is mapped to a single feature vector which summarizes the relevant information about the whole bag X. After this transform you can use ordinary supervised classification methods. For the business-level (bag-level) feature extraction I used: Averaging of photo-level features Simple, but very efficient in the case of outputs of neural networks. Fisher Vectors Fisher Vector was the best performing image classification method before “Advent” of deep learning in 2012. Usually FV was used as a global image descriptor obtained from a set of local image features (e.g. SIFT), but in this competition I used them as an aggregation of the set of photo-level features into the business-level feature. With Fisher Vectors you can take into account multi-instance nature of the problem. VLAD descriptor Very similar to Fisher Vectors. After some experimentation, I ended up with a set of the following business-level features: Averaging of L2 normalized features obtained from the penultimate layer of [Full ImageNet Inception-BN] Averaging of L2 normalized features obtained from the penultimate layer of [Inception-V3] Averaging of PCA projected features (from 50716 to 2048) obtained from the antepenultimate layer of [Full ImageNet Inception-BN] L2 normalized concatenation of 2., 3. PCA projected 4. to 128 components. Fisher Vectors over PCA projected 3. to 64 components. VLAD over PCA projected 3. to 64 components. How did you deal with the multi-label aspect of this problem? I used Binary Relevance (BR) and Ensemble of Classifier Chains (ECC) with binary classification methods in order to handle the multi-label aspect of the problem. But my best performing single model was the multi-output neural network with the following simple structure: This network shares weights for the different label learning tasks, and performs better than several BR or ECC neural networks with binary outputs, because it takes into account the multi-label aspect of the problem. Classification Neural network has much higher weight(6) compared to the LR(1) and XGB(1) at the weighing stage. After all, 0, 1 labels were obtained with a simple thresholding, and for all labels a threshold value was the same. What didn’t work for you? Label powerset for multi-label classification RAkEL for multi-label classification Variants of ML-KNN for multi-label classification Removing duplicate photos XGBoost. I added some XGBoost models to the ensemble just out of respect to this great tool, although local CV score was lower. MISVM for multi-instance classification More image crops in the feature extractor Stacking. It’s pretty easy to overfit with a such small dataset, which has only 2000 samples. Were you surprised by any of your findings? Features extracted from the Inception-V3 had a better performance compared to the ResNet features. Not always better error rates on ImageNet led to the better performance in other tasks. Simple Logistic Regression outperforms almost all of the widely used models such as Random Forest, GBDT, SVM. Binary Relevance is a very good baseline for the multi-label classification. Which tools did you use? MXNet , scikit-learn , Torch , VLFeat , OpenCV , XGBoost , Caffe How did you spend your time on this competition? 50% feature engineering, 50% machine learning What was the run time for both training and prediction of your winning solution? Feature extraction: 10 hours Model training: 2-3 hours Words of wisdom: What have you taken away from this competition? A “Prize Winner” badge and a lot of Kaggle points. Do you have any advice for those just getting started in data science? Kaggle is a great platform for getting new knowledge. Just for fun: If you could run a Kaggle competition, what problem would you want to pose to other Kagglers? I’d like to see reinforcement learning or some kind of unsupervised learning problems on Kaggle. Bio Dmitrii Tsybulevskii is a Software Engineer at a photo stock agency. He holds a degree in Applied Mathematics, and mainly focuses on machine learning, information retrieval and computer vision. Tweet Become a Part of the Data Revolution at Yelp Interested in using machine learning to unlock information contained in Yelp's data through problems like this? Apply to become a Data-Mining Engineer. View Job Back to blog", "date": "2016-04-28"}, {"website": "Yelp", "title": "Announcing the Winners of Yelp's Online Hackathon", "author": ["\n        \n  Quy L., Product Mananger\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/05/yelp-online-hackathon-winners.html", "abstract": "The results are in for the inaugural Yelp online hackathon which was held a few months ago (Jan. 25-Mar. 25).  We were looking for cool, innovative uses of the Yelp API and received 60 submissions with 759 developers signed up to participate from around the world.  Winners were chosen based on originality, user experience and effective use of the Yelp API.  Our esteemed panel of judges poured over the entries and chose the following winners: 1st Place - Grand Prize Winner Yelpify by Raj Nagasubramanian Yelpify is a Chrome extension that adds Yelp ratings to Google search results, OpenTable, GrubHub and TripAdvisor.  Just install the extension on your Chrome browser and you can see Yelp ratings and review counts instantly without having to search on Yelp. 2nd Place - Runner-up onrute by Aaron Ludwig onrute is a website that searches for businesses from point A to point B. If you’re driving to a destination and craving for a hamburger, use onrute to show you a map of highly rated burger joints along the way. 3rd Place SnapPea by the team of Daisy Tsao, Carl Bernardo, Justin Tan and Shin Adachi SnapPea recommends restaurants based on your food preferences and allows you to invite your friends. You pick your preferred food item from a series of photos and SnapPea automatically recommends restaurants based on your choices. Congratulations to the winners! We’re really excited to see all of the apps that were developed. Check out the rest of the hackathon submissions on the gallery page . We’re planning more hackathons in the future so please follow Yelp Engineering on Twitter and Facebook for future announcements. Tweet Back to blog", "date": "2016-05-02"}, {"website": "Yelp", "title": "Yelp Restaurant Photo Classification, Winner's Interview: 2nd Place, Thuyen Ngo", "author": ["\n        \n  Fang-Chieh C., Data Mining Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/05/yelp-kaggle-photo-challenge-interview-2.html", "abstract": "The Yelp Restaurant Photo Classification competition challenged Kagglers to assign attribute labels to restaurants based on a collection of user-submitted photos. At the final tally, 355 players tackled this unique machine learning problem. Last week, we published Dmitrii Tsybulevskii’s 1st place solution here ; Thuyen Ngo finished in 2nd place, with an F1 score of 0.83168, and describes his strategy in this week’s interview. The Basics What was your background prior to entering this challenge? I am a PhD student in Electrical and Computer Engineering at UC Santa Barbara. I am doing research in human vision and computer vision. In a nutshell I try to understand how humans explore the scene and apply that knowledge to computer vision systems. Thuyen (AKA Plankton) on Kaggle How did you get started competing on Kaggle? My labmate introduced Kaggle to me about a year ago and I participated in several competitions since then. Do you have any prior experience or domain knowledge that helped you succeed in this competition? All of my projects involved images. So It’s fair to say that I have some “domain knowledge”. However, for this competition I think knowledge from image processing is not as important as machine learning. Since everyone uses similar image features (from pre-trained convolutional neural networks, which have been shown to contain good global descriptions of images and therefore are very suitable to our problem), the difficult part is to choose a learning framework that can combine information from different instances in an effective way. What made you decide to enter this competition? I am interested in image data in general (even though in the end there was not much image analysis involved). The problem is very interesting itself since there’s no black-box solution that can be applied directly. Let’s get technical What preprocessing and supervised learning methods did you use? Like most participants, I used pre-trained convolutional networks to extract image features, I didn’t do any other preprocessing or network fine-tuning. I started with the inception-v3 network from google but ended up using the pre-trained resnet-152 provided by Facebook. My approach is super simple. It’s just a neural network. How did you deal with the multi-instance and multi-label aspect of this problem? I used multilayer perceptron since it gave me the flexibility to handle both the multiple label and multiple instance at the same time. For multiple label, I simply used 9 sigmoid units and for multiple instance, I employed something like the attention mechanism in the neural network literature. The idea is to let the network learn by itself how to combine information from many instances (which instance to look at). Each business is represented by a matrix of size N x 2048, where N is the number of images for that business. The network is composed of four fully connected (FC) layers. Attention model is a normal FC layer but the activation is a softmax over images, weighting the importance of each image for a particular feature. I experimented with many different architectures, in the end, the typical architecture for the final submission is as follows: The model is trained using the business-level labels (each business is a training example) as opposed to image-level labels (like many others). I used the standard cross entropy as the loss function. The training is done with Nesterov’s accelerated SGD. What was your most important insight into the data? Finding the reliable local validation is quite challenging to me. It’s a multiple label problem, and thus there’s no standard stratified split. I tried some greedy methods to do stratified 5-fold split but it didn’t perform very well. At the end I resorted to a random 5-fold split. My submission is normally the average of 5 models from 5-fold validation. Another problem is that we only have 2000 businesses for training and another 2000 test cases. Even though it sounds a lot of data, training signals (the labels) are not that many. In combination with the instability of F measure, it makes the validation even more difficult. Since the evaluation metric is F1 score, it is reasonable to use F-measure as the loss function, but somehow I couldn’t make it work as well as the cross entropy loss. With limited labeled data, my approach would have badly overfitted the data (it has more than 2M parameters). I used dropout for almost all layers, applied L2 regularization and early stopping to mitigate overfitting. How did you spend your time on this competition? Most of the time for machine learning (training), 1% for preprocessing I guess. Which tools did you use? I used Tensorflow/torch for feature extraction (with the provided code from Google/Facebook) and Lasagne (Theano) for my training. What was the run time for both training and prediction of your winning solution? It takes about two and a half hours to train one model and about a minute to make the predictions for all test images. Words of wisdom What have you taken away from this competition? Neural networks can do any (weird) thing :) Do you have any advice for those just getting started in data science? Just do it, join Kaggle, participate and you will improve. Bio Thuyen Ngo is a PhD student in Electrical and Computer Engineering at the University of California, Santa Barbara. Tweet Become a Part of the Data Revolution at Yelp Interested in using machine learning to unlock information contained in Yelp's data through problems like this? Apply to become a Data-Mining Engineer. View Job Back to blog", "date": "2016-05-04"}, {"website": "Yelp", "title": "Yelp Android App Went On A Diet", "author": ["\n        \n  Coltin Caverhill, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/05/yelp-android-app-went-on-a-diet.html", "abstract": "Whether it’s battery usage, network, or time, we care a lot about our users’ resources. A big app creates a barrier of entry for users on metered or low speed networks. Last Thanksgiving, our automated alerts let us know that our app size was getting larger than we’d like, and making it harder for low resource users to download our app. Here we saw an upward trend in app size as we added more features and built a more compelling app. In order to tackle app size, we first had to understand where that size was coming from. Breakdown of a release build around September 2015. Looking at these two graphs we could see that images took up a large percentage of the overall APK size. But why did they take up a larger percentage of space once it was compressed? The reason was that images are not compressed during the APKs zipping process. This is to allow images to be memory mapped from disk rather than loading them into RAM. This is a great optimization of modern operating systems like Android, but means that we cannot rely on the APK process to optimize images for us. Every byte in an image translates to a byte in the final APK. It started to look like we could make a big difference by reducing the file size of our images. On our hunt for potential improvements, we learned that Android 4.2.1 introduced a new image format called WebP that claimed better compression than JPEG or PNG . Fortunately, the Yelp app supports Android 4.4+, and its images are mostly PNGs with a few JPEGs. To test WebP’s claims, we converted over 2000 of our PNGs with this command: cwebp -pass 10 -m 6 -mt -lossless [ 1 ] Only 8 of the over 2000 PNGs we compressed did not benefit from the conversion to WebP, and the difference for those was extremely slight (only around 400 bytes). Overall our compressed APK size dropped from 27.1MB to 23.1MB with no loss in the quality of images. Our breakdown still favoured images by quite a bit, but not as significantly. This size reduction was great, but we had concerns about runtime performance. Would WebP require a lot more processing time to decode/render? Would we end up using more memory? Using Android’s performance tools , we didn’t find any increase in memory or CPU load when loading WebP images compared to their PNG variants on a variety of devices. Similar results were reported by this paper on WebP , which is Android agnostic. Having no remaining concerns, we moved this to production. We set up a pre-commit hook that converts PNGs automatically to lossless WebP . This allowed us to enjoy a much smaller app without having to do any extra development work - win! However the story doesn’t end there… What about lossy content like JPEGs for photos? As part of an experiment for onboarding, we added three new photos to dazzle and amaze new users. However, when this experiment was pushed to master, our alerting let us know it was very unhappy. Why was it so mad about this new picture!? It turned out that some of these images were pretty large JPEGs, the largest being 1.4MB. Remember that these will not be compressed in the final APK, so each byte of every image will ship to users. All the new assets combined to give us a 34% increase in APK size over our previous release, from 21.88MB to 29.39MB! To compress these images, we tried converting to lossless WebP like we did before. However, these new images were rich photos with lots of color, so we didn’t see similar gains from a lossless compression. With lossless compression a dead end, we tried using lossy compression instead: cwebp -pass 10 -m 6 -mt -jpeg_like -q 60 [ 2 ] 1,463KB lossless 92KB 60 quality lossy 62KB 40 quality lossy The reductions were huge! After converting all the new images, our final APK size was 22.76MB, which was a great improvement over 29.39MB. Because this process was highly subjective, we did not add this to our pre-commit hooks. Some images, like icons, should always be very high quality and only be compressed losslessly; while others, like large photos, will benefit from various degrees of lossy compression. Making that decision is hard to automate. For those stuck with PNGs (supporting a minSdkVersion less than 17) or those who aren’t quite ready to jump ship to WebP, Colt McAnlis wrote a very detailed article on PNG compression . It includes many methods to removing unnecessary bytes from images that fall outside the scope of this post. WebP conversion was a quick and easy way for us to dramatically reduce our APK size. If your app uses a lot of images, this strategy should help you too! While the effects of reducing APK size are not well known, it’s still pretty much a no-brainer: when we can easily reduce our impact on our users’ storage and data usage, we will! Footnotes Cwebp is a program provided by Google to convert images to WebP. Here, we let it do the maximum amount of analysis passes (-pass 10). We set the compression method to the slowest (-m 6) in order to get the best compression. We enabled multithreading (-mt) for some speed improvements. And finally, we specified that we want the image to be lossless (-lossless) so that we’ll have no quality lost in our produced WebP image. There are many more options available for the astute reader. ↩ Here we’ve removed the lossless (-lossless) option so that cwebp will actually remove image quality. We told cwebp to use a compression method similar to JPEG (-jpeg_like) because we know these are all photos. And finally we have the quality setting (-q 60) which is similar but different from JPEG quality settings . The value of 60 was selected after experimenting with different values for these specific photos, and seemed to give us the biggest reduction in file size before much quality loss was noticeable. Your specific images and quality thresholds will vary. ↩ Tweet Back to blog", "date": "2016-05-12"}, {"website": "Yelp", "title": "It's The Holiday Season and We’re Giving You A Present: PaaSTA!", "author": ["\n        \n  Julia N., Senior Events and Partnerships Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/12/its-the-holiday-season-and-were-giving-you-a-present-paasta.html", "abstract": "In case you missed it last month, we open sourced PaaSTA , our platform-as-a-service which received a lot of excitement and support from the community. We want to bring our friends together to learn more, so we’re hosting our last Yelp Tech Talk of the year in conjunction with Mesosphere on December 17th, featuring PaaSTA + Mesos! Our lead engineer on PaaSTA, Kyle Anderson, will introduce the platform and speak to how it uses Mesos to power our SOA architecture while Sunil Shah, from Mesosphere, will speak on Mesosphere’s DCOS and how it enables PaaS tools like PaaSTA to exist. We’ll also be hosting the SF Dev Ops meetup that same week featuring Mitchell Hashimoto, the creator of Vagrant, so make sure to mark your calendars! Tuesday, December 8, 2015 - 6:45PM - Products That Count Holiday Party ( Products That Count ) Thursday, December 10, 2015 - 6:30PM - Drawing from Life: San Francisco, Whiskey and Other Issues of Importance ( Designers + Geeks ) Monday, December 14, 2015 - 7:00PM - Mitchell Hashimoto, creator of Vagrant, speaks about Nomad and Otto ( SF Dev Ops ) Thursday, December 17, 2015 - 6:15PM - Yelp Tech Talk Series: PaaSTA + Mesos with Mesosphere ( Yelp Engineering ) Tweet Back to blog", "date": "2015-12-01"}, {"website": "Yelp", "title": "How We Made Yelp Search Filters Data Driven", "author": ["\n        \n  Ray M. G., Software Engineer Data Mining\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/12/how-we-made-yelp-search-filters-data-driven.html", "abstract": "Yelp has an incredible amount of information about businesses to power local search experiences. Many of our users use our advanced search filters to explore and find exactly the place they are looking for. While most people don’t have any trouble filtering their searches using filters such as price, distance, and rating, it was harder for users to employ our more specialized filters such as “Outdoor Seating” or “Live Music”. We set off on a mission to make our advanced filters more approachable for casual users without hindering the experience for our advanced users. Before designing the new filters, we wanted to better understand how our users engaged with them, so we began digging into the data. People typically choose no more than a few filters at a time. The filter(s) they chose largely depended on the search query they used. We started by doing a lengthy data exploration. We came up with a set of exploratory data questions that would guide our design, such as: What are the most popular filters? When and how often do people use filters? Which filters have the highest impact for search? Which filters do we over-show and are under-used and vise versa? For example, the plot below shows that of all the searches that use filters, most of them only use a single filter. This tells us that we want a simple design to highlight only a few filters relevant to the search. Based on the exploration, we came up with a new design shown below. Not only does the new design take significantly less space, reducing the time it takes to get to the search results, but hides less relevant filters for the users that want to use them. By revealing the most relevant filters based on a search, we are able to cater to almost all users. If we are unable to accurately make a prediction, all of Yelp’s powerful filters are still available a click away. Old Search Filters New Search Filters Building a model to recommend filters We wanted to build a model that can take a set of informative features such as the query, date & time, location, personal preferences and a variety of other feature to suggest a set of most relevant filters to show to our users. One of the most important features here is the query string: what the user is searching for will have a huge impact on the filters they plan to use. We can see this easily from our data. For example, searching for “birthday dinners” or “romantic dinners” often leads to the make a reservation filter. When people search for “coffee shops”, they often want to look for free-wifi or outdoor-seating. Unfortunately, the query text is a super-sparse, long-tailed, feature with high cardinality (we receive tens of millions of distinct queries in a year in over 30 different countries), making it difficult to engineer and feed into a model.  We wanted to build a function that maps the query text into a single number that tells us how relevant is this query to a particular filter. Ideally this function should be continuous and smooth; “Birthday dinner” and “Best birthday dinners” should produce very similar scores. The simplest approach here is look at our click data and see which filters people use for which queries and generate a signal based on that. However, this is not a very flexible approach as click data and can be very sparse and drops off quickly after the top queries. In addition, approximately 10 - 15% of queries are ones that we have never seen before. This is where language models become useful. A language model can be thought of as a function that takes as an input a sequence of words and returns a probability (likelihood) estimate of that sequence. If we can build a language model for every search filter, we can use it to calculate the posterior probability of all filters being used given the query and thus help decide which filters are most relevant to our query. To calculate the posterior probability of P(filter | query), we use a simple application of Bayes rule (see diagram above). Intuitively, we want to ask how probable a query is to be generated by some particular filter language model versus how common the query is overall in the language of Yelp queries. In the diagram above searching for “after work bars”, HappyHour and OutdoorSeating are given a positive score in our model and deemed relevant whereas GoodForBrunch and GoodForKids are deemed as not relevant. Data Analysis We launched the new filter redesign as an experiment and waited for results to roll in. The data analysis here is rather tricky; we completely rebuilt the filter panel and needed to carefully define what metrics we want to measure. Filter Engagement: Increase user engagement with Yelp search filters because they help our users discover relevant content faster. Search Quality: Improve the search experience by providing more relevant content through recommending better filters. Filter Engagement The first metric appears simple: loosely speaking we want to increase total number of filter clicks normalized by the total number of searches in each cohort. But as with all data analysis, there are many caveats to consider. For example: We are reducing the number of filters we show and the total real-estate of the filter panel. The more rigorous metric here would be click density, or clicks per pixel. Since this an optimization problem over a large parameter space (of filters), we are inevitably going to increase usage for some filters but decrease for others. Which ones do we care about more? In the new filter panel, users can actually access all the old filters by clicking on the all-filters button. We need to keep track of the source of the clicks to make sure the users are actually interacting with the new panel. Being such a prominent feature, we need to account for the novelty effect and run our experiment for a sufficient amount of time. However for simplicity, the filter usage in the experimental cohort was about 20% more than that in the status quo. Certain filters such as Open Now and Happy Hour received huge gains in usage while others saw slight decreases. This is important feedback that tells us which filters we over-weighted and which ones are under-weighted. Search Quality Increasing filter engagement is good but ultimately our goal is to improve the search experience for our users. But how do we measure “search experience”? The most frequently used success metrics are clicked based. (ie: Click Through Rate (CTR), MAP (Mean Average Precision), or RR (Reciprocal Rank)). The graph below shows that over 10 days, we saw a consistent, statistically significant (p-value « 0.05), increase in search session CTR in the experimental cohort. While clicks are easy to measure, they can often be the most deceptive metrics. Just because a user clicks on a result, doesn’t make it relevant.  Instead, we use a number of other metrics that more closely couples to user interactions with the site to define success. For example, this includes user engagement metrics on the business page following a search and time it takes for a user to find relevant results. Future In this blog, we mostly discussed the usage of filter language models to build an engine to power our new search filters UI. These new filters were very well received and beat our expectations on almost every metric. However, as mentioned earlier, the filter language model is one of many powerful features that can be combined to make an even better prediction. Stay tuned to hear about follow up work on building a machine learning model to combine these features. Acknowledgements : Special thanks to the whole filter redesign team. Back-end by Ray M. G.; Front-end by Peter R; Design by Taron G.; Product Management by Natarajan S. Tweet Become a Part of the Data Revolution at Yelp Interested in using natural language processing and machine learning to build amazing features like this for Yelp and our users? Apply to become a Data-Mining Engineer. View Job Back to blog", "date": "2015-12-02"}, {"website": "Yelp", "title": "Introducing the Yelp Restaurant Photo Classification Challenge", "author": ["\n        \n  Daniel Y., Data Mining Tech Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/12/yelp-restaurant-photo-classification-kaggle.html", "abstract": "We’re excited to release our first image dataset with hundreds of thousands of user-submitted photos as part of a challenge to all data scientists, launching this week on Kaggle ! Yelp’s users provide several kinds of “unstructured” data such as reviews, photos, and videos.  They can also answer structured questions like, “Is the restaurant romantic?”  These structured answers are incredibly useful to users who want a quick summary of important attributes of a business. We want to know: can you extract these attributes from our photos dataset, and what is the right way to approach this problem? If this type of challenge excites you, we have good news for you: we have dozens of similar problems and want your help solving them!  Think of this challenge as a way to get to know Yelp, and your submission as a way for us to get to know you.  The best solutions will get invitations to interview directly with the engineers that tackle these problems routinely. Join us in exploring this data and help us push the envelope for using machine learning to unlock structured information from our user generated content. Introducing the competition For this competition, participants are tasked with creating a model which predicts binary business attribute labels for restaurants based on the restaurant’s photos.  A dataset has been created which consists of user-submitted photos for a subset of restaurants on Yelp, where each restaurant has a variable number of photos. Nine user-reported business attributes are also attached to each restaurant; these are binary tags which can only take on “yes” or “no” values, such as “good for lunch”, “classy ambience”, or “is expensive”. Since the data is user-submitted, there is a certain amount of noise - for example, some images may be duplicated due to a user submitting the same photo twice. As per other Kaggle competitions, this data is split into a “training” data set, where the business attributes are supplied, and a “test” data set, where the business attributes are withheld. The challenge is to develop a model or algorithm based on the training data, and apply it to the test data. The test data submissions are used to evaluate the model’s accuracy via a metric known as the mean F1 score , which ranges from 0 (worst) to 1 (best). We have also prepared several baseline models to facilitate benchmarking and comparisons. Our first model is just a naive random guesser: it makes a random assignment for each attribute with equal probability. Random guessing results in a score of 0.4347. Our second model plays with color: it calculates the color distribution of all the images of a test business, compares that to the average color distribution of businesses with positive attribute values and negative attribute values respectively, and assigns the value with a more similar color distribution to the test business. This is harder to beat - our color feature benchmark results in a score of 0.6459. While our color model benchmark is simple, it’s still able to perform quite a bit better than random guessing. The rest is up to you - will you train models on single image samples, or should you use a multi-instance cost function? Will the latest and greatest deep neural networks be effective? Let us know through the leaderboard ! Prizes The prize for this competition is a fast track through Yelp’s recruiting process and an opportunity to show our data mining teams just what you’ve got! Keep in mind though, Yelp is not just looking for your best model; we are looking for data mining engineers that can help us use our data in novel ways while also pushing high-quality code to production. For more information about opportunities to use your machine learning and statistics skills at Yelp, check out our data mining career page . Check it out! Head over to the Kaggle competition to get a taste of the challenges our engineers solve every day. Tweet Become a Part of the Data Revolution at Yelp Interested in using machine learning to unlock information contained in Yelp's data through problems like this? Apply to become a Data-Mining Engineer. View Job Back to blog", "date": "2015-12-23"}, {"website": "Yelp", "title": "Introducing dumb-init, an init system for Docker containers", "author": ["\n        \n  Chris K., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/01/dumb-init-an-init-for-docker.html", "abstract": "At Yelp we use Docker containers everywhere: we run tests in them, build tools\naround them, and even deploy them into production . In this post\nwe introduce dumb-init , a simple init system written in C which we\nuse inside our containers. Lightweight containers have made running a single process without normal init\nsystems like systemd or sysvinit practical. However, omitting an init system\noften leads to incorrect handling of processes and signals, and can result in\nproblems such as containers which can’t be gracefully stopped, or leaking\ncontainers which should have been destroyed. dumb-init is simple to use and solves many of these problems: you can just add\nit to the front of any container’s command, and it will take on the role of PID\n1 for itself. It immediately spawns your process as PID ~2, and then proxies on\nany signals it receives. This helps to avoid special kernel behavior applied to\nPID 1, while also handling regular responsibilities of the init system (like\nreaping orphaned zombie processes). The motivation: modeling Docker containers as regular processes What we really want is to be able to treat Docker containers just like regular\nprocesses, so that we can slowly migrate our tools and infrastructure toward\nDocker. Instead of forcing developers to unlearn their existing workflow, we\ncan move individual commands into containers without developers even realizing\nthey’re spawning a Docker container on each invocation. It also lets us take a practical approach to Docker in development: rather than\nrequire that everything live in a container, we can choose to use containers\nwhen it makes sense from a business or technical perspective. To achieve this goal, we want processes to behave just as if they weren’t\nrunning inside a container. That means handling user input, responding the same\nway to signals, and dying when we expect them to. In particular, when we signal\nthe docker run command, we want that same signal to be received by the process\ninside. Our quest to model Docker containers as regular processes led us to discovering\nmore than we ever wanted to know about how the Linux kernel handles processes,\nsessions, and signals. Process behavior inside Docker containers Containers are unique in that they often run just a single process, unlike\ntraditional servers where even a minimal install usually runs at least a\ncomplex init system, cron, syslog, and an SSH daemon. While single-process containers are quick to start and light on resources, it’s\nimportant to remember that, for most intents and purposes, these containers are\nfull Linux systems. Inside your container, the process running as PID 1 has\nspecial rules and responsibilities as the init system. What is PID 1 inside a container? There are two common scenarios. Scenario 1: A shell as PID 1 A quirk of Dockerfiles is that if you specify your container’s command without\nusing the recommended JSON syntax , it will feed your\ncommand into a shell for execution. That results in a process tree that looks like: docker run (on the host machine) /bin/sh (PID 1, inside container) python my_server.py (PID ~2, inside container) Having a shell as PID 1 actually makes signaling your process almost\nimpossible. Signals sent to the shell won’t be forwarded to the subprocess, and\nthe shell won’t exit until your process does. The only way to kill your\ncontainer is by sending it SIGKILL (or if your process happens to die). For this reason, you should always try to avoid spawning a shell. If you can’t\neasily avoid that (for example, if you want to spawn two processes), you should exec your last process so that it replaces the shell. Scenario 2: Your process as PID 1 When you use the recommended syntax in your Dockerfile, your process is started\nimmediately and acts as the init system for your container, resulting in a\nprocess tree that looks like: docker run (on the host machine) python my_server.py (PID 1, inside container) This is better than the first scenario; your process will now actually receive\nsignals you send it. However, being PID 1, it might not respond to them quite\nas you expect it to. Trouble signaling PID 1 The Linux kernel treats PID 1 as a special case, and applies different rules\nfor how it handles signals. This special handling often breaks the assumptions\nthat programs or engineers make. First, some background. Any process can register its own handlers for TERM and use them to perform cleanup before exiting. If a process hasn’t registered\na custom signal handler, the kernel will normally fall back to the default\nbehavior for a TERM signal: killing the process. For PID 1, though, the kernel won’t fall back to any default behavior when\nforwarding TERM . If your process hasn’t registered its own handlers (which\nmost processes don’t), TERM will have no effect on the process. Since we’re modeling containers as processes, we’d like to just send SIGTERM to the docker run command and have the container stop. Unfortunately, this\nusually doesn’t work. When docker run receives SIGTERM , it forwards the signal to the container and\nthen exits, even if the container itself never dies. In fact, the TERM signal\nwill often bounce right off of your process without stopping it because of the\nPID 1 special case. Even using the command docker stop won’t do what you want; it sends TERM (which the Python process won’t notice), waits ten seconds, and then sends KILL\nwhen the process still hasn’t stopped, immediately stopping it without any\nchance to do cleanup. Not being able to properly signal services running inside your Docker container\nhas lots of implications, both in development and in production. For example,\nwhen deploying a new version of your app, it might have to kill the previous\nservice version without letting it clean up (potentially dying in the middle of\nserving a request, or leaving connections open to your database). It also leads\nto a common problem in CI systems (such as Jenkins) where aborted tests leave\nDocker containers still running in the background. The same problem applies to other signals. The most notable case is SIGINT ,\nthe signal generated when you press ^C in a terminal. Since this signal is\ncaught even less frequently than SIGTERM , it can be especially troublesome\ntrying to manually kill servers running in your development environment. dumb-init to the rescue To address this need, we created dumb-init, a minimal init system intended to\nbe used in Linux containers. Instead of executing your server process directly,\nyou instead prefix it with dumb-init in your Dockerfile, such as CMD\n[\"dumb-init\", \"python\", \"my_server.py\"] . This creates a process tree that looks\nlike: docker run (on the host machine) dumb-init (PID 1, inside container) python my_server.py (PID ~2, inside container) dumb-init registers signal handlers for every signal that can be caught, and\nforwards those signals on to a session rooted at your process. Since your\nPython process is no longer running as PID 1, when dumb-init forwards it a\nsignal like TERM , the kernel will still apply the default behavior (killing\nyour process) if it hasn’t registered any other handlers. Using a regular init system also solves these problems, but at the expense of\nincreased complexity and resource usage. dumb-init is a simpler way to do\nthings properly: it spawns your process as its only child, and proxies signals\nto it. dumb-init won’t actually die until your process dies, allowing you to do\nproper cleanup. dumb-init is deployed as a statically-linked binary with no extra dependencies;\nit’s ideal to serve as a simple init system, and can typically be added to any\ncontainer. We recommend using it in basically any Docker container; not only\ndoes dumb-init improve signal handling, but it also takes care of other\nfunctions of an init system, such as reaping orphaned zombie processes. You can find much more information about the role of dumb-init and how to start\nusing it on its GitHub page . Tweet Back to blog", "date": "2016-01-06"}, {"website": "Yelp", "title": "Critical CSS Middleware: Inlining The Important CSS rules On-The-Fly", "author": ["\n        \n  Adam W., Software Engineer Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/01/critical-css-middleware-inlining-the-important-css-rules-on-the-fly.html", "abstract": "Website performance can be judged in a lot of ways, but perhaps the most\nimportant is user-perceived performance: the amount of time that is taken\nbetween clicking a link and having the desired page rendered on the screen. A\nbig part of keeping things feeling snappy is understanding which bits of\ncontent are blocking the “critical rendering path,” and coming up with ways to\nshorten or unblock them. At Yelp we focused on shortening the process of\nloading our CSS stylesheets. Before the browser can begin rendering the page, it needs to have its HTML\nmarkup and CSS rules. Usually, these rules are included in an external\nstylesheet linked from the document’s <head> : <link rel= \"stylesheet\" type= \"text/css\" media= \"all\" href= \"http://example.com/coolstyles.css\" > A tag like this tells the browser “before you render anything, download,\nparse and evaluate coolstyles.css .” This requires another whole HTTP\nrequest! That’s costly, especially on high latency connections. Furthermore,\nit’s likely that we don’t need all of the rules in that stylesheet to\nproperly render the first view of the page. A lot of them are only used by\ninteractive elements like modals and search drop-downs, which aren’t visible\ninitially. Blocking page render to fetch potentially useless CSS rules is wasteful. Can we\ndo better? Elements such as modals aren’t visible on initial page load, therefore these\nstyles are non-critical. We can avoid the extra HTTP request by including our entire ruleset in an\ninline <style> tag instead of an external one, but that has its own set of\nissues. If we inline our entire CSS ruleset, we’ve severely increased the HTML\ndocument size. The browser’s ability to cache the CSS files for future visits\nhas also been removed, which is generally not good. So we can’t inline all of our rules, but what about a critical subset of them?\nIt’s already been established that there are probably way more rules in the\npackage than needed for a first page render, so if these extra rules can be\nfound and eliminated, the size of the inlined rules would be significantly\nreduced. There are actually several existing ways to do this sort of elimination. One\nexample is CriticalCSS , a Grunt task that can be used during a\nbuild process to generate critical CSS files. Other options include Critical , a Node.js package,\nand Penthouse . Most of these alternatives work by rendering the\npage in a headless browser process and determining from there which rules end\nup being important to the final page. We decided to take a different approach. Yelp is a big site with a lot of\ndifferent pages, and permutations of those pages which may require different\nsets of rules. Accounting for all of them at build time would be very\nchallenging. Instead, our solution acts as a middleware which intercepts every\nHTML response before it’s sent to the client and alters the markup to inline\ncritical CSS rules. It looks like this: The middleware processing occurs after our main application is done generating\nthe response, right before we send it out to users. It functions very similarly\nto Google’s Critical CSS PageSpeed module . However, that\nsolution works at the Apache level and is designed to be useful in all\napplications generally, whereas our middleware is tailored specifically to\nYelp. Our middleware needs to: parse the HTML document to figure out which elements exist on the page parse the CSS and determine what rule selectors we’re dealing with use this information to eliminate non-essential rules Lacking a headless browser to do a full rendering of the DOM, we took a simpler\napproach to rule elimination. By constructing a unique set of all the classes\nand IDs present on the page, we can confidently eliminate CSS rules referencing\nelements which do not exist. What’s left is any rule that uses nothing but\ngeneral tag types (like <div> , <a> etc.) and rules that describe classes and\nIDs we did find on the page. Let’s say we have the following HTML document sent to us by the server: <html> <head> <link rel= \"stylesheet\" href= \"http://yelp.com/stylesheet.css\" > </head> <body> <div class= \"my-class\" ></div> <div id= \"my-id\" ></div> </body> </html> We have one external stylesheet in the <head> , as well as two <div> elements and a total of one unique class and one unique ID. Now let’s say the external stylesheet contains the following: a { color : blue ; } .my-class { display : block ; } div .unused-class { display : none ; } Given that the first rule applies to all <a> elements and does not specify\nany classes or IDs, it is included in the critical set automatically. The\nsecond rule does specify a class, which is also present in the document, so it\nis included as well. The third rule specifies a class which does not exist on\nthe DOM, so it is excluded from the critical set. The reduced set of rules is then inserted as an inline <style> tag, and the\noriginal external link tag is moved to the bottom of the document to make it\nload asynchronously. <html> <head> <style> a { color : blue } .my-class { display : block } </style> </head> <body> <div class= \"my-class\" ></div> <div id= \"my-id\" ></div> <link rel= \"stylesheet\" href= \"http://yelp.com/stylesheet.css\" > </body> </html> Even with this cautious approach, the results are promising: we went from 4000+\nCSS rules to just over 800 when running our middleware. That’s an 80%\nreduction! A more useful result is the number of bytes this reduction saves us when\ninlining the critical styles. The original stylesheets were 62 KB in total, but\nthe reduced set is only 31.3 KB. Eliminating half the total size is obviously a\nbig win, and with a more aggressive elimination approach this could be reduced\neven further. With this reduced set of CSS rules, we can create an inline style tag and move\nthe external stylesheets out of the <head> , unblocking the critical rendering\npath! Barring any other blocking resources, it should now be possible to render\nthe page as soon as the HTML is finished downloading, instead of waiting for\nthe CSS. We can then load the full stylesheet asynchronously using a special\nJavaScript library called loadCSS . The page will render quicker\nwhile also receiving the full set of rules – needed for things like\nmodals – before they appear on the page. Once this asynchronous download\ncompletes, the browser will cache the CSS. Using a cookie to track the version\nof our assets seen by users on their last visit, we can determine whether the\ncurrent version is already cached and skip the optimization if the latest CSS\npackages are in their cache. Results The results of this optimization are generally positive. A way to measure its\neffect is by using Chrome/IE’s firstPaint event, which occurs when the page\nis painted on the screen for the first time. Below are the 50th, 75th and 95th\npercentile timings when our middleware is enabled and disabled: First Paint Timings (available in Chrome and IE) Percentile Enabled (ms) Disabled (ms) Difference 50th 209.0 293.0 29% 75th 462.0 602.5 23% 95th 1702.0 1823.25 7% Data like this shows a very promising improvement, but this optimization isn’t\nwithout its drawbacks. Due to the parallel nature of browser resource\ndownloading, there are many cases where the external CSS request is kicked off\nas soon as the link tag is encountered in the head, and it may finish before\nthe full HTML document has finished downloading. A case like this might happen\nwhen, for example, the CSS is much smaller than the full HTML document.\nApplying the middleware optimization in these situations actually hurts\nperformance, because it prevents the browser from downloading the CSS and HTML\nat the same time. Luckily, most of Yelp’s pages have relatively slim HTML\ndocuments compared to the size of the CSS, so the middleware makes sense in the\nmajority of cases. Another issue is the added server-side processing time. We use a popular\nlibrary called tinycss to parse through our CSS stylesheets, but\nthis process is slow. This pushed us to implement this as a build step. With CSS parsing out of the way, on-the-fly operations are HTML parsing and CSS\nrule matching, which add up to ~50ms. The vast majority of which is spent\nparsing the HTML with the lxml library. Unfortunately, this operation is\nimpossible to cache since Yelp’s pages are dynamic. Therefore, there is a\ntradeoff between lowering time to first paint time and increasing backend\nresponse time. The important thing is to make sure there is a net performance\ngain from the user perspective. Acknowledgements : special thanks to Kaisen C. and Arnaud B. for ideas,\nimplementation feedback, and mentoring throughout the course of this project! Tweet Help us build a better, faster Yelp! Interested in performance? Do you have what it takes to make Yelp faster and better for everyone? Apply to join Yelp's engineering team! View Job Back to blog", "date": "2016-01-12"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 5 Winner", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/01/yelp-dataset-challenge-round5-winner.html", "abstract": "Yelp Dataset Challenge Round 5 Winners The fifth round of the Yelp Dataset Challenge ran throughout the first half of 2015 and we were quite impressed with the projects and concepts that came out of the challenge.\nToday, we are proud to announce the grand prize winner of the $5,000 award:\n“From Group to Individual Labels Using Deep Features” by Dimitrios Kotzias, Misha Denil, Nando De Freitas, and Padhraic Smyth (from the University of California, Irvine, the University of Oxford, and the Canadian Institute for Advanced Research).\nThis paper proposes a novel approach to using group-level labels (e.g. the category of an entire review) to learn instance-level classification (e.g. the category of specific sentences inside this review). The authors designed a new objective (cost) function for training a model which uses features from a deep-learning convolutional neural network. This trained neural network can, in turn, be used as a classifier predicting which category a specific instance belongs to. Their innovative research has broad implications for a variety of fields, and not just text classification. This entry was selected from many submissions for its technical and academic merit. A PDF copy of the winning paper can be found at this location . This paper was published in the Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\nFor a full list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Thanks to all who participated! Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset.\nThese examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They recently wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Next Yelp Dataset Challenge Round Submissions for Round 6 closed on December 31, 2015, but Round 7 starts on January 15, 2016 and a new dataset has been released. We are excited to see what you will come up with! Tweet Back to blog", "date": "2016-01-19"}, {"website": "Yelp", "title": "Announcing the Yelp Dataset Challenge Round 7", "author": ["\n        \n  Sébastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/02/yelp-dataset-challenge-round7-announcement.html", "abstract": "What can you learn from a Photo? Show us with the Yelp Dataset Challenge Round 7! The Challenge The Yelp Dataset Challenge provides the academic community with a real-world dataset over which to apply their research. We encourage students to take advantage of this wealth of data to develop and extend their own research in data science and machine learning. Students who submit their research are eligible for cash awards and incentives for publishing and presenting their findings.\nA new round of the Yelp Dataset Challenge (our seventh already!) opened on January 15, 2016, giving students access to reviews and businesses from 10 cities scattered over 4 different countries. The challenge is also open to international students. See the terms and conditions for more information. New Data: Now Including 200,000 Photos Deep learning has changed the game for researchers and companies alike over the last couple of years. To keep up with the trend, we are proud to announce that we are updating the previous dataset by adding more businesses, reviews, tips, check-ins, and users across the 10 cities we selected! Moreover, we’re excited to announce a great new feature: a dataset of 200,000 photos taken by our users in the businesses selected for round 7. These photos nicely complement reviews, business attributes, check-ins, and tips, and open the door to even more exciting research. This addition to the dataset is released as an auxiliary 5.9 Gb tar file.\nWith this update, the new dataset is comprised of reviews, businesses and user information from: Phoenix, AZ Pittsburgh, PA Charlotte, NC Urbana-Champaign, IL Las Vegas, NV Madison, WI Waterloo, Canada Montreal, Canada Karlsruhe, Germany Edinburgh, UK This new dataset and the photo auxiliary file are available for immediate download. Compared to the round 6 dataset, here are all the new features this dataset includes: Businesses: 77,445 (+27%) Business Attributes: 566,610 (+18%) Check-in Sets: 55,569 (+23%) Tips: 591,864 (+18%) Users: 552,339 (+51%!) User Connections: 3,563,817 (+23%) Reviews: 2,225,213 (+42%) Photos: 200,000 from 41,658 businesses Round 7 is Now Live The challenge is now open to students around the world and will run from January 15, 2016 to June 30, 2016. See the website for the full terms and conditions. These data can be used to train a myriad of models and extend research in many fields. So download the dataset now and start using this real-world dataset right away! Tweet Back to blog", "date": "2016-02-03"}, {"website": "Yelp", "title": "Leaping into February", "author": ["\n        \n  Julia N., Senior Events and Partnerships Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2016/02/leaping-into-february.html", "abstract": "February is going to be an exciting month! We’re looking forward to hosting our third Girl Geek Dinner next week which will feature talks from Yelp engineers on topics like learning, system performance metrics, and service monitoring. We’ll be present at the WSDM Conference that focuses on data mining and search, and at the Lesbians Who Tech Summit in San Francisco. If you’re in the area, swing by to say hi! We’ll be the ones giving away the infamous Yelp mints. This month we’ll also be hosting SF Python, Designers + Geeks, and Products that Count in the office. At SF Python, we’ll hear from Mike Starr, one of Facebook’s Data Infrastructure engineers, about how they use Dataswarm for their data processing pipeline. Wes McKinney, the creator of Ibis , will go over how Python can be used for data science scaling. Designers + Geeks will then teach us about design standards while Products that Count will further our understanding about the importance of marketing. We hope to see you around, especially since there’s an extra day for you to catch up with us this month. Tuesday, February 9, 2015 - 6:15PM - Yelp Girl Geek Dinner ( Yelp Engineering ) Wednesday, February 10, 2016 - 6:15PM - Learn more about Dataswarm and Ibis ( SF Python ) Thursday, February 11, 2016 - 6:30PM - Popular versus Brilliant ( Designers + Geeks ) Wednesday, February 24, 2015 - 6:30PM - Netflix from Good to Great ( Products That Count ) Tweet Back to blog", "date": "2016-02-05"}, {"website": "Yelp", "title": "Intern Project: Building the Yelp iOS Widget", "author": ["\n        \n  Sabrina R., Engineering Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/10/intern-project-building-the-yelp-ios-widget.html", "abstract": "We recently launched Yelp’s new Today Widget, so now you can search Yelp with a single tap from anywhere on your iPhone or iPad (iOS 8+). Today Widgets live in Notification Center, which can be accessed anywhere on the device by swiping on the top of the screen. You can search directly for a category near your current location, or open up the app directly to search input. App Extensions Today Widgets are a type of app extension (which include action, share, and photo editing extensions, custom keyboards, and document pickers). An app extension is a separate binary from the containing app, running in a separate process, and requiring a new target in Xcode. App extensions are packaged as a part of a containing app, and launched by a host app. In the case of something like a share extension, the share extension is launched in the context of a ‘host app’ and linked to its ‘containing app’. The containing app is the app downloaded from the app store, and the ‘host app’ is a third party app. For a Today Widget, its host app is the Notification Center. Today Widgets Today Widgets use standard UIKit classes, meaning you can subclass UIViewController and UIView. The design for the Yelp widget is static so using a storyboard could have worked, but we’ve found it hard to use storyboards and handle lots of merge conflicts. Instead, the Yelp widget views are constructed programmatically and use Auto Layout. Auto Layout was chosen because it makes supporting multiple screen sizes easy (more on this later in the Localization section). To display widgets, the system takes snapshots of the widget’s view, and displays the previous one until a new one is available. To update the view for a snapshot, a widget’s main view controller can conform to the NCWidgetProviding protocol. The protocol’s widgetPerformUpdateWithCompletionHandler method is opportunistically called by the system when the widget is about to be shown and in the background. The widget can update its content in this method, then call the completion handler and tell the system whether there was a view update from new data. Today widgets have several limitations. It can’t access sharedApplication, perform long-running background tasks, and has an unpredictable life cycle since it can be killed at anytime. The widget has three ways to communicate: The widget can tell its NSExtensionContext to open an URL in the containing app With app groups enabled, the widget and containing app can both read from and write to shared app group resources. This is done by instantiating a shared NSUserDefaults for the app group, that is separate from the standard NSUserDefaults in the main app. It can make requests through NSURLSession . NSURLSession does networking in a separate process from the containing app or the today widget. If the extension’s process is killed before the request responds, the request response is sent to the containing app. To launch the containing Yelp app, we used NSExtensionContext openURL: . Yelp has several public deep links for searching, opening a business, and viewing check-ins, and we used the same links to open up the app from the widget. Analytics The widget can send analytics if you use NSURLSession , and frameworks like Google Analytics also work well with the widget. When we use deep links to open the app, we add some tracking parameters from the widget. These parameters are passed to our main app and allows us to track how an app session originated. Localization The widget is available in all the countries and languages of the Yelp app. We updated our localization scripts to work for this new target, and Auto Layout allowed us to ensure that text wraps correctly for all the different languages we support. Localization in the widget works like the main app, using the NSLocalized set of macros. The widget should have its own set of Localizable.strings separate from the main app, since it builds into a different target. Next Steps You can add a Today Widget by adding a new target in your project. The default template will create a storyboard for you, but if you prefer to create your views in code you can also define the starting view controller by specifying the NSExtensionPrincipalClass in the widget’s Info.plist. If you’re interested in storyboards, we also have a blog post on our Apple Watch app here , which does use storyboards. We’re just getting started with our widget, so stay tuned for more! Tweet Intern at Yelp and build products that matter Our interns get a hands-on experience of building products that reach millions of consumers. Think you could make the cut? Apply here! View Job Back to blog", "date": "2015-10-01"}, {"website": "Yelp", "title": "Start Fall With a Little Yelp", "author": ["\n        \n  Julia N., Senior Events and Partnerships Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/10/start-fall-with-a-little-yelp.html", "abstract": "In two weeks we’ll be attending Grace Hopper for the third time in a row and we’re really excited to support Susanne L., Wei W., and Jen W. as they present about “ Crucial Conversations in Your Career .” Our team learned a lot last year and we look forward to sharing more lessons with you again after we get back! If you’re attending the conference, be sure to swing by our booth (121). You might be a few of the lucky attendees to score a ticket to our after party! We’re also happy to announce that later this month, we’ll be hosting the Ubuntu Linux launch party and participating in the Manifesto Conference , focused on product design. Meet some members of the Yelp Design Team and listen in as Eric Singley, our VP of Consumer and Mobile Products, takes the stage! Don’t forget about our regular meetup events: Wednesday, October 14, 2015 - 6:15PM - The Nitty Gritty of Concurrency for Python ( SF Python ) Thursday, October 15, 2015 - 6:30PM - When You Get a Napkin Sketch (Advice for Designers, Devs and Airline Pilots) ( Designers + Geeks ) Thursday, October 22, 2015 - 6:30PM - San Francisco Wily Release Party ( Ubuntu California ) Hope to see you at Yelp! Tweet Back to blog", "date": "2015-10-02"}, {"website": "Yelp", "title": "ElastAlert: Alerting At Scale With Elasticsearch, Part 1", "author": ["\n        \n  Quentin L., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/10/elastalert-alerting-at-scale-with-elasticsearch.html", "abstract": "Elasticsearch at Yelp Yelp’s web servers log data from the millions of sessions that our users initiate with Yelp every day. Our engineering teams can learn a lot from this data and use it to help monitor many critical systems. If you know what you’re looking for, archiving log files and retrieving them manually might be sufficient, but this process is tedious. As your infrastructure scales, so does the volume of log files, and the need for a log management system becomes apparent. Having already used it very successfully for other purposes , we decided to use Elasticsearch for indexing our logs for fast retrieval, powerful search tools and great visualizations. For those unfamiliar, Elasticsearch is an open source project that acts as a database and search engine for JSON documents. Along with Logstash and Kibana, it forms the ELK stack. Logstash is a document ingestion and transformation pipeline and Kibana is a visual front end service. With ELK, we are able to parse and ingest logs, store them, create dashboards for them, and perform full text search on them. An example Kibana dashboard. ELK scales well and has helped with incident response, comparing metrics, tracking bugs, etc. However, as the number of dashboards and amount of data grew, we realized the need for automation. Unless someone was actively looking at a dashboard or searching for the right thing, we missed a lot. How can we get alerted if this happens? We needed a way to monitor the data we had in Elasticsearch in near real time. We looked at several other projects, but they weren’t quite what we needed. We wanted a generic way to look for certain patterns in our data, without duplicating our data somewhere or spinning up a heavyweight service. We needed this to be accessible to engineers from every team across the organization to use with their own logs. Enter ElastAlert ElastAlert was developed to automatically query and analyze the log data in our Elasticsearch clusters and generate alerts based on easy-to-write rules. Our initial goal was to create a comprehensive log management system for our data. We had a few basic alerts we wanted, such as “Send us an email if a user fails login X times in a day” or “Send a Sensu alert if the number of error messages spikes.” This led us to a general architecture which could suit almost any scenario we needed across the company, not just on the security team. ElastAlert takes a set of “rules”, each of which has a pattern that matches data and a specific alert action it will take when triggered. For each rule, ElastAlert will query Elasticsearch periodically to grab relevant data in near real time. We designed ElastAlert with a few principles in mind It should be easy to understand and human readable. For this, we chose a YAML format with intuitive option names. It should be resilient to outages. It records every query it makes, and can pick up exactly where it left off when it turns back on. We designed it to be modular. The major components, rule types, enhancements and alerts, can all be imported or customized by implementing a base class. ElastAlert Rules Lets look at an example ElastAlert rule and break it down into its three major components. name: Large Number of 500 Responses\nes_host: elasticsearch.example.com\nes_port: 9200\nindex: logstash-responses-*\nfilter:\n  - term:\n      response_code: 500\ntype: frequency\nnum_events: 100\ntimeframe:\n  hours: 1\nalert:\n  - email\nemail: example@example.com 1. Define which documents to monitor es_host: elasticsearch.example.com\nes_port: 9200\nindex: logstash-responses-*\nfilter:\n  - term:\n      response_code: 500 In the first component, we are defining documents that this particular rule will be monitoring. A host, port, index and optional set of filters let you narrow down the rule to a specific set of documents. The filters are written in the Elasticsearch query DSL, which gives you powerful search tools like regular expression matching, range, and analyzed strings. The filters can even be copied directly from your Kibana dashboard without having to manually type them. filter:\n  - regexp:\n       ip_address: “10\\\\..*”\n  - term:\n       _type: ssh_login\n  - term:\n       outcome: failure You can also set different options for how ElastAlert queries Elasticsearch. You can control how often the queries are run, how big each query window is, whether to use the search or count API, and whether to run the query delayed from real time. There is also basic support for doing terms aggregations. 2. Which patterns to look for Each rule has a type , which is the type of pattern that the data will be checked against. In this example, we are matching “100 events in one hour”. This type of pattern is looking at the frequency of events occurring, so it’s called frequency . type: frequency\nnum_events: 100\ntimeframe:\n  hours: 1 Each type also has a few required parameters. For frequency, we care about how many events must occur within a specific timeframe for an alert to fire. Almost all of the rule types use rolling windows, with an associated timeframe, to store and process data. There are several other rule types available in ElastAlert. Spike The spike rule uses two sliding windows to compare the relative number of events. It matches when the current window has more or less than X times as many documents as the reference window. In this example, the current window contains 7 times as many documents as the reference window, and the reference window has at least 25 events. Flatline The flatline rule matches when the number of documents in a sliding window falls below a threshold. New Term The new term rule matches when there is a never-before-seen value for a field. This is useful for auditing new items while ignoring commonly occuring values. If only all malware was this aptly named. Change The change rule matches when a field has changed, within a time limit, grouped by another field. In this example, the country field for user1 has changed from from United States to Romania. Additional custom rule types can either be imported or created by implementing a simple base class . 3. How to alert There are several types of included alerters. Of course, as in the example, you can send emails. You can also open JIRA issues, run arbitrary commands, and custom python code. Each alerter has it’s own specific options, but there are several that can apply to any type, such as realert , which is the minimum time before sending a subsequent alert for a given rule, and aggregation , which allows you to aggregate all alerts which occur within a timeframe for a rule together. When a rule is triggered, it outputs a dictionary as the match. For some types of rules which match on a single document, this dictionary is just the document from Elasticsearch which triggered the alert. For types which alert on a large number of documents, the match can include additional information, such as aggregate counts for different fields. For most alerts, this dictionary is then converted into a string in one of several ways, and that string is the body of the alert. You can also set the text yourself: alert_text: |\n    ElastAlert has detected suspicious activity for {0}.\n    At {1}, an {2} error occured. Do something about it!\nalert_text_args:\n  - username\n  - timestamp\n  - error_type With this option, we can use templated text, optionally populated with fields from the match, as the alert text.  There is another option to add a link back to Kibana, which has the filters prepopulated from the rule, and the time settings set to the time of the alert. Assuming there exists a field called url , and by adding an option, top_count_keys , to our original example rule, we might get an alert that looks something like this: Large Number of 500 Responses\n\nAt least 100 events occurred between 8-18 4:20 PDT and 8-18 5:20 PDT\n\nurl:\n/some/url/path: 73\n/foo/bar: 14\n/index.html: 13\n\n@timestamp: 2015-08-18T12:20:36Z There is also another layer, between matches and alerts, called enhancements. Here, custom code can be run to transform, perform additional processing on, or drop the matches before they alert. Look out for part 2 of this blog post, with more practical examples and information about how we make use of ElastAlert as part of our Security Incident and Event Management infrastructure! For a full list of features, as well as a tutorial for getting started, check out the documentation . Source can be found on Github . Pull requests and bug reports are always welcome! If you have any questions, jump into our Gitter channel . Tweet Securing The Yelps If you're interested in building tools like ElastAlert that help us secure Yelp and its users, apply to become a Software Security Engineer View Job Back to blog", "date": "2015-10-06"}, {"website": "Yelp", "title": "How We Use Deep Learning to Classify Business Photos at Yelp", "author": ["\n        \n  Wei-Hong C., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html", "abstract": "Yelp hosts tens of millions of photos uploaded by Yelpers from all around the world. The wide variety of these photos provides a rich window into local businesses, a window we’re only just peeking through today. One way we’re trying to open that window is by developing a photo understanding system which allows us to create semantic data about individual photographs. The data generated by the system has been powering our recent launch of tabbed photo browsing as well as our first attempts at content-based photo diversification. Building a Photo Classifier One can imagine a variety of ways to tackle the ambitious goal of holistically understanding pictures. To help simplify our problem, we focused initially on only sorting photos into a handful of predefined classes. Further, we focused only on categories of photos directly relevant to restaurants as shown below: food drink inside outside menu To develop a classifier that can put a photo into one of these groups, we need to first collect many photos with known labels. We collected this information through a few different ways: Photo captions : A good number of “menu” photos have the word “menu” in their captions. Similarly, we can find photos titled “sushi” or “burger” that are likely to be food. For the former, we had to worry about false positives because it’s not uncommon to see “food” or “drink” photos whose captions are of the pattern “Best on Their Menu!”, and as a result some cleanup was necessary. To aid us in identifying food items, we relied on Yelp’s menu structures (e.g., http://www.yelp.com/menu/gary-danko-san-francisco/ ) which maintain each business’ list of food items. We found that matching food items from the list to the captions of photos yielded a dataset of high precision. Photo attributes : When uploading photos to Yelp, users are allowed to mark a few attributes about the photo, such as “Is it the storefront?” They are not always accurate, but still serve as a good source of candidate photos. Crowdsourcing : We ran additional tasks through a crowdsourcing partner to correct our guesses for what label should be applied to each photo and to collect more “inside” and “outside” photos. We have found that this led to generally good quality labels at a reasonable cost (both in time and money). Once we had our labeled data, we employed deep convolutional neural networks (CNNs) in the form of ” AlexNet ” to recognize those classes. CNNs usually consist of a deep stack of multiple convolutional layers (for extracting spatially local and translation-invariant features), ReLU (Rectified Linear Units) layers (for non-saturating activations), pooling layers (for down-sampling and translation-invariance), local response normalization layers (for better generalization) and fully-connected layers as in conventional feedforward neural networks. Softmax outputs and regularization methods such as dropout are also commonly used. Our CNN was built  on AWS EC2 GPU instances based on the Caffe framework. We like Caffe because it’s easy to use, performant, open source (BSD 2-clause), and under active development. To address Caffe’s software dependencies, we wrapped our CNN using Docker so that it could be more easily deployed. We also created abstractions to ensure that our CNN could be easily integrated with other possible forms of classifiers, including different instances of CNNs. As illustrated below, our baseline is a “Caffe Classifier” that runs the CNN by means of Caffe; it’s a special form of an abstract classifier that can take different signals and perform different classification algorithms. Our current “facade” classifier is an ensemble that takes the weight average of classification results from two independently trained Caffe Classifiers. It would be quite straightforward if we decide to further incorporate new classifiers relying on other signals, such as photo captions. On an evenly split gold test set of 2,500 photos, our current classifier shown above has an overall precision of 94% of precision and recall of 70%. While these numbers can definitely be improved, we found them reasonably good for the applications described below. Photo Classification Service Yelp uses SOA (Service-Oriented Architecture) and we made a RESTful photo classification service to support existing or upcoming Yelp applications. The main applications of the service, as detailed below, are based on a business’s classified photos. Since the service is expected to host more than one classifier (e.g., of different versions, or for different types of businesses), the service APIs take a classifier ID, a business ID, and an optional class, and then return all photos belonging to the business that have been classified (into the optional class, if specified) by the classifier, e.g.: We use a standard MySQL database server to host all classification results, and all service requests can be handled by simple database queries. To avoid more expensive real-time classifications, and because our current applications do not hinge on latest photos’ classifications, we only perform offline classifications. The architecture is shown below: for every new classifier, we sweep over all photos and store their classification results in a database. The sweep is computationally intensive, but Yelp’s infrastructure allows us to alleviate this by running our classifiers in parallel on arbitrarily many machines. After the sweep, everyday we also automatically collect new photos and send them into a batch for both classification and database load: Application: Cover Photo Diversification Once we have the photo classification service in place, it can immediately power many key features at Yelp. For one thing, our business detail pages show a set of “cover photos” which are recommended by our photo scoring engine based on user feedback and certain photo attributes. A typical issue with our current cover photos is that the selected photos lack “diversity”: for example, all the cover photos shown below are about food (ramen), and users could not see other aspects of the business unless they click on the “See all” button. Photo classification now allows us to diversify cover photos by classes – we can readily identify highest-scored non-food photos and incorporate them into cover photos. Through a rigorous A/B test, we confirmed that our restaurant viewers prefer to see a highlighted “food” photo and a highlighted “non-food” photo, as well as two smaller “food” photos and another two “non-food” photos, as illustrated below. Diversification has substantially increase our users’ interactions with photos. Application: Tabbed Photo Browsing As anyone who’s looked through Yelp’s photos before knows, the vast majority of Yelp’s photos from restaurants are food, but we’ve heard feedback from users that they find Yelp photos useful for more than just finding the most aesthetically pleasing burger. Some folks use Yelp photos for checking out the atmosphere for a special event or navigating to a venue for the first time, while others use Yelp photos for more serious applications like finding out if a restaurant can accommodate a handicapped patron. All of these tasks are now easier and more efficient with the launch of tabbed photo browsing Tabbed photo browsing is our most significant application to date of our photo classification service. Photos are now organized under their respective tabs (classes); as we can see below, it is now way easier to jump to the exact information you’re looking for. What’s Next No machine learning system is perfect. If you want to help contribute to improving our photo classification quality, feel free to flag any misclassified photos you see. We’re only at the beginning of exploring what we can do with Yelp’s vast photo corpus. Stay tuned to see where we’ll go! Acknowledgements : The photo classification service was designed and implemented by Wei-Hong C., Prasanna S., Joel O., Colin P., and Mohini T. The web front end of tabbed photo browsing was designed by Taron G. and implemented by Lawrence W. Tweet Become a Data-Mining Engineer at Yelp Interested in using machine learning to exploit Yelp's data? Apply to become a Data-Mining Engineer at Yelp. View Job Back to blog", "date": "2015-10-19"}, {"website": "Yelp", "title": "Fullerite: A New Mineral To Collect Metrics", "author": ["\n        \n  Ryan N., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/10/fullerite-a-new-mineral-to-collect-metrics.html", "abstract": "Collecting System Metrics at Scale Monitoring system metrics (e.g. CPU utilization, loadavg, memory) is not a new problem. It’s been necessary for a long time and there are a few that already exist, most notably diamond and collectd . At Yelp, we run thousands of different machines hosted all over the world. The simple volume of metrics we monitor quickly starts to complicate how we collect them. After trying out a few different solutions (and breaking many things) we decided to write our own: fullerite . A History of System Metrics Collection We started using diamond to report our system metrics. Everything seemed to be running great so we started to add more collectors. First we had a few smaller collectors, causing little bumps in the total volume of metrics. Finally, we added our service layer metrics, which roughly doubled the total volume. Things seemed fine; charts were populating and the system was humming along. Shortly thereafter we noticed that some machines were running out of memory. After some digging we saw that diamond was eating all the memory on the boxes. It was consuming upwards of 14GB ! We started to put in solutions to help mitigate this problem, but we knew that they were mostly stop gap solutions. Over the course of the next month we had a few high severity tickets, so we put in more safety checks and continued to investigate and test diamond. We discovered that the concurrency model that diamond employs (one handler process and a process per collector) was too sensitive to latency. A single handler could block the other handlers, causing diamond’s internal queues of metrics to back up. This would result in a not-so-gradual rise in diamond’s handler process’s memory. With a hackathon coming up, a few developers got together and said “We could write a replacement in 2 days!” Our goals with building fullerite were: Have a predictable and low memory footprint Provide a lot of isolation and concurrency in the tool Support dimensional metrics Have an easy build and deploy system Introducing Fullerite A few weeks later, once we’d gotten most of the kinks out, we released our replacement: fullerite . Fullerite follows a similar pattern as both diamond and collectd; it has a configurable set of collectors that multicast to a set of handlers. Interesting features include: Fully compatible with diamond collectors Written in Go for easy reliable concurrency Configurable set of handlers and collectors Native support for dimensionalized metrics Internal metrics to track handler performance The concurrency model in fullerite is important, we strove to isolate and decouple the components as much as possible. Because it is mostly written in Go, almost everything that can, is running in its own goroutine. This allows us to isolate the different handlers and collectors; latency in a single component won’t affect the other components. The fullerite and fullerite diamond bridge are running as their own linux services (using upstart). Fullerite spawns go routines for each configured handler and collector and then collectors report their metrics into separate channels. Fullerite passes through the metrics to each handler so that it can, in its own time, process that metric. The handlers often spawn further goroutines to deal with the batching or actual HTTP sending. The fullerite diamond bridge service starts each of the configured collectors. Each Python collector has its own embedded connector to the fullerite bridge to continue the paradigm of isolation. These collectors independently report their metrics back to the main diamond collector (over TCP) running in fullerite which propagates them just like any other collector. Where We Are Here is some info around what the current state of fullerite is at Yelp: Deployed to thousands of machines Running in both AWS and local data centers all over the world Running 10-12 collectors per instance Generating upwards of 3-4,000 metrics per minute per instance Generating over 10M metrics per minute across the whole cluster Consuming on average 10 MB of memory, with peaks up to 40 MB (relative to the GBs of memory diamond was using) These metrics make us happy with our little project. It has been extremely stable both in its performance and consistency. That is to say, based on the role of the box (e.g. service_machine, web, database) we can predict how much CPU, memory, and load fullerite will utilize. We have some great ideas for future work, most notably: expose the multidimensional interface better to diamond style collectors make the golang collectors and handlers pluggable improve performance (e.g. CPU utilization, memory footprint) This project grew out of a few days of a few engineers thinking we could do better. It has been a real blast to take this little idea and see it grow into an integral part of our performance and monitoring infrastructure. Go check it out on github ! Tweet Build the Infrastructure that Powers Yelp If finding new ways to build and manage servers and services all over the world excites you, join our team! View Job Back to blog", "date": "2015-10-26"}, {"website": "Yelp", "title": "Yelp Hackathon 17: Jurassic Hacks", "author": ["\n        \n  Srivatsan S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/10/hackathon-17-jurassic-hacks.html", "abstract": "To an outsider visiting the Yelp offices last month, it would’ve been quite a fascinating sight. People were huddled around whiteboards brainstorming the next killer idea. Floors were lined up with copious fresh fruits, snacks and catered gourmet food. And most noticeably, our all-hands space was transformed into a Jurassic World-like set complete with inflatable dinos, prehistoric plants and laser guns to shoot down malicious targets. Welcome to the 17th edition of our internal Product & Engineering Hackathon! Our SVP of Engineering kicks off proceedings with a Mimosa Toast Engineers, product managers and designers across all of our offices came together to produce more than 80 projects over the course of 2 days. From constructing an electronic circuit that could deploy web applications (yes, it works!) to inventing a programming language based on quotes from Seinfeld (yes, it compiles!), projects spanned themes ranging from hardware hacks, cool visualizations of Yelp’s rich dataset, killer developer productivity tools and even creative photo, music & videography experiments. Clockwise from left : Projects on display during our science-fair style exhibition; Playing with lasers in our jurassic arena (no dinos were harmed in the making of this :P); Food, coffee, hacks and smiles; It’s tea-time! On the creative side, a team comprising of Rachel Z., Jen W., Luis F., Tim K., William T., Vanessa S., Michelle L. and Zeke K. decided to capture the look and style of Yelp Engineering. What started off as a casual discussion about our distinct schools of dressing styles evolved into a full-fledged online lookbook. The lookbook team busy at work To realize their vision, the team needed photographers, photo editors, set managers and copywriters. The team rented studio lights and softboxes, and set up ad-hoc studios to capture photographs both inside our office and outside on the street. Once the photos were edited and the funny taglines were crafted, the team put  together a sleek lookbook website using Node.js hosted on Heroku. The Yelp Engineering Fashion Catalog! On the other side of the spectrum,  team comprising of Arnaud B., Buck G. and Kaisen C. tried to solve a nasty, old problem in software engineering: dead code. Large and complex production codebases maintained over several years by numerous developers often run the risk of leaving behind dead code and this dead code can be incredibly hard to find and eliminate. The team came up with an ingenious solution to this problem. They wrote a service that compares the set of functions which appear in aggregated cprofile data from production ( Yelp profiles a small percentage of all requests, and aggregates them by leveraging AWS ) with the ones defined in our codebase. If some functions defined in Yelp’s codebase don’t appear in recent production profiles, they get flagged as “dead” and become candidate for deletion. Die dead code! Die! The Dead Code Highlighter Service discovered this snippet of code (highlighted) in our webapp that’s no longer used anywhere Tweet Ready for Hackathon 18? Are those creative juices flowing through your head right now? Why don’t you come join forces as we embark on our next awesome Hackathon this winter! View Job Back to blog", "date": "2015-10-29"}, {"website": "Yelp", "title": "Getting Festive In November", "author": ["\n        \n  Julia N., Senior Events and Partnerships Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/11/getting-festive-in-november.html", "abstract": "Last month a group of Yelp engineers attended the Grace Hopper Conference in Houston. We had a great time presenting on crucial conversations and talking to amazing women at our after party. To keep the momentum going through the holiday season, we’re ramping up to launch our 18th internal hackathon at the beginning of this month! On November 7th, we’re excited to be partnering with Next.ML to bring machine learning workshops and panels to Yelp HQ. We’ll be hearing from leaders in the industry on a number of topics and will contribute to the presentations by sharing some of the recent work we’ve been doing on our photo classification systems . Later this month, for the first time in its history, the United Negro College Fund (UNCF) will be touring technical companies in the Bay Area. They’ll be coming to Yelp to gain career insight and advice from our team as part of the Historically Black College and University (HBCU) Innovation Summit! This organization focuses on minority education and provides operating funds for colleges and universities, in addition to providing scholarships and internships for students as well as professional training. During their tour, UNCF will also be hosting workshop sessions with Rachel Williams, Yelp’s Head of Diversity and Inclusion, who will be leading a few of them! We’re delighted to work with UNCF as they make their way around the Bay and are looking forward to building out our partnership. Saturday, November 7, 2015 - 9:00AM - Hands On Workshops & Industry Use Cases ( Next.ML ) Tuesday, November 10, 2015 - 6:30PM - Apache Kafka and the Rise of The Stream Data Platform ( SF Big Analytics ) Wednesday, November 18, 2015 - 6:45PM - Slack Growth Strategies ( Products That Count ) Tweet Back to blog", "date": "2015-11-01"}, {"website": "Yelp", "title": "Introducing PaaSTA: An Open, Distributed, Platform as a Service", "author": ["\n        \n  Kyle Anderson, Site Reliability Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/11/introducing-paasta-an-open-platform-as-a-service.html", "abstract": "As an Operations engineer, my first priority is to keep the site up. A close\nsecond is enabling developers to quickly go from an idea to running code in\nproduction. Once an organization grows, the only sane way to ship code with any reasonable\nfrequency is to split it up into microservices, also known as building a\nService Oriented Architecture (SOA). We’ve previously talked about the\nphilosophy behind services and why we build them. This blog post explains the\ntool we use to make those services available to developers: PaaSTA ! What is PaaSTA? PaaSTA is Yelp’s platform-as-a-service. It allows developers to\ndeclare, in config files, exactly how they want the code in their git repo to\nbe built, deployed, routed, and monitored. PaaSTA powers a significant number\nof production services at Yelp, and has been running in production for more\nthan 1.5 years. It even powers this very blog! Before PaaSTA, Yelp’s internal “platform” was so manual, I would just call it\n“Mechanical Turk Operations.” In other words, getting a service into production\nwas composed of manual steps, blocked on the operations team, taking days or\nmonths of setup time. Sound familiar? Almost every team has some sort of\nprocedure like this, and like most procedures, it starts off as a manual\nprocess. Eventually, Operations decided that our developers would be more powerful and,\nas an organization, we could ship things faster if we modernized our process\nand built a coherent set of tooling around shipping services. So PaaSTA was\ncreated! How Does PaaSTA Work? PaaSTA works by integrating existing open-source components that do their jobs\nvery well: Docker for code delivery and containment Apache Mesos for code execution and scheduling (runs Docker containers) Mesosphere’s Marathon for managing long-running services Chronos for running things on a timer (nightly batches) SmartStack for service registration and discovery Sensu for monitoring/alerting Jenkins (optionally) for continuous deployment Why are there so many pieces? Why isn’t PaaSTA a single Go binary? At Yelp we\nbelieve in not reinventing the wheel where possible. We want “seams” so we can\ntake a technology and swap it out as we grow and scale. PaaSTA provides a place\nfor these seams. That’s Just a Laundry List of Tools. How Does it Actually Work? It Starts with a Git Repo A developer starts by setting up their service to run in a container by\nbuilding a Docker image. The Operations team might help another team do this a\nlittle bit, but for the most part it is up to the service author to explicitly\ndeclare their runtime dependencies. Containers are great! They allow the\ndevelopers to have the flexibility to setup the runtime environment as they see\nfit, and it gives the infrastructure a common “binary” to run, regardless of\nthe underlying operating system. Once a Docker image is built, the PaaSTA command line tool gives developers or\nJenkins a way to upload their image and mark it as ready to be deployed. Ok I’ve Got A Docker Image. Now What? Once a Docker image is ready, the developer needs to declare how and where they\nwant their service to be run. PaaSTA provides a DSL to describe this. Here is\nan example of a Service definition: ---\ndemo:\n  cpus: 1\n  mem: 500\n  instances: 10\n  monitoring:\n    team: operations This is what it looks like to build a declarative, implementation-agnostic\nplatform-as-a-service for the long haul. There are no implementation-specific\ndetails in here. PaaSTA takes in these generic definitions and deals with the\nimplementation-specific details behind the scenes. To a developer, the interface is clean and straight-forward. All options are documented . The Plate of PaaSTA is Set. Time to Dig In! We have an image and a declarative language describing how to run it. Now\nPaaSTA can take over and put that code into Production. This is actually way\neasier than it sounds. Thanks to the existing engineering work put into Mesos,\nMarathon, and Chronos, PaaSTA can simply communicate with those API’s to make\nthe developer’s wishes come true. PaaSTA constantly syncs the service\ndefinitions with the actual APIs: But it is more than just POSTing to an API. With PaaSTA you are declaring your\nintent. Things don’t show up in prod because someone ran a deploy script.\nThings are deployed because they were declared that way in a config file, the\ncruise control was set, and PaaSTA stepped on the gas! If something\ncatastrophic happens, no developer has to “redeploy” their service. PaaSTA will\njust try to get things back to the way that they were originally declared in\nthe config files. This is such a big idea that it justifies some more examples. Let’s say that Yelp\nengineers are trying to build out a whole new region. What does that look like?\nWell, in the PaaSTA world it looks like a big diff with new config files that\ndeclare that a bunch of services should be running in the new environment.\nRight after the git commit, PaaSTA will make it so. If we need to nuke that\nenvironment for some reason, no developer interaction is needed. We already\ntold PaaSTA what we want, it will try to make it happen again. My Service Is Running. How Do I Find It? With any platform, service discovery is a BIG deal. If a service is deployed in\nan automatic way, then it needs to be discovered in an automatic way as well. At Yelp we use SmartStack for\nservice discovery. We love SmartStack at Yelp because it is platform and\nprotocol agnostic. It takes a config file and configures loadbalancers on\nlocalhost. PaaSTA provides the glue, telling SmartStack which services to\nadvertise and taking in hints from Docker, Mesos, or whatever technologies we\nadopt in the future. A flexible service discovery mechanism that uses “boring” components is\nessential to the healthy long-term evolution of your infrastructure. We can\ntightly control SmartStack’s behavior because it is un-opinionated, and in the\nend reads whatever config file we generate. SmartStack doesn’t know anything\nabout Mesos, or Docker, or Marathon, or whatever. This is important to us at\nYelp, because we don’t want to be locked into a particular tool that tightly\nintegrates with one particular piece of technology. Want to know more about SmartStack? Check out this Video / Slides on how we use SmartStack at Yelp. But It’s Not Done Till It’s Shipped Monitored! Monitoring services in a dynamic world like this is a difficult challenge.\nSensu is the only open-source monitoring tool I’ve seen that is up to this\nchallenge. But Sensu also doesn’t know anything about Mesos, Docker, Marathon, SmartStack,\nor PaaSTA. Its job is just to route alerts. PaaSTA is the glue that knows what\nservices are out there and which team is responsible for them. Once you tie\nthose pieces together, your monitoring system becomes, well, awesome! Sensu is super flexible, and allows us to route our alerts in a team-centric way. Then, we let PaaSTA tell Sensu which services own which teams. No “Ops” team has\nto update the monitoring system when new services are created, Sensu can handle\nnew events on the fly! But What About Terraform, ECS, Nomad, BeanStalk, Flynn, Deis, Heroku, OpenStack… There is certainly no shortage of platforms and tools out there, but with\nPaaSTA we can grow and use different tools as we need them, and developers\nshouldn’t have to change anything. After all, they have already told the system\nwhat code to run and how to run it. PaaSTA can figure out the rest. In the infrastructure world, there is pretty much no such thing as a drop-in\npiece of software. Each component requires integration with the others. You\ncan’t just plop down something like OpenStack in front of developers and say:\n“Here you go! Have at it!” For Yelp infrastructure, we want to take raw, un-opinionated tools, and glue\nthem together in a cohesive, opinionated, and sustainable way. PaaSTA is that\nopinionated glue! If you would like to know more technical details about how\nPaaSTA compares to other tools, check our our comparison doc . Interested? Has PaaSTA given you any cool ideas with what you want in your own PaaS?\nCheckout PaaSTA on GitHub to see how we glue the pieces together. Also check\nout this video to watch PaaSTA in action. If you\nwant to talk to us in real time in #paasta on Freenode. Check out our meetup on December 17, 2015 where we’ll talk more about the technology that powers PaaSTA and why\nthis PaaS has worked out so well for us. Also, to learn more about Yelp’s experience with Mesos, Docker and building a\nnext-generation development environment, check out this case study . Tweet Want to help cook PaaSTA? Like building this sort of thing? At Yelp we love building systems we can be proud of, and we are proud of PaaSTA. Check out the Site Reliability Engineer positions on our careers page if you like building systems you can be proud of too! View Job Back to blog", "date": "2015-11-10"}, {"website": "Yelp", "title": "Glide - How Yelp’s Android App Loads Images", "author": ["\n        \n  Michael S., Android Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/07/glide-how-yelps-android-app-loads-images.html", "abstract": "Dynamically loaded images are a cornerstone of many Android applications. At Yelp, images are critical to how we connect consumers with great businesses. As network connections and device hardware have become more powerful, the quantity and quality of images that users have come to expect has continued to increase. Images can easily become the largest consumer of device memory and network data, and handling the downloading and management of this data becomes a daunting task. We explored several solutions to this problem, and ultimately decided that Glide provided a great combination of performance, ease of use, and a robust feature set. Glide, in the simplest use case, will load images either from a remote server or the local file system, put them into disk and memory based caches, and then load them into views. While it can be used for pretty much all images in an app, Glide is optimized for scrolling lists that contain images as smoothly as possible. The Object Pool At the center of Glide’s approach is keeping an object pool data structure for bitmaps. The main goal of an object pool is to increase performance by reducing the number of large object allocations and instead reuse them (for an overview of object pools, check out this Android performance pattern video ). Both the Dalvik virtual machine and ART (for the moment) don’t use a compacting garbage collector , a model in which the GC will run through the heap and move living objects into adjacent memory locations, leaving larger chunks of memory available for future allocations. Because Android doesn’t use this model, the heap can end up in a situation where allocated objects are spread out with only small chunks of available memory in between them. If the application tries to allocate an object that is bigger than the largest contiguous chunk of free memory it will run into an OutOfMemoryError and crash, even if the total amount of free memory was greater than the size of the object. Using an object pool also helps scrolling performance because reusing bitmaps means fewer objects are created and garbage collected. Garbage collection passes cause “stop the world” events where all threads (including UI) are paused while the collector is executing. During this time, frames can’t be rendered and the UI may stall, which is especially noticeable during scrolling. Using Glide Glide is simple to get up and running, and it automatically includes bitmap pooling without any special configuration. DrawableRequestBuilder requestBuilder = Glide.with(context).load(imageUrl); requestBuilder.into(imageView); That’s all that is required to begin loading an image. As always with Android, it can be unclear what type of context to pass into the with() method. It is important to note that the type of context passed in affects how much Glide can optimize loading. If an Activity context is passed in, Glide will monitor the activity lifecycle methods and automatically cancel pending requests if it sees the activity is starting to tear down. However, if an Application context is used, you lose out on this optimization. Optimization Features Along the same lines, Glide automatically cancels pending requests for images in a ListView if the associated list item is scrolled off the screen. Since most developers take advantage of view recycling in their adapters, it does this by attaching a tag to the ImageView when requesting an image, checking for that tag before loading another image, and canceling the first request if it exists. Glide provides a few features that give the impression of faster image loading. The first is the ability to prefetch images in a list before they are shown on the screen. It provides a ListPreloader class, which is instantiated with the number of items ahead it should prefetch. That is then passed to the ListView through setOnScrollListener(OnScrollListener) . Need to prefetch images outside of a ListView as well? Not a problem, that is supported as well. Using the builder object above, simply call builder.downloadOnly(); . We’ve found the out-of-the-box utility that Glide provides has greatly increased the performance, reliability, and aesthetics of the areas in our Android application that load images. The availability of these additional features and optimizations really allow applications to fine tune their image loading experience into something that creates a delightful experience for the user. Tweet Back to blog", "date": "2015-07-17"}, {"website": "Yelp", "title": "Know Thyself: My Internship Quest to Build a Framework to Analyze Our Yelp Feed", "author": ["\n        \n  Arthur J., Software Engineer Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/07/know-thyself-my-internship-quest-to-build-a-framework-to-analyze-our-yelp-feed.html", "abstract": "Over the years Yelp has built cool integrations with partners such as Apple , Microsoft , Yahoo and many others that have enabled users to interact with our rich dataset in unique ways. One of the methods we’ve used to achieve these integrations is by building a robust, scalable data feeds system, which contains information for over 50 million businesses. As we allow partners to have their own configurations, information included in these feeds may vary from one partner to another and we did not have an easy way to access the history of data feeds that were delivered to our partners. Having the ability to explore our data feeds is crucial as it allows us to perform quality assurance, pick up on trends and enable us to make future business partnership decisions. Such insights into our feeds would allow us to answer questions like: How many of the business listings we sent to partner-A yesterday were merged into other business listings because they were duplicates? Is it true that a business listing we sent to a partner has actually been closed since last Saturday? How many business listings in the US don’t have phone numbers specified on them?\nAs you can imagine, the answers to these questions could be critical for partners consuming this data and could also help us catch bugs in our feed generation process. This problem statement fascinated me, and I got a chance to implement a solution for this as my intern project this past spring. Since this was a new system, I had a lot of freedom in its implementation, and after researching the various available options and evaluating their tradeoffs, I decided to use ElasticSearch , Kibana , ElastAlert (Yelp’s open source project), Amazon S3 and Amazon EMR . Architecture The main objective of this project was to build a visualization system on top of our data feeds. The most important piece was to choose a visualization tool that is easily navigable and has enriched functionality, and there are many visualization tools that can output data in a graphical form. At the beginning of the project, we considered either Kibana , Redash or Grafana , all have their pros and cons. We decided to use Kibana because it provides us with 11 different types of graphs, flexible filters and simple queries. Redash and Grafana are also good visualization tools but are not suitable in our use case. For example, Redash has great support for using arbitrary and complex queries on a desired data set but, in our project, it would have introduced the overhead of maintaining queries for more than 25 partners with 13 different graphs individually (325 queries in total). By using Kibana, it was a natural choice to use an ElasticSearch cluster as our backend data store for the feeds. Yelp has an internal ElasticSearch cluster template which allowed us to easily spin up our own cluster for this project on AWS. Before spinning up a new ElasticSearch cluster, we had to estimate how much data we could afford to store. This estimation is important for two reasons - we want to optimize the monetary cost of provisioning AWS machines and we want to optimize Kibana’s efficiency in visualizing the data. We estimated at least 45GB of new data per day, a representative subset of our overall feeds. We decided to go with a one month retention policy along with one replica for each shard. With this setup, we would index roughly 111M documents every day. These calculations led us to choose three dedicated master nodes and 5 data nodes, master nodes would use m3.medium and data nodes would use c4.8xlarge AWS machines. Now that the architecture and configurations were decided, I went on to implement a two-step workflow to import these data feeds into the newly provisioned ElasticSearch cluster. The first step takes data feeds generated by the Feeds system (which dumps its output to S3) from S3 and uses our MRJob package to initiate an EMR job. This cycle, called Data Feeds Transformation, aims to retain only necessary information from data feeds. The results, data metrics, will be uploaded back to S3 from EMR. The second step reads these data metrics from S3 and then dumps them to the ElasticSearch cluster. This two-step process is applied to each individual partner that we generate feeds for. Problems As mentioned earlier, the feeds system is highly customizable and gives partners a lot of freedom to select what information they want to receive. Every partner has its own configuration that controls the output of its data feed. These configurations are specified through our own DSL that uses YAML as its representation. Data feeds are generated by applying these partner configurations to our underlying source data. For each configuration, it explicitly states what information to keep and what operations to perform. For example, a snippet of partner configuration in YAML format can look like: location:\n    city: 'some_city',\n    state_code:\n        _rename: 'state' Given the above configuration, the data feed for the partner will be structured like: {\n    \"location\": {\n        \"city\": \"some_city\",\n        \"state\": \"ABC\"\n    }\n} This data feed includes the city and state information for the given partner. Notice that the field changes to ‘state’ from ‘state_code’ inside of the location structure as a ‘_rename’ operation has been applied on that field. Besides rename operation, there are other operations that can be performed, such as flattening addresses. Some partners choose to have operations on certain fields, whereas some choose not to. Data feeds will have different structures and different fields depending on the selected operations. As a result, our feed data is not uniform across partners. This makes it hard for us to visualize them on the same scale, or even analyze them for the same patterns. Thus, we had to build a generic workflow that is capable of processing every data feed and bring uniformity to the final data metrics. In order to bring back uniformity, I expanded on the same DSL to define operations that would essentially reverse the effect of the partner-specific transformations, bringing all partner feeds into a uniform format. Once we generate this unified format, we can easily apply another configuration to filter what we need to get the final data metrics. Monitoring Once we got our data metrics into ElasticSearch, we wanted to have a way to monitor our data. We used ElastAlert to notify us if any of the metrics at any given time has spikes or any anomalous patterns that we have defined. For example, we implemented an alert to check if the number of businesses sent to any partner drops or increases by 10% from one day to the next. This can help us to detect any possible problems with our feed generation service immediately. A snippet of our rule configuration is below, # ...\ntype: spike\nspike_height: 1.10\nspike_type: both\ntimeframe:\n    hours: 24\n# ...\nalert:\n- \"modules.sensu_alert.SensuAlerter\"\nsensu:\n    team: TEAM\n    status: 1\n    notification_email:TEAM@yelp.com\n    irc_channels: \"#TEAM\"\n    page: true\n# ... In the configuration, we simply specified the type of rule in the ‘type’ field. Each rule has its own required fields to be configured. In our case, ‘spike_height’, ‘spike_type’ and ‘timeframe’ are mandatory in ‘spike’ rule type. In addition, we configured our rule to trigger an alert when the given condition is met. The ‘alert’ field allows us to specify an alert type. ElastAlert is integrated with two default types of alert - Email and JIRA. Users can also create their own alert types. The snippet above shows a use case of our customized Sensu alert. If you are interested in using the ElastAlert for your project, a more complete walkthrough of ElastAlert can be found here . Results This Feed Metrics system that we built has been online since April of 2015. Below is part of the configured dashboard showing one partner with fake numbers. The upper histogram shows the total number of business counts we sent to a subset of our partners in the last seven days. The lower chart shows the number of businesses that are closed or not closed. With the dashboard, we can easily navigate through all the preconfigured graphs to visualize daily data feeds. In addition, we are able to perform numerous quality assurance operations by using preconfigured filters and queries on data we are delivering. Tweet Back to blog", "date": "2015-07-31"}, {"website": "Yelp", "title": "iPhone vs. Android: What Does Your Phone Say About You?", "author": ["\n        \n  Sebastien C., Data Scientist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/08/iphone-vs-android-what-does-your-phone-say-about-you.html", "abstract": "It’s the battle of our generation - iPhone vs. Android - and Yelp has some novel data on what makes these devices different. Users of both are aligned in their passion of using Yelp to find restaurants as well as nightlife, plumbers and even dentists. However, when we dug into the search queries, some interesting differences were revealed - some stereotypical (brunch, Starbucks, cheap, luxury, whole foods/organic), some not so much (beer vs. wine, ramen vs. steaks). Regardless of the differences, we discovered some things for certain: everyone loves sushi, pizza, and delivery of all kinds! Let’s start with a few similarities. On both iPhone and Android, food terms are by far the most searched terms. “Restaurants,” “bars,” “coffee,” and “food” account for a good amount of all searches on iPhones, based on a study we did of the past 6 month’s search queries globally. The first explicitly “non-food” item is “shopping” on Android (ranked number 5) as well as on iPhone (ranked number 18). The most searched drink on both systems is “coffee.” Surprisingly, “beer” appears only at number 118 on iPhone and 152 on Android, while “wine” ranks 170 on iPhone and a distant 514 on Android. However, both “wine bars” and “beer and wine” rank higher on Android than on iPhone (“wine bars” ranks 113 on Android vs. 152 on iPhone; “beer and wine” ranks 115 on Android vs. 196 on iPhone). The most searched dish on both platforms is “pizza” (ranked 5 on iPhone vs. 6 on Android), and Yelpers also seem to love “sushi” (ranked 6 on iPhone vs. 7 on Android), “pho” (ranked 20 on iPhone vs. 23 on Android), and “seafood” (ranked 24 on iPhone vs. 20 on Android). However, there are some obvious differences between both platforms. Android users seem to look for deals a lot more than iPhone users (“deals” is ranked 24 on Android and 221 on iPhone). In general, the word “cheap” is more often used on Android searches than iPhone ones: “cheap eats” (ranked 398 on Android vs. 639 on iPhone), “cheap dinner” (ranked 446 on Android vs. 846 on iPhone), “cheap motel” (ranked 1,172 on Android vs. 1,296 on iPhone), and just “cheap” (ranked 1,367 on Android vs. 2,272 on iPhone). Conversely, iPhone users might be more attracted to “luxury”: “luxury nail salon” (ranked 4,240 on iPhone vs. 7,863 on Android), “luxury hotel” (ranked 25,719 on iPhone vs. 37,570 on Android), or just “luxury” (ranked 24,090 on iPhone vs. 43,484 on Android). The same goes for “luxe” (ranked 8,045 on iPhone vs. 10,358 on Android). Android Yelpers seem more interested in “nightlife” (ranked 11 on Android vs. 52 on iPhone), especially “dance clubs” (96 vs 156). On the other hand, iPhone users have a soft spot for brunch (rank 14 vs 26) and Starbucks (15 vs 30). As far as coffee chains go, they are also more interested in Peet’s Coffee (649 vs 1676). After the staple foods that are pizzas, sushi, and pho, iPhone Yelpers have a taste for ramen noodles (22 vs 28) while Android Yelpers favor steaks (25 vs 28). When dessert is considered, iPhone users enjoy ice cream (ranked 31) more so than Android ones (ranked 50). iPhone Yelpers are also looking for Whole Foods grocery stores more often than Android ones (159 vs 660 for “whole foods market”, and 166 vs 421 for “whole foods”): this is perhaps not surprising as they also search for organic food more often (621 vs 911). Finally, Android users are Yelping more for “hotels” (ranked 59) than iPhone users are (ranked 80), and the same is true for “hair” (ranked 21 on Android vs. 29 on iPhone). Acknowledgements Thanks to Travis Brooks and Alexa Herasimchuk Tweet Become a Data Scientist at Yelp Looking at the differences between iPhone vs. Android users is only scratching the surface. If you're interested in digging into more of our data, apply to become a Data Scientist at Yelp. View Job Back to blog", "date": "2015-08-07"}, {"website": "Yelp", "title": "Roaring into August", "author": ["\n        \n  Julia N., Senior Events and Partnerships Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/08/roaring-into-august.html", "abstract": "Most of July was spent ramping up for our internal hackathon so it was a little slower for us. Last month, we hosted a small PyLadies workshop and celebrated the Designers + Geeks 4 year anniversary (2.5 years of which have been hosted at Yelp!). We also had Tera- Nicholle Nelson, the VP of Marketing for MyFitnessPal and Under Armour, stop by our office to discuss how to “disrupt” yourself. Since hackathon recently wrapped up, we’re slowly recovering from dinosaurs, hacking, and our food comas (look out for our official hackathon post coming out soon!). Coming up this month , we’ve got some talks happening back to back at Yelp HQ and also at the BSides and DefCon security conferences in Vegas! Tasneem Minadakis, one of our Ads Managers, will be sharing her personal career story with students from HackingEDU as well. Later this month, we’ll be hosting our third tech talk in the Yelp Tech Talk Series. This time, we’ll be focusing on ads and how we display and target local advertising. Be sure to RSVP to this event early because tickets go quickly! Wednesday, August 12, 2015 - 6:15PM - Practical Python 3: Fixing Things and Moving On + Jython 2.7/3.x ( SF Python ) Thursday, August 13, 2015 - 5:30PM - Macbrained’s August Meet-Up @ Yelp ( MacBrained ) Wednesday, August 19, 2015 - 6:00PM - HackingEDU’s Fireside Chat ( HackingEDU ) Thursday, August 20, 2015 - 6:30PM - Designers vs. Geeks ( Designers + Geeks ) Tuesday, August 25, 2015 - 6:30PM - Yelp Tech Talks- The Science and Systems Behind Local Advertising ( Yelp Engineering ) Wednesday, August 26, 2015 - 6:45PM - TiVo Chief Engineering and Design on Disrupting TV ( Products That Count ) Tweet Back to blog", "date": "2015-08-10"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Rides Again", "author": ["\n        \n  Travis B., Group Product Manager of Search & Data Science\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/08/yelp-dataset-challenge-rides-again.html", "abstract": "6. An impressive number. It’s 1 x 2 x 3 AND 1 + 2 + 3. It’s brilliant AND the number of degrees of freedom a rigid object has to move in three dimensions. It’s where we are in the history of the Yelp Academic Dataset challenge. We’ve had 5 rounds, hundreds of academic papers written , and we are excited to go at it again. Our dataset for this iteration includes information about local businesses in 10 cities across 4 countries. This dataset contains 1.6M reviews and 500K tips by 366K users for 61K businesses. It also comes with rich attributes data (such as hours of operation, ambience, parking availability) for these businesses, social network information about the users, as well as aggregated check-ins over time for all these users. At Yelp, one of our missions is to engage with the academic community and help them by providing real-world data to aid their research. Our dataset should be useful to researchers in data mining, machine learning, economics and urban planning alike. Whether you’re building a cutting-edge Natural Language Parsing (NLP) algorithm that mines sentiments expressed by our reviewers, figuring out what business attributes (service quality, ambience, etc.) make a local business popular, or designing better cities and communities by mining local business data – our dataset has everything you need to put your research together. New Competition: Deadline is Dec 31, 2015 Download the new dataset and remember to submit your entry by December 31, 2015 in order to be eligible for one of our prizes of $5,000. We expect that many folks will be putting the finishing touches on their projects instead of partying that evening, so it’s important to note that we’ll be using Pacific Standard Time for that deadline. Please note that the contest itself is open only to students, though others can use the dataset for academic purposes if they otherwise follow our standard terms and conditions . Check out this webpage for more details. Fifth Round We’re still judging the fifth round entries, and we’ll publish a list of winners in the next months, but in the mean time here are a few interesting findings to whet your appetite: Yelpers have written over 80 million reviews about local businesses as diverse as the wave organ in San Francisco and Daiwa Sushi in Tokyo . Many of those reviews have been voted “Useful”, “Funny”, or “Cool” by our users, but R. Cheng et al. from Santa Clara University decided that they couldn’t read them all. Instead they studied reviews with Useful, Funny and Cool votes in the dataset and distilled them into a single “koan-like” phrase: \"like good food, just one great place.\" We suspect there is more than one great place, but we can’t disagree with the sentiment. Overall, Yelp reviews tend to be more positive than negative, and they also tend to contain a lot of useful information about local businesses. D. Song et al. from University of Washington wondered if there was a correlation between length and sentiment of reviews, and they produced a nice visualization of review lengths on Yelp as it relates to rating. They also compared it to readability guidelines and useful votes: It turns out positive reviews tend to be shorter than negative ones, but useful reviews tend towards the magical middle. Tweet Back to blog", "date": "2015-08-17"}, {"website": "Yelp", "title": "Docker in the Real World at Yelp", "author": ["\n        \n  Matt B., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/08/docker-in-the-real-world-at-yelp.html", "abstract": "Thousands of businesses use Yelp SeatMe every day to manage their seating and reservations.  Having a stable system is incredibly important to us, given how critical this system is to many businesses. This blog post is going to dive into how we use Docker to reliably develop and deploy Yelp SeatMe. Docker is an incredibly powerful productivity booster and has simplified our deployment pipeline. Hopefully, from this post, you’ll understand how to do it for your team. First, we’ll give a little bit of background on what Yelp SeatMe is, how it’s developed, and deployed. What is Yelp SeatMe? Restaurants manage and accept reservations via our native iPad app and/or web interface. We keep all devices in sync in real-time and also support offline changes if clients become disconnected. Our web technology stack is: Javascript on the client, using Backbone for our single page apps Python is our backend language, with Django as our framework, running under uWSGI Celery and RabbitMQ for asynchronous work Postgres database, using triggers for data validation and update notifications to support our sync protocol and long polling engines We host the entire platform in AWS and we use Chef as our primary tool for bootstrapping servers, managing deploys, and organizing our testing, staging, and production environments. Getting Docker Into Production A year ago, we started down a path to make our environments (testing, staging, and production) more consistent and to simplify our deployment process using Docker containers. In our pre-container setup, all the logic for deploying a web app server was kept in a Chef recipe. The process looked something like: set directory permissions install required Python libraries and other packages download a specific git tag to the local git repo maintain a cache of the last few releases to ensure we can quickly roll back These Chef recipes weren’t really accessible for new engineers, since they were in a different language (Ruby) and used a complex framework. This setup made the deploy brittle and difficult to change. Containers to the rescue! By using Docker, we were able to simplify the parts of our deploy that were Chef managed, down to just Docker container manipulation: pull a specific Docker image to the server bring the server out of service, by health checking it down stop the existing container tag the new image with a human readable name to point out it’s the current release (www:latest) start the image (with all of its filesystem mappings, etc) as a new named container (www) Benefits of the Docker setup: increased developer control of the environment via source-control managed edits to the Dockerfile elimination of server environment drift, over and above what Chef already provided reduced the amount of Chef code required to configure the application from thousands to hundreds of LOC centralized repository of deployable images that can always be mapped to an exact git commit we continuously build our Docker images, so every completed code review always has a deployable image built when its tests pass developers can now modify system level packages, without requiring the operations team to do it After two months of development and testing, we rolled Docker into production early October 2014 with no major hiccups. Things to Keep in Mind as you Transition Docker is a fairly new technology, so of course there are going to be some ‘gotchas’ as you try to roll things out. Here are some issues/tips while you think about deploying Docker from development to production in your environment. Make sure you thoroughly test your storage drivers There are several storage drivers available for Docker, with AUFS typically considered the prior generation, and Device Mapper being the current generation. One thing we discovered while testing was local filesystem corruption while using Device Mapper, as well as full system hangs. While it’s very likely these were kernel and distribution version specific, we found AUFS to be very stable for us in production. Always thoroughly test your storage driver, especially via repeated deployments, before moving out of a staging environment. Build times for Docker images are slow and disk images can become big quickly Due to how the Docker file system layers work, if you change something early in the layer order, you must rebuild all successive layers. Figuring out what steps are expensive in terms of time or network access and how to order your build steps to minimize future rebuilding isn’t always straightforward. There is a fine art to combining commands into a single command line to minimize layers and rebuilding time. Consider caching Docker images from successful builds in order to decrease build times. Always have explicit commands for building and launching containers kept in source control alongside your Dockerfile We used make to do this. Once you have a few arguments to docker run, you want these arguments documented and easily extractable. If these arguments change over time or with a new version of Docker, you don’t want to have to change a configuration file or shell alias on the server; it should be a code change. Images and containers don’t automatically clean themselves up This has been talked about at length, but we discovered the hard way when all our server hard drives were mysteriously full, that every image we deployed was on the servers until we removed it. We have reduced our disk usage with some simple image management scripts, but are exploring tools like docker-custodian to really solve the issue. Consider layering your images into multiple Dockerfiles to speed up build times Over and above what we discussed earlier with image caching, we’ve implemented a multi-layer Docker image strategy, designing images to inherit from each other. The base image, which contains things like system packages, is very slow to build but changes infrequently. The production web image inherits from the base container, and contains a fully built set of Python packages as well as a point in time snapshot of the source code. This is fast to build except in the event of a requirements.txt change, which then triggers a rebuild of the virtualenv . The developer web image inherits from the production image, adding the tools to run selenium tests (Xvnc, google-chrome), as well as additional developer tools. The developers then use file system mappings to bring their live source code into this container, overlaying the snapshot of source in the production web image. In an emergency, you can still reach inside the container to debug your code Initially, when you run your code inside a Docker container, it can feel like putting a layer between you and the code that prohibits a lot of normal debugging techniques. While it’s not considered best practice for production systems, when in testing (especially development) it’s fully possible to open a shell with docker exec -ti <containerid> bash and then use normal tools like strace on your running code or interactively examine the file system. You need a strategy for log management While an extremely mundane topic, log management with Docker turns out to be a lot trickier than it sounds. While there are simple techniques you can use to map host file systems into the container and simply log to disk, this quickly fails to scale when you decide to start more than one container on a given host. In the end, after a lot of experimentation, we’ve had the most success with the application logging directly to the host rsyslog over UDP. Before version 1.7 of Docker, doing this required a technique for finding the correct IP address to syslog to, which we solved by passing it in through environment variables. To the Future In conclusion, containerization has been a big win. For example, when the Shellshock vulnerability was announced, our incident response was almost completely taken care of by a container rebuild and a Chef run. Despite growing pains, Docker has been a change we are happy with and continue to invest in. With a stable web infrastructure building block, we have found newer and more efficient ways to test, deploy, and manage our infrastructure, as well as iterate faster in development. We hope to share these uses in future articles, so stay tuned! If you are interested in learning more about Docker, Yelp is hosting a Docker meetup this Thursday at our office. We would love to continue the discussion in person! We are also hiring developers excited to work alongside us with Docker as we continue to grow. Acknowledgements Many thanks to Kris Wehner and Charles Guenther for their experience and help writing this blog post. Tweet Work on Docker at Yelp Interested in working on our Docker deployments and continuing to help improve our Docker setup? Join our SRE team! View Job Back to blog", "date": "2015-08-25"}, {"website": "Yelp", "title": "Designing the Android Navigation Drawer", "author": ["\n        \n  Yoni D., Product Designer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/09/designing-the-android-navigation-drawer.html", "abstract": "Since the dawn of time – or a very long time at least – the Yelp Android app has relied on a springboard design pattern for its navigation. This once-popular pattern served us well through the years as it allowed users quick access to Yelp’s most-used features. However, the design came with a series of trade-offs. The Yelp Android app back in 2009 and 2011 Navigating Yelp is inherently a free-form action. Users may search for a business, look through some of that business’ reviews, then check Talk for the latest news in their community, before finally liking some of their friend’s Check-Ins. With the springboard design this meant backing out to the springboard before drilling down again on a regular basis. Besides being cumbersome to navigate, it also provided an overwhelming first impression to new users. We aspired for our users’ first impression to be the amazing content created by their incredible peers, not just a grid of features. Given that we are also constantly adding great new features, it’s fair to point out that the springboard doesn’t scale well. We started exploring the idea of a navigation drawer accessible from almost anywhere in the app. This would not only make navigating our app easier, it would also allow us to better focus our app through the hierarchy set in the menu. Presenting our users with rich nearby content would be achieved through the default collapsed nature of a drawer, defaulting to Nearby. The new Yelp Android app With more exploration we discovered a number of additional improvements we could make to the drawer. For example, highlighting our users’ core profile statistics evokes a sense of ownership and accomplishment. Surfacing contribution links offered both easy access for existing users while communicating the user-generated nature of our content to new users. And best of all, the bottom of the drawer provided us with an almost unlimited canvas to add a shiny new easter egg to our stable of platform mascots . Yelp Android app easter egg However, we weren’t done yet. We had gone from an app that revolved around too many features to an app that felt a tad shallow and oversimplified. Although the drawer hierarchy did a great job at emphasizing our core functionality, it did lack significance and impact. We were drawn to the idea of a persistent navigation that highlights Yelp’s most essential features (nearby discovery, search, community activity) and gives new users an idea of what the app’s core value is. Yelp Android app persistent navigation This impactful persistent navigation creates both an easier path to the most essential features while also representing the core value of Yelp. We think this will lead to greater retention of new users, but we also believe it’s just a better experience that’s more engaging for all users. Through a series of experiments and partial roll outs we released this new navigation paradigm into the wild. It’s never a fun experience when an app you frequently use makes radical changes, even if they’re for the better. Because of this, one of our main concerns was the sentiment of our users. We also always try to be very thoughtful when it comes to how features look and behave on a specific platform. We believe the specific visual identity of an operating system is both important part of the user’s identity as well as a great tool for ease of use. This is one of the main reasons why we didn’t simply copy the navigation we have on our iOS app. Yelp mobile patterns Yet, working with a product as unique and complex as ours, we often find ourselves having to extend, modify and or even reinvent patterns set forth by operating systems. It’s often unclear if we did a great job until your users tell you they love or hate the creative liberties we took. Luckily, our users seem to love the changes we made. Especially the persistent navigation we introduced appeared to be very well received by our users. Although we’ll always be iterating and experimenting with our navigation, this milestone has been an extremely rewarding project. Tweet Join the Yelp Product Design Team If our redesign of the Android navigation inspired you to design beautiful and functional products, apply to be a UI designer at Yelp! View Job Back to blog", "date": "2015-09-14"}, {"website": "Yelp", "title": "Automatically Categorizing Yelp Businesses", "author": ["\n        \n  n. tung, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/09/automatically-categorizing-yelp-businesses.html", "abstract": "At Yelp, we use categories for businesses to help determine search results,\nimprove search relevance, inform our users of what a business does, as well as\nhelp our sales team target groups of businesses. For example, Starbucks is\ncategorized as Coffee & Tea , and Walgreens\nstores are often categorized as both Drugstores and Convenience Stores . A number of our businesses don’t have categories, but they do hold other clues\nwhich can be used to infer categories. They tend to be businesses with\nrelatively few reviews and photos, since more popular businesses would have\neventually been assigned a category by Yelp users, or our manual data curation\nteams. In the Impact section (down the page), we show that businesses received\nalmost twice the number of clicks after being categorized. Our machine-learning system infers the category Hair Salons. In the review, you can see text like “Great place for a haircut”, which is very indicative of this categorization. Problem statement Given a business on Yelp, we want to assign it a set of applicable categories.\nBecause a business can be assigned multiple categories, this is a multi-label\nclassification problem. Categorization is challenging because: We have nearly 1000 possible categories (see our category\nlist ) Our current categorizations (training data) aren’t perfect Uncategorized businesses (inputs) can be ambiguous The difference between some categories is subtle or ambiguous\n(e.g. Real Estate Services vs. Real Estate Agents ) Further, we wish to automatically apply categorizations with minimal user\nintervention. We are willing to trade recall for\nprecision , i.e. assign\naccurate categories to fewer businesses. Typically (in our work) this is\nachieved by choosing confidence thresholds, which can later be adjusted, though\none can also weight negative training examples more. Classification approach Our first attempt at categorizing businesses tried to determine the single best\ncategory for a business. This is called a “one-vs-all” multi-class\nclassification\napproach .\nWe would then try to recover multi-label\nclassification aspects by looking at how the probability scores for each classifier fell off.\nFor example, if a business was marked as 70% Restaurants , 20% Coffee\n& Tea , and the remaining 10% weight shared by remaining categories, we\nmight try to guess that the labels [ Restaurants , Coffee & Tea ] should apply. This didn’t work very\nwell, because we couldn’t distinguish between cases where: We weren’t sure of any categorization None of our existing categories matched the business well Multiple categories matched the business well Not having reasonable confidences severely limited our ability to auto-apply\ncategorizations. Our re-designed approach uses a series of binary classifiers: one for each\ncategory. Each classifier answers a simple question: is a business of a given\ncategory? For example, the Mexican Restaurant classifier would respond “no” to the example business above (A & E Styles),\nwhereas the Hair Salons classifier\nwould respond “yes”. A naive approach could simply assign categories of the top-scoring classifiers.\nFor example, if the Mexican and Italian classifiers both responded “yes”, those\nwould be the new categories of the business. However, we find that using a\nsecond-level classifier, which we uncreatively call the “multi-label\nclassifier”, is a better approach (we compare the two later). To construct multi-label classifications, we take the 5 top-scoring categories,\nand consider subsets of 1, 2, and 3 categories. Each subset is scored by the multi-label classifier. The classifier will output\nprobabilities that: The subset is wrong (includes categories that aren’t correct) The subset is correct but not complete (a strict subset of the correct\ncategories) The subset is correct and complete (equal to the set of correct categories) Binary classifiers: features and design The binary classifiers use scikit-learn’s logistic\nregression class. These classifiers support a large number of features. We extract terms\nfrom names and reviews, using standard lexical analysis techniques of tokenization ,\nnormalization (e.g. lowercasing), and stop word filtering. If the business has\nbeen categorized as part of a chain (which we’ll describe in an upcoming blog\npost!) we’ll include that chain’s URL as a feature, and if the business has NAICS codes from one of our data partners,\nwe’ll include those as well. For the hair salon example above, our binary classifier will work on bag-of-words features from the name, review text, country code, and NAICS codes from data providers. The classifier will then determine which of these features are relevant. For\nexample, the Shopping classifier is highly correlated with businesses that have\n“Pharmacy”, “Dollar”, “Antiques”, or “Battery” in their business name. Our initial design used simple unnormalized linear term frequencies, which\nworked well enough for classifying businesses with few reviews. However, when\nthere are many reviews, the chance of some unrelated word being brought up in\none of its 500 reviews is higher. For example, one of our Mexican businesses had a spurious review comparing\nit to a Tex-Mex place, which resulted in a\nhigh score from the Tex-Mex classifier. We\nare still in the process of choosing an effective normalization scheme (simply\nusing scikit-learn’s TF-IDF transformer\ndidn’t help), but have achieved good preliminary results by normalizing review\nterms separately, with maximum TF\nnormalization ,\nwhich divides term frequencies by the maximum term frequency. Infrastructural challenges While not necessary for understanding the core concepts of our system, another\ninteresting aspect of our binary classifiers is that we are able to train good\nclassifiers on only 400,000 businesses (or fewer, for classifiers which don’t\nhave many positive training examples). This is nice for keeping the memory\nfootprint and training time reasonable. However, we need to train each\nclassifier on the most relevant examples. For example, the Mexican classifier is\nlikely to be confused with Tex-Mex or Traditional American , but is unlikely to be\nconfused with Real Estate Lawyers . Therefore\nwe will show it more Tex-Mex or Traditional American examples. We accomplish this with some clever boosting of the training data. Some very\nrough preliminary classifiers are trained using 30,000 input samples. We then\nboost negative training samples according to the formula: p (classifier is wrong) ~ p (actual category is X ) * p (preliminary\nclassifier miscategorizes X ) This is entirely analogous to normal boosting procedures (where incorrect\nsamples from one training iteration are used as input to the next), except that\nwe use categories as a proxy for whether the classifier will be wrong on a\nsample. To sample and join the data, we currently use Redshift. We make frequent use of\nthe ORDER BY RANDOM() LIMIT N trick to generate uniform samples, the RANK() window function to select up to 3 random reviews for a business, as well as\ncareful sortkey / distkey use, materialized TEMP tables, and UNLOAD in parallel\nwithout sorting. Despite the inconvenient access patterns doing this much random\nsampling for so many classifiers, Redshift performs admirably, and is far from a\nbottleneck. For processing data, most of the project was developed with vimap , providing local parallelism,\nbut we were pushing its limits. We have begun to use Spark, to train and\nevaluate all of our classifiers in parallel, and the preliminary results are\ngood. Note: For practitioners, it’s worth mentioning that currently, doing ORDER BY RANDOM() on many-column tables doesn’t perform well. Therefore, it’s\ngood to sample one table (e.g. the primary table) to a reified TEMP table, and\nthen perform INNER JOIN s with distkey s on auxiliary tables. Multi-label classifier: features and design The final multi-label classifier uses scikit-learn’s random forest\nclassifier .\nRecall that as input, the multi-label classifier operates on subsets of category\nlabels, restricted to reasonable subsets generated from the highest-scoring\nbinary classifiers. For example, if the highest scoring binary classifiers are\n[ Restaurants , Mexican , Italian , Shopping ],\none subset we might score could be\n[ Restaurants , Italian ], but we won’t consider a subset like\n[ Restaurants , DUI\nLawyers ]. Each of these subsets is scored with probabilities it is\ncorrect and/or complete. The classifier operates on features that don’t precisely\nspecify which concrete categories are part of the subset, to make it generalize\nwell. Denote “chosen binary classifiers” as the classifiers for categories in\nthe subset under consideration. It will look at features like: The best/worst score of any chosen binary classifier on the business The best/worst F-scores for chosen binary classifiers (this is a metric of\nhow good the classifiers are overall / how innately ambiguous the categories\nare) A score representing whether there are any high-scoring binary classifiers\nwhich were not chosen. Whether the chosen categories often co-occur (for example, it’s unlikely a\nrestaurant is both Mexican and Italian ) Features like the last will drop a few special-case businesses, in exchange for\nimproving overall classification accuracy. For example, a gas station\nconvenience store known for its\npizza is categorized by\nour system as Pizza instead of both Pizza and Convenience Stores . The role of the multi-label classifier is pretty important. Here is a comparison\nto using a naive approach, that only selects binary classifiers whose score is\nabove a threshold, Approach Percent classified All-correct precision, of those classified Complete precision, of those classified multi-label classifier 69.5% 89.3% 12.0% multi-label classifier, weighted more towards complete 67.0% 89.3% 13.1% Naive, 0.95 threshold 48.8% 75.9% 14.7% Naive, 0.97 threshold 38.7% 81.4% 13.2% Naive, 0.98 threshold 31.6% 83.6% 11.4% We explain the two precision scores in detail in the evaluation section, but the\nupshot is that the multi-label classifier swamps the naive approach in terms of\nrecall, while providing higher precision. Its lower percent of complete\ncategorizations is offset by the the fact that it is making many more\ncategorizations. As a minor detail (in the name of scientific honesty), we\nmodified the multi-label classifier to skew it towards complete classifications,\nsince by default it’ll be biased towards giving top-level category predictions;\nwe hope this makes the comparison with the naive approach more fair. We posit that in the naive approach, errors in our component classifiers can get\namplified–even if each classifier only had a 0.1% false positive rate, quite\noften, for a given business, at least one of our hundreds of classifiers would\nbe giving a false positive. The multi-label classifier can help correct some of\nthese errors by examining all classifiers’ outputs. Getting labelled data To train the binary classifiers, we used categorized and reviewed Yelp\nbusinesses. The binary classifiers benefit from having a large amount of data,\nand anecdotally this matters much more than having perfectly-labeled training\ndata. Also, for the sake of pristine evaluation, we remove any businesses in the\nevaluation dataset from the binary classifiers’ training dataset. For evaluation of the binary classifiers, and the multi-label classifier (which\nwe evaluate/train cross-validation style), we’ve gathered labelled data in two\nways. The first was to take Yelp’s top-reviewed businesses, and modify them so\nthat they reflected data to be categorized. The top-reviewed businesses are\ncategorized very accurately, but are also way too easy: each has hundreds of\nreviews. So, we only used the first review (that the business ever got). This served well for our initial evaluation, but there was another problem: our\ntop-reviewed businesses also fell under a different distribution of categories\n(mostly restaurants) than new or newly-reviewed businesses. Therefore, we took a\nuniform sample of businesses we were trying to categorize, and used CrowdFlower\nto get crowdsourced labelings. A sample question would look like so: When evaluating our system, we would sometimes see cases where it was badly\nwrong (e.g. assigning Auto Detailing to a boat detailing place), versus cases\nwhere it selected a secondary purpose of the business, or a related category,\nlike Barbers instead of Hair Stylists. The inclusion of an “Unsure” option, and\nnatural distribution of crowdsourced answers, allowed us to analyze these cases\nseparately. The downside of using CrowdFlower was that its users, unlike our\nmanual data curation teams, are not as familiarized with the peculiarities of\nour category tree, and not as invested in giving precise answers. Our current gold dataset contains 5000 of the modified top-reviewed businesses,\nand 3800 CrowdFlower-labeled businesses. Evaluation We evaluated binary classifiers and the multi-label classifier separately. The\nbinary classifiers differed widely in their accuracy. Some categories, like Local Flavor and Ethnic Food , are relatively ambiguous, and their\napplication to\nbusinesses isn’t uniform. Most classifiers for top-level categories performed\nwell, achieving F 0.1 scores around 0.95 and AUC scores around 0.97. In a previous (2011) blog\npost ,\nwe evaluated Naive Bayes classifiers on a sample of businesses, that was easier\nthan our current gold dataset. The previous system achieved around 80% accuracy\n(between 75% and 88%). On our current gold dataset, our new Autocategorizer\nachieves around 96% accuracy (between 88% and 99%). Our specific per-category\nresults are: Category Accuracy Automotive 98.5% Health 99.1% Home Services 96.7% Real Estate 98.8% Restaurants 87.9% Shopping 95.6% We hypothesize that the Restaurants classifier having significantly lower accuracy is in part accounted by the\nambiguity between Restaurants and Food categories in our category tree. On our current settings, the multi-label classifier returns confident results\nfor 70% of the businesses in the gold dataset. Of these, the classifications are\ncorrect 90% of the time (recall “correct” means the predicted category set is a\nsubset of the gold category set). However, when we factor in ambiguity of our\ncategory tree, we typically find that it gives acceptable results 95% of the\ntime (that it gives a confident result). The multi-label classifier optimizes\nfor precision of its output, but we manually bias it back towards more\ninteresting categorizations; currently, 45% of our categorizations feature a\nnon-root category, and around 10% are complete. Impact We’ve been able to successfully categorize approximately 65% of uncategorized\nbusinesses that have at least 1 review in the US, Canada, and the UK. This has\nbeen a significant benefit to our Sales team, increased traffic for those\nbusinesses, and provided our users with more relevant results. To quantify the\nimpact on search relevance, we found that the click-through volume from search\nto a business nearly doubled after being categorized, for US businesses. The orange line shows that after categorization, businesses received approximately twice as many clicks (on average). The blue and red control lines, representing businesses which were not categorized, show that this change is due to categorization, instead of extraneous factors, like increased overall traffic or faulty metrics. (We abstracted the exact number of clicks, since that is non-public information.) We also categorize about 1-2% of businesses in the US with no review text,\nshould they have a strongly-indicative name. We hope that eventually this number\ncan double, but for now, we are starting with the most confident\ncategorizations. Future Work We wish to increase the impact of this project by: Correcting categories on businesses. Review vector normalization experiments\nare yielding good quality output. We may also want to remove bad\ncategorizations. Categorizing businesses in non-English-speaking countries. There is\nsometimes an issue of data sparsity, but preliminary findings are promising. For improving classifier quality, we want to: Add features by scraping individual websites of businesses Improve interpretation of review text with unsupervised models (e.g. topic\nmodeling), and possibly basic translation models (e.g. dictionaries) for\nforeign languages. We also want to maximize the impact of this project by showing less-confident\ncategory suggestions to either Yelp users when they add or edit businesses, or\nour human data curation team. These suggestions may not be good enough to\nauto-apply, but they are often much better than searching through our large list\nof categories for the appropriate one. We hope our automatic categorization project will ensure Yelp businesses have\nthe most accurate categories possible, and that our users can use this\ninformation to better discover great local businesses! Tweet Become a Data-Mining Engineer at Yelp Interested in using machine learning to improve Yelp's data? Apply to become a Data-Mining Engineer at Yelp. View Job Back to blog", "date": "2015-09-02"}, {"website": "Yelp", "title": "Mycroft - Load Data into Redshift Automatically", "author": ["\n        \n  John R., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/04/mycroft-load-data-into-redshift-automatically.html", "abstract": "Yelp generates terabytes of logs every day. Starting in 2010 with the release of mrjob , Yelp has relied heavily on Amazon Elastic MapReduce (EMR) and MapReduce jobs to analyze this data. While MapReduce works well to repeatedly answer the same question, it’s not a great tool to answer questions that are not well defined or that need to be answered only once. Consequently, we started using Redshift , Amazon’s Postgres-compatible column-oriented data warehouse, to explore our data. Yelp’s log data already lands on S3 every day making it a convenient location to stage data for loading into Redshift. Unfortunately, most of our logs aren’t in a format that can be directly loaded but instead need to be lightly transformed, then converted into JSON or CSV for loading. mrjob is the perfect tool to perform these light transformations - so much so that we started building infrastructure to make this extremely common pattern as easy as possible. Mycroft is an orchestrator that coordinates mrjob, S3, and Redshift to automatically perform light transformations on daily log data. Just specify a cluster, schema version, S3 path, and start date, and Mycroft will watch S3 for new data, transforming and loading data without user action. Mycroft’s web interface can be used to monitor the progress of in-flight data loading jobs, and can pause, resume, cancel or delete existing jobs. Mycroft will notify users via email when new data is successfully loaded or if any issues arise. It also provides tools to automatically generate schemas from log data, and even manages the expiration of old data as well as vacuuming and analyzing data. Mycroft provides a web interface that makes it easy to create new data loading jobs. Mycroft ships as a set of Docker containers which use several AWS services, so we’ve provided a small configuration script to ease the initial customization. Once configured, the service itself can be started using docker-compose , making getting it up and running relatively painless. A comprehensive Quickstart is available for getting Mycroft up and running. The guide steps through getting a copy of Mycroft, configuring Mycroft and launching the required AWS services, and culminates in generating a schema for some example data and loading that data into Redshift. Mycroft is available on GitHub . Please let us know if you encounter any issues with Mycroft, and don’t hesitate to submit pull requests with any great features you decide to develop. Acknowledgements Thanks to the team and everyone that helped build Mycroft: John Roy, Boris Senderzon, Anusha Rajan, and Justin Cunningham. Tweet Back to blog", "date": "2015-04-30"}, {"website": "Yelp", "title": "Yelp Tech Talks: Mobile Testing 1, 2, 3 Wrap Up", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/05/yelp-tech-talks-mobile-testing-1-2-3-wrap-up.html", "abstract": "Last week we held our second tech talk, focusing on mobile, in a new internal series we launched this year. The presenters covered two very important topics in mobile: wearable apps and testing. Building Our Apple Watch App The evening started with a talk by Bill M. who lead the efforts to build our Apple Watch app. We knew we had to build the app in order to provide our users with the best possible access and experience. Since the platform was brand new and kept changing, it came with its own challenges. Development needed to be planned carefully to make sure we could deliver an app with a set of features that added up to a useful experience but still hit the launch date. The Yelp Apple Watch storyboard With a few key features in mind, our iOS developers dove in and started coding away. The storyboard (shown above), defines all of the functionality for the app. The team faced a lot of challenges with how to handle network requests, images, location, and phone-watch communication. Testing After learning how to build an Apple Watch app, Mason G. and Tim M. followed up on how we test our iOS and Android mobile apps (respectively) to ensure the best possible end product. Both apps share a common API. Whenever we want to change this API, we start by writing documentation and examples. This allows us to define a contract between the API and the clients, and work out major problems before we write any code. Additionally, we can then take the examples and use them as mock data to test the clients. Mason then led us into the world of iOS testing. Testing is critical in order to prevent regressions and give developers confidence that their changes work. The team relies on unit, integration, and acceptance tests to make sure that different components all work correctly. Our test suite leverages several tools, including KIF , to provide great test coverage. Tim then spoke on the challenging world of Android testing. The major concerns with Android testing include test scalability, reliability, and speed. Despite the many obstacles to overcome, we’ve created a solid setup here at Yelp, largely due to the great open source libraries available for the platform, including Spoon and Espresso . Screenshot from a tech talk showing all the different testing frameworks. We have the full video and slides online: On to the next talks! If you weren’t able to attend our tech talk this time around, don’t worry! You’ll still get a chance to see our offices at our 3rd annual WWDC party. RSVP here . If you’re interested in future events at Yelp or in engineering opportunities, let us know ! Tweet Back to blog", "date": "2015-05-08"}, {"website": "Yelp", "title": "HTTPS Client Testing Made Easy", "author": ["\n        \n  Thomas R., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/05/https-client-testing-made-easy.html", "abstract": "You’ve probably read about the recent AFNetworking vulnerability . Nowadays, it’s not sufficient to just test your SSL certificates. You must also test how your clients use these certificates to have confidence your users aren’t getting pwned. There are plenty of tools to test broken cipher suites and cryptographic vulnerabilities but, until now, there wasn’t a readily-available, free, and simple tool for testing how your client handles certificate requests and X.509 verification over the wire. Enter tlspretense-service . This tool provides a simple Docker container built around iSEC Partner’s tlspretense certificate testing suite that acts as a MitM to test your clients. It works very similarly to tlspretense-docker, except instead of routing through the container by making (potentially problematic) networking changes, you instead connect to the container directly. This means that if your client accepts a service URL to connect to, you can just point it at tlspretense-service to test the robustness of your certificates. The tlspretense’s configuration file contains more information on which tests are run. tlspretense-service is fairly easy to get up and running , since the Docker container does the bulk of the work for you. When it’s done running, you’ll get a handy report like this: Each test tells you what it expected, what the actual result was, and the complete duration of each connection. The above example demonstrates the default behaviour of curl ( curl https://localhost:8443 ). Note that curl rejected every connection in this naive test. This is because tlspretense provides its own CA for use in testing. Here’s what happens if we trust it ( curl https://localhost:8443 --cacert tlspretense/ca/goodcacert.pem ): This time, our client connected in the majority of cases. The output then proceeds to show whether curl continued the connection, what expected behavior would be, and whether the test passed or failed. Here are a few interesting things you can do with this container: Rigorously test certs for your public HTTPS clients, including web browsers and web proxies that connect to the outside world. Intercept and test certs for your internal HTTPS and HSTS service traffic safely, without disclosing information to a third party. Create regression tests and test harnesses around your clients, to ensure they’re always X.509 compliant, without having those tests perform invasive networking changes. This tool is still a work in progress, so feel free to report any bugs or issues you find with it. Test on! Tweet Back to blog", "date": "2015-05-18"}, {"website": "Yelp", "title": "Seeing Double On Yelp", "author": ["\n        \n  Tobi O., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/05/seeing-double-on-yelp.html", "abstract": "Being able to easily find what you want on Yelp is a critical part in ensuring the best user experience. One thing that can negatively affect that experience is displaying duplicate business listings in search results, and if you use Yelp often enough, you might have run into duplicate listings yourself. We constantly receive new business information from a variety of sources including external partners, business owners, and Yelp users. It isn’t always easy to tie different updates from different sources to the same business listing, so we sometimes mistakenly generate duplicates. Duplicates are especially bad when both listings have user-generated content as they lead to user confusion over which page is the “right” one to add a review or check-in to. The problem of detecting and merging duplicates isn’t trivial. Merging two businesses involves moving and destroying information from multiple tables which is difficult for us to undo without significant manual effort. A pair of businesses can have slightly different names, categories, and addresses while still being duplicates, so trying to be safe by only merging exact matches isn’t good enough. On the other hand, using simple text similarity measures generates a lot of false positives by misclassifying cases like: two businesses that are part of the same chain and are located close to one another one business that is a sub-business of another (e.g. Monterey Bay Aquarium and Jellies Experience at the Monterey Bay Aquarium ) a professional and the practice that they work at (e.g. Coldwell Banker and Rose Parmelee - Coldwell Banker ) Business Match The first step in our deduplication system is our Business Match service. Using a wrapper over Apache Kafka , every time a new business is created or a business’s attribute is changed, a batch that consumes messages published to the new_business and business_changed topics calls Business Match to find any potential duplicates of the affected business above a relatively low confidence threshold. Business Match works by taking partial business information (such as name, location, and phone) as input, querying Elasticsearch, reranking the results with a learned model, and returning any businesses that the model scores above a scoring threshold. If Business Match returns any results, the business pairs are added to a table of potential duplicates. Our User Operations team is responsible for going through pairs of businesses in the table and either merging them or marking them as not duplicates. However, the rate at which duplicates are added to the queue far outpaces the rate that humans can manually verify them which motivated us to develop a high-precision duplicate business classifier that would allow us to automatically merge duplicate pairs of businesses. Getting Labelled Data In order for our classifier to work, we needed to get thousands of instances of correctly labelled training data. For this, we sampled rows from our business duplicate table and created Crowdflower tasks to get crowdsourced labelings. We’ve launched public tasks as well as internal-only tasks for our User Operations team which let us easily create a gold dataset of thousands of accurately labelled business pairs. In the future, we are planning on trying an active learning approach where only data that our classifier scores with low confidence is sent to Crowdflower, which would minimize the amount of necessary human effort and allow our classifier to reach a high accuracy with a minimal number of training instances. Features Our classifier takes as input a pair of businesses and generates features based on analyzing and comparing the business fields. It uses the same model ( scikit_learn’s Random Forests ) and many of the same basic features as Business Match like geographical distance, synonym-aware field matching, and edit distance / Jaccard similarity on text fields. In order to capture the kinds of false positives described earlier, we also added two intermediate classifiers whose output was used as features for the final classifier. We created a named entity recognizer to detect and label business names that indicate a person (e.g. lawyers, doctors, real estate agents) in order to detect the differences between a professional and their practice or two professionals working at the same practice. Another feature we added is a logistic regression classifier that works by running a word aligner on both business names, finding which terms occur on one or both business names, and determining how discriminative the similarities and differences between the two names are. It outputs a confidence score, the number of uncommon words that appeared in one name but not the other, and the number of uncommon words that appeared in both names, which are used as features in the duplicate classifier. # clf = sklearn.linear_model.LogisticRegression\n# significant_terms = set of terms appearing more than n times in training\ndef classify(left_name, right_name):\n    \"\"\"\n    Classifies names using delta term analysis.\n    :return:\n        A tuple (p_is_duplicate, exact_match_rare_terms, one_side_rare_terms).\n\n        * p_is_duplicate is the score from the log-linear classifier. It's\n          probably the most relevant signal.\n        * exact_match_rare_terms is the number of terms that are RARE and\n          exact matches. This is generally a positive signal.\n        * one_side_rare_terms is a list of terms that are RARE and either\n          only occur on one side, or are partial matches. This is generally\n          a negative signal.\n    :rtype: (float, bool, bool)\n    \"\"\"\n    # calls an word aligner on the two names. Returns a list of\n    # ExactMatchTerm(string)/OneSideTerm(string) objects\n    delta_terms = get_name_delta_terms(left_name, right_name)\n\n    significant_delta_terms = [term for term in delta_terms if term.word in significant_terms]\n\n    (p_false, p_true), = clf.predict_proba(feature_vectorize(significant_delta_terms))\n\n    # Signals outside of the classifier prediction\n    exact_match_rare_terms = sum(\n        int(term.is_rare() and isinstance(term, ExactMatchTerm))\n        for term in significant_delta_terms\n    )\n    one_side_rare_terms = sum(\n        int(term.is_rare() and isinstance(term, OneSideTerm))\n        for term in significant_delta_terms\n    )\n\n    return p_true, exact_match_rare_terms, one_side_rare_terms Evaluation Since merges are hard to undo, false positives are costly so the focus of our classifier was on precision rather than recall. Our main evaluation metric was F 0.1 score, which treats precision as 10 times more important than recall. With all of our classifier’s features, we achieved a F 0.1 score of 0.966 (99.1% precision, 27.7% recall) on a held-out data set, compared to a baseline F 0.1 = 0.915 (97.1% precision, 13.4% recall) for the strategy of only merging exact (name/address) matches and F 0.1 = 0.9415 (96.6% precision, 26.4% recall) using only the basic Business Match feature set. Future Work With the work done on our duplicate classifier and automatic business merging, we’ve been able to merge over 500,000 duplicate listings. However, there’s still room for improvements on deduplication. Some things slated for future work are: language and geographical area-specific features focusing deduplication efforts on high-impact duplicates (based on number of search result impressions) extracting our named entity and discriminative word classifiers into libraries for use in other projects\nWith the improvements to our classifier, we hope to be able to detect merge all high confidence duplicate business listings and minimize the necessary amount of human intervention. Tweet Back to blog", "date": "2015-05-27"}, {"website": "Yelp", "title": "Things To Do Outside of WWDC", "author": ["\n        \n  Julia N., Senior Events and Partnerships Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/06/things-to-do-outside-of-wwdc.html", "abstract": "WWDC is coming up soon and it’s going to be a blast! We’ve got our Apple Watch (with our lovely Yelp App installed, of course!) and are ready to party. To celebrate, we’re hosting our third annual WWDC party on June 8 and raffling off an Apple Watch (so you can use our app too)! While you’re in town for WWDC, make sure to catch some of the other great meetups we’ve got lined up. We’ll also be speaking at a Women in Tech event at Alpine Labs and at a DBA Happy Hour co-hosted with Box! We’re also really looking forward to hearing presentations by the talented girls of Technovation . We’ll help judge their World Pitch competition where girls ranging from the ages of 10-18 partner up to develop ways of incorporating technology into their everyday lives. The top 10 finalist teams from around the world will be visiting us, competing for the chance to win $10,000 towards their project. Come meet the girls and support their hard work and dedication! Thursday, June 4, 2014 - 6:00PM - Introduction to Functional Reactive Programming on Android ( SF Android User Group ) Monday, June 8, 2015 - 6:30PM - Yelp WWDC Afterparty ( Yelp Engineering ) Tuesday, June 9, 2015 - 6:00PM - How to Design Habit-Forming Products Workshop ( Nir Eyal ) Wednesday, June 10, 2015 - 6:15PM - Learn about the inner workings of the Internet and Twisted ( SF Python ) Thursday, June 11, 2015 - 6:00PM - Interactive Data Science and Sharing with Jupyter and IPython ( SF Big Analytics ) Thursday, June 18, 2015 - 6:30PM - Failure and Success ( Designers + Geeks ) Tuesday, June 23, 2015 - 6:45PM - Bigger Data, Bigger Impact ( Products That Count ) Wednesday, June 24, 2015 - 5:30PM - World Pitch Competition ( Technovation ) Tweet Back to blog", "date": "2015-06-01"}, {"website": "Yelp", "title": "Advanced UITableViews Made Simple: YLTableView", "author": ["\n        \n  Mason G., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/06/advanced-uitableviews-made-simple-yltableview.html", "abstract": "Let’s say you had a great new idea for a Todo app and set out to build an awesome iPhone application. After firing up your copy of Xcode, you’d probably start by creating a UITableView – a critical part of the iOS SDK that allows you to build scrolling views. At Yelp, we’ve been playing around with different table view architectures for years. After a lot of trial and error, we’ve come up with a framework that easily scales from the simplest list all the way up to our business page. We’re open sourcing it today – check out YLTableView on GitHub! The Activity Feed and the Business View both make use of table views A table view contains a number of sections, each of which can be separated by a small margin. Each section can contain an arbitrary number of cells. We use table views all over the Yelp app, in places like the business page and Activity Feed above. The feed contains a list of ‘items’ representing actions other users have taken – like posting photos or checking-in at a business. In the Activity Feed, each feed item is a single section made up of multiple cells. A section (green) has multiple cells (blue) To implement something like the Activity Feed, you need to tell UITableView about the sections and cells in your table view by implementing a few methods from the UITableViewDataSource protocol: - (NSInteger)numberOfSectionsInTableView:(UITableView *)tableView;\n- (NSInteger)tableView:(UITableView *)tableView numberOfRowsInSection:(NSInteger)section;\n- (UITableViewCell *)tableView:(UITableView *)tableView cellForRowAtIndexPath:(NSIndexPath *)indexPath; However, you’re not done yet – if you want to have section headers or make your cells do something when tapped, you’ll need to implement some of the UITableViewDelegate protocol. The two protocols have a lot of overlap and it can get confusing really quickly. When trying to figure all of this out, you might come across UITableViewController , and end up with an architecture that looks like this: This might work for a while, but the view controller is going to get complex very quickly since it will violate the single responsibility principle . The view controller has to deal with loading content, micromanaging the table view cells, and pushing on new view controllers for cells. It might work for a small table view, but it’ll get complicated very quickly. Enter: YLTableview. At it’s core, YLTableView works with a series of models. Each cell expects a given type of model and, once it’s given that model, is able to configure itself. Separating out the model helps encourage modularity through your application, and makes it easy to build interfaces out of common components. To support the models, we’ve created YLTableViewDataSource which implements both UITableViewDelegate and UITableViewDataSource . The two protocols typically need the same data, and we’ve found that combining them helps simplify your code. To use YLTableViewDataSource, make a subclass and implement a few easy methods: - (NSString *)tableView:(UITableView *)tableView reuseIdentifierForCellAtIndexPath:(NSIndexPath *)indexPath;\n- (NSObject *)tableView:(UITableView *)tableView modelForCellAtIndexPath:(NSIndexPath *)indexPath;\n- (NSInteger)numberOfSectionsInTableView:(UITableView *)tableView;\n- (NSInteger)tableView:(UITableView *)tableView numberOfRowsInSection:(NSInteger)section; YLTableViewDataSource will then take care of creating the cell, configuring it with your model, and even telling the table view how tall it should be. When using YLTableView, your architecture will look a bit like this: Unlike the UITableViewController design we looked at earlier, this architecture does a great job of separating out responsibilities. Your view controller loads content – like reviews or businesses – and passes them off to the data source. The data source then turns that content into table view models and displays them as cells. Before telling the view controller to take an action, the data source will translate the cell models back into content to pass to the view controller. The view controller doesn’t need to know anything about the models or cells: you could change your entire UI and it’s underlying data implementation without changing the logic in the view controller. As we became better and better at building table views, we started to tackle some more complex cells. Take a look at this cell from the Activity Feed: This cell has a swipeable photos cell. Swiping back and forth will reveal and load in more photos, while tapping on a photo will make it full-screen. As you can imagine, this is pretty tricky to do! Our first approach to enabling complex cells like this was to have the cell delegate back to the data source, and the data source would delegate up to the view controller. This prevented the photos cell from being modular – every time you wanted to use it, you had to duplicate the entire chain of delegates. We knew there had to be a better way. After thinking about it for a while, we decided to try creating child view controllers for our table view cells. Child view controllers allow you to add a new view controller to manage a subsection of your view. They behave like real view controllers and can push new view controllers onto the stack. Normal table views don’t support child view controllers, but we figured out how to do it with YLTableView. Simply have your cell implement YLTableViewCellChildViewController and set the parent view controller property on your data source. Then, instead of having to deal with the mess of delegates, you have a much simpler view controller: In addition to having a simpler architecture, this cell is now significantly more reusable. We can use it directly on the business page without having to duplicate any code. Taking it a step further, we can even use the PhotosViewController by itself. A little bit of configuration later, and we can now use the same view controller all over the app. The cell's child view controller can also be reused by itself. YLTableView has helped us simplify our architecture through the app. Using models has allowed us to build up a system of reusable components, making it easy to build interfaces out of common components. Using child view controllers has made this even more powerful, letting us reuse entire view controllers in table view cells. And, as more and more of our app is written with YLTableView, we’ve found that having a consistent architecture gives developers a base level of familiarity with features the haven’t worked on before. Give YLTableView a shot, and let us know what you think! Tweet Back to blog", "date": "2015-06-12"}, {"website": "Yelp", "title": "New Yelp API Console: Life’s Easier in v2", "author": ["\n        \n  Sandeep C., Product Management\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/06/new-yelp-api-console-lifes-easier-in-v2.html", "abstract": "Yelp’s API allows any developer to build rich user experiences by integrating Yelp’s local business information, reviews, and pictures into their web and mobile applications. We announced in January that we have ambitious plans to make it easier than ever for developers to integrate a local layer into their apps. Today, we’re excited to present one of these efforts: a brand new API console that allows developers to explore the detail and depth of responses returned by the Yelp API, without having to write a single line of code. Sign up for a free account, create your API v2 credentials, and check it out on the Yelp Developer Site . API V1 End of Life As we continue to invest in Version 2 of the API, we will be discontinuing the previously deprecated Version 1 of the API. All v1 API endpoints will be shut down on July 15, 2015. If you are currently using any v1 API endpoints you can find out how to migrate to v2 endpoints on the Yelp developer site. Build Your Yelp API Powered App Today Finally, if this has got you hungry to build something, you can head over to our Github Repo to find examples and libraries to get you underway with your Yelp API powered App. We love hearing from fellow developers, so share your cool creations with us on Twitter via @YelpEngineering or write us at api@yelp.com and stay tuned for even more API goodies coming your way. Tweet Back to blog", "date": "2015-06-30"}, {"website": "Yelp", "title": "Yelp API Now Returns Action Links", "author": ["\n        \n  Hemant B., Product Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/07/yelp-api-now-returns-action-links.html", "abstract": "Savvy developers like you already know that the Yelp API is the best place to get information on local businesses. Today, as a part of our on-going integration with our friends at both Yelp SeatMe and Yelp Eat24, we are excited to announce an additional feature of the API: “Action Links.” Action Links will allow your users to directly make a reservation or even start a food order for delivery or pickup from wherever you’re displaying Yelp data. To see this in action (pardon the pun), next time you use the Search endpoint or the Business endpoint , simply pass in the optional “ actionlinks=true ” parameter. When requested, action links will be returned in fields called reservation_url and eat24_url with the response for businesses that support them. Checkout the handy API documentation for more details. You can use these links within your application to launch the Yelp Eat24 ordering experience or into the Yelp SeatMe reservation experience. Ordering Food on Yelp Eat24 Making A Yelp SeatMe Reservation Go ahead, give it a whirl and let us know what you think. We love hearing from fellow developers, so share your cool creations with us on Twitter via @YelpEngineering or write us at api@yelp.com and stay tuned for even more API goodies coming your way. Tweet Back to blog", "date": "2015-07-02"}, {"website": "Yelp", "title": "Reading Between the Lines: How We Make Sense of Users' Searches", "author": ["\n        \n  Heath V., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/02/reading-between-the-lines-how-we-make-sense-of-users-searches.html", "abstract": "The Problem People expect a lot out of search queries on Yelp. Understanding exact intent from a relatively vague text input is challenging. A few months ago, the Search Quality team felt like we needed to take a step back and reassess how we were thinking about a user’s search so that we could return better results for a richer set of searches. Our main business search stack takes into account many kinds of features that can each be classified as being related to one of distance, quality and relevance. However, sometimes these signals encode related meaning, making the equivalence between them difficult to understand (e.g., we know a business is marked as “$ – inexpensive” and that people mention “cheap” in their reviews), but often they capture orthogonal information about a business (e.g., reviews are not a reliable source of a business’ hours). We need a stage where we discern which parts of a query tell us what types of information should be searched over. It’s also useful to embellish the plain text of a search query with context. Are there related words that are very relevant to a substring of text? Is it possible that the user misspelled part of the query? Having this extended information would enhance our ability to recall relevant businesses, especially in low content areas. Let’s consider a more concrete example, such as a search for “mimosa brunch” in San Francisco. The above search results show some features that our desired (and now current) query understanding system should be able to extract: “mimosa” has the related search term “champagne” “brunch” maps to the attribute “Good for Brunch” All these enhancements sounded great but were becoming increasingly difficult to implement. Our old search stack combined the understanding of the intent and semantics of a query and its execution into a single step. This resulted in engineers having a hard time adding new quality features: concepts were overloaded (the word “synonym” had very different meanings in different parts of the code), questions of semantics were tied up in execution implementation, and there was no way for other parts of Yelp to understand the semantic knowledge locked up in our Lucene search stack. Semantics as Abstraction To help solve these problems, we found it useful to formally split up the process of extracting meaning from the process of executing a search query. To that end, we created a component named Soxhlet (named after a kind of extractor used in chemistry labs) which runs before search execution, and whose sole job is to extract meaning from a raw text query. It turns the input text into a Rich Query that holds structured semantics in the context of business search. This Rich Query then becomes the preferred abstraction for the rest of the search stack to execute upon. Encoding Meaning What does this Rich Query data structure look like? When we were first whiteboarding possible representations we tended to underline parts of the query and assign semantics to each part. As you can see from the above representation, there are complex semantics encoded in a signal that is mostly textual (the query text). Therefore, we think that annotating the query string is a good way to represent a Rich Query: ranges of the original query text are marked as having typed information via various kinds of Annotations. For those of you familiar with Lucene, this data structure is very similar to a TokenStream but with annotations instead of attributes. This makes consumption of the Rich Query in the recall and relevance steps straightforward while at the same time giving enough abstraction so that non-search components can also easily consume Rich Queries. As an example, the extracted RichQuery for a query popular with our team is: {\n  \"query\" : \"cheap restaurants\",\n  \"language\" : \"en\",\n  \"queryAnnotations\" : [ \n    {\n      \"type\" : \"RawTokenQueryAnnotation\",\n      \"offset\" : 0,\n      \"length\" : 5,\n      \"queryText\" : \"cheap\"\n    },\n    {\n      \"type\" : \"RawTokenQueryAnnotation\",\n      \"offset\" : 6,\n      \"length\" : 11,\n      \"queryText\": \"restaurants\"\n    },\n    {\n      \"type\" : \"AttributeQueryAnnotation\",\n      \"offset\" : 0,\n      \"length\" : 5,\n      \"attribute\" : \"$\",\n      \"queryText\" : \"cheap\",\n      \"confidence\" : 1.0\n    }\n  ]\n} When the above Rich Query is analyzed by our core relevance stack (a service called “Lucy”) from the AttributeQueryAnnotation , it knows to also search over the attribute field of businesses for the “$” attribute. Furthermore, the confidence of 1.0 implies that an attribute match should be considered equivalent to a textual match for “cheap.” Other annotations may have lower confidences associated with them if their extraction process is less precise and this is factored into the scoring function of constructed Lucene queries. By being able to use the way Annotations overlap in the original search query, it’s now possible to more easily cross-reference Annotations with each other and with the original text. The recall and relevance stages of the search stack use this ability to ensure that the right constraints are used. Extraction Process What is the architecture within Soxhlet that allows us to turn a plain query into a Rich Query? Our first observation is that we should have specialized components for producing each type of Annotation. Our second observation is that basic Annotations can help identify more complex ones; you could imagine taking advantage of detected synonyms to properly identify cost attributes in the search query string, etc. From these observations, we decided that a good starting design was a series of transformation steps that iteratively adds new Annotation types. Each one of these steps is called a Transformer. Each transformer takes a Rich Query as input, looks at all existing Annotations and the original query text, and produces a modified Rich Query as output that possibly contains new annotations. Stripped of boilerplate, the code for the top-level Soxhlet class is very simple: public class Soxhlet {\n    private final List<QueryTransformer> queryTransformers;\n\n    public RichQuery extract(PoorQuery poorQuery) {\n        // Step 1: Build the initial rich query.\n        RichQuery richQuery = new RichQuery(poorQuery);\n\n        // Step 2: Apply query transformations.\n        for (QueryTransformer qf : this.queryTransformers) {\n            richQuery = qf.transform(richQuery);\n        }\n\n        // Step 3: Profit.\n        return richQuery;\n    }\n} This architecture also provides us with a mechanism for sharing prerequisite work between Transformers, while not forcing coupling between earlier Transformers or the search recall and relevance steps. As an example, although all Transformers could operate on the raw query string, many kinds of text analysis work on the token level, so many Transformers would have to duplicate the work of tokenizing the query string. Instead, we can have a tokenization Transformer create token Annotations from the raw string, and then every subsequent Transformer has access to that tokenization information. This optimization should be used carefully though with Soxhlet being internationalized into dozens of languages and more being regularly added, normalization (accent removal, stemming etc.) is an important part of extracting rich queries. It’s tempting to store to normalized queries at beginning of the extraction process but we have avoided doing so to prevent couplings among Transformers and between Soxhlet and other services such as the recall and relevance stage of the search stack. The Transformers themselves can vary in complexity so let’s dive into some details on how they actually work: Spelling Transformer We actually use two different types of spelling transformations. “Spelling Suggestion” is where we still perform the Rich Query extraction and search on the original query but offer a “Did You Mean” link in the results for queries that could be good suggestions. These suggestions are computed at query time by generating candidate suggestions within a small edit distance of the original query and then scoring them with a noisy channel model that takes into account query priors and likelihood of edits. “Spelling Correction” occurs when we are very confident about a correction and perform the Rich Query extraction and search on the corrected query. These corrections are generated by promoting Spelling Suggestions that have performed well in the past based on click rates for their “Did You Mean” link. At query time we simply lookup a map from query to correction. This two-tiered approach is very effective in that it allows us to achieve high precision with Spelling Correction without sacrificing recall by allowing Spelling Suggestions for lower confidence suggestions. Our future plans for spelling include improving internationalization and incorporating more advanced features such as a phonetic algorithm into our suggestion scorer. Synonyms Transformer Context is very important to synonym extraction and this drove a number of our design decisions: The same word can have different synonyms in different query contexts. For example, the word “sitting” in the query “pet sitting” has the synonym “boarding”. However, the same word in the query “house sitting” probably shouldn’t have “boarding” as a synonym. The same word in the same query can have different synonyms in different domain contexts. For example the word “haircut” could have the synonym “barber” in a local business search engine such as Yelp. However, on an online shopping website a better synonym would be “hair clippers”. The same word can be used very differently in queries compared to documents. For example, it might be common for a review to mention “I got my car repaired here” but it would be very unusual to write “I got my auto repaired here”. As a result, an algorithm that uses only document data to extract synonyms would likely not recall “auto repair” having the synonym “car repair” despite this being a very good query synonym. In fact, research has shown that the similarity between queries and documents is typically very low. To solve the above problems we adopted a two-step approach to synonym extraction. In the first step we extract possible candidate synonyms from user’s query refinements. In the second step we create vectors for each query containing the distribution of clicked businesses and then score synonyms based on the cosine similarity of their query vectors. We store this information in a lookup table that is accessed at query time to find synonyms. Attributes Transformer We found a simple lookup table mapping between phrases and attributes to be very effective for extracting attributes. This transformer also illustrates one of the benefits of a unified query understanding framework - by piggy-backing off the results of the Synonyms Transformer we are able to use the knowledge that “cheap” is a synonym of “inexpensive” to extract the attribute of “$” from “inexpensive restaurant” without explicitly having the mapping of the “inexpensive” to “$” in our attribute lookup. Although the Attributes Transformer is very simple but precise, you could imagine ways of improving the recall such as using a language model for each attribute. For example the Synonyms Transformer simply contains a dictionary to lookup synonyms that have been pre-generated by batch jobs from query logs. On the other hand, the Spelling Correction Transformer uses a more complex noisy channel model at query time to determine likely corrections. Conclusion Using Soxhlet in our business search stack, we’ve been able to roll out the ability to use query synonyms to enhance recall and relevance, more intelligently spell-correct query mistakes, better detect attributes in queries and most importantly we now have an elegant framework for extending our query understanding functionality by adding new transformers. We have already seen significant search results CTR gains as a result of these changes and have set a foundation for sharing with other parts of Yelp our concept of meaning for business search queries. Although it is a major undertaking to move many of the existing “meaning extraction,” like parts of our search stack into Soxhlet, we believe the benefits from this architecture are well worth it. Special thanks to David S. for helping author this blog post and the Query Understanding team who helped build the platform: Chris F., Ray G., Benjamin G., Natarajan S., Maria C., and Denis L. Tweet Back to blog", "date": "2015-02-11"}, {"website": "Yelp", "title": "March Events @ Yelp", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/03/march-events-yelp.html", "abstract": "This month we’re ramping up and preparing for an awesome time at PyCon . We’ll be there in full force next month so look for us there at booth 606! Be sure to catch a presentation by our own Soups R. on Friday, April 10 at 12:10 where he’ll be speaking on Data Science in Advertising: Or a future when we love ads . In the meantime, hopefully you aren’t too sleepy from daylight savings time to attend some great events this month: Wednesday, March 11, 2015 - 6:00PM - Tech talks and PyCon Startup Row Pitches ( SF Python ) Thursday, March 19, 2015 - 7:00PM - The Best Interface Is No Interface ( Designers + Geeks ) Tuesday, March 24, 2015 - 6:00PM - Mini PyCon 2015 ( PyLadies ) Wednesday, March 25, 2015 - 7:00PM - Workplace Evolved ( Products That Count ) Thursday, March 26, 2015 - 7:00PM - Analytics + Visualization for Large-Scale Neuroscience ( SF Big Analytics ) Tweet Back to blog", "date": "2015-03-11"}, {"website": "Yelp", "title": "Using Services to Break Down Monoliths", "author": ["\n        \n  John B., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/03/using-services-to-break-down-monoliths.html", "abstract": "Introduction At Yelp we value our ability to quickly ship code. We’re constantly pushing changes out to production, and we even encourage our interns to ship code on their first day. We’ve managed to maintain this pace even as the engineering team has grown to over 300 people and our codebase has reached several million lines of Python code (with a helping of Java on the side). One key factor in maintaining our iteration speed has been our move to a service oriented architecture. Initially, most of our development work occurred in a single, monolithic web application, creatively named ‘yelp-main.’ As the company grew, our developers were spending increasing amounts of time trying to ship code in yelp-main. After recognizing this pain point, we started experimenting with a service oriented architecture to scale the development process, and so far it’s been a resounding success. Over the course of the last three years, we’ve gone from writing our first service to having over seventy production services. What kinds of services are we writing at Yelp? As an example, let’s suppose you want to order a burrito for delivery. First of all, you’ll probably make use of our autocompletion service as you type in what you’re looking for. Then, when you click ‘search,’ your query will hit half a dozen backend services as we try to understand your query and select the best businesses for you. Once you’ve found your ideal taqueria and made your selection from the menu, details of your order will hit a number of services to handle payment, communication with partners, etc. What about yelp-main? There’s a huge amount of code here, and it’s definitely not going to disappear anytime soon (if ever). However we do see yelp-main increasingly becoming more of a frontend application, responsible for aggregating and rendering data from our growing number of backend services. We are also actively experimenting with creating frontend services so that we can further modularize the yelp-main development process. The rest of this blog post roughly falls into two categories: general development practices and infrastructure. For the former, we discuss how we help developers write robust services, and then follow up with details on how we’ve tackled the problems of designing good interfaces and testing services. We then switch focus to our infrastructure, and discuss the particular technology choices we’ve made for our service stack. This is followed by discussions on datastores and service discovery. We conclude with details on future directions. Education Service oriented architecture forces developers to confront the realities of distributed systems such as partial failure (oops, that service you were talking to just crashed) and distributed ownership (it’s going to take a few weeks before all the other teams update their code to start using v2 of your fancy REST interface). Of course, all of these challenges are still present when developing a monolithic web application, but to a lesser extent. We strive to mitigate many of these problems with infrastructure (more on that later!), but we’ve found that there’s no substitute for helping developers really understand the realities of the systems that they’re building. One approach that we’ve taken is to hold a weekly ‘services office hours,’ where any developer is free to drop in and ask questions about services. These questions cover the full gamut, from performance (“What’s the best way to cache this data?”) to architecture (“We’ve realized that we should have really implemented these two separate services as a single service - what’s the best way to combine them?”). We’ve also created a set of basic principles for writing and maintaining services. These principles include architecture guidelines, such as how to determine whether a feature is better suited to a library or whether the overhead of a service is justified. They also contain a set of operational guidelines so that service authors are aware of what they can expect once their service is up and running in production. We’ve found that it can often be useful to refer to these principles during code reviews or design discussions, as well as during developer onboarding. Finally, we recognize that we’re sometimes going to make mistakes that negatively impact either our customers or other developers. In such cases we write postmortems to help the engineering team learn from these mistakes. We work hard to eliminate any blame from the postmortem process so that engineers do not feel discouraged from participating. Interfaces Most services expose HTTP interfaces and pass around any structured data using JSON. Many service authors also provide a Python client library that wraps the raw HTTP interface so that it’s easier for other teams to use their service. There are definite tradeoffs in our choice to use HTTP and JSON. A huge benefit of standardizing on HTTP is that there is great tooling to help with debugging, caching and load balancing. One of the more significant downsides is that there’s no standard solution for defining service interfaces independently of their implementation (in contrast to technologies such as Thrift). This makes it hard to precisely specify and check interfaces, which can lead to nasty bugs (“I thought your service returned a ‘username’ field?”). We’ve addressed this issue by using Swagger . Swagger builds on the JSON Schema standard to provide a language for documenting the interface of HTTP/JSON services. We’ve found that it’s possible to retrofit Swagger specifications onto most our our services without having to change any of their interfaces. Given a Swagger specification for a service, we use swagger-py to automatically create a Python client for that service. We also use Swagger UI to provide a centralized directory of all service interfaces. This allows developers from across the engineering team to easily discover what services are available, and helps prevent duplicated effort. Testing Testing within a service is fairly standard. We replace any external service calls with mocks and make assertions about the call. The complexity arises when we wish to perform tests that span services. This is one of the areas where a service oriented architecture brings additional costs. Our first attempt at this involved maintaining canonical ‘testing’ instances of services. This approach lead to test pollution for stateful services, general flakiness due to remote service calls and an increased maintenance burden for developers. In response to this issue we’re now using Docker containers to spin up private test instances of services (including databases). The key idea here is that service authors are responsible for publishing Docker images of their services. These images can then be pulled in as dependencies by other service authors for acceptance testing their services. Service stack The majority of our services are built on a Python service stack that we’ve assembled from several different open-source components. We use Pyramid as our web framework and SQLAlchemy for our database access layer, with everything running on top of uWSGI . We find that these components are stable, well-documented and have almost all of the features that we need. We’ve also had a lot of success with using gevent to eke additional performance out of Python services where needed. Our Elasticsearch proxy uses this approach to scale to thousands of requests per second for each worker process. We use Pyramid tweens to hook into request lifecycles so that we can log query and response data for monitoring and historical analysis. Each service instance also runs uwsgi_metrics to capture and locally aggregate performance metrics. These metrics are then published on a standard HTTP endpoint for ingestion into our Graphite-based metrics system. Another topic is how services access third-party libraries. Initially we used system-installed packages that were shared across all service instances. However, we quickly discovered that rolling out upgrades to core packages was an almost impossible task due to the large number of services that could potentially be affected. In response to this, all Python services now run in virtualenvs and source their dependencies from a private PyPI server. Our PyPI server hosts both the open-source libraries that we depend upon, and our internally-released libraries (built using a Jenkins continuous deployment pipeline). Finally, it’s important to note that our underlying infrastructure is agnostic with respect to the language that a service is written in. The Search Team has used this flexibility to write several of their more performance-critical services in Java. Datastores A significant proportion of our services need to persist data. In such cases, we try to give service authors the flexibility to choose the datastore that is the best match for their needs. For example, some services use MySQL databases, whereas others make use of Cassandra or ElasticSearch. We also have some services that make use of precomputed, read-only data files. Irrespective of the choice of datastore, we try to keep implementation details private to the owning service. This gives service authors the long-term flexibility to change the underlying data representation or even the datastore. The canonical version of much of our core data, such as businesses, users and reviews, still resides in the yelp-main database. We’ve found that extracting this type of highly relational data out into services is difficult, and we’re still in the process of finding the right way to do it. So how do services access this data? In addition to exposing the familiar web frontend, yelp-main also provides an internal API for use by backend services in our datacenters. This API is defined using a Swagger specification, and for most purposes can be viewed as just another service interface. Discovery A core problem in a service oriented architecture is discovering the locations of other service instances. Our initial approach was to manually configure a centralized pair of HAProxy load balancers in each of our datacenters, and embed the virtual IP address of these load balancers in configuration files. We quickly found that this approach did not scale. It was labor intensive and error prone both to deploy new services and also to move existing services between machines. We also started seeing performance issues due to the load balancers becoming overloaded. We addressed these issues by switching to a service discovery system built around SmartStack . Briefly, each client host now runs an HAProxy instance that is bound to localhost. The load balancer is dynamically configured from service registration information stored in ZooKeeper . A client can then contact a service by connecting to its localhost load balancer, which will then proxy the request to a healthy service instance. This facility has proved itself highly reliable, and the majority of our production services are now using it. Future directions We are currently working on a next-generation service platform called Paasta . This uses the Marathon framework (running on top of Apache Mesos ) to allocate containerized service instances across clusters of machines, and integrates with SmartStack for service discovery. This platform will allow us to treat our servers as a much more fungible resource, freeing us from the issues associated with statically assigning services to hosts. We will be publishing more details about this project later this year. Acknowledgements The work described in this blog post has been carried out and supported by numerous members of the Engineering Team here at Yelp. Particular credit goes to Prateek Agarwal, Ben Chess, Sam Eaton, Julian Krause, Reed Lipman, Joey Lynch, Daniel Nephin and Semir Patel. Tweet Back to blog", "date": "2015-03-11"}, {"website": "Yelp", "title": "Analyzing the Web For the Price of a Sandwich", "author": ["\n        \n  Ben C., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/03/analyzing-the-web-for-the-price-of-a-sandwich.html", "abstract": "I geek out about the Common Crawl . It’s an open source crawl of huge parts of the Internet, accessible for anyone to use. You have full access to the HTML and text of billions of web pages. What’s more, you can scan the entire thing, tens of terabytes, for just a few bucks on Amazon EC2. These days they’re releasing a new dataset every month. It’s awesome. People frequently use mrjob to scan the Common Crawl , so it seems like a fitting tool for us to use. mrjob , if you’re not familiar, is a Python framework written by Yelp to help run Hadoop jobs locally or on Amazon’s EMR service. Since the Common Crawl is stored in Amazon’s S3, it makes a lot of sense to use EMR to access it. The Problem I wanted to explore the Common Crawl in more depth, so I came up with a (somewhat contrived) use case of helping consumers find the web pages for local businesses. Yelp has millions of businesses in its index and we like to provide links back to a business’s own web page wherever possible, but there are plenty of cases where we just don’t have that information. Let’s try to use mrjob and the Common Crawl to help match businesses from Yelp’s database to the possible web pages for those businesses on the Internet. The Approach Right away I realized that this is a huge problem space. Sophisticated solutions would use NLP, fuzzy matching, cluster analysis, a whole slew of signals and methods. I wanted to come up with something simple, more of a proof-of-concept. If some basic approaches yielded decent results, then I could easily justify developing more sophisticated methods to explore this dataset. I started thinking about phone numbers. They’re easy to parse, identify, and don’t require any fuzzy matching. A phone number either matches, or it doesn’t. If I could match phone numbers from the Common Crawl with phone numbers of businesses in the Yelp database, it may lead to actually finding the web pages for those businesses. I started planning my MapReduce job: I start with a mapper that takes both Common Crawl WET pages (WET pages are just web page text) and Yelp business data. For each page from the Common Crawl, I parse the page looking for phone numbers and yield the URLs keyed off of each phone number that I find. For the Yelp business data, I yield the ID of the business, keyed off of the phone number that we have on record, which lets me start combining any businesses and URLs that match the same phone number in my reduce step. Spammy Data I ran this against a small set of input data to make sure it was working correctly but already knew that I was going to have a problem. There are some websites that just like to list every possible phone number and then there are sites like Yelp which have pages dedicated to individual businesses. A website that simply lists every possible phone number is definitely not going to be a legitimate business page. Likewise, a site like Yelp that has a lot of phone numbers on it is unlikely to be the true home page of any particular business. I need a way to filter these results. So let’s set up a step where we organize all the entries keyed by domain. That way if it looks like there are too many businesses hosted on that single domain, we can filter them out. Granted this isn’t perfect, it may exclude large national chains, but it’s a place to start. The Input The Common Crawl data files are in WARC format. The warc python package can help us parse these files, but WARC itself isn’t a line-by-line format that’d be suitable as direct input to our job. Instead, the input to our mapper will be the paths to WARC files stored in S3. Conveniently, the Common Crawl provides a file that is exactly that. According to the Common Crawl blog , we know that the WET paths for the December 2014 crawl live at: https://aws-publicdatasets.s3.amazonaws.com/common-crawl/crawl-data/CC-MAIN-2014-52/wet.paths.gz We’ll supply that path and a path to a similar file of our Yelp dataset to our mrjob: $ python mr_crawl_phonenumbers.py -r emr s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2014-52/wet.paths.gz s3://yelp/business_data.txt Here’s an excerpt from the full job that illustrates how we load the WARC files and map them into phone numbers and URLs: import warc\n\nclass MRCrawlPhoneNumbers(MRJob):\n    # ...\n    \n    def map_to_phone_number(self, _, s3_path):\n        s3_url_parsed = urlparse.urlparse(s3_url)\n        bucket_name = s3_url_parsed.netloc\n        key_path = s3_url_parsed.path[1:]\n\n        conn = boto.connect_s3()\n\n        if s3_path.startswith('common-crawl'):\n            # Assume it's a commoncrawl object\n            for phone_number, value in self.map_commoncrawl(s3_path):\n                yield phone_number, value\n        else:\n            # Parse the Yelp business data\n            for phone_number, value in self.map_biz_phone_csv(s3_path):\n                yield phone_number, value\n\n    def map_commoncrawl(self, s3_path):\n        conn = boto.connect_s3()\n        bucket = conn.get_bucket('aws-publicdatasets', validate=False)\n        key = Key(bucket, s3_path)\n\n        warc_file = warc.WARCFile(fileobj=GzipStreamFile(key))\n        for record in warc_file:\n            for key, value in self.process_warc_record(record):\n                yield key, value\n\n    def process_warc_record(self, record):\n        if record['content-type'] != 'text/plain':\n            return\n\n        webpage_text = record.payload.read()\n        for phone_number in us_phone_regex.findall(webpage_text):\n            yield standardize_phone_number(phone_number), {'url': record.header['warc-target-uri']} Performance Problem: Distribute Our Input To More Mappers The default Hadoop configuration assumes that there are many lines of input and each individual line is relatively fast to process. It designates many lines to a single map task. In this case, given that our input is a list of WET paths, there’s a relatively small amount of line input, but each line is relatively expensive to process. Each WET file is about 150MB so each line actually represents a whole lot more input than just that single line. For best performance, we actually want to dole out each input to a separate mapper. Fortunately this is pretty easy by simply specifying the input format to NLineInputFormat : class MRCrawlPhoneNumbers(MRJob):\n   HADOOP_INPUT_FORMAT = 'org.apache.hadoop.mapred.lib.NLineInputFormat'\n   # ... Note that NLineInputFormat will reformat our input a bit. It will change our input lines S3 paths into '\\t' so we need to treat our input as tab-delimited key, value pairs and simply ignore the key. MRJob’s RawProtocol can do this: class MRCrawlPhoneNumbers(MRJob):\n    HADOOP_INPUT_FORMAT = 'org.apache.hadoop.mapred.lib.NLineInputFormat'\n    INPUT_PROTOCOL = RawProtocol\n\n    def mapper(self, _, s3_path):\n        # _ is the byte offset, ignore this and just use the s3_path value\n        ... Identifying Phone Numbers This part isn’t too complicated. To find possible phone numbers on a given page, I use a simple regex. (I also tried the excellent phonenumbers Python package, which is far more robust but also much slower, it wasn’t worth it.) It can be expanded to work on international phone numbers, but for now we’re only looking at US-based businesses. PHONE_SEP = r\"[\\-. ()+]\"\nUS_PHONE = {\n    \"prefix\": r\"[\\D\\b](1?\",\n    \"first_digit\": \"[2-9]\",\n    \"digits\": [2,3,4],\n    \"suffix\": r\")[\\D\\b]\"\n}\n\ndef make_phone_format(phone_format = US_PHONE):\n    format_list = [phone_format[\"prefix\"]]\n    format_list += [phone_format[\"first_digit\"]]\n    for digit in phone_format[\"digits\"][:-1]:\n        format_list += [r\"\\d{\", str(digit), \"}\", PHONE_SEP, r\"+\"]\n    format_list += [r\"\\d{\", str(phone_format[\"digits\"][-1]), \"}\"]\n    format_list += [phone_format[\"suffix\"]]\n    return r\"\".join(format_list)\n\nus_phone_regex = re.compile(make_phone_format()) Running the Job (On the Cheap!) The WET files for the December 2014 Common Crawl are about 4TB compressed. That’s a fair bit of data, but we can process it pretty quickly with a few high-powered machines. Given that AWS charges you by the instance-hour, it’s most cost effective to just use as many instances required to process your job in a little less than 60 minutes. I found that I could process this job in about an hour with 20 c3.8xlarge instances. The normal rate for a c3.8xlarge is currently $1.68/hour. Plus $.270/hr for EMR. Thus under standard pricing, our job would cost ($1.68 + $0.27) * 20 = $39. But with spot pricing, we can do much better! Spot pricing lets you get EC2 instances at much lower rates when there is unused capacity. The current spot price for a c3.8xlarge in us-east-1 is around $0.26. We still have to pay the additional EMR charge, but this gets us to a much more reasonable ($0.26 + $0.27) * 20 = $10.60. Results The mrjob found approximately 748 million US phone numbers in the Common Crawl December 2014 dataset. Of the ones we were able to match against businesses in the Yelp database, 48%, already had URLs associated with them. If we assume the Yelp businesses that have URLs are correct URLs, we can get a rough estimate of accuracy by comparing the URLs in the Yelp database against the URLs that our MRJob identified. So of the businesses that matched and already had URLs, 48% of them were the same URLs that we already had. 61% had matching domains. If we wanted to use this for real, we’d probably want to combine this with other signals to get higher accuracy. But this isn’t a bad first step. Conclusion The Common Crawl is a great public resource. You can scan over huge portions of the web with some simple tools and a price of a sandwich. MRJob makes this super easy with Python. Give it a try! # -*- coding: utf-8 -*-\nimport itertools\nimport re\nimport urlparse\n \nimport boto\nimport warc\n \nfrom boto.s3.key import Key\nfrom gzipstream import GzipStreamFile\nfrom mrjob.job import MRJob\nfrom mrjob.protocol import RawProtocol\n \n \nPHONE_SEP = r\"[\\-. ()+]\"\nUS_PHONE = {\n    \"prefix\": r\"[\\D\\b](1?\",\n    \"first_digit\": \"[2-9]\",\n    \"digits\": [2, 3, 4],\n    \"suffix\": r\")[\\D\\b]\"\n}\n \n \ndef standardize_phone_number(number):\n    number_sep = sep_re.split(number)\n    number = \"\".join(number_sep)\n    if len(number) > 7:\n        if number[-1] not in '0123456789':\n            number = number[:-1]\n        if number[0] not in '0123456789':\n            number = number[1:]\n    if len(number) <= 10:\n        return \"+1\" + number\n    else:\n        return \"+\" + number\n \n \ndef make_phone_format(phone_format=US_PHONE):\n    format_list = [phone_format[\"prefix\"]]\n    format_list += [phone_format[\"first_digit\"]]\n    for digit in phone_format[\"digits\"][:-1]:\n        format_list += [r\"\\d{\", str(digit), \"}\", PHONE_SEP, r\"+\"]\n    format_list += [r\"\\d{\", str(phone_format[\"digits\"][-1]), \"}\"]\n    format_list += [phone_format[\"suffix\"]]\n    return r\"\".join(format_list)\n \n \nus_phone_regex = re.compile(make_phone_format())\nsep_re = re.compile(PHONE_SEP)\n \n \nclass MRExtractPhoneNumbers(MRJob):\n    HADOOP_INPUT_FORMAT = 'org.apache.hadoop.mapred.lib.NLineInputFormat'\n    INPUT_PROTOCOL = RawProtocol\n \n    def map_commoncrawl(self, s3_object):\n        f = warc.WARCFile(fileobj=GzipStreamFile(s3_object))\n        for record in f:\n            for key, value in self.process_record(record):\n                yield key, value\n \n    def process_record(self, record):\n        if record['content-type'] != 'text/plain':\n            return\n \n        payload = record.payload.read()\n        for phone_number in us_phone_regex.findall(payload):\n            yield standardize_phone_number(phone_number), {'url': record.header['warc-target-uri']}\n \n    def fetch_yelp_business_data(self, s3_object):\n        # Fake example data, for illustration\n        yield '+14159083801', {'biz_id': 1000}\n        yield '+14155550100', {'biz_id': 1001}\n        yield '+14155550101', {'biz_id': 1002}\n \n    def map_to_phone_number(self, _, s3_url):\n        s3_url_parsed = urlparse.urlparse(s3_url)\n        bucket_name = s3_url_parsed.netloc\n        key_path = s3_url_parsed.path[1:]\n \n        conn = boto.connect_s3()\n        bucket = conn.get_bucket(bucket_name, validate=False)\n        key = Key(bucket, key_path)\n \n        if key_path.startswith('common-crawl'):\n            # Assume it's a commoncrawl object\n            for phone_number, data in self.map_commoncrawl(key):\n                yield phone_number, data\n        else:\n            for phone_number, data in self.fetch_yelp_business_data(key):\n                yield phone_number, data\n \n    def reduce_by_phone_number(self, phone_number, values):\n        biz_ids = []\n        urls = []\n        for each_value in values:\n            if 'biz_id' in each_value:\n                biz_ids.append(each_value['biz_id'])\n            if 'url' in each_value:\n                urls.append(each_value['url'])\n \n            if len(biz_ids) > 1000 or len(urls) > 1000:\n                break\n \n        if not urls or not biz_ids:\n            return\n \n        # multiple businesses share same phone number, treat them as one id\n        biz_id = ','.join(sorted(biz_ids))\n        for each_url in urls:\n            yield biz_id, each_url\n \n    def map_to_domain(self, biz_id, url):\n        domain = urlparse.urlparse(url).netloc.split(':')[0]\n        if domain:\n            yield domain, (url, biz_id)\n \n    def filter_large_domains(self, domain, url_biz_id_pairs):\n        # Filter out any domain with > 10 businesses\n        first_pairs = list(itertools.islice(url_biz_id_pairs, 11))\n        if len(first_pairs) > 10:\n            yield 'filtered', domain\n            return\n \n        for url, biz_ids in first_pairs:\n            yield biz_ids, url\n \n    def steps(self):\n        return [self.mr(mapper=self.map_to_phone_number, reducer=self.reduce_by_phone_number),\n                self.mr(mapper=self.map_to_domain, reducer=self.filter_large_domains)]\n \n \nif __name__ == '__main__':\n    MRExtractPhoneNumbers.run() Tweet Back to blog", "date": "2015-03-20"}, {"website": "Yelp", "title": "Yelp Hackathon 16: What the hack is a Kühlkdebeulefotoapparat", "author": ["\n        \n  Srivatsan S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/03/yelp-hackathon-16-what-the-hack-is-a-kuhlkdebeulefotoapparat.html", "abstract": "A couple of weeks ago, the Yelp Product & Engineering teams put their creative minds together to work on our coolest, funniest and hardcore-est of ideas at the 16th edition of our internal Hackathon . As always, the food was plentiful- our kitchens were stacked with delicious catered food, fresh fruits and snacks, gourmet coffee, and, wait for it, ice-cream sandwiches! While all that deliciousness fueled our bodies, our minds were fueled by do-it-yourself Metal Earth 3D model kits and metal-etching workshops. If that wasn’t enough, we even got a chance to race these amazing Anki cars . Our VP of Engineering kicking off the proceedings with a transatlantic mimosa toast Over 60 fantastic projects came out of our engineering offices in San Francisco, Hamburg and London - ideas that ranged from visualizations of our rich dataset to hardcore machine learning applications to nifty internal tools designed to skyrocket developer productivity and of course, the incredible out-of-the-box projects. The unwavering constant of Yelp Hackathons - a medley of work, play and food Speaking of out-of-the-box projects, the dynamic duo of Yoni D. and Matthew K. decided to reinvigorate a certain 19th century invention. What started off as an idea to self-develop photographic film quickly evolved into a project to create their very own camera using the swanky 3D printer that we have here in our San Francisco office. Using the design for an existing pinhole camera , they 3D printed parts of the camera, painted and assembled them to create a product as hardcore as its name - the Kühlkdebeulefotoapparat. Don’t know what means? Rumor has it that it has something to do with the duo’s last names. And it takes pretty good photographs too. Another fancy piece of gadgetry that came out of our top-secret 3D printing lab While some folks were busy fashioning things out of thin air, others joined forces to solve an interesting problem that we face here at Yelp. We build a ton of amazing features every single day that leave us with a lot of data with varying tools to visualize them. We’ve gotten really comfortable using Kibana to expose our data in Elasticsearch , but could we somehow use the same UI that we know and love to visualize our great data on Redshift ? A team comprised of Garrett J., Jimming C. and Tobi O. decided to answer exactly that. Poking into the Kibana code (yay open source!), they realized that while the project was still under active development, it had a well-defined interface to query ElasticSearch. They built a translation server that modeled a similar interface for Redshift that Kibana could understand. The result - the ability to create new dashboards and visualizations that will help us gain more insight into our data! Our hackers showing off their projects at a “science-fair” style exhibition While some folks spent their time coming up with elegant solutions to hard problems, a team comprised of Duncan C. and Matthew M. did something, um, quite the opposite. In the true spirit of being “unboring,” they created “DumbStack” - an insanely complicated web of machinery comprised of Webfaction , Slack , AppEngine , SQS , EC2 , Heroku , Tumblr , Netcat , Github and Google Translate to solve a simple problem - posting a tweet. Definitely the easiest way to post to Twitter, right? This project certainly would’ve made Rube Goldberg proud! Have some creative ideas brewing in your head? Why don’t you come join forces as we embark on our next awesome Hackathon this summer. We are constantly on the lookout for amazing engineers, product managers and data scientists to bolster our team. Check out our exciting product and engineering job openings at www.yelp.com/careers and apply today. Hack on! Tweet Back to blog", "date": "2015-03-27"}, {"website": "Yelp", "title": "CocoaPods (or How to Stop Worrying About Dependency Management)", "author": ["\n        \n  Mason G., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/04/cocoapods-or-how-to-stop-worrying-about-dependency-management.html", "abstract": "Yelp has had an iOS app for as long as third-party iOS apps have existed. Maintaining a codebase with that much history is always interesting and sometimes challenging, and one of the biggest challenges is dependency management. For a long time, git submodules met most of our needs and caused relatively few headaches. However, the submodule approach made it difficult to understand what unanticipated or even breaking changes will be introduced when bumping a submodule by a commit – or several. A Git SHA has no concept of versioning. Additionally, adding a new library often required changes to the build settings of the Yelp app. Sometimes new header search paths were required, other times special build flags had to be added. As the project became larger and more complicated, the process of adding a new dependency became more and more difficult. It was not uncommon for the integration of a new library to require around a day of developer time. There had to be a better way. Enter CocoaPods : a dependency manager for Objective-C projects.Ruby has gem+bundler, and Node.js has npm. Each of these tools operate in a similar way. Your application specifies the libraries and which versions to use. The concept of a dependency manager is nothing new. Python has pip and easy_install, depends on, and the dependency manager recursively resolves these dependencies until a final list is compiled. The manager then downloads the required libraries and makes them available to your project. Since Objective-C is a compiled language, CocoaPods provides the additional convenience of configuring header search paths, setting build flags, etc., on a per-library basis. These options are specified by the author of each library as part of its podspec , neatly shifting the cognitive load off of you, the developer. Libraries in CocoaPods – called pods – are versioned according to the Semantic Versioning standard . This scheme makes it infinitely easier to understand when changing a library version is safe and when it will introduce backwards-incompatible changes. CocoaPods even has built-in support for all of our internal libraries – converting our existing submodules was a breeze. We first created a new Git repository, YelpSpecs, as an internal counterpart to the main CocoaPods Specs repository . Then, after writing a podspec for each internal library and pushing those specs to YelpSpecs, our internal libraries could be referenced alongside the external libraries in our Podfile. At last count, we had eleven internal libraries referenced in YelpSpecs. Extracting our code into pods enabled us to do a lot of cleanup. Submodules no longer needed an xcodeproj file, as CocoaPods handled all of the configuration for us. In the end, we were able to delete thousands of lines from configuration files. Having an easy dependency management system has encouraged us to better organize our code. Before CocoaPods, we only had 1 internal library which covered everything from networking and caching to utility views. Making a new library was so painful that we opted to throw everything together. CocoaPods has completely changed that. We’ve begun to dismantle the mega-library, and have created several smaller and more focused libraries. This has made sharing code with our new Business Owner App incredibly easy. Our current set of libraries would have been nearly impossible to manage before CocoaPods. For example, our small utility library, YLUtils, is included in every single one of our internal libraries. Getting Xcode to link this properly would have been a nightmare, but CocoaPods has made it a breeze. We’ve been using CocoaPods for nearly a year, and the change has allowed for some great improvements in how we work and develop. However, the transition wasn’t always easy – we had been using submodules for so long that we weren’t used to the CocoaPods workflow, where you have to checkout a local version of the pod before making changes. If you don’t checkout a local version first, changes won’t be picked up by git and might be lost the next time you run ‘pod install.’ In order to help our developers, we wrote a small script that goes in the podfile. When running pod install, it removes the write permission from all pod-related files that aren’t installed via a local pod. Xcode will then give you a helpful message, noting that the file is locked. This small change really helped with the transition, preventing future frustration. def set_permissions(installer, permissions)\n  for pod in installer.pods\n    # If the pod is actually in Pods/, and not a :path linked pod\n    if pod.root.parent == installer.sandbox_root\n      for file in pod.source_files\n        File.chmod(permissions, file)\n      end\n    end\n  end\nend\n \npre_install do |installer|\n  # Unlock all of the files before trying to install them\n  set_permissions(installer, 0644)\nend\n \npost_install do |installer|\n  # And lock them back up after the installer is finished\n  set_permissions(installer, 0444)\nend After we had happily been using CocoaPods for several months, a new version was released. However, this new release was not backwards-compatible – if developers installed the new release on their system, they would be unable to build older versions of the app or branches that hadn’t recently been updated with master. In order to solve this problem, we realized that we would need a dependency manager to manage our dependency manager. Since CocoaPods is itself a Ruby gem, the obvious choice was bundler . We added a shim that installs the correct version of CocoaPods locally via bundler. With this setup, developers always have access to the correct version of CocoaPods regardless of what branch or version they’re working on. If you have a large team or frequently switch between newer and older branches, we strongly recommend a configuration like this. We’ve set up a sample project to demonstrate our system – check out the readme for more details. The work we’ve put into tuning our CocoaPods setup has already paid off. It’s made it extremely simple to create and try new libraries, which has in turn helped us make our code base more modular. Switching to CocoaPods has allowed us to spend less time fighting configurations, and more time developing great new features. Update: Samuel Giddins tipped us off about CocoaPods plugins, so you can now lock your Pods without making any changes to your Podfile! Just install the cocoapods-readonly gem . Tweet Back to blog", "date": "2015-04-01"}, {"website": "Yelp", "title": "Data Science Contest “Keeping it Fresh”: Predict Restaurant Health Scores", "author": ["\n        \n  Soups R., Ph.D., Software Engineer (Data Mining)\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/04/data-science-contest-keeping-it-fresh-predict-restaurant-health-scores.html", "abstract": "Yelp connects people with local businesses and along the way we’ve gathered rich data about customers’ experiences at those businesses via reviews, tips, check-ins and business attributes. We are constantly asking ourselves how the collective wisdom of Yelpers can be used to better inform cities in their efforts around protecting the health of our communities. In particular, could we use Yelp’s reviews and business information to make the process of sending Health Inspectors to restaurants more efficient? According to the Centers for Disease Control, more than 48 million Americans per year become sick from food , and an estimated 75% of the outbreaks came from food prepared by caterers, delis, and restaurants . Currently, inspectors are sent to restaurants in a mostly random fashion. Since cities only have a limited number of health inspectors, quite often their time is wasted on spot checks at clean, rule-abiding restaurants. This also means that sometimes restaurants with poor health and safety records are discovered too late. It turns out that with Yelp’s data, cities can improve the process of assigning Health Inspectors drastically. A research study by Prof. Michael Luca from Harvard Business School and Prof. Yejin Choi from Stony Brook University and their graduate students found that a machine learnt model built using Yelp’s reviews data and past health inspection records is able to successfully predict future inspection scores for restaurants 82% of the time. Can we do better? We want to challenge data scientists worldwide to design a better health inspection prediction algorithm. Yelp is co-sponsoring a new Data Science contest “Keeping it Fresh” in collaboration with the City of Boston, DrivenData.org and Harvard University economists ( Ed , Andrew , Scott , and Mike ). Using Yelp’s data for restaurants, food and nightlife businesses in Boston as well as past history of health inspections, we are asking contestants to predict the future health score that will be assigned to a business at their next health inspection. Winning algorithms will be awarded financial prizes — but the real prize is the opportunity to help the City of Boston, which is committed to examining ways to integrate the winning algorithm into its day-to-day inspection operations. The goal for this competition is to use data from social media to narrow the search for health code violations in Boston. Competitors will have access to historical hygiene violation records from the City of Boston — a leader in open government data — and Yelp’s consumer reviews. The challenge: Figure out the words, phrases, ratings, and patterns that predict violations, to help public health inspectors do their job better. The first-place winner will receive $3,000, and two runners-up will receive $1,000 each. All prizes are provided by Yelp. Figure 1: Health inspection history for a popular San Francisco restaurant. This restaurant's health score is predicted in Figure 2 below. Yelp engineers have been fascinated by this problem. In a recent internal hackathon , a team of Yelp engineers comprising of Wing Y., Srivathsan R., Florian H., Jon C. and Srivatsan S., decided to dive deep into our rich user-generated content to find out correlations between reviews and actual health scores for businesses. They modelled Yelp reviews as a bag-of-words and used machine learning techniques like logistic regression to predict health scores. They then went on to overlay their algorithmically predicted scores with the actual city-issued health scores for those businesses over a period of time. Consider this example of a popular San Francisco restaurant: Figure 2: Health score of a popular San Francisco restaurant in black and the predicted health score in green. It turns out that the actual health scores issued by the city of San Francisco (in black) follow very closely with the algorithmically predicted health scores (in green). That doesn’t come as a surprise because Yelpers are quite often talking about the same things that health inspectors look for - cleanliness, ambience, methods of preparation. This restaurant also exemplifies our observation from a randomized, controlled trial in one large city that restaurants whose low hygiene ratings are posted on Yelp tend to respond by cleaning up and performing better on their next inspection. In the end such public/private partnerships with cities will enable us to enrich the experiences of all parties: consumers who avoid getting sick, businesses who are able to use actionable information to improve their health standards and cities who are able to optimize their finite resources of Health Inspectors to match them with restaurants more efficiently. The competition opened yesterday (Monday, April 27th) and will accept submissions for eight weeks. Submissions will be evaluated on fresh hygiene inspection results during the six weeks following the competition; after that, the prizes will be awarded. Your submission will not only put you in the running for the prize – it has the chance to transform how city governments ensure public health. We are excited to see what machine learning algorithms our contestants will build. So wait no more. Go sign up for the “Keeping it Fresh” Contest here . Acknowledgements Thanks to our partners from: City of Boston, Harvard Business School and DrivenData for making this contest a reality. Special thanks to Luther L. for leading the charge and bringing together all the parties involved. Thanks to Srivatsan S. for helping author this blog post. Tweet Back to blog", "date": "2015-04-28"}, {"website": "Yelp", "title": "True Zero Downtime HAProxy Reloads", "author": ["\n        \n  Joseph Lynch, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/04/true-zero-downtime-haproxy-reloads.html", "abstract": "We have since migrated to a more robust solution that uses NGINX and HAProxy\ntogether to achieve our goals. See that solution here HAProxy: Cornerstone of Reliable Websites One primary goal of the infrastructure teams here at Yelp is to get as close to zero downtime as possible. This means that when users make requests for www.yelp.com we want to ensure that they get a response, and that they get a response as fast as possible. One way we do that at Yelp is by using the excellent HAProxy load balancer. We use it everywhere: for our external load balancing, internal load balance, and with our move to a Service Oriented Architecture , we find ourselves running HAProxy on every machine at Yelp as part of SmartStack . We love the flexibility that SmartStack gives us in developing our SOA, but that flexibility comes at a cost. When services or service backends are added or permanently removed, HAProxy has to reload across our entire infrastructure. These reloads can cause reliability problems because while HAProxy is top notch at not dropping traffic while it is running, it can (and does) drop traffic during reloads. HAProxy Reloads Drop Traffic As of version 1.5.11, HAProxy does not support zero downtime restarts or reloads of configuration. Instead, it supports fast reloads where a new HAProxy instance starts up, attempts to use SO_REUSEPORT to bind to the same ports that the old HAProxy is listening to and sends a signal to the old HAProxy instance to shut down. This technique is very close to zero downtime on modern Linux kernels, but there is a brief period of time during which both processes are bound to the port. During this critical time, it is possible for traffic to get dropped due to the way that the Linux kernel (mis)handles multiple accepting processes. In particular, the issue lies with the potential for new connections to result in a RST from HAProxy. The issue is that SYN packets can get put into the old HAProxy’s socket queue right before it calls close , which results in a RST of those connections. There are various workarounds to this issue. For example, Willy Tarreau, the primary maintainer of HAProxy, has suggested that users can drop SYN packets for the duration of the HAProxy restart so that TCP automatically recovers. Unfortunately, RFC 6298 dictates that the initial SYN timeout should be 1s, and the Linux kernel faithfully hardcodes this . As such, dropping SYNs mean that any connections that attempt to establish during the 20-50ms of an HAProxy reload will encounter an extra second of latency or more. The exact latency depends on the TCP implementation of the client, and while some mobile devices retry as fast as 200ms, many devices only retry after 3s. Given the number of HAProxy reloads and the level of traffic Yelp has, this becomes a barrier to the reliability of our services. Making HAProxy Reloads Not Drop Traffic To avoid this latency, we built on the solution proposed by Willy. His solution actually works very well at not dropping traffic, but the extra second of latency is a problem. A better solution for us would be to delay the SYN packets until the reload was done, as that would only impose the latency of the HAProxy reload on new connections. To do this, we turned to Linux queueing disciplines (qdiscs). Queueing disciplines manipulate how network packets are handled within the Linux kernel. Specifically you can control how packets are enqueued and dequeued, which provides the ability to rate limit, prioritize, or otherwise order outgoing network traffic. For more information on qdiscs, I highly recommend reading the lartc howto as well as the relevant man pages. After some light bedtime reading of the Linux kernel source code, one of our SREs, Josh Snyder, discovered a relatively undocumented qdisc that has been available since Linux 3.4: the plug queueing discipline. Using the plug qdisc, we were able to implement zero downtime HAProxy reloads with the following standard Linux technologies: - tc : Linux traffic control. This allows us to set up queueing disciplines that route traffic based on filters. On newer Linux distributions there is also libnl-utils which provide interfaces to some of the newer qdiscs (such as the plug qdisc). - iptables : Linux tool for packet filtering and NAT. This allows us to mark incoming SYN packets. Smartstack clients connect to the loopback interface to make a request to HAProxy, which fortunately turns incoming traffic into outgoing traffic. This means that we can set up a queuing discipline on the loopback interface that looks something like Figure 1. Figure 1: Queueing Discipline This sets up a classful implementation of the standard pfifo_fast queueing discipline using the prio qdisc, but with a fourth “plug” lane . A plug qdisc has the capability to queue packets without dequeuing them, and then on command flush those packets. This capability in combination with an iptables rule allows us to redirect SYN packets to the plug during a reload of HAProxy and then unplug after the reload. The handles (e.g ‘1:1’, ‘30:’) are labels that allow us to connect qdiscs together and send packets to particular qdiscs using filters; for more information consult the lartc howto referenced above. We then programmed this functionality into a script we call qdisc_tool . This tool allows our infrastructure to “protect” a HAProxy reload where we plug traffic, restart haproxy, and then release the plug, delivering all the delayed SYN packets. This invocation looks something like: qdisc_tool protect <normal HAProxy reload command> We can easily reproduce this technique with standard userspace utilities on modern Linux distributions such as Ubuntu Trusty. If your setup does not have nl-qdisc-add but does have a 3.4+ Linux kernel, you can manipulate the plug via netlink manually. Set up the Queuing Disciplines Before we can do graceful HAProxy reloads, we must first set up the queueing discipline described above using tc and nl-qdisc-add. Note that every command must be run as root. # Set up the queuing discipline\ntc qdisc add dev lo root handle 1: prio bands 4\ntc qdisc add dev lo parent 1:1 handle 10: pfifo limit 1000\ntc qdisc add dev lo parent 1:2 handle 20: pfifo limit 1000\ntc qdisc add dev lo parent 1:3 handle 30: pfifo limit 1000\n\n# Create a plug qdisc with 1 meg of buffer\nnl-qdisc-add --dev=lo --parent=1:4 --id=40: plug --limit 1048576\n# Release the plug\nnl-qdisc-add --dev=lo --parent=1:4 --id=40: --update plug --release-indefinite\n\n# Set up the filter, any packet marked with \"1\" will be\n# directed to the plug\ntc filter add dev lo protocol ip parent 1:0 prio 1 handle 1 fw classid 1:4 Mark SYN Packets We want all SYN packets to be routed to the plug lane, which we can accomplish with iptables . We use a link local address so that we redirect only the traffic we want to during the reload, and clients always have the option of making a request to 127.0.0.1 if they wish to avoid the plug. Note that this assumes you have set up a link local connection at 169.254.255.254 . iptables -t mangle -I OUTPUT -p tcp -s 169.254.255.254 --syn -j MARK --set-mark 1 Toggle the Plug While Reloading Once everything is set up, all we need to do to gracefully reload HAProxy is to buffer SYNs before the reload, do the reload, and then release all SYNs after the reload. This will cause any connections that attempt to establish during the restart to observe latency equal to the amount of time it takes HAProxy to restart. nl-qdisc-add --dev=lo --parent=1:4 --id=40: --update plug --buffer\nservice haproxy reload\nnl-qdisc-add --dev=lo --parent=1:4 --id=40: --update plug --release-indefinite In production we observe that this technique adds about 20ms of latency to incoming connections during the restart, but drops no requests. Design Tradeoffs This design has some benefits and some drawbacks. The largest drawback is that this works only for outgoing links and not for incoming traffic. This is because of the way that queueing disciplines work in Linux, namely that you can only shape outgoing traffic. For incoming traffic, one must redirect to an intermediary interface and then shape the outgoing traffic from that intermediary. We are working on integrating a solution similar to this for our external load balancers, but it is not yet in production. Furthermore, the qdiscs could also probably be tuned more efficiently. For example, we could insert the plug qdisc at the first prio lane and adjust the priomap accordingly to ensure that SYNs always get processed before other packets or we could tune buffer sizes on the pfifo/plug qdiscs. I believe that for this to work with an interface that is not loopback, the plug lane would have to be moved to the first lane to ensure SYN deliverability. The reason that we decided to go with this solution over something like huptime , hacking file descriptor passing into HAProxy, or dancing between multiple local instances of HAProxy is because we deemed our qdisc solution the lowest risk. Huptime was ruled out quickly as we were unable to get it to function on our machines due to an old libc version, and we were uncertain if the LD_PRELOAD mechanism would even work for something as complicated as HAProxy. One engineer did implement a proof of concept file descriptor patch during a hackathon but the complexity of the patch and the potential for a large fork caused us to abandon that approach; it turns out that doing file descriptor passing properly is really hard . Of the three options, we most seriously considered running multiple HAProxy instances on the same machine and using either NAT, nginx, or another HAProxy instance to switch traffic between them. Ultimately we decided against it because of the number of unknowns in implementation, and the level of maintenance that would be required for the infrastructure. With our solution, we maintain basically zero infrastructure and trust the Linux kernel and HAProxy to handle the heavy lifting. This trust appears to be well placed as in the months this has been running in production we have observed no issues. Experimental Setup To demonstrate that this solution really works, we can fire up an nginx HTTP backend with HAProxy sitting in front, generate some traffic with Apache Benchmark, and see what happens when we restart HAProxy. We can then evaluate a few different solutions this way. All tests were carried out on a freshly provisioned c3.large AWS machine running Ubuntu Trusty and a 3.13 Linux kernel. HAProxy 1.5.11 was compiled locally with TARGET=linux2628. Nginx was started locally with the default configuration except that it listens on port 8001 and serves a simple “pong” reply instead of the default html. Our compiled HAProxy was started locally with a basic configuration that had a single backend at port 8001 and a corresponding frontend at port 16000. Just Reload HAProxy In this experiment, we only restart HAProxy with the ‘-sf’ option, which initiates the fast reload process. This is a pretty unrealistic test because we are restarting HAProxy every 100ms, but it illustrates the point. Experiment # Restart haproxy every 100ms\nwhile [ 1 ]; do\n  ./haproxy -f /tmp/haproxy.cfg -p /tmp/haproxy.pid -sf $(cat /tmp/haproxy.pid)\n  sleep 0.1\ndone Results $ ab -c 10 -n 10000 169.254.255.254:16000/\nBenchmarking 169.254.255.254 (be patient)\n...\napr_socket_recv: Connection reset by peer (104)\nTotal of 3652 requests completed $ ab -r -c 10 -n 200000 169.254.255.254:16000/  \nBenchmarking 169.254.255.254 (be patient)\n...\nComplete requests:      200000\nFailed requests:        504\n...\n  50%      2\n  95%      2\n  99%      3\n 100%     15 (longest request) Only 0.25% of requests failed. This is not too bad, but well above our goal of zero. Drop SYNs and Let TCP Do the Rest Now we try the method where we drop SYNs. This method seems to completely break with high restart rate as you end up with exponentially backing off connections, so to get reliable results I could only restart HAProxy every second. Experiment # Restart haproxy every second\nwhile [ 1 ]; do \n  sudo iptables -I INPUT -p tcp --dport 16000 --syn -j DROP\n  sleep 0.2\n  ./haproxy -f /tmp/haproxy.cfg -p /tmp/haproxy.pid -sf $(cat /tmp/haproxy.pid) \n  sudo iptables -D INPUT -p tcp --dport 16000 --syn -j DROP\n  sleep 1\ndone Results $ ab -c 10 -n 200000 169.254.255.254:16000/  \nBenchmarking 169.254.255.254 (be patient)\n...\nComplete requests:      200000\nFailed requests:        0\n...\n  50%      2\n  95%      2\n  99%      6\n 100%   1002 (longest request) Figure 2: Iptables Experiment Results As expected, we drop no requests but incur an additional one second of latency. When request timings are plotted in Figure 2 we see a clear bimodal distribution where any requests that hit the restart take a full second to complete. Less than one percent of the test requests observe the high latency, but that is still enough to be a problem. Use Our Graceful Restart Method In this experiment, we restart HAProxy with the ‘-sf’ option and use our queueing strategy to delay incoming SYNs. Just to be sure we are not getting lucky, we do one million requests. In the process of this test we restarted HAProxy over 1500 times. Experiment 400: Invalid request Results 400: Invalid request Figure 3: TC Experiment Results Success! Restarting HAProxy has basically no effect on our traffic, causing only minor delays as can be seen in Figure 3. Note that this method is heavily dependent on how long HAProxy takes to load its configuration, and because we are running such a reduced configuration, these results are deceivingly fast. In our production environment we do observe about a 20ms penalty during HAProxy restarts. Conclusion This technique appears to work quite well to achieve our goal of providing a rock-solid service infrastructure for our developers to build on. By delaying SYN packets coming into our HAProxy load balancers that run on each machine, we are able to minimally impact traffic during HAProxy reloads, which allows us to add, remove, and change service backends within our SOA without fear of significantly impacting user traffic. Acknowledgements Thanks to Josh Snyder, John Billings and Evan Krall for excellent design and implementation discussions. Tweet Back to blog", "date": "2015-04-13"}, {"website": "Yelp", "title": "MySQL @ Yelp", "author": ["\n        \n  Jenni S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/01/mysql-at-yelp.html", "abstract": "This past November, Yelp Engineering hosted a Bay Area Girl Geek Dinner and I had the pleasure of presenting a brief overview of how Yelp uses MySQL . I received a lot of great questions and wanted to take a few minutes to share the information with our wider blog audience. Overview: MySQL @ Yelp Yelp has used MySQL since the beginning, and in more recent years has adopted the Percona fork . We like MySQL because it fits our data needs, is configurable, fast, and scalable. The Percona fork includes performance enhancements and additional features on top of stock MySQL, we have used their training services in the past, and are currently a support customer. MySQL also has a *huge* online community. I love that a quick search using your favorite search engine will quickly get you started down the right path to solving most problems! A Little More Detail We’re running roughly 100 database servers between our dev, staging, and production environments. Our database is split up into several functional shards. Different database servers house different data sets, with different tables and schemas, but all of the data for a single table resides within one shard. The InnoDB storage engine provides us with the ability to handle a fair number of concurrent transactions at a time on masters, and we rely upon it for our transactional consistency. To scale database reads, we use MySQL statement-based replication to propagate data from master DBs to local and remote replicas, which reside in 2+ read-only data centers/locations. Our replication hierarchy uses an intermediate master database in each datacenter to reduce replication bandwidth and to facility easy master promotions: MySQL is also extremely portable, running on almost anything, and has a massive number of variables to tune your performance if you’re inclined. We’re a Python shop, so we use the MySQLdb connector , and SQLAlchemy . Tools we use with MySQL At Yelp, we have a saying: “it’s not in production until it’s monitored.” We monitor MySQL with Nagios and Sensu, and graph metrics using Ganglia, Graphite, and Grafana. We also use the Percona Toolkit ’s command-line utilities to analyze database query traffic, log deadlocks, collect data during times of database stress, verify the consistency of our data between replicas and MySQL versions, online schema change, and to kill poorly performing queries. Yelp DBAs In order to keep Yelp fresh and deliver new features, we push new versions of our application multiple times a day. This means we also have to support frequent schema changes too! There are two rules for our schema changes to keep things moving smoothly: They must be backwards-compatible with the previous version of the application They must either be online operations, or we must be able to run them using pt-online-schema-change What’s so important about backwards-compatible schema changes? While I love dropping columns and tables just as much as (if not more!) than the next person, part of pushing often is also being able to safely roll back. If we need to do something like drop a column, we will first deploy code that no longer uses it, take the extra step of renaming the column first, verify that things look good, and then drop the column, each over its own push. We also want to keep in mind that neither database changes nor rolling out a new version of our application happens in a single instant. We always make our database/schema changes before the code deployment part of a push, and there can be a period of time during deployment when two versions of our application are running. Thus we make our database changes before the code is deployed, and we use pt-online-schema-change to prevent replication delay as the schema change is made. pt-online-schema-change alters a table online by: Creating an empty copy of the table to alter Running the alter statement against the new, empty copy Copying the rows from the original table into the new table Replacing the original table with the new one Yelp DBAs This wasn’t part of my original presentation but there was an interesting question asked after my talk: do we use an ORM ( Object Relational Mapping ) or do we write out all of our SQL as direct queries or stored procedures? As I mentioned above, we do use SQLALchemy and while we have some hand-written queries, we prefer to use the ORM. This debate exists because ORMs can write some, well, interesting (read: awful) queries that can really hurt database performance. However, they also greatly enhance developer productivity! We choose to favor getting new tables and the code that uses them developed and out in the wild quickly, and then iterate from there if we find that specific queries need to be optimized by hand. We have a couple of practices and/or tools that assist us here: Monitoring: we monitor our databases to alert us if application or database timings change We have a “ gross query checker ,” which I’ve blogged about before: it alerts us to poorly performing queries before we leave the development environment We run pt-kill : a tool from the Percona Toolkit that will kill long-running queries in production What else? In the slides, you can also find a couple of notes about backups, where to find additional resources when working with MySQL and some book titles I’ve read that I think are worth checking out! Also, if you’re already a member of the MySQL Community, I will be presenting , and Yelp will be sponsoring a booth, at Percona Live this April . Please come say hi! The open, supportive community surrounding MySQL fits in well with Yelp Engineering’s culture of collaboration, playing well with others, and technical tenacity. On top of providing the databases, documentation, and tools, it’s important to make sure that our developers have an easy venue to discuss ideas and ask questions. Once a week, the DBA team at Yelp hosts DBA office hours - a time when we’re available to answer any and all database questions, talk about performance, swap ideas, or just generally chat. Tweet Back to blog", "date": "2015-01-07"}, {"website": "Yelp", "title": "OSXCollector: Forensic Collection and Automated Analysis for OS X", "author": ["\n        \n  Ivan L., Engineering Manager - Security\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/01/osxcollector-forensic-collection-and-automated-analysis-for-os-x.html", "abstract": "Introducing OSXCollector We use Macs a lot at Yelp, which means that we see our fair share of Mac-specific security alerts. Host based detectors will tell us about known malware infestations or weird new startup items. Network based detectors see potential C2 callouts or DNS requests to resolve suspicious domains. Sometimes our awesome employees just let us know, “I think I have like Stuxnet or conficker or something on my laptop.” When alerts fire, our incident response team’s first goal is to “stop the bleeding” - to contain and then eradicate the threat. Next, we move to “root cause the alert” - figuring out exactly what happened and how we’ll prevent it in the future. One of our primary tools for root causing OS X alerts is OSXCollector . OSXCollector is an open source forensic evidence collection and analysis toolkit for OS X. It was developed in-house at Yelp to automate the digital forensics and incident response (DFIR) our crack team of responders had been doing manually. Performing Forensics Collection The first step in DFIR is gathering information about what’s going on - forensic artifact collection if you like fancy terms. OSXCollector gathers information from plists, sqlite databases and the local filesystem then packages them in an easy to read and easier to parse JSON file. osxcollector.py is a single Python file that runs without any dependencies on a standard OS X machine. This makes it really easy to run collection on any machine - no fussing with brew, pip, config files, or environment variables. Just copy the single file onto the machine and run it. sudo osxcollector.py is all it takes. $ sudo osxcollector.py\nWrote 35394 lines.\nOutput in osxcollect-2014_12_21-08_49_39.tar.gz Details of Collection The collector outputs a .tar.gz containing all the collected artifacts. The archive contains a JSON file with the majority of information. Additionally, a set of useful logs from the target system logs are included. The collector gathers many different types of data including: install history and file hashes for kernel extensions and installed applications details on startup items including LaunchAgents , LaunchDaemons, ScriptingAdditions, and other login items OS quarantine, the information OS X uses to show ‘Are you sure you wanna run this?’ when a user is trying to open a file downloaded from the internet file hashes and source URL for downloaded files a snapshot of browser history, cookies, extensions, and cached data for Chrome, Firefox, and Safari user account details email attachment hashes The docs page on GitHub contains a more in depth description of collected data. Performing Basic Forensic Analysis Forensic analysis is a bit of an art and a bit of a science. Every analyst will see a bit of a different story when reading the output from OSXCollector - that’s part of what makes analysis fun. Generally, collection is performed on a target machine because something is hinky: anti-virus found a file it doesn’t like, deep packet inspect observed a callout, endpoint monitoring noticed a new startup item, etc. The details of this initial alert - a file path, a timestamp, a hash, a domain, an IP, etc. - is enough to get going. OSXCollector output is very easy to sort, filter, and search for manual forensic analysis. By mixing a bit of command-line-fu with some powerful tools like like grep and jq a lot of questions can be answered. Here’s just a few examples: Get everything that happened around 11:35 $ cat INCIDENT32.json | grep '2014-01-01 11:3[2-8]' Just the URLs from that time period $ cat INCIDENT32.json | grep '2014-01-01 11:3[2-8]' | jq 'select(has(\"url\"))|.url' Just details on a single user $ cat INCIDENT32.json | jq 'select(.osxcollector_username==\"ivanlei\")|.' Performing Automated Analysis with OutputFilters Output filters process and transform the output of OSXCollector. The goal of filters is to make it easy to analyze OSXCollector output. Each filter has a single purpose. They do one thing and they do it right. For example, the FindDomainsFilter does just what it sounds like: it finds domain names within a JSON entry. The domains are added as a new key to the JSON entry. For example, given the input: {\n  \"visit_time\": \"2014-10-16 09:44:57\",\n  \"title\": \"Pizza New York, NY\",\n  \"url\": \"http://www.yelp.com/search?find_desc=pizza&find_loc=NYC\"\n} the FindDomainsFilter would add an osxcollector_domains key to the output: {\n  \"visit_time\": \"2014-10-16 09:44:57\",\n  \"title\": \"Pizza New York, NY\",\n  \"url\": \"http://www.yelp.com/search?find_desc=pizza&find_loc=NYC\",\n  \"osxcollector_domains\": [\"yelp.com\",\"www.yelp.com\"]\n} This enhanced JSON entry can now be fed into additional OutputFilters that perform actions like matching domains against a blacklist or querying a passive DNS service for domain reputation information. Basic Filters FindDomainsFilter Finds domain names in OSXCollector output and adds an osxcollector_domains key to JSON entries. FindBlacklistedFilter Compares data against user defined blacklists and adds an osxcollector_blacklist key to matching JSON entries. Analysts should create blacklists for domains, file hashes, file names, and any known hinky stuff. RelatedFilesFilter Breaks an initial set of file paths into individual file and directory names and then greps for these terms. The RelatedFilesFilter is smart and ignores usernames and common terms like bin or Library . This filter is great for figuring out how evil_invoice.pdf landed up on a machine. It’ll find browser history, quarantines, email messages, etc. related to a file. ChromeHistoryFilter and FirefoxHistoryFilter Builds a really nice browser history sorted in descending time order. The output is comparable to looking at the history tab in the browser but contains more info such as whether the URL was visited because of a direct user click or visited in a hidden iframe. Threat API Filters OSXCollector output typically has thousands of potential indicators of compromise like domains, urls, and file hashes. Most are benign; some indicate a serious threat. Sorting the wheat from the chaff is quite a challenge. Threat APIs like OpenDNS, VirusTotal, and ShadowServer use a mix confirmed intelligence information with heuristics to augment and classify indicators and help find the needle in the haystack. OpenDNS RelatedDomainsFilter Looks up an initial set of domains and IP with the OpenDNS Umbrella API and finds related domains. Threats often involve relatively unknown domains or IPs. However, the 2nd generation related domains, often relate back to known malicious sources. OpenDNS & VirusTotal LookupDomainsFilter Looks up domain reputation and threat information in VirusTotal and OpenDNS. The filters uses a heuristic to determine what is suspicious. These can create false positives but usually a download from a domain marked as suspicious is a good lead. ShadowServer & VirusTotal LookupHashesFilter Looks up hashes with the VirusTotal and ShadowServer APIs. VirusTotal acts as a blacklist of known malicious hashes while ShadowServer acts as a whitelist of known good file hashes. AnalyzeFilter - The One Filter to Rule Them All AnalyzeFilter is Yelp’s one filter to rule them all. It chains all the previous filters into one monster analysis. The results, enhanced with blacklist info, threat APIs, related files and domains, and even pretty browser history is written to a new output file. Then Very Readable Output Bot takes over and prints out an easy-to-digest, human-readable, nearly-English summary of what it found. It’s basically equivalent to running: $ cat SlickApocalypse.json | \\\n    python -m osxcollector.output_filters.find_domains | \\\n    python -m osxcollector.output_filters.shadowserver.lookup_hashes | \\\n    python -m osxcollector.output_filters.virustotal.lookup_hashes | \\\n    python -m osxcollector.output_filters.find_blacklisted | \\\n    python -m osxcollector.output_filters.related_files | \\\n    python -m osxcollector.output_filters.opendns.related_domains | \\\n    python -m osxcollector.output_filters.opendns.lookup_domains | \\\n    python -m osxcollector.output_filters.virustotal.lookup_domains | \\\n    python -m osxcollector.output_filters.chrome_history | \\\n    python -m osxcollector.output_filters.firefox_history | \\\n    tee analyze_SlickApocalypse.json | \\\n    jq 'select(false == has(\"osxcollector_shadowserver\")) |\n        select(has(\"osxcollector_vthash\") or\n               has(\"osxcollector_vtdomain\") or\n               has(\"osxcollector_opendns\") or\n               has(\"osxcollector_blacklist\") or\n               has(\"osxcollector_related\"))' and then letting a wise-cracking analyst explain the results to you. The Very Readable Output Bot even suggests new values to add to your blacklists. This thing is the real deal and our analysts don’t even look at OSXCollector output until after they’ve run the AnalyzeFilter. Give It a Try The code for OSXCollector is available on GitHub - https://github.com/Yelp/osxcollector . If you’d like to talk more about OS X disk forensics feel free to reach out to me on Twitter at @c0wl . Tweet Back to blog", "date": "2015-01-12"}, {"website": "Yelp", "title": "2014 Highlights from the Yelp Engineering Team", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/01/2014-highlights-from-the-yelp-engineering-team.html", "abstract": "2014 was a busy year for us at Yelp Engineering and with 2015 having just kicked off, we wanted to highlight some of our most exciting events from last year. Open Source Yelp loves open source. We love contributing back to the developer and open source communities and last year was no exception to that. We saw a large number of our engineers contributing to many different projects. In July, to help organize all of the projects we worked on, we launched a new open source site along with the revamped developer site . Over the course of the year, we open sourced 16 new projects, and saw 3,000 commits across the 48 projects we have on GitHub. Some of the biggest projects we released, including MOE ( source ), dockersh ( source ), pre-commit ( source ), and pyleus ( source ), saw a lot of excitement from the developer community and we’re very happy to have open sourced them! The Community Yelp Engineers at Grace Hopper We partner a lot with the developer community to give people and groups ample opportunities to to learn, interact, and share knowledge with each other. Throughout the year, our engineers attended 15 different conferences, like LAUNCH Scale and Grace Hopper . At LAUNCH Scale, VP of Engineering Michael Stoppelman shared a little bit about how we’ve scaled traffic at Yelp , discussing some of the changes we made to our infrastructure to handle the increased traffic over time. Our engineers returning from Grace Hopper were excited to share some of their experiences and feedback from the conference, hoping to help more women enter the tech industry. We hosted 70 meetups at Yelp HQ from different groups like SF Python , Designers and Geeks , SF Machine Learning , Docker , and Women Who Code . They brought with them many great speakers such as author Nir Eyal who spoke on how to build habit forming products and Python core developer Raymond Hettinger who taught us about high performance Python . One of the things we’re very proud of is our partnership with Women Who Code, which, in September, grew even stronger when we became one of the first official sponsors , helping them achieve their goal of connecting 1 million women in tech. Hackathon Three times a year, all the engineers at Yelp gather for two days of hacking, food, and fun. We put together some Yelpy puzzles, flew remote-controlled sharks, and designed our own henna tattoos. Last year, our engineers participated in three separate hackathons where we saw a total of 221 projects between the three. On to 2015! There’s already a lot of fun and exciting things planned for 2015 so if you want to be part of the fun follow us on Twitter and Facebook to catch all the announcements! Tweet Back to blog", "date": "2015-01-14"}, {"website": "Yelp", "title": "CTEs and Window Functions: Unleashing the Power of Redshift", "author": ["\n        \n  Kris W., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/01/title-ctes-and-window-functions-unleashing-the-power-of-redshift.html", "abstract": "At Yelp, we’re very big fans of Amazon’s RedShift data warehouse. We have multiple deployments of RedShift with different data sets in use by product management, sales analytics, ads, SeatMe and many other teams. While it minimizes a lot of the work the RedShift team has done to call RedShift a simple fork of Postgres 8.4, RedShift does share a common code ancestry with PG 8.4. This means that much of the advanced query functionality of Postgres is available, which, when combined with the petabyte scale of RedShift, offers some amazingly powerful analytics tools. Because most of the PG 8.4 query syntax is available, I often find that directly referencing the Postgres 8.4 documentation for query syntax is more readable and useful than trying to navigate Amazon’s version of the same documentation. CTEs (Common Table Expressions): Scaling Your Queries When dealing with OLAP (online analytical processing, or warehousing) queries, especially with more snowflake schemas, it’s very common for the number of joins in a query to get large. Snowflake schemas are those where dimension tables are designed to be joined to other dimension tables, which is typical when portions of a transaction schema are mirrored into the data warehouse. RedShift (and Postgres) are well optimized for large numbers of joins, but unfortunately our brains are not. Correctness of analytics queries is paramount; basing your business decisions on faulty data can be an extremely costly mistake. One of the reasons SQL has gotten a bad reputation for doing analytics work is complexity; a traditional procedural language has functions and procedures that let you encapsulate and compose blocks of logic, while SQL does not. CTEs (Common Table Expressions) bring this same power of encapsulation and composability to SQL; by allowing you to compose multiple independent queries into a single statement, you can write SQL that is much clearer and easier to verify and debug. An Example CTE: Using The WITH Statement One of the common things we have to do inside the SeatMe codebase is determine when a restaurant’s opening and closing times for various meals occur (internally referred to as scheduled shifts). It’s very common to compute things based on these scheduled times, such as how busy the restaurant is. A (much simplified) version of this query looks like: SELECT\n    r.id as restaurant_id,\n    rez_sheet.shift as shift,\n    start AT TIME ZONE r.timezone as start_time,\n    \"end\" AT TIME ZONE r.timezone as end_time\nFROM\n    rez_sheet,rez_schedule rs,rez_restaurant r\nWHERE\n    r.id = rez_sheet.restaurant_id AND\n    rs.sheet_id = rez_sheet.id; The query itself, with its 2 joins, is understandable and independently verifiable. We then use this with a CTE in our analytics to compute things like reservations per shift. WITH scheduled_shifts AS (\n    -- The same query above; referenced as scheduled_shifts\n    -- in the main query\n    SELECT\n        r.id as restaurant_id,\n        rez_sheet.shift as shift,\n        start AT TIME ZONE r.timezone as start_time,\n        \"end\" AT TIME ZONE r.timezone as end_time\n    FROM\n        rez_sheet,rez_schedule rs,\n        rez_restaurant r\n    WHERE\n        r.id = rez_sheet.restaurant_id AND\n        rs.sheet_id = rez_sheet.id\n)\nSELECT\n    count(r.id) as parties,\n    sum(r.covers) as people,\n    ss.shift as shift,\n    ss.restaurant_id as restaurant_id\nFROM\n    -- This references the query in the body of the WITH statement\n    -- as if it were a typical table join\n    scheduled_shifts ss\nJOIN\n    rez_reservation r\nON (\n    ss.restaurant_id = r.restaurant_id AND\n    r.when::time between ss.start_time and ss.end_time\n)\nGROUP BY 3,4; Conceptually you’ve created a temporary table called scheduled_shifts with the results of the first query that you can join against in the second query. One of the benefits of using CTEs when composing queries is that if they are getting re-used frequently, you can create a view over the same statement. If performance of the statement being used in the CTE is a concern and the data can be cached without hurting correctness, you can also trivially create a temporary table with the results of the CTE with only minimal change and very low risk to the overall query correctness. It does bear saying: CTEs in both RedShift and Postgres represent an optimization barrier. When using a CTE the optimizer is unable to perform optimizations across the query in the body of the CTE and the main query, though it does optimize each of them individually. While this can be an issue, in the real world we’ve found the conceptual benefits greatly outweigh the performance drawbacks. Window Functions or “Dang, I didn’t know SQL could do that.” PostgreSQL Window Functions, which are available in RedShift, are extremely complex and difficult to explain. My goal here is to give a broad overview of the concepts and enough information to encourage people to try them out. Ultimately you’ll need to read and refer to the PostgreSQL documentation on Window Functions and Window Function Calls , along with the tutorial when using them in your own queries. Defining Windows Window functions are a special class of analytic functions that are applied to windows of rows. Windows are defined by an OVER (...) clause which defines a set of rows related to the current row to which the function applies. While that’s extremely abstract, the diverse functionality available from the different window functions doesn’t really lend itself to a simpler definition. The two main components of the window are: PARTITION BY which is the logical analog of GROUP BY in a traditional query. Window functions that aggregate will do so across all rows in the partition. ORDER BY which is required for certain functions that look forward or backward in the window. The FRAME clause, which is harder to cover, and I’ll not go into in depth. The frame is another logical concept only used by functions that are relative to the frame (like first_value / last_value ) I think of window functions as falling into two categories: Functions that are also available as traditional analytics functions, such as count , sum , avg , etc. Functions that are only available when using windows, such as lead , lag , ntile , etc. These expose truly novel functionality unavailable without using windows. For functions that are also available when using GROUP BY , the primary advantage of using them with window functions is it becomes possible to do multiple different grouping operations in a single query. When combined with the power of subqueries and CTEs, this can let you do very powerful business logic all in a single statement. It would be natural to assume that doing multiple grouping operations in a single query would be just as costly in terms of execution time as doing multiple single operations. In practice, we haven’t seen this to be the case. There is of course a cost, but we typically see it be much smaller than a 100% overhead depending on the query and the grouping. Window Function Examples: Using Multiple Traditional Aggregates The following query illustrates the use of multiple count functions over different partitions to compute the percent of reservations that a given restaurant accounts for by locality (city). The things to note in this query are: Use of DISTINCT : Since window functions append columns to each row, without a DISTINCT operator, the query will give you back 1 row for every row in the join. For this query, this would be 1 row per reservation rather than a row per restaurant and city. The two count operations each have a different PARTITION BY clause, one counting by restaurant and the other counting by locality. The final query, which references the two columns produced by the window function in a CTE and computes a percentage using them. WITH counts_per_res_and_city as (\n    SELECT DISTINCT\n        rez_restaurant.id as restaurant_id,\n        rez_restaurant.locality as city,\n        count(r.id) OVER (PARTITION BY rez_restaurant.id) as per_restaurant,\n        count(r.id) OVER (PARTITION BY rez_restaurant.locality) as per_city\n    FROM\n        rez_restaurant JOIN \n        rez_reservation r ON (\n            rez_restaurant.id = r.restaurant_id\n    )\n)\nSELECT restaurant_id, \n    city, \n    ((per_restaurant * 100)/per_city) as restaurant_city_percent\nFROM counts_per_res_and_city; Window Function Examples: Using ntile To Compute Percentiles Frequently, Yelp needs to look at distributions of user activity and compute percentile buckets based on their activity. The query below uses the ntile function to augment a per-user count of lifetime review behavior. Things to note about this query: The ntile(100) is PARTITION BY signup_country , so it will compute the percentile per signup country. The ntile(100) is ORDER BY review_count , which means the rows will be bucketed in order of the review_count. Each row will get a number from 1-100, that is the logical bucket that the row falls into, added as a new column called percentile . with user_signup_counts as (\n    -- Query that produces 1 row per user, with the number\n    -- of reviews that user has created over all time, their\n    -- country and signup date.\n)\nselect distinct\n    ntile(100) OVER (\n        PARTITION BY signup_country \n        ORDER BY review_count) \n   as percentile,\n   review_count,\n   signup_country,\n   quarter\nFROM\n   user_signup_counts; Further Reading I’ve touched on two of the most powerful features for Redshift analytics, window functions and CTEs, but there’s a lot more functionality in Postgres, much of which is also in RedShift. One of my favorite Postgres sessions is Postgres: The Bits You Haven’t Found , which showed me a whole huge set of Postgres functionality, including first exposing me to window functions. In addition, brushing up on your psql chops pays dividends over time as you start to become fluid with the advanced functionality in the Postgres CLI. If you write a lot of date based reports, which I suspect we all do, I would also recommend digging into the date/time functionality (in particular date_trunc). date_trunc makes doing date based roll ups extremely fast and easy, letting you quickly truncate dates to useful things to months, quarters, weeks, etc. Tweet Back to blog", "date": "2015-01-21"}, {"website": "Yelp", "title": "Animating the Mobile Web", "author": ["\n        \n  Zain M., Engineering Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/01/animating-the-mobile-web.html", "abstract": "One of the most engaging features of Yelp is our photos and videos gallery. When you visit a Yelp Business Page inside of the mobile app, there is a photo at the top of the page to provide visual context. It also serves as a compelling entry point to our photo viewer if you pull it down. We wanted to have this same effect on our mobile site, so we set out to develop a nice, smooth animation to pull down this photo and delight mobile web users with the same experience they’re used to on our mobile applications. The Beginning I was tasked with implementing this animation as part of my internship. Having little prior experience, all I knew was that when the user touches to pull down on the photo, its CSS properties should update over time to generate what is pictured above. To feel smooth, this animation needs to run at 60 frames per second (fps). This sets our frame budget to 16ms. 16ms to perform all animation-related work necessary to render each frame: sounds like a challenge! Scoping Out Animation The first step was to place the background image behind the top of the page so that we can expand and scale it in the future. After completing this, I started to manually test out how the image would expand and thought about which CSS properties were needed to accomplish the animation effect. When the photo is being pulled down, several CSS properties should be animated over time: margin-top , to control the photo’s top offset opacity , to fade-in the photo as the user pulls down and to fade-out the rest of the business details page as the user pulls down height/width , to scale the photo up as the user pulls down Using Chrome Developer Tools I manually fiddled with the CSS properties of the respective DOM Elements to replicate the desired animation and after some experimentation, things looked okay. Still left to do: animate those CSS properties based on a user’s touch movements. Handling Touch Movements When researching about touch events, I learned that, on mobile devices, there are three main events triggered when a user touches a screen: touchstart , touchmove , and touchend . These three events enable JavaScript to see when and where a user’s finger starts, moves and stops. In our case, we care about the distance between their current touch point and their initial touch point. Hence the following action plan to handle touch movements: On touchstart : keep track of the initial y coordinate (let’s call it initialY ) and store it for future comparisons. On touchmove : get the current vertical coordinate ( currentY ) and compare: if currentY > initialY , the user is pulling down on the photo. In this case, ( currentY - initialY ) represents how much a user has pulled down thus far, and can serve as a basis for CSS properties updates (more on that later.) if currentY initialY, user is trying to scroll normally On touchend : redirect the user to our photo viewer or animate the page back to its original state After coding this, I started to test how well touch events integrate with the CSS animations. Initial Test Results As seen below, we were blowing through our 16ms frame budget, resulting in a significantly laggy and choppy animation. This is a screenshot from Chrome's profiling tool in frames mode while the pull-down animation is running. Each vertical bar represents a frame. Its height indicates the time it took to compute it. Its coloring indicate the type of work done by the browser to compute it. Read more here . Two things are causing this: margin-top , height , and width are poor CSS properties to animate. Since updates can’t be offloaded to the GPU, animating on any of these properties takes a heavy toll on the browser, especially on mobile. Each touchmove event triggers the rendering of a new frame. This is too much rendering work for the renderer, which explain the frames dropped and the choppy animation.\nAs Jon Raasch explains in a post on HTML5 Hub : “The renderer tends to choke on the large number of rendering tasks, and often isn’t able to render a certain frame before it has already received instructions to render the next. So, even though the browser renders as many frames of the animation as possible, the dropped frames still make for a choppy-looking animation, which is not to mention the performance implications of overloading the processor with as many tasks as possible.” Animating Faster To tackle the first problem of expensive CSS properties, I read Paul Lewis’ and Paul Irish’s post on High Performance Animations to find more efficient replacements. Their post explained which CSS properties are best for animating on the web and lead us to use: transform: translateY() , to control the photo’s top offset opacity , to fade-in the photo as the user pulls down opacity , to fade-out the rest of the business details page as the user pulls down transform: scale() , to scale the photo up as the user pulls down In addition to using more efficient CSS properties, I promoted each DOM element taking part in this animation to its own layer by styling them with transform:translateZ(0) . This is essential because it offloads rendering work to the GPU and prevents layout thrashing (since the animated elements are on their own layers, the non-animated elements don’t need to be re-laid-out/re-painted). Animating Smoother To prevent frames from getting dropped due to too many rendering requests, I used requestAnimationFrame . requestAnimationFrame takes a callback that executes when the browser pushes a new frame to the screen. Essentially, the browser pulls for work at each frame, instead of us pushing work for each new touch event. This allows for concurrent animation to fit into one reflow/repaint cycle. As a result, it makes animations look much smoother because the frame rate is consistent. Problems solved but could implementation be better? I had the essentials for a neat photo pull down animation. However, the animation was composed of several independent animations on different DOM elements. Manually computing CSS properties’ values at each frame was unnecessarily complex. I needed a more standard & organized solution to create and run DOM animations. GitHub and Shifty to the Rescue! I found on GitHub a tweening engine to abstract most of the difficulties of creating an animation, called Shifty , an open-sourced lightweight tweening engine for JavaScript created by Jeremy Kahn . Using Shifty would provide us with: the calculation of CSS properties at a certain point in an animation, given a start/end value and a desired duration the ability to easily seek to a certain point in an animation However, the things Shifty wouldn’t provide us were: the ability to directly apply the calculated CSS properties to specific DOM elements the ability to orchestrate multiple animations simultaneously How can we build on top of Shifty to help us with our use cases? As a result, I created two JavaScript classes which extend from the Shifty tweening engine. DomTweenable The first class is called DomTweenable. It’s the same as a Tweenable object from Shifty except that you can attach additional DOM element to the Tween. Moreover, when you seek to a specific part of the DomTweenable’s tween, the CSS properties are automatically applied to the DOM element. Timeline The second class is called Timeline. Same as a Tweenable object from Shifty except that you can attach multiple DomTweenable objects at specific point in the Timeline’s tween. When you seek to a specific part of the Timeline’s tween, it seeks on each of the DomTweenable objects at (specified position - starting position on timeline.) Final Result We now have an easy way to animate on multiple DOM Elements and create smooth animations! And hey, look! Under 60 frames per second: Thanks to Simon Boudrias and Arnaud Brousseau for the help! Resources on animations High Performance Animations by Paul Lewis and Paul Irish requestAnimationFrame for Smart Animating by Paul Irish requestAnimationFrame for Better Performance by Jon Raasch Leaner, Meaner, Faster Animations with requestAnimationFrame by Paul Lewis Tweet Back to blog", "date": "2015-01-27"}, {"website": "Yelp", "title": "February Events At Yelp", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/02/february-events-at-yelp.html", "abstract": "The Northeast may be covered in snow, but here in sunny San Francisco, we’re operating at full steam! We got the year off to a great start with a few meetups and the launch of our new tech talk series (more on that later). This month, on top of the meetups we’re hosting, we’re also getting involved with GirlDevWeek . We’ll be hosting a panel here at Yelp HQ, as well as throwing the official after party in conjunction with Pandora . So if you’re going to be at GirlDevWeek, we hope to see you at both events! Meetups at Yelp HQ: SF Machine Learning , February 5th SF Python, February 11th (registration link will be available soon) Designers & Geeks , February 19th Products That Count , February 25th Tweet Back to blog", "date": "2015-02-03"}, {"website": "Yelp", "title": "assert_called_once: Threat or Menace", "author": ["\n        \n  Tyler R., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/02/assert_called_once-threat-or-menace.html", "abstract": "I remember the first time I laid eyes on the beast: Summer, 2012. The air conditioning at Yelp HQ hummed imperceptibly as I reviewed code for a colleague. This wasn’t my first rodeo, but I was new to Yelp and to working in a large Python codebase. I painstakingly scanned the code for lurking bugs, but couldn’t find any. “Ship it!” I declared electronically, freeing my colleague to deploy his changes. It is chilling to think that on that day, I looked the beast squarely in the eyes and never realized it. Cunning camouflage allowed it to slip past me and into production. Hunt the Wumpus So what is this horrific beast? I will show you, and you still might not spot it! Imagine you are that starry-eyed engineer from the Summer of ‘12 and you’re reviewing production code that looks something like this: def restart_server(server):\n    \"\"\"Safely restart a server by removing it from the load balancer,\n    temporarily silencing its monitoring, telling its mom it will be home\n    before dark, etc.\n\n    (The implementation doesn't matter so we'll leave it out, but assume it\n    makes remote API calls and invokes system commands and generally\n    performs expensive operations.)\n    \"\"\"\n    pass\n\ndef restart_servers_in_datacenter(servers, datacenter):\n    for server in servers:\n        if server.endswith(\"-%s\" % datacenter):\n            restart_server(server) This feature even comes with a unit test: import mock\n\nmodule_under_test = __name__\n\ndef test_restart_servers_in_datacenter_with_match():\n    servers = set([\"web1-sfo\", \"web2-phx\", \"web3-jfk\"])\n    with mock.patch(\"%s.restart_server\" % module_under_test) as mock_restart:\n        restart_servers_in_datacenter(servers, \"sfo\")\n        mock_restart.assert_called_once() The main idea is that restarting actual servers whenever someone runs the test suite is not a great strategy. So instead of calling the real restart_server() , we use mock.patch() from the Python Mock library to swap in a test double . The fake restart_server() , which we call mock_restart , is a MagicMock which remembers how it has been called and provides convenience methods like assert_called_once_with to validate its experiences. The code review includes output showing that this test passes. You can paste the code above into a file and run it with py.test if you don’t trust me: $ py.test beast.py\n============================================================================\ntest session starts\n============================================================================\nplatform linux2 -- Python 2.6.7 -- py-1.4.20 -- pytest-2.5.2\ncollected 1 items\n\nbeast.py .\n\n============================================================================\n1 passed in 0.01 seconds\n============================================================================ Did you spot the beast? It is Pitch Black Here’s a clue: the test will happily continue to pass if you change the assertion to: mock_restart.assert_two_wrongs_dont_make_a_right() Or: mock_restart.assert_two_plus_two_equals(5) Or even: mock_restart.ASDFASDFASDFASDFASDFASDFASDF() What’s going on here? Remember that a mock’s job is to say, “You got it, boss” whenever anyone calls it. It will do real work, like raising an exception, when one of its convenience methods is called, like assert_called_once_with . But it won’t do real work when you call a method that only resembles a convenience method, such as assert_called_once (no _with !). You Are Likely to be Eaten by a Grue More troubling, the test passes even if you remove the call to restart_server() from the code-under-test entirely! def restart_servers_in_datacenter(servers, datacenter):\n    pass This should make your blood run cold. Our test looks reasonable, emits a green “1 test passed,” but doesn’t test anything! Production code with no test coverage is pretty bad, but this test actually has negative value. Someone spent energy writing it. More engineers will spend energy reading it. Future maintainers, not realizing that they are bedding down with the beast, will add features based on this test and these new features will also be untested. Unwary newcomers will see the useful-looking but ultimately poisonous assert_called_once() and spread it to other modules. Kill The Beast Now that we know the beast and how it ticks, how can we defeat it? Fear of Commitment Fellow Yelpers Cheng C. and Nicholas N. added a check to our pre-commit hooks that uses the compiler module to walk the AST looking for assert_called_once and a few other disguises commonly used by the beast. If the beast’s signature is detected, the commit is rejected.[1] This band-aid was cheap to implement and provides a final, automated chance to slay the beast before it can sneak into the master branch. Blacklists are a crude instrument, however, so this hook is not a perfect defense. You Can Go Your Own Way After my colleague Keith M.’s first encounter with the beast (pointed out to him by battle-hardened Autumn 2014 me in a code review), he vowed to avoid the Mock convenience methods entirely: I’ve switched to an idiom of checking mock objects that should be a lot more resilient to the fact that they are pernicious liars. So instead of: mock_restart.assert_called_once_with(\"web1-sfo\") Keith writes: assert mock_restart.call_count == 1 assert mock_restart.call_args == mock.call(“web1-sfo”) His rationale: The helper method saves a line, but calling things on mocks and expecting them to have side effects (like raising an error) is just asking to get bit by typos and misspellings. Meanwhile, it’s much harder for a mock to accidentally end up with very specific values occupying a random attribute. Glasses for Your Subaru? The authors of Mock are aware of the beast and have built a defense into the library – autospec: If you use a class or instance as the spec for a mock then you can only access attributes on the mock that exist on the real class. If we change our test to use the autospec defense: with mock.patch(\"%s.restart_server\" % module_under_test, autospec=True) as mock_restart:\n    restart_servers_in_datacenter(servers, \"sfo\")\n    mock_restart.assert_called_once() Then the beast is exposed! $ py.test beast.py\n============================================================================\ntest session starts\n============================================================================\nplatform linux2 -- Python 2.6.7 -- py-1.4.20 -- pytest-2.5.2\ncollected 1 items\n\nbeast.py F\n\n============================================================================\nFAILURES\n============================================================================\n_________________________________________________________________\ntest_restart_servers_in_datacenter_with_match\n_________________________________________________________________\n\nE           AttributeError: 'function' object has no attribute 'assert_called_once'\n\nbeast.py:22: AttributeError\n============================================================================\n1 failed in 0.01 seconds\n============================================================================ autospec is very effective… if you remember to use it. It’s not on by default due to “ caveats and limitations ”, so the beast relies on the uninitiated and forgetful to avoid being trapped by autospec . Test-Driven Defense The best defense against the beast is not a blacklist or a workaround or a clever library feature. It is a change in how you write software. When I see a test containing a bogus method like assert_called_once , it tells me that the author never verified that the test could fail. A test that cannot fail is no test at all. The easiest way to insure that your test can fail is to write the test first, watch it fail, then write production code until the test passes. This is Test-Driven Development (TDD) and in addition to being a relatively foolproof means of defeating the beast, it is the best way I know to write well-tested, well-factored, maintainable software. TDD is what allowed me to outsmart the beast the first time. While adding a feature, I had copied from an existing, beast-infiltrated test. But because I had written the test first, I knew something was amiss when the test suite cheerfully passed. Suspicion and ipdb soon laid bare the beast’s tricks, and now I pass my experiences on to you. So go, my friends, into the great wilderness of software. May your test suite remain faithful, honest, and beast-free. [1] You can see this hook at https://gist.github.com/mrtyler/995fcb4282a9d15de625 . #!/usr/bin/python\n\nimport compiler\nimport compiler.ast\nimport optparse\nimport sys\n\nclass MockChecker(object):\n    def __init__(self):\n        self.errors = 0\n        self.current_filename = \"\"\n        self.non_existent_methods = [\n            'assert_calls',\n            'assert_not_called',\n            'assert_called',\n            'assert_called_once',\n            'not_called',\n            'called_once',\n            'called_once_with',\n        ]\n\n    def check_files(self, files):\n        for file in files:\n           self.check_file(file)\n\n    def check_file(self, filename):\n        self.current_filename = filename\n        try:\n            ast = compiler.parseFile(filename)\n        except SyntaxError, error:\n            print >>sys.stderr, \"SyntaxError on file %s:%d\" % (filename, error.lineno)\n            return\n        compiler.walk(ast, self)\n\n    def visitGetattr(self, node):\n        if node.attrname in self.non_existent_methods:\n            print >>sys.stderr, \"%s:%d: you may have called a nonexistent method on mock\" % (self.current_filename, node.lineno)\n            self.errors += 1\n\ndef main():\n    parser = optparse.OptionParser(usage=\"%prog [options] file [files]\", description=\"Checks that the test file does not contain non-existent mock methods\")\n    (opts, files) = parser.parse_args()\n    if len(files) == 0:\n        parser.error(\"No filenames provided\")\n\n    checker = MockChecker()\n    checker.check_files(files)\n    return 1 if checker.errors else 0\n\nif __name__ == '__main__':\n    sys.exit(main()) Tweet Back to blog", "date": "2015-02-04"}, {"website": "Yelp", "title": "Yelp Dataset Challenge is Doubling Up!", "author": ["\n        \n  Soups R., Ph.D., Software Engineer (Data Mining)\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2015/02/yelp-dataset-challenge-is-doubling-up.html", "abstract": "Two years, four highly competitive rounds, over $35,000 in cash prizes awarded and several hundred peer-reviewed papers later: the Yelp Dataset Challenge is doubling up. We are proud to announce our latest dataset that includes information about local businesses in 10 cities across 4 countries. This dataset contains 1.6M reviews and 500K tips by 366K users for 61K businesses along with rich attributes data (such as hours of operation, ambience, parking availability) for these businesses, social network information about the users, as well as aggregated check-ins over time for all these users. This treasure trove of local business data is waiting to be mined and we can’t wait to see you push the frontiers of data science research with our data. Figure 1: Cities included in the Yelp Dataset Challenge Round 5 At Yelp , one of our missions is to engage with the academic community and help them by providing real-world data to aid their research. Our dataset should appeal to researchers in data mining, machine learning, economics and urban planning alike. So whether you are building a cutting-edge Natural Language Parsing (NLP) algorithm that mines sentiments expressed by our reviewers, figuring out what business attributes (service quality, ambience, etc.) make a local business popular, or designing better cities and communities by mining local business data – our dataset has everything you need to put your research together. New Competition: Deadline is June 30th 2015 Download the new dataset and remember to submit your entry by June 30, 2015 in order to be eligible for one of our top-project prizes of $5,000. Please note that the contest itself is open only to students. The contest will run from Feb 5, 2015 to June 30, 2015. See the website for the full terms and conditions. New Dataset: 10 cities, 4 countries The most recent Yelp Dataset Challenge ( our fourth round ) ran from August 1 – Dec 31 2014, giving students access to reviews and businesses from five cities worldwide: _ Phoenix, Las Vegas, and Madison in the U.S., Waterloo in Canada and Edinburgh in U.K. _. In Round 5, open now, we are expanding the dataset to include data from five new cities: _ Pittsburgh, Urbana-Champaign, and Charlotte in the U.S., Montreal in Canada and Karlsruhe in Germany _. We have also updated the data from the original five cities with any businesses that were added or new reviews and tips that were written in those cities since August 1, 2014. To get your creative juices flowing, here are a few things that you could do with this dataset and some interesting projects from the last round: Cultural Trends By adding a diverse set of cities, we want participants to compare and contrast what makes a particular city different. For example, are people in international cities less concerned about driving in to a business, indicated by their lack of mention about parking? What cuisines are Yelpers raving about in these different countries? Do Americans tend to eat out late compared to the Germans and English? In which countries are Yelpers sticklers for service quality? And what about that old adage: is success of a business just location, location and location? Seasonal Trends What about seasonal effects: Are HVAC contractors being reviewed just at onset of winter, and manicure salons at onset of summer? Do you see any non-intuitive correlations between business categories e.g., how many karaoke bars also offer Korean food, and vice versa? Are there more reviews for sports bars on major game days and if so, could you predict that? Natural Language Processing (NLP) What are the most common positive and negative words used in our reviews? Are Yelpers a sarcastic bunch? And what kinds of correlations do you see between tips and reviews: could you extract tips from reviews? In international cities such as Montreal, are French speakers reviewing places differently than English speakers? Some creative projects from Round 4: We are still in the process of reviewing the submissions from Round 4. The response has been overwhelming and we have received over 60 well thought-out and insightful submissions. Yelp’s team of data mining engineers is still reviewing the submissions to decide the winners of the grand prizes. Meanwhile, we thought we’d share a few interesting submissions. Figure 2: Good Food Bad Service Restaurants A team of Stanford Data Science majors mined our review dataset to identify the characteristics of restaurants that are consistently ranked for their good food, but bad service. If you live in Phoenix, Las Vegas, Madison, Waterloo or Edinburgh, then you can check out these Good Food Bad Service Restaurants they’ve identified. For instance, the words in blue above are those that are mentioned more often in 5-star reviews than in 1-star reviews, e.g., “duck,” “cod,” “poached,” and “crepes.” In contrast, “airport,” “occupied,” “issues,” “serving,” and “7 pm” are words used much more often in lower rated reviews. Their recommendation to “Good Food Bad Service” restaurants: hire more wait-staff for the 7 pm rush! Figure 3: Thanksgiving is all about food while Christmas is about shopping! Data Science Society of UCSD wrote up a very interesting blog post describing Naïve Bayes and Random Forests based approaches towards predicting star rating of users solely based on the content in their reviews. They also created some beautiful word clouds to visualize what Yelpers like to do during the holidays. It turns out Thanksgiving is all about food while Christmas is all about shopping! So go ahead and take our data for a spin. We can’t wait to see what you create! Tweet Back to blog", "date": "2015-02-06"}, {"website": "Yelp", "title": "Reflections from Grace Hopper (Part 1)", "author": ["\n        \n  Tasneem M., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/11/reflections-from-grace-hopper-part-1.html", "abstract": "As you probably heard, Yelp attended Grace Hopper this year. Nine software engineers from different teams attended and, for many of us, it was our first time. It was a unique experience to see so many talented women in one place. In addition to the talks and panel discussions, we also had the opportunity and pleasure to represent Yelp at the career fair. It was amazing to see a consistent flow of students and industry talent, all happy customers, stop by our booth to speak with us and tell us their stories of using Yelp. We had such a great time that we wanted to share some highlights with you. This is the first of two posts where each of us have added an excerpt from our experience in our own words. Our first day at the career fair. From left: Jen F., Wei W., Susanne L., Tasneem M., Carmen J., Emily F., Virginia T. Tasneem M. Engineering Manager, Ads I joined Yelp about a month ago and was super excited to be part of this journey with fun and interesting women from the company. I have worked in the software industry since the early 2000s and have grown as an engineer, a manager and a leader partly because I have had inspiring role models throughout my career. My mentors have challenged me to take on opportunities that I felt I wasn’t quite ready for. My experience at Grace Hopper was a reminder of how far I have come and yet how far I still can go. It has re-inspired me to continue mentoring women and help them take charge of their careers in a male dominated industry. I am also motivated to help tackle the diversity issues within my local community. The highlights for me were being at the career fair, attending the keynotes and technology lightning talks. Having spent a lot of time at the career fair, I was impressed by the talent and deep interest in data mining and machine learning. I look forward to seeing some of you join our growing community of awesome engineers at Yelp. I also loved Pat Kirkland ’s talk on “Executive Presence.” I appreciated her role-playing and practical guidance on the different personas (prey, predator and partner) and have been able to successfully experiment with some of her tips at work. The lightning talks were a great way to hear several unique stories and perspectives within an hour. I hope that we can be on stage next year sharing some of our experiences. Susanne L. Database Engineer, Operations I am a database engineer at Yelp, working on a team where we all make sure that our persistent stores are reliable, scalable, fast and give our Yelp consumers a pleasant experience with our product. I thought it was awesome to see so many great women in computing coming to Grace Hopper. Some of them were looking for an internship or a full time position and they stopped by our booth genuinely interested in Yelp. It was exciting to hear how our product makes consumers happy. A lot of these young women had remarkable experiences but didn’t really know what to highlight and how to sell themselves. My suggestion to some of them was to: Consider having an “elevator pitch” prepared. Sell yourself in 60 seconds: try a pitch that includes your name, year, major, minor (if you have one), and why you want this job. Prepare to talk about a distinguishing skill/project. For example, “I did a hackathon, where I developed an android app..” Be prepared to answer any questions about the project and what you learned from it. Know what you are looking for. “What kind of internship at Yelp would you be interested in?” Backend/Frontend/Mobile/Web? Which programming language do you like? Why? Avoid “I don’t know, anything is fine” unless you are a freshman! Jen F. Systems Administrator, Corporate Infrastructure I have been at Yelp since August 2012. For me,  the talk I will remember most is “Beyond the Buzzwords: Test-Driven Development” where I got a quick overview of how one can practice test driven development (TDD). While TDD isn’t the most exciting technical topic, the speaker, Sabrina B. Williams from Google, did an awesome job making it relatable. She was able to share a live demo of a project from concept to the tests to fully-implemented program. The keynotes were full of rock stars of the technology world that were also surprisingly engaging (and occasionally controversial!).  Who hasn’t gone to a technology conference keynote and expected a glorified sales pitch that was easily forgettable?  None of that was here at Grace Hopper. I also appreciated how diverse the talks were - from recommendations on how to get your first job to dealing with office politics to in-depth technical talks. However, one thing I’d love to see more at Grace Hopper next year is talks about advancing your career in non-managerial roles. All in all, this is a great conference and I only wish I had been able to participate when I was just getting into the industry! Tweet Back to blog", "date": "2014-11-04"}, {"website": "Yelp", "title": "Reflections from Grace Hopper (Part 2)", "author": ["\n        \n  Tasneem M., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/11/reflections-from-grace-hopper-part-2.html", "abstract": "Welcome back! Today we have Wei, Rachel, Jen, Virginia and Anusha sharing their experiences. Wei is an engineer on the consumer team and she brings amazing user experiences to our customers. Rachel and Jen are both engineers on our international team, bringing the power of Yelp to all of our international communities. Virginia works as an engineer on our partnerships team and Anusha is an engineer on our infrastructure team. Overall, we all had a blast getting to know each other, meeting other amazing women in the industry, hearing some great stories from inspiring women role models, and sourcing some future talent for Yelp. If you missed us at the career fair and are interested in working at Yelp,  check out yelp.com/careers . We are always interested in talented women engineers to grow our community. Wei W. Software Engineer, Mobile Site I was very inspired by Yoky Matsuoka’s talk. She reminded us to constantly evaluate our level of passion and engagement with what we’re doing, and to change courses if we find those levels waning. As a professional tennis player turned robotics professor turned VP of technology at Nest, Yoky is proof that our interests may lead us on many different paths throughout our life and that, it turns out, is actually okay. We loved the afternoon ice-cream snacks Rachel Z. Software Engineer, International My time at Grace Hopper was split between interviewing and attending talks. I was a bit overwhelmed by the volume of students coming by our career booth everyday but I was also really glad that we got to interview some strong women engineers while at the conference. It was great to see the interest in technical as well as future leadership possibilities. Jo Miller’s talk on Winning at the Game of Office Politics was fascinating. I always viewed office politics as evil. Her talk provided a way to look at it from a different angle and assured us that it’s possible to navigate office politics without becoming a political animal. Some other personal highlights: speaking with industry folks during workshops, eating ice cream, dancing at the parties, and reconnecting with friends. A couple things I hope the conference improves upon next year are to have less scheduling conflicts between interesting talks, and increased availability for the more popular talks. Jen W. taking a break from interviews Jen W. Software Engineer, International I have been working in the industry for over 10 years, always in male-dominated workplaces. What struck me the most was being surrounded by so many women. It was inspiring to meet people from such a wide diversity of experiences and backgrounds, all of them bright, eager, and happy to chat (Hi Yenny! Hey Kanak!). Those who are in school or looking at career growth will truly benefit from attending future Grace Hopper conferences. For the rest of us, it’s still a great opportunity to meet new people, attend the amazing (in more ways than one!) plenary panels and learn what’s going on in tech in the rest of the world. Our booth looked awesome and was very popular for its swag! Clearly all of us were trying to tweet about it while we took this picture! Virginia T. Software Engineer, Partnerships For me, the most useful and interesting workshop was “The Dynamics of Hyper-Effective Teams.” I loved that there was role-playing involved. For instance, do you know someone on your team who’s an “Airtime Dominator,” someone who speaks at least 20% of the time? Or a “Silent Expert,” the one who has all the knowledge but rarely speaks up? What about “The Naysayer” who constantly refutes others’ ideas? The workshop concluded with volunteers sharing their stories and advice on working with these personalities. Having personally been through some similar experiences, it was an eye-opener to learn how these (sometimes clashing) personalities can effectively work together. Can’t wait to experiment with it! Anusha R. Software Engineer, Infrastructure I loved attending Grace Hopper for the first time this year! There were a bunch of inspirational speakers at the conference. Just like my co-worker and friend Wei, I also enjoyed Yoky’s talk about how her passions led to her finding her career path. I was also inspired by Barbara Birungi , one of the award winners. Her non-profit is helping women in Uganda get involved in  technology through mentorship and coaching. There was a good mix of technical and career building talks and workshops. I attended several career building sessions and enjoyed the workshops on office politics, difficult conversations and building hyper-effective teams. But most of all, I enjoyed talking to the students who visited our booth at career fair. Many of them had interesting projects to talk about. I was excited to see this level of talent and wish them the best in their careers. Tweet Back to blog", "date": "2014-11-05"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 3 Winners and Dataset Tools for Round 4", "author": ["\n        \n  Dr. Scott Clark, Ad Targeting Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/11/yelp-dataset-challenge-round-3-winners-and-dataset-tools-for-round-4.html", "abstract": "Yelp Dataset Challenge Round 3 Winners We recently opened the fourth round of the Yelp Dataset Challenge . This announcement included an update to the dataset, adding four new international cities and bringing the total number of reviews in the dataset to over one million. You can download it and participate in the challenge here . Submissions for this round are open until December 31, 2014. See the full terms for more details. With the opening of our fourth iteration of the challenge, we closed the third round, which ran from February 1, 2014 to July 31, 2014. We are proud to announce two grand prize winners of the $5,000 award from round three: Jack Linshi from Yale University with his entry “Personalizing Yelp Star Ratings: A Semantic Topic Modeling Approach.” Jack proposed an approximation of a modified latent Dirichlet allocation (LDA) in which term distributions of topics are conditional on star ratings, allowing topics to have an explicit sentiment associated with them. Felix W. from Princeton University with his entry “On the Efficiency of Social Recommender Networks.” Felix constructed metrics for measuring the efficiency of a network in disseminating information/recommendations and applied them to the Yelp social graph, discovering that it is quite efficient. These entries were selected from many submissions for their technical and academic merit. For a full list of all previous winners of the Yelp Dataset Challenge, head over to the challenge site . Dataset Example Code We maintain a repository of example code to help you get started playing with the dataset. These examples show different ways to interact with the data and how to use our open source Python MapReduce tool mrjob with the data. The repository includes scripts for Converting the dataset from JSON to CSV Predicting likely categories given review text Finishing reviews using Markov Chains Finding the sentiment of words in the dataset Other Tools There are many ways to explore the vast data within the Yelp Dataset Challenge Dataset. Below are some examples of some of the many cool tools that can be used with our data: CartoDB is a cloud based mapping, analysis, and visualization engine that shows you how you can transform reviews into insightful visualizations. They recently wrote a blog post demonstrating how to use their tools to gain interesting insights about the Las Vegas part of the dataset. Statwing is a tool used to clean data, explore relationships, and create charts quickly. They loaded the dataset into their system for people to play with and explore interesting insights. Yelp Dataset Challenge Round 4 Submissions for this round are open until December 31, 2014. See the full terms for more details. This dataset contains over one million reviews from five cities around the world, along with all of the associated businesses, tips, check-ins, and users along with the social graph. We are excited to see what you come up with! Tweet Back to blog", "date": "2014-11-06"}, {"website": "Yelp", "title": "November Events at Yelp", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/11/november-events-at-yelp.html", "abstract": "This month we have a handful of exciting events and a few new ones! We kicked off the month with Hackathon 15.0 where Yelpers created and shared some amazing projects (more on that in a few weeks!). Now that the hackathon dust has settled, we’re starting off by hosting the Bay Area Girl Geek Dinner to help encourage networking between girl geeks. On top of that, we’ll be at AnDevCon November 18 - 21 so make sure to find us there too. Events happening at Yelp HQ: Tuesday, November 11, 2014 - 5:30PM - Yelp Girl Geek Dinner ( Bay Area Girl Geek Dinner ) Thursday, November 13, 2014 - 7:00PM - Hooked: How to Build Habit-Forming Products ( Designers + Geeks ) Tuesday, November 18, 2014 - 6:30PM - Docker Meetup at Yelp ( Docker ) Wednesday, November 19, 2014 - 6:45PM - You Are What You Buy ( Products That Count ) Thursday, November 20, 2014 - 6:30PM - Become a Mad Scientist through Failure and Determination ( BAKG ) Tweet Back to blog", "date": "2014-11-11"}, {"website": "Yelp", "title": "Scaling Elasticsearch to Hundreds of Developers", "author": ["\n        \n  Joseph Lynch, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/11/scaling-elasticsearch-to-hundreds-of-developers.html", "abstract": "Yelp uses Elasticsearch to rapidly prototype and launch new search applications, and moving quickly at our scale raises challenges. In particular, we often encounter difficulty making changes to query logic without impacting users, as well as finding client library bugs, problems with multi-tenancy, and general reliability issues. As the number of engineers at Yelp writing new Elasticsearch queries grew, our Search Infrastructure team was having difficulty supporting the multitude of ways engineers were finding to send queries to our Elasticsearch clusters. The infrastructure we designed for a single team to communicate with a single cluster did not scale to tens of teams and tens of clusters. Problems we Faced with Elasticsearch at Yelp Elasticsearch is a fantastic distributed search engine, but it is also a relatively young datastore with an immature ecosystem. Until September 2013, there was no official Python client. Elasticsearch 1.0 only came out in February of 2014. Meanwhile, Yelp has been scaling search using Elasticsearch since September of 2012 and as early adopters, we have hit bumps along the way. We have hundreds of developers working on tens of services that talk to tens of clusters. Different services use different client libraries and different clusters run different versions of Elasticsearch. Historically, this looked something like: Figure 1: Yelp Elasticsearch Infrastructure What are the Problems? Developers use many different client libraries, we have to support this. We run multiple version of Elasticsearch, mostly 0.90.1, 1.0.1 and 1.2.1 clusters. Multi-tenancy is often not acceptable for business critical clients because Elasticsearch cannot offer machine level resource controls and its JVM level isolation is still in development. Having client code spread over a multitude of services and applications makes auditing and changing client code hard. These problems all derive from Elasticsearch’s inevitably wide interface. Elasticsearch developers have explicitly chosen a wide interface that is hard to defend due to lack of access controls, which makes sense given the complexity they are trying to express with an HTTP interface. However, it means that treating Elasticsearch as just another service in a Service Oriented Architecture rapidly becomes difficult to maintain. The Elasticsearch API is continually evolving, sometimes in backwards incompatible ways, and the client libraries built on top of that API are continually changing as well, which ultimately means that iteration speeds suffer. Change is Hard As we scaled usage of Elasticsearch here at Yelp, it became harder and harder to change existing code. To illustrate these concerns let us consider two examples of developer requests on the infrastructure mentioned in Figure 1: Convert Main Web App to use the RequestsES Client Library This involves finding all the query code in our main web app and then, for each one: Create secondary paths that use RequestsES Setup RequestBucketer groups. Write duplicate tests. Deploy the change. Remove duplicate tests. We can make the code changes fairly easily but deploying our main web app takes a few hours and we have a lot of query code that needs to be ported. This would take significant developer time due to the amount of complexity involved in deploying our main web application. The high developer cost of changing this code outweighs the infrastructure benefits, which means this change is not pursued. Convert Service 4 to elasticsearch-py and move them to Cluster 4 Service 4’s SLA has become stricter and they can no longer tolerate the downtime caused by Service 1’s occasionally expensive facet queries . Service 4’s developers also want the awesome reliability features that Elasticsearch 1.0 brought such as snapshot and restore . Unfortunately, our version of the YelpES client library does not support 1.X clusters, but the official Python client does, which is ok because engineers in Search Infrastructure are experts in porting YelpES code to the official Python client. Alas, we do not know anything about Service 4. This means we have to work with the team that owns Service 4, have them build parallel paths, and tell them how to communicate with the new cluster. This takes significant developer time because of coordination overhead between our teams. It is easy to see that as the number of developers grows, these development patterns just do not scale. Developers are continually adding new query code in various services, using various client libraries, in various programming languages. Furthermore, developers are afraid to change existing code because of long deployment times and business risk. Infrastructure and operations engineers must maintain multi-tenant clusters housing clients with completely different uptime requirements and usage patterns. Everything about this is bad. It is bad for developers, infrastructure engineers, and operations engineers, and it leads to the following lesson learned: Systems that use Elasticsearch are more maintainable when query code is separated from business logic Our Solution Search Infrastructure at Yelp has been employing a proxy service we call Apollo to separate the concerns of the developers from the implementation details so that now our infrastructure looks like this: Figure 2: Apollo Key Design Decisions Isolate infrastructure complexity The first and foremost purpose of Apollo is to isolate the complexity of our search infrastructure from developers. If a developer wants to search reviews from their service, they post a json blob: {\"query_text\": \"chicken tikka masala\", \"business_ids\": [1, 2, 3] } to an Apollo url: apollo-host:1234/review/v3/search The developer need never know that this is doing an Elasticsearch query using the elasicsearch-py client library, against an Elasticsearch cluster running in our datacenter that happens to run Elasticsearch version 1.0.1. Validation of all incoming and exiting json objects using json-schema ensures that interfaces are respected and because these schemas ship with our client libraries we are able to check interfaces in calling code, even when that calling code is written in Python. Make it easy to iterate on query code Every query client is isolated in their own client module within Apollo, and each client is required to provide an input and output schema that governs what types of objects their client should accept and return. Each such interface is bound to a single implementation of a query client, which means that in order to write a non-backwards compatible interface change, one must write an entirely new client that binds to a new version of the interface. For example, if the interface to review search changes, developers write a separate module and bind it to /review/v4/search , while continuing to have the old module bound to /review/v3/search . No more “if else” experiments, just self contained modules that focus on doing one thing well. A key feature of per module versioning is that developers can iterate on their query client independently and the Apollo service is continuously delivered, ensuring that new query code hits production in tens of minutes. Each client can also be selectively turned off or redirected to another cluster if they are causing problems in production. As for language, we chose Python due to Yelp’s mature Python infrastructure and the ease in which consumers could quickly define simple and complicated query clients. For a high throughput service like Apollo, Python (or at least Python 2) is usually the wrong choice due to high resource usage and poor concurrency support, but by using the excellent gevent library for concurrency and the highly optimized json parsing library ujson , we were able to scale Apollo to extremely high query loads. In addition, these libraries are all drop-ins so clients do not have to design concurrency into their query logic, it comes for free. At peak load Apollo with gevent can do thousands if not tens of thousands of concurrent Elasticsearch queries on a single uwsgi worker process, which is pretty good compared to the single concurrent query that normal Python uwsgi workers can achieve. Make it easy to iterate on infrastructure Because the only thing that lives in Apollo is code that creates Elasticsearch queries, it is easy to port clients to new libraries or move their client to a different cluster in a matter of minutes. The interface stays the same and end to end tests ensure functionality is not broken. Another key capability is that from the start we designed these modules to implement a simple interface that is composable. This composable-first architecture has allowed us to provide wrappers like: SlowQueryLogger : A unary wrapper that logs any slow requests to a log for auditing and monitoring. Tee : A binary wrapper that allows us to make requests to two clients but only wait on results from one of them. This is useful for dark launching new clients or load testing new clusters. Mux : A n-ary wrapper that directs traffic between many clients. This is useful for gradual rollouts of new query code or infrastructure. As an example, let us assume there are two query clients which differ only in the client library they use and Elasticsearch version they expect: ReviewSearchClient and OfficialReviewSearchClient. Furthermore, let us say our operations engineer has just provisioned a new shiny cluster running Elasticsearch 1.2.1 that lives in the cloud and is ready to be load tested. An example composition of these clients within Apollo might be: # Create base clients\nPyESClient = ReviewSearchClient(query_timeout_s=0.300)\nOfficialClient = OfficialReviewSearchClient(query_timeout_s=0.300)\nOfficialClientCloud = OfficialReviewSearchClient(\n    service_name='elasticsearch-cloud',\n    query_timeout_s=0.300\n)\n \n# Compose base clients together\nComposedReviewSearchClient = SlowQueryLogger(\n    log_threshold_ms=500,\n    client=Tee(\n        toggle='toggles.reviews.enable_dark_launch',\n        client=Mux(\n            fallback=ClientWithToggle(\n                client=PyESClient,\n                toggle='toggles.reviews.use_pyes_client' # set to 0.50\n            ),\n            client_configs=[\n                ClientWithToggle(\n                    client=OfficialClient,\n                    toggle='toggles.reviews.use_official_client' # set to 0.50\n                )\n            ])\n        tee_client=OfficialClientCloud\n    )\n) This maps to the following request path: Figure 3: Life of a Request in Apollo In this short amount of Python code we achieved the following: If any query takes longer than 500ms, log it to our slow query log for inspection Send all traffic to a Mux that muxes between an old PyES implementation and our new official client implementation. We can change the Mux weights at runtime without a code push Separately send traffic to a cloud cluster that we want to load test. Do not wait for the result. Most importantly of all, we never had to worry about which consumers are making review search requests because there is a well defined interface that is well tested. Additionally, because Apollo uses Yelp’s mature Python service stack we have performance and quality metrics that can be monitored for this client, meaning that we do not have to be afraid to make these kinds of changes. Revisiting developer requests Now that Apollo exists, making changes goes from weeks to days, which means our organization can continue to be agile in the face of changing developer needs and backwards incompatible Elasticsearch versions. Let us revisit those developer requests now that we have Apollo: Convert Main Web App to use the RequestsES Client Library We have to find all the clients in Apollo that the Main Web App queries and implement their interfaces using the RequestsES client library. Then we wire up a Mux for each client that allows us to switch between the two implementations of the interface, deploy our code (~10 minutes) and gradually roll out the new code using configuration changes. From experience, query code like this can get ported in an afternoon . Having minute long deploys to production makes all the difference because it means that you can get multiple pushes to production in one day instead of one week. Also, because the elasticsearch query crafting code is separate from all the other business logic, it is easier to reason about and feel confident in changes. Convert Service 4 to elasticsearch-py and move them to Cluster 4 We can implement Service 4’s interface using the new client library, re-using existing tests to ensure functional equivalence between the two implementations. Then we set up a Tee to the new cluster to make sure our new code works and the cluster can handle Service 4’s load. Finally, we wait a few days to ensure everything works and then we change the query client to point at the new cluster. If we really want to be safe we can setup a Mux and gradually roll it over. This whole process takes a few days or less of developer time. Infrastructure Win Now that Yelp engineers can leverage Apollo, along with our real time indexing system and dynamic Elasticsearch cluster provisioning, they can develop search applications faster than ever. Whereas before Search Infrastructure was accustomed to telling engineers “unfortunately we can’t do that yet”, today we have the flexibility to support even the most ambitious projects. Since the release of Apollo just a few months ago, we have ported every major Yelp search engine running on Elasticsearch to use Apollo as well as enabled dozens of new features to be developed by other teams. Furthermore, due to the power of Apollo we were able to seamlessly upgrade to Elasticsearch 1.X for a number of our clients where prior to this that would have been nearly impossible given our uptime requirements. As for performance, we have found that the slight overhead of running this proxy have proved more than worth it in deployment, cluster reconfiguration, and developer iteration time, enabling us to make up for the request overhead by deploying big win refactors that improve performance. At the end of the day Apollo gives us flexibility, fast deploys, new Elasticsearch versions, performant queries, fault tolerance and isolation of complexity. A small abstraction and the right interface turns out to be a big win. Tweet Back to blog", "date": "2014-11-12"}, {"website": "Yelp", "title": "Yelp Hackathon 15: Sharks, Sock Puppets and Spectacular Projects", "author": ["\n        \n  Srivatsan S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/11/yelp-hackathon-15-sharks-sock-puppets-and-spectacular-projects.html", "abstract": "We recently held the 15th edition of Hackathon , a two-day event comprising of pure, unadulterated hacking, innovation, creativity and fun. Folks clustered together, hammering on their keyboards, scribbling on whiteboards, and some electrical engineers were plugging away on their breadboards. The kitchens were filled with delicious catered food, and fresh fruits and snacks lined up in the common spaces. We even had a school of remote-controlled sharks lurking around the office and taking over the meeting rooms. Those ominous-looking RC sharks do not seem to deter our intrepid hackers! Close to 60 great projects came out of the event from our offices in San Francisco, Palo Alto, Hamburg, and London - projects that ranged from cool data mining and visualizations, useful product features, funny utilities, and some serious hardcore hacking. Are they building a quadcopter to fight those sharks? Speaking of hardcore hacking, a team of engineers comprised of Cameron P., Ben B. and Kevin L. designed an 8-bit ISA (Instruction Set Architecture) with a 16-bit stack-based math co-processor. They implemented an emulator for the architecture and also designed/built an assembler to target it. With their assembler they built an interpreter for Brainfuck , a popular esoteric and Turing-complete language. To top it all off, they wrote their emulator and assembler in Go. Their project idea echoes a collective sentiment that runs through the fabric of our engineering team: taking on a hard challenge! Could they design a useful ISA in the constrained space of 8-bit fixed width instructions? If they designed it could they actually build it? And if they built it could they make a non-trivial program to prove it was useful? For them, it was fun just trying to accomplish all of that. The Creative team at Yelp came up with a fun Yelp branding campaign for the Hackathon, complete with a set of mascots and a selfie station. They seem to have had some high-profile guests that day. Like most of our Hackathons in the past, this one resulted in a whole array of projects focussed on Yelp data. Why? Well, we have a fantastic dataset generated by a passionate and dedicated community of Yelpers around the world. A team of data scientists and product managers comprised of Farid H., Natarajan S. and Daniel F. built a pretty cool tool that allows users to explore a city by its neighborhoods. Say you’re a tourist visiting San Francisco and want to dine in a touristy neighborhood with fancy restaurants. Or maybe you are a budget traveller and want to check out the local nightlife scene. All you need to do is to tweak the filters on this nifty tool, and voila! Touristy neighborhoods with fancy restaurants! Local favorites for the budget traveler! To build this tool, the team wrote an algorithm to compute different attribute scores. For example, this algorithm would compute a tourist_score by analyzing the percentage of reviews for businesses in a given neighborhood written by someone who is not from that city. As engineers, the words reliability and performance are near and dear to our hearts, so much so that some of us use Hackathon as a time to dive into our codebase and explore ways to speed things up. A team of mobile developers comprising of Alex H., Mason G. and Ben A. did just exactly that with some serious iOS hacking. One of the most complex views on the Yelp iOS app is the business view, the page that lets you learn all about the business you searched for. A lot of the complexity comes from all of the features it has which includes rendering maps, reviews, review highlights, and a swipe-able photoview, just to name a few. How could they make this faster? Well, they figured out that the bottleneck was due to large amounts of both layout and CPU-bound drawing. The team rebuilt the entire view with collection cell automatic sizing and caching (which is available on iOS 8 and has a fallback available for iOS 7 users). This gave them incremental layout and rendering instead of full view layout and rendering, which sped things up! They modernized the view to be fully compatible with future screen size changes by using Auto Layout instead of frame based layouts. Even though an analogous Auto Layout view is slower than a similar frame based layout, they still saw large performance improvements from the incremental rendering and rewritten views. These changes dropped rendering times and cut memory usage by roughly 50%. Our hackers showing off their projects in a science-fair style exhibition Good job, everyone! Or as Darwin would say with staunch approval, “Woof! Woof!” Thinking about that next killer Hackathon idea, aren’t you? Check out our exciting product and engineering job openings at www.yelp.com/careers and apply today. Hackathon 16 isn’t too far off! Tweet Back to blog", "date": "2014-11-26"}, {"website": "Yelp", "title": "December Events At Yelp", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/12/december-events-at-yelp.html", "abstract": "While we’re all still recovering from our Thanksgiving-induced food comas, we wanted to share some of our upcoming events for December! This Thursday, some of our engineers will be giving lightning talks at the Women Who Code meetup so make sure to come! We’re also very excited to be sponsoring the Women Who Code Holiday Party in Waterloo Ontario on December 11th. If you’re in the area, RSVP and stop by! For the rest of the week (through December 5th), Yelp will also be matching donations to Women Who Code (up to $5,000 total). Please consider donating and helping them connect women in tech. Wednesday, December 3, 2014 - 6:15PM - Lightning talks and High Performance Python by Raymond Hettinger ( Python ) Thursday, December 4, 2014 - 6:30PM - Lightning Talks ( Women Who Code ) Thursday, December 11, 2014 - 7:00PM - Experiences Everywhere: Designing for a Multi-Device World // Holiday Party ( Designers + Geeks ) Wednesday, December 17, 2014 - 6:15PM - Advanced JS: D3.js Workshop ( Girl Develop It ) Tweet Back to blog", "date": "2014-12-03"}, {"website": "Yelp", "title": "Learning to Rank for Business Matching", "author": ["\n        \n  Rui W., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/12/learning-to-rank-for-business-matching.html", "abstract": "At Yelp, we solve a lot of different information retrieval problems ranging from highlighting reviews for businesses to recommending nearby restaurants. We recently improved one system called business matching, moving from a naive ranking function to a machine learned approach using Learn to Rank . The Business Matching Problem Business matching is a system that accepts a description of a business (e.g. name, location, phone number) and returns the Yelp businesses that description best fits. We can use this in several different contexts. For example, in 2012 Yelp announced a partnership spearheaded by the City of San Francisco to allow municipalities to publish restaurant inspection information to Yelp. The data feed shared by the City of San Francisco contained restaurants with business pages on Yelp but we didn’t know the exact business IDs needed to correctly update their information. Originally, we had a number of similar ad-hoc implementations across different engineering teams but, as we grew, concerns like codebase complexity, testability and maintenance costs, as well as matching quality started coming up. To solve these problems, we built a unified system to replace all the existing ad-hoc implementations. The system, similar to other information retrieval systems, takes a semi-structured description of a business (e.g. name, location, phone number) as input, searches through our database and returns a ranked list of Yelp businesses with scores that measure how good they match the description. Architecture We use Elasticsearch a lot at Yelp . It should come as no surprise that here we use it as our core engine. Originally, the system architecture looked fairly simple: it normalized the input, built a big Elasticsearch query, searched the index, got the results, and did some filtering. In order to get the most relevant results, we needed to leverage information included in the input for each component. Components can be things like “name,” “location text,” “geo-distance” “phone number,” etc., which meant that each query we sent to Elasticsearch contained subqueries for each component. For example, we could build a name subquery that matches business names using TF-IDF scoring logic and a distance subquery that gives businesses higher scores if they are closer to the input location. The final query logic would linearly combine scores for each subquery or component and output a final score for each business candidate. To measure the effectiveness of the system, we built an evaluation framework for it. A sample of some past business matching requests were pulled and we manually labeled the returned businesses as correct or incorrect matches. These results created the “gold dataset” and the system was then evaluated by running against this dataset and recording whether it returns the correct matching businesses for each request. Some standard information retrieval metrics like Precision/Recall/F1 are used to measure the matching quality. Addressing The Downsides The system worked fairly well but there’s always room for improvement. As you can imagine, there are many things we need to balance in the scoring logic here: How much weight should we give for address text matching? Is phone number a good indicator for matching? Is linearly combining each component score a good solution? Originally, we addressed these questions by doing ad hoc experiments manually. We would poke around with different ideas, changing values and seeing if the F score improved. If we found a change where the evaluations looked good, we would push this change. However, performing these manual ad-hoc experiments is expensive and time-consuming. To make things even harder, different clients may want to use the business matching system to solve different problems. Each client might want a slightly different ranking logic. Combining this time consuming task with the expense of ad-hoc manual parameter tuning led to a large amount of human time being spent on tuning the system. We reached a point where we felt that improving ranking logic by running ad-hoc experiments manually was reaching a bottleneck. In order to optimize the system further we wanted to have some automated, algorithmic solution. Learning to Rank Learning to Rank is a method of automatically creating a ranking model by using machine learning. In our business matching problem, we wanted to build a machine learning model that would optimize our ranking algorithm in an automatic and systematic way. How it works Given the nature of our problem, we adopted the “ Pointwise Approach ,” which approximates matching businesses as a regression problem. More specifically, for each candidate business we want the machine learning model to predict a numeric score, which serves as the estimation for how good this business matches the input description. As for features, we included the scores for each component returned from Elasticsearch. Some additional features reflect the original ranking given by Elasticsearch are included as well. We generated the training data by running regular evaluation on the gold dataset and recording relevant information returned from Elasticsearch. Putting all of this together, we now have our new architecture. We still construct the query to Elasticsearch and get results containing a list of candidate businesses with scores for each component/subquery. But instead of linearly combining them, we throw them into the trained rescoring model and, finally, a ranked list of rescored business candidates is returned. Essentially, for this system we’re using Elasticsearch (which is really good at getting a pool of potential relevant candidates) as a “recall engine”. We extract the core ranking logic out of Elasticsearch and use the more powerful machine learning algorithms to achieve better matching quality. Improvements Instead of searching for a set of optimal parameters of a linear ranking function manually, the learning algorithm makes this optimization process flexible. Now the ranking function can be either linear or non-linear by applying different learning algorithms with the parameters of the ranking function being learned in an automatic and systematic way. Moreover, since we are frequently updating the database, the TF/IDF and other scoring factors change over time. The new machine learning ranking model provides certain stability on top of Elasticsearch. Our evaluation results showed that our new learning to rank approach boosted F1 score from 91% to 95%. With these improvements, we can treat our business matching system as a general business retrieval system framework that can be configured for new problems or clients, solving a much broader set of problems. For example, we are currently working on improving our business deduplication system, which discovers and automatically merges duplicate businesses in our data. To use business matching system we just need to retrain the machine learning model with a modified set of features on a different training set. By using machine learning to approach our business matching problem, our retrieval system’s matching quality significantly improved and also became more flexible, more stable and more powerful. Tweet Back to blog", "date": "2014-12-10"}, {"website": "Yelp", "title": "Yelp Sponsors Women Who Code (WWCode)", "author": ["\n        \n  Michael, VP of Engineering\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/09/yelp-sponsors-women-who-code-wwcode.html", "abstract": "We’re happy to announce that we are one of the first official sponsors of Women Who Code ! WWCode is an organization whose goal is to help women excel in technology careers. WWCode and Yelp started working together three years ago when the meetup group was created. We’ve hosted many of their events ranging from Ruby workshops to discussion panels including CEOs and CTOs. Since their launch, WWCode has grown to 14,000 members across 14 countries worldwide. By sponsoring their new non-profit (as of this July!), we’re excited and happy to help them achieve their goals of expanding into 50 cities worldwide by 2015 with 1 million members by 2019. As WWCode expands, they’re looking to reach out to top technical universities around the nation in order to introduce women to engineering at a younger age. We will be partnering with them at their first pilot university, Waterloo, this fall. “Yelp has supported most of Women Who Code’s major events over the past three years,” said Alaina Percival, WWCode CEO. “Collaborating with Yelp will be key in reaching our goal of being in 50 cities by the end of the year.” Want to get involved? You can help support WWCode by attending one of their upcoming events listed here: Grace Hopper Practice Talks September 23, 2014 Doors open: 6:30PM Yelp HQ Women Who Code Fundraiser - Applaud Her October 23, 2014 Doors open: 6:00PM Zendesk HQ Tweet Back to blog", "date": "2014-09-10"}, {"website": "Yelp", "title": "Hackathon 14: Puzzles, Pizza and Projects Galore!", "author": ["\n        \n  Srivatsan S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/09/hackathon-14-puzzles-pizza-and-projects-galore.html", "abstract": "Three times a year the entire engineering team at Yelp gets together and does innovative (sometimes crazy) things like launching a 3D printer into space or flying a quadcopter with human wings…err…arms or figuring out whether Cronuts are more popular than Donuts . We call this tri-annual event…Hackathon! It’s a festival celebrating innovation, creativity, and technical badassery where our smart, talented and witty engineers get 48 hours to work on anything they like. Needless to say, a relentless supply of delicious food also plays a key role in this event. Our hackers showing off their projects in a science fair style exhibition The 14th version of our Hackathon, which was held this past month, saw around 80 projects across all of our engineering offices, covering a wide variety of topics ranging from mining our rich dataset to developing visualization tools to building robots. Sometimes our engineers de-stress by attempting to put together ridiculously hard monochromatic jigsaw puzzles custom created with an inside joke Shahid C., one of our intern extraordinaires this summer, worked on a project that he calls “Yelp Boost,” a nifty visualization tool that tries to address the age-old economics question of supply and demand. Shahid echoes what sounds like a fundamental tenet of Yelponomics 101, “If we can figure out where the demand for a product greatly outweighs supply, we could recommend business owners to set up their shops in those locations in order to meet this demand and boost their sales!” To determine these supply and demand logistics, Shahid dug deep into our search logs and came up with real time visualizations that look like this: The heat map (the light blue to intense red) represents an increasing demand for pizza, while the red dots with green halos represent pizzerias in San Francisco. You see those big red blobs with dropped pins inside them? There is a high demand for pizza there, but unfortunately, there aren’t many pizzerias nearby. Hmm…wonder what could be done about that. Did I mention that we also built robots during Hackathon 14? Apart from the physical ones that could roam around and shoot nerf darts at you, a team of engineers, Aditya M., Anthony M., Jon M. and Kurtis F., built a different kind of robot - a robot that tries to understand Yelp the same way traditional web crawlers do. It’s affectionately called BotBot, it’s a web crawler that shows our engineering team how crawlers like Googlebot, Yahoo Slurp, and Bingbot discover and index our content. The team created this useful simulation by using Scrapy to crawl the site, pull out links, and used selenium to process pages that had javascript content. Pretty cool, eh? Have the creative engineering gears in your brain started turning? Check out our exciting product and engineering job openings at www.yelp.com/careers and apply today! Who knows, you may be showing off your killer idea at Yelp Hackathon 15. Tweet Back to blog", "date": "2014-09-18"}, {"website": "Yelp", "title": "Intern Project: Real-Time Log Tailing with Franz, our Kafka Wrapper ", "author": ["\n        \n  Qui N., Engineering Intern\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/09/intern-project-real-time-log-tailing-with-franz-our-kafka-wrapper.html", "abstract": "At Yelp, we often need to analyze data in real time. Right now most of our data is aggregated and logged using a system called Scribe . This means that real-time processes currently depend on tailing Scribe, reading messages as they are added to the end of the logs. Simply tailing the logs means that any time a tailer is not running, due to maintenance or an unexpected error, it misses all the information logged during that time. We want a more flexible system that allows processes to pick up where they left off, so our analyses account for all of the data available. In comes Kafka , a different kind of logging system. One of the big differences from Scribe is that Kafka provides you with the ability to start tailing from any point in the log, allowing past messages to be reprocessed if necessary. We’re already starting to use Kafka for some applications at Yelp, and all the data being written to Scribe is also available from Kafka. One thing has been stopping us from switching to Kafka for real-time processing: the lack of a simple, efficient way to tail Kafka. What we do have is an internal service with an HTTP API for interacting with our Kafka clusters, named Franz. As an HTTP service, though, it can only respond to requests, not initiate communication. That means a client tailing Kafka with this API must poll continuously to see if new messages have been written. In addition, Franz’s current API requires clients to keep track of low-level, Kafka-specific details about their position in the logs, further inhibiting adoption. This summer, I worked on adding a high-level WebSocket endpoint to Franz, designed to improve the Kafka tailing experience. WebSocket is a relatively new internet protocol that lets clients and servers establish long-lived, two-way connections with each other. With such an endpoint, a client could simply start one WebSocket connection with Franz to start tailing Kafka. As messages became available, Franz could then use the connection to forward the messages, without any further action from the client. This endpoint also manages the position of clients, so that reconnecting clients automatically start at the position they left off at. Because this is the first time we’re using WebSocket at Yelp, I had a lot of freedom in the implementation. The existing parts of Franz were implemented in Java with Dropwizard , but Dropwizard is only designed for regular HTTP endpoints. Ultimately, I decided to use Atmosphere , a Java framework that supports WebSocket, and added an Atmosphere servlet to the Dropwizard environment. Using the endpoint is fairly straightforward: you can use a Python client like ws4py , tornado.websocket , or even a Chrome extension to establish a connection to the WebSocket endpoint. Once you’re connected, you send a topic message {\n    \"topic\": \"service_log\",\n    \"batch_size\": 20\n} where the batch_size specifies how many Kafka messages you want, max, per message from Franz. Franz will then start streaming messages back to you, max batch_size at a time. A response from Franz is pretty simple, it’s just an object containing an array of messages with some metadata on them: {\n\n    \"messages\": [\n        { \"topic\": \"<topic_name>\", \"partition_id\": <partition_id>, \"offset\": <offset>, \"message\": <json_dict>},\n        ...\n    ]\n} We’re currently working on wrapping up the project and many people and teams at Yelp are excited to use it. The first user will likely be the real-time ad metrics display. Since most Yelp applications are written in Python, there is also a Python client in progress to facilitate service use of the new interface. I’m looking forward to my project being used across the organization! Tweet Back to blog", "date": "2014-09-29"}, {"website": "Yelp", "title": "Yelp At Grace Hopper", "author": ["\n        \n  Jen W., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/10/yelp-at-grace-hopper.html", "abstract": "Yelp is happy to announce that we’re headed to the Grace Hopper Celebration of Women in Computing conference in October! Admiral Grace Hopper was a pioneer in computer science. She was a driving force behind the development of English-based programming languages and an inspiring example of how intelligence and perseverance can overcome technical and cultural adversity. The Grace Hopper conference, presented by the Anita Borg Institute and ACM , is the largest gathering of women in computing in the world. Its mission to “connect, inspire, and guide women in computing and organizations that view technology innovation as a strategic imperative” is a perfect tribute to the incredible Grace Hopper. Yelp’s delegation this year is composed of engineers from a diverse selection of teams, from mobile to frontend to backend to infrastructure. We’re looking forward to meeting tech women from all over the world to swap stories, experience, and expertise. We’re also excited about this year’s talks. Our ‘must see’ lists include: “ New Managers - What’s your challenge? ” “ Web Services: Grow, Refactor, Rebuild ” “ Male Allies Plenary Panel ” “ Volunteering to Promote STEM education (K-12) ” and many more, of course! If you’d like to learn more about life as a Yelp Engineer, have great ideas about how to make our app better, want to know what’s up with all those dog photos , or have the urge to shoot the technological breeze, come find us - we’d love to chat! We’ll be wandering around the conference and you can also come find us at booth 532 in the Career Fair. Speaking of the Career Fair, we will be accepting resumes there as well as online. If you’re attending Grace Hopper, here’s a Super Special Grace Hopper link . The page might say New Grad, but we’re happy to take all applications through this special link. Stay awesome, keep coding, and see you in Phoenix! Tweet Back to blog", "date": "2014-10-01"}, {"website": "Yelp", "title": "October Events at Yelp", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/10/october-events-at-yelp.html", "abstract": "October will be a busy month for all of us, with a lot of great conferences and events happening back-to-back. We start the month off with an SF Python meetup in collaboration with PyLadies. Zach Musgrave will be presenting on performance profiling in production. Later this month, Scott Clark will be presenting on MOE for SF Machine Learning here at Yelp HQ too! If you’re not familiar with MOE, it’s our black box optimization engine to help you with real world metric optimization. We’ll also be at Grace Hopper on October 8-10, StrataConf on October 15-17, and at HTML5DevConf on October 20-24. Michael Stoppelman will be presenting at StrataConf on Yelp’s Data Pipeline and Ken Struys will be presenting at HTML5DevConf on Scaling Selenium Testing at Yelp so make sure to catch both of those! Events happening at Yelp HQ Wednesday, October 8, 2014 - 6:15PM - Python in Handling E-Crime and Massive Web Requests in Production ( SF Python ) Thursday, October 9, 2014 - 6:00PM - Android Tour plus Hands-On Session ( Learning Android Development ) Wednesday, October 15, 2014 - 6:00PM - Introducing the Metric Optimization Engine (MOE) ( SF Machine Learning ) Thursday, October 16, 2014 - 6:30PM - Designing for Longevity ( Designers + Geeks ) Wednesday, October 22, 2014 - 6:45PM - How Product Managers Make Your Business Scale ( Products That Count ) Thursday, October 30, 2014 - 6:00PM - Large-scale Linear Classification: Status and Challenges by Prof. C.J. Lin ( SF Machine Learning ) Conferences Yelp is attending Grace Hopper - October 8-10 @ Phoenix Convention Center StrataConf - October 15-17 @ Javits Center HTML5DevConf - October 20-24 @ Moscone Center Tweet Back to blog", "date": "2014-10-06"}, {"website": "Yelp", "title": "Using MOE, the Metric Optimization Engine, to Optimize an A/B Testing Experiment Framework", "author": ["\n        \n  Dr. Scott Clark, Ad Targeting Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/10/using-moe-the-metric-optimization-engine-to-optimize-an-ab-testing-experiment-framework.html", "abstract": "A/B Testing Experiment Frameworks and MOE We recently open sourced MOE, the Metric Optimization Engine , a machine learning tool for solving global, black box optimization problems. An example application for such a system is optimally running online A/B experiments. A/B testing segments the users that come to a site into buckets, or cohorts, and show different versions of the site to different cohorts of users. One can show 50% of users one version of a site (version A) and 50% of users another version of a site (version B). After some amount of time we can see which version of the site performs better on various metrics (like Click Through Rate (CTR), conversions, or total revenue) and shift all of your traffic over to the better version. This can be repeated as we converge towards the best version of the site under the metric(s). It can take a long time to attain statistically significant results for experiments. We want to know with high confidence that one version of the site is better than another. Depending on user traffic it can take days, or even weeks, to run a single iteration. These experiments are also expensive; by showing a suboptimal version of your site, you are sacrificing potential revenue. There is also an opportunity cost associated with creating, developing, and running any particular experiment. So the fewer experiment iterations to get to the optimum, the better. Furthermore, many A/B tests are merely parameter selection problems (an A/A’ test, where a feature stays the same, and only the underlying parameters change). Given a feature, we want to find the optimal configuration values for its parameters as quickly as possible. MOE was designed to optimally solve problems like this, when we want to find an optimal set of parameters (inputs) of a function (a metric, like CTR) when sampling this function is time consuming or expensive (like running an A/B test experiment on live traffic). By leveraging MOE we can build a general experiments platform that automatically promotes good cohorts and pushes traffic away from under-performing cohorts, then replacing them with new, winning configurations. Figure 1: MOE can be used to optimally assign traffic allocations in an A/B testing framework and to suggest new parameters to sample given the historical values already sampled. Using Multi-Armed Bandits for Optimal Traffic Allocation Instead of naively allocating traffic in some uniform way (like a 50/50 split) we can use the information about how an individual cohort is performing to dynamically and optimally allocate the best percentage of traffic to it. This is fundamentally a tradeoff between exploration and exploitation. Exploration is equivalent to gaining more knowledge about the system by continuing to allocate traffic to a wide variety of points. Exploitation on the other hand, is using the knowledge we have already gained to get the most expected return out of the system as we currently understand it. Others have shown that using Multi-Armed Bandits can achieve better results, faster, in online A/B tests. MOE has many different bandit policies implemented and allows the user to select a policy that best fits their desired trade-off between the exploration and exploitation of the data. By using a Multi-Armed Bandit approach we can limit the amount of suboptimal traffic allocation in an A/B test, leaving as few gains on the table as possible. Using Bayesian Global Optimization to Suggest Parameters As the Multi-Armed Bandit system starts lowering the traffic allocation for a certain experiment we have two choices. We can turn off that cohort, and allow the remaining cohorts to battle it out until we have one clear winner (within whatever confidence we desire). Alternatively, we can have MOE suggest a new cohort using Bayesian Global Optimization (with a new set of underlying parameters) for the system to try. This allows the system to play an internal game of king-of-the-hill, where better and better parameters are being sampled as the objective continues to rise higher and higher. MOE uses the historical data gained so far to decide which points to sample next. It suggests new parameters to try that have the highest Expected Improvement under a Gaussian Process model. These are the parameters that are expected beat the current best set of parameters by the most, using whatever metric we provide as the objective function . This method converges much faster to global optima than other heuristic methods like grid search, random search or basic hill climbing. By constantly spinning down underperforming cohorts in our experiment and replacing them with optimal new cohorts we allow our experiment framework to continually search the underlying parameter space for the best possible parameters. Putting it all Together Let’s assume that we want to optimize some underlying parameter that affects our ad Click Through Rate (CTR) in some way. This parameter could represent anything from a threshold in our system to some hyperparameter to a feature in our ad targeting system. Let’s say we currently are running with this parameter set to the value 0.2 in production, our status quo . We would like to run an experiment where we sample other values of parameter in an attempt to maximize CTR. We will need to run time consuming and expensive experiments every time we want to measure CTR, so we will use MOE to help us find the optimal value in as few samples as possible. We simulate the observed CTR of the system for 1 million users by sampling from a Bernoulli distribution with the given true CTR. In a real world system we do not know exactly how the underlying parameter affects CTR, which is why we need to sample various values with real traffic to find the optima. For the purposes of this example, we will define the exact function of CTR with respect to the underlying parameter for simulation purposes, keeping it hidden from MOE. Note that the function need not be convex or even continuous. Figure 2: The graph of the true CTR vs the underlying parameter. There is a local CTR maximum at 0.143 and a global CTR maximum at 0.714. We begin the experiment by asking MOE for 2 new parameter values to sample, in addition to our current status quo value of 0.2. We will set our objective function for MOE to optimize to be the relative gain over the status quo CTR. We note that this gives our status quo parameter a value of 0, and any parameter that has a higher CTR than the status quo will have a positive value, while any parameter that has a lower CTR will have a negative value. Our initial representation of the underlying system, and the Gaussian Process , looks like Figure 3. Figure 3: The blue (mean) and green (variance) plot is the initial Gaussian Process representation of the space that MOE optimizes. The dashed gray line is the true, unknown objective function, the relative CTR gain vs status quo. MOE suggests two points as the initial parameters to test. These will be the corresponding parameters for cohort 1 and cohort 2 respectively. Initially we will conservatively allocate new cohorts to have 5% of traffic each, using the epsilon greedy bandit policy with epsilon set to 0.15. The remaining 90% of traffic will go towards the best parameter value observed so far. Once we have the simulated data we can determine which cohorts have performed well and which ones should be turned off. We turn off any cohort that has a CTR more than two standard deviations worse than the current best observed CTR. We then query MOE for new, optimal parameters to sample given how all historical parameters have performed. This is repeated multiple times as poorly performing parameter values are culled and new parameter values are suggested by MOE. See Figure 4 for the evolution of the underlying Gaussian Process and watch MOE converge to the globally optimal parameter value. Figure 4: The new points be sampled each day (red x) and previous points (blue x) influence the Gaussian Process representation of the relative CTR gain that MOE uses to optimize the underlying parameter. We can see the Gaussian Process getting more accurate with respect to the true function (dashed gray) as more points are sampled. By iterating to the global optimal parameter value in as few samples as possible, we can increase the system CTR far beyond the initial status quo value, or even the local maxima. In this example, by the second set of sampled parameters MOE is beating both the status quo CTR value and the CTR the best local parameter value, achieving the best global parameter value within the third set of sampled points. Figure 5: The simulated CTR during for each round of sampled parameter values. Initially the system CTR is equal to the status quo CTR. MOE quickly finds better and better values of the underlying parameter, converging to the global optima in 3 short sets of samples. Using MOE in your experiment framework All of the code used to generate this example can be found here . Other examples of using MOE can be found here . The Multi-Armed Bandit portion of MOE is documented here . The Bayesian Global Optimization components are documented here . Any issues can be reported in the issue tracker . Questions or comments can be sent to opensource+moe@yelp.com . Tweet Back to blog", "date": "2014-10-09"}, {"website": "Yelp", "title": "Introducing Pyleus: An Open-source Framework for Building Storm Topologies in Pure Python", "author": ["\n        \n  Patrick L., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/10/introducing-pyleus.html", "abstract": "Yelp loves Python, and we use it at scale to power our websites and process the huge amount of data we produce. Pyleus is a new open-source framework that aims to do for Storm what mrjob , another open-source Yelp project, does for Hadoop: let developers process large amounts of data in pure Python and iterate quickly, spending more time solving business-related problems and less time concerned with the underlying platform. First, a brief introduction to Storm. From the project’s website , “Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing.” A Pyleus topology consists of, at minimum, a YAML file describing the structure of the topology, declaring each component and how tuples flow between them. The pyleus command-line tool builds a self-contained Storm JAR which can be submitted to any Storm cluster. When it comes to massive data processing demos, “word count” is a classic. Since Storm operates on “unbounded streams of data”, we can’t calculate a total count for each unique word, since the input stream could continue indefinitely. Instead, our topology will maintain a monotonically increasing counter for each unique word, and log the value each time we see it. So how can you build a word count Storm topology using Pyleus? All you need is a pyleus_topology.yaml and a few Python components. For a simple demonstration, you need to know about the three core concepts in Storm: A tuple is the unit of data within a Storm topology, flowing into and out of processing components. Spouts are components that feed tuples into a topology. Usually, a spout consumes data from an external source, like Kafka or Kinesis , then emits records as tuples. Bolts subscribe to the output streams of one or more other spouts and bolts, do some processing, then emit tuples of their own. This topology has three components: A spout that emits a random line of “lorem ipsum” text, a bolt that splits lines into words, and bolt that counts and logs occurrences of the same word. pyleus_topology.yaml\nword_count/\n    __init__.py\n    line_spout.py\n    split_words.py\n    count_words.py Here are the contents of pyleus_topology.yaml : name: word_count\n\ntopology:\n\n    - spout:\n        name: line-spout\n        module: word_count.line_spout\n\n    - bolt:\n        name: split-words\n        module: word_count.split_words\n        groupings:\n            - shuffle_grouping: line-spout\n\n    - bolt:\n        name: count-words\n        module: word_count.count_words\n        groupings:\n            - fields_grouping:\n                component: split-words\n                fields:\n                    - word The spout configuration is self-explanatory, but the bolts must indicate the tuple streams to which they are to subscribe. The split-words bolt subscribes to line-spout with a shuffle_grouping —this means that tuples emitted from line-spout should be evenly and randomly distributed amongst all instances of split-words , be there one, five, or fifty. count-words , however uses a fields_grouping on the ‘word’ field. This forces all tuples emitted from split-words with the same value of ‘word’ to go to the same instance of count-words . This allows the code in word_count.count_words to make the assumption that it will “see” all occurrences of the same word within the same process. word_count/line_spout.py : import random\n\nfrom pyleus.storm import Spout\n\nLINES = \"\"\"\nLorem ipsum dolor sit amet, consectetur\nadipiscing elit. Curabitur pharetra ante eget\nnunc blandit vestibulum. Curabitur tempus mi\n...\nvitae cursus leo, a congue justo.\n\"\"\".strip().split('\\n')\n\n\nclass LineSpout(Spout):\n\n    OUTPUT_FIELDS = [\"line\"]\n\n    def next_tuple(self):\n        line = random.choice(LINES)\n        tup = (line,)\n        self.emit(tup)\n\n\nif __name__ == '__main__':\n    LineSpout().run() word_count/count_words.py : from collections import defaultdict\n\nfrom pyleus.storm import SimpleBolt\n\n\nclass CountWordsBolt(SimpleBolt):\n\n    def initialize(self):\n        self.words = defaultdict(int)\n\n    def process_tuple(self, tup):\n        word, = tup.values\n\n        self.words[word] += 1\n\n        msg = \"'{0}' has been seen {1} times\\n\".format(word, self.words[word])\n        with open(\"/tmp/word_counts.txt\", 'a') as f:\n            f.write(msg)\n\n\nif __name__ == '__main__':\n    CountWordsBolt().run() word_count/split_words.py is left as an exercise for the reader. (Or, you could just check out the full example on GitHub) Now, running pyleus build in this directory will produce a file word_count.jar which you can submit to a Storm cluster with pyleus submit , or run locally for testing with pyleus local . The code for a Pyleus topology can be very simple, but one feature in particular we are excited about is built-in virtualenv integration. Simply include a requirements.txt file alongside your pyleus_topology.yaml , and pyleus build will build a virtualenv that your code can use and embed it in the JAR. You can even re-use packaged Pyleus components, and refer to them directly in pyleus_topology.yaml ! Another team at Yelp has already developed a Pyleus spout for consuming data from an internal source and built a Python package for it. Now, others within the company can add one line to their requirements.txt and use the spout in their pyleus_topology.yaml without writing a single line of code. Pyleus is beta software, but feedback and pull requests are happily accepted. Get started using Pyleus by installing it with pip install pyleus , then check out the source on GitHub for more examples. Tweet Back to blog", "date": "2014-10-15"}, {"website": "Yelp", "title": "Scaling Traffic from 0 to 139 Million Unique Visitors", "author": ["\n        \n  Michael, VP of Engineering\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/10/scaling-traffic-from-0-to-139-million-unique-visitors.html", "abstract": "At LAUNCH Scale last week, I gave a talk to over 75 co-founders (CEOs and CTOs) on how we’ve scaled traffic here at Yelp. It brought back memories of Darwin biting through our ethernet cable and reminded me of the run up to our IPO, making sure we had enough capacity to handle the expected surge in traffic from the world’s press (and more recently, the launch of Yelp in Hong Kong! ). For close to 8 years, I’ve had the privilege to work alongside some of the best engineers in the world and have seen the meticulous work and thought it takes to scale a site to serve over a hundred million unique visitors. I joined Yelp in early 2007 as a software engineer coming from Google, where I had spent the previous 4 years. On my very first day I was handed the search engine and asked to “improve it.” At the time we were handling approximately 200,000 searches per day and of those searches 86,400 of them were from load balancer doing health checks! We had one primary database running MySQL with a couple of replication slaves running a mix of InnoDB and MyISAM tables (side note: MyISAM isn’t great when your databases hard fail). We were using Apache without gzip enabled (pro tip: enable it!) and our data science toolkit was: cat, grep, wc, gnuplot, and awk. Because Yelp was born pre-AWS, we had to operate our own data centers. As our traffic grew, our infrastructure had to scale with it. We started using a CDN to host our static content. The one MySQL database with slaves was scaled by vertically sharding it, moving tasks that were write heavy and non-user-interactive to different databases (e.g. clicks/impression/phone calls for advertisers). Another high-impact win our team had for improving database performance was moving our hosts to FusionIO PCIe cards as our primary storage. On the data center side, our operations team moved from having one data center to having many. Given our traffic make-up, we decided that our new data centers would be “read-only” and that we would have a separate primary data center where all writes happen. This made scaling our read only traffic much more straightforward. We now had data centers closer to users, allowing us to use DNS to geographically load balance our traffic, making the experience faster for users. We’ve also been able to leverage Amazon EC2 using AWS Direct Connect , which allows our engineering teams to bring up hardware whenever they need. It’s been awesome removing the hardware barrier for getting to production. As our traffic scaled, our logging infrastructure needed to keep up as well. We started off using syslog-ng and rsync to handle logs stored on a NFS server and lots of disks. In October 2008 we moved to using scribe (now a custom branch), which has served us very well over the past 5+ years that we’ve been using it. We take the logs scribe aggregates and move them into Amazon S3 for storage, which makes using EMR on AWS seamless. This is why in 2009 we open sourced mrjob , which allows any engineer to write a MapReduce job without contending for resources. We’re only limited by the amount of machines in an Amazon data center (which is an issue we’ve rarely encountered). Real-time analytics are much better than periodically run batch jobs, so recently we open sourced Pyleus which allows anyone to write Storm topologies using Python. Another powerful tool we created, called MOE , allows anyone to optimize the parameters of any function (e.g. ad or search ranking functions). We’re extremely proud of our open source contributions and hope to have many more in the future. For the past 10 years, the Yelp Engineering team has worked really hard to build a scalable architecture with the ability to develop and push code to the site multiple times a day. For those of you who enjoy working on these types of problems, make sure to check out our careers page . We’re always hiring! If you’d like to know more on how we scaled the site, check out the slides from my presentation at LAUNCH Scale and feel free to reach out to me on Twitter at @stopman . Tweet Back to blog", "date": "2014-10-31"}, {"website": "Yelp", "title": "The Yelp Dataset Challenge Goes International! New Data, New Cities, Open to Students Worldwide!", "author": ["\n        \n  Dr. Scott Clark, Ad Targeting Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/08/the-yelp-dataset-challenge-goes-international-new-data-new-cities-open-to-students-worldwide.html", "abstract": "The Challenge The Yelp Dataset Challenge provides the academic community with a real-world dataset over which to apply their research. We encourage students to take advantage of this wealth of data to develop and extend their own research in data science and machine learning. Students who submit their research are eligible for cash awards and incentives for publishing and presenting their findings. The most recent Yelp Dataset Challenge (our third round) opened in February 2014, giving students access to our Phoenix Academic Dataset, with reviews and businesses from the greater Phoenix metro area. In the fourth round, open now, we are expanding the dataset to include data from four new cities from around the world. We are also opening up the challenge to international students, see the terms and conditions for more information. New Data We are proud to announce that we are extending the popular Phoenix Academic Dataset to include four new cities! By adding a diverse set of cities we hope to encourage students to compare and contrast the different aspects of each city and find new insights about what makes each city unique. The dataset is comprised of reviews, businesses and user information from: Phoenix, AZ Las Vegas, NV (new!) Madison, WI (new!) Waterloo, CAN (new!) Edinburgh, UK (new!) This new dataset increases the data included in the previous Phoenix Academic Dataset with the following new data and is available for immediate download : Businesses - 42,153 (+26,568 new businesses!) Business Attributes - 320,002 (+208,441 new attributes!) Check-in Sets - 31,617 (+20,183 new check-in sets!) Tips - 403,210 (+289,217 new tips!) Users - 252,898 (+182,081 new users!) User Connections - 955,999 (+804,482 new edges!) Reviews - 1,125,458 (+790,436 new reviews!) Round 4 is Now Live Along with the updated dataset, we’re also happy to announce the next iteration of the Yelp Dataset Challenge . The challenge will be open to students around the world and will run from August 1st, 2014 to December 31, 2014. See the website for the full terms and conditions . This data can be used to train a myriad of models and extend research in many fields. So download the dataset now and start using this real-world dataset right away! Tweet Back to blog", "date": "2014-08-01"}, {"website": "Yelp", "title": "Keeping Yelp Safe at Security BSides Las Vegas!", "author": ["\n        \n  Ioannis K., Security Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/08/keeping-yelp-safe-at-security-bsides-las-vegas.html", "abstract": "People say that security is hard and that’s exactly why we have a dedicated security team here at Yelp! We place tremendous importance on securing our environment, our employees and the millions of visitors who trust Yelp every month. Information Security is not a solo endeavor. You have to exchange information with fellow security engineers and researchers, get informed of new vulnerabilities and threats and build a “web of trust” containing security practitioners that you can count on. “Community” is the keyword in this case. This is why we are officially sponsoring Security BSides Las Vegas 2014 ! BSides is a great community-driven security convention held in Las Vegas August 5th and 6th, at Tuscany Suites & Casino. Our own security team will be there and would be more than happy to meet and exchange GPG keys, errr… ideas and knowledge! It’s also my great pleasure to have been selected to conduct a 4 hour long workshop for 28 lucky participants on one of my favorite research topics: honeypots! Unfortunately, all the available spots were filled within the first few days, but make sure to catch me at BSides if you are interested! In the field of computer security, honeypots are systems aimed at deceiving malicious users or software that launch attacks against the servers and network infrastructure of various organizations. Essentially, they are systems running fake or emulated services with security holes that are open for exploitation. Everything that an attacker or malware does can be recorded for further analysis. Thus, honeypots can be deployed as protection mechanisms for an organization’s real systems, or as research units to study and analyze the methods employed by human hackers or malware. At the BSides workshop, we will talk at length about the use cases and the value of honeypots, what problems they solve (or create), how to get the best out of the available deployment scenarios, what you can do with the data you can capture and how to get a better understanding of them. This will be followed by a hands-on portion where participants will create and test several research honeypots by manually deploying and testing in real time. One honeypot system will undertake the role of a web trap for attackers who target the SSH service in order to gain illegal server access. SSH is the most common way sysadmins manage their systems and it’s always an easy entry point if public key authentication is not in place. Another one will undertake the role of a malware collector, a device usually deployed by malware analysts and anti-virus companies to gather and securely store malicious binary samples. We will also talk about post-capturing activities and further analysis techniques. I will present some useful visualization tools, plus a honeypot bundle Linux distribution that contains many pre-configured versions of the aforementioned honeypots and tools, which can make the deployment of honeypots in small or large networks an easy task. The latter is a project by me called HoneyDrive and you can find the latest version (released only a few days ago) here: http://sourceforge.net/projects/honeydrive/ Do you think all of these sound interesting? We surely do! If you want to be part of a security team in one of the most exciting companies to work for, take a look at our careers page . We are currently hiring security engineers in our San Francisco, New York and London offices! Tweet Back to blog", "date": "2014-08-04"}, {"website": "Yelp", "title": "August Events at Yelp: Our Schedule is Heating Up!", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/08/august-events-at-yelp-our-schedule-is-heating-up.html", "abstract": "We’ve got a busy month ahead of us with several great events being held at Yelp HQ. We hope everyone was able to catch our security team over at BSides LV this past week where one of our engineers, Ioannis, presented on honeypots and ran a workshop teaching people how to set up one of their own. From panel discussions to hands-on workshops on everything from fashionable tech to Android development and growth hacking, there’s plenty to keep you busy at Yelp HQ. We’ve got six great events this month, and spaces are going fast. Make sure to sign up at the event links below. Hope to see you at Yelp! Tuesday, August 12, 2014 - 6:00PM - Mobile Debators ( Mobile Debators ) Wednesday, August 13, 2014 - 6:15PM - Learn About PyToolz with Matthew Rocklin ( San Francisco Python Meetup Group ) Thursday, August 14, 2014 - 6:00PM - Android Tour Plus Hands-On Session ( Learning Android Development ) Wednesday, August 20, 2014 - 6:00PM - Talk by Professor Michael Jordan, UC Berkeley ( SF Machine Learning ) Thursday, August 21, 2014 - Fashion Tech Design ( Designers + Geeks ) Wednesday, August 27, 2014 - 6:45PM - Growth Hacking for Great Products, with Chris Neumann from CROmetrics ( Products That Count ) Tweet Back to blog", "date": "2014-08-11"}, {"website": "Yelp", "title": "Meet Yelp Trends: A Fun Way to Visualize Trends From Millions of Reviews on Yelp", "author": ["\n        \n  Marcin S., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/08/meet-yelp-trends-a-fun-way-to-visualize-trends-from-millions-of-reviews-on-yelp.html", "abstract": "Yelp is celebrating its 10th anniversary this year. That’s right, a decade of marvelous reviews from Yelpers all around the world. What better way to celebrate the big 1-0 than to build a tool that would take a sneak peak into over 61 million reviews from our community and let you discover real world trends in cities all over the globe? Meet Yelp Trends . You might have seen it hit the press recently . As announced in our official blog , Yelp Trends is a fun way to visualize the review frequency of how often specific words are used in reviews and their development over the past 10 years. From popular trends in the culinary world to popular slang terms to what’s hot in fitness, users are encouraged to explore the world from the local communities’ perspective. Yelp Trends started as a hackathon project when a group of engineers indexed words used in reviews into Elasticsearch. We leveraged its powerful facet queries in the backend and built the UI to graph the normalized review frequencies. At Yelp, Elasticsearch is an important part of our search infrastructure, as it provides a robust, distributed platform that is easy to integrate with other services. On top of Elasticsearch we also have our own custom frameworks for creating clients as well as indexers called Apollo and ElasticIndexer, respectively. Apollo (of course named after Apollo Creed) allows us to quickly build Elasticsearch clients with a fixed interface and also provides us with many default features such as monitoring and a managed infrastructure. Our Apollo client queries the reviews index and returns a JSON formatted time series of queries frequency. What does a request out to Elasticsearch look like in Apollo? Not that much different than the regular JSON you would send but as Python data structures instead. 400: Invalid request This sample shows how we built the request to correctly search through all of the indexed data for relevant results. We have three different filters here, all being selected with ‘and’ to make sure that each data point from Elasticsearch matches all three requirements: the city (San Francisco here), the restaurants, and the review language. These all get applied to the phrase we’re matching on, pizza. After we have all of this data, we need to structure it in a manner that makes it easy for us to visualize. Using facets we’re able to have Elasticsearch format the data into an easily consumable format. 400: Invalid request ElasticIndexer, the other framework that complements Apollo, is an indexing pipeline for loading Yelp data into Elasticsearch. Building a new index can take several days for some of our larger indices, so to help us avoid doing this, ElasticIndexer constantly monitors database tables for changes and re-indexes documents as they are modified or added. It also has the ability to determine field dependencies, which enables us to re-index only the fields that actually change when the database changes. Leveraging both Apollo and ElasticIndexer, the hackathon project was at first branded Wordtime. We used AngularJS , D3.js , Rickshaw and adopted design templates from Bootstrap . Rickshaw provided the framework to display interactive graphs, drawn with SVG that are highly customizable and easily styled using the standard CSS techniques. It quickly became obvious that the tool is addictive and people enjoyed trying out new examples. That’s when some of the other teams learned about the project, with the anniversary in mind, we decided to productionize the tool. Wordtime originally supported English reviews only, while for Yelp Trends we wanted to add support for other review languages as well (through specialized search analysers). While we could have indexed the review text of reviews in other languages in the same field we use for English, this would not have worked well. Elasticsearch only allows a fixed analyzer per field and an English analyzer would not work well for a completely different language such as Japanese. We ended up adding new fields for each language and then reindexed the entire review corpus over a two day period. Our final reviews index grew to over 150GB but queries still only took under a second. Yelp Trends is an inspiring tool! Give it a try yourself. We can’t wait to see what trends you will uncover in your city. And don’t forget to share your findings with us. Have fun! Tweet Back to blog", "date": "2014-08-15"}, {"website": "Yelp", "title": "HACK209 - dockersh", "author": ["\n        \n  Tomas D., Site Reliability Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/08/hack209-dockersh.html", "abstract": "Yelp’s engineering team loves Docker . We’re already using it for a growing number of projects internally but there are applications that Docker isn’t a great fit for, such as providing independent (VM like) containers on shared hardware for people to interactively ssh into. You can, of course, run sshd inside a Docker container but you can only run one sshd per port. If you wanted multiple users to be able to ssh into the same server, without having a custom port allocated per user, you’re out of luck. To solve this problem we built dockersh , a user shell for isolated, containerized environments in Docker. It’s available as a container on dockerhub with a 1 step install. This is a fully functional and usable implementation that you can play with. I’m already using it on some of my home servers to separate people’s screen/tmux sessions for irc into separate containers. Originally I solved this by using the ssh ForceCommand setting so that users would log into the host and then immediately be forced to ssh to a Docker container. This was not ideal as key management and mapping user’s ForceCommand setting was complex. After discussing this problem with a couple of my colleagues, and some inspiration from a recent blog post about sshd in containers, we had a rough idea of what we wanted to experiment with: making a shell which located a user inside a container using nsenter! In brief, we planned to write a utility which would: Be invoked by the login system as a user shell (when a user sshs into the host) Start up a Docker container for the user (if it wasn’t already running), with the user’s home directory mounted Lastly, exec nsenter to give the user a shell inside this container Theoretically, this could give us isolated environments, as each user would have their own network stack, process and memory namespaces, etc. If subsequent ssh sessions just enter the pre-existing container, it would look (to the user) like they had their own dedicated machine. We thought we could hack together a prototype of this pretty quickly. Turns out we were right ; with a couple of patches , and a custom nsenter version we were in business, at least the ‘terrible but kinda functional prototype’ business, perfect for our upcoming hackathon. We decided to rewrite the prototype version in Go, which was a first for me. At the time, this was mostly an excuse to play with Go but it quickly proved to be a very sound decision. We were able to make use of some excellent libraries, like libcontainer , which did a lot of the heavy lifting for us. dockersh can be explicitly invoked for testing, but more usually will be setup as an ssh ForceCommand, or added to /etc/shells and used as the shell for users in /etc/passwd. It reads a global config file and per user configuration files, and then sets up a container and invokes a real shell process for the user. This user shell process inside their container should be more secure than access to the host, as the container’s kernel namespaces and cgroups are applied to the new shell, in addition to dropping additional ‘capabilities’ for the process including SUID and SGID bits. The utility we have now is just a start - I’ve already got a list of future improvements and further work (in the issue tracker ), and I encourage you to have a play with the project and let us know what you think. Tweet Back to blog", "date": "2014-08-26"}, {"website": "Yelp", "title": "Announcing pre-commit: Yelp’s Multi-Language Package Manager For Pre-Commit Hooks", "author": ["\n        \n  Ken S., Technical Lead\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/08/announcing-pre-commit-yelps-multi-language-package-manager-for-pre-commit-hooks.html", "abstract": "At Yelp we rely heavily on pre-commit hooks to find and fix common issues before changes are submitted for code review. We run our hooks before every commit to automatically point out issues like missing semicolons, whitespace problems, and testing statements in code. Automatically fixing these issues before posting code reviews allows our code reviewer to pay attention to the architecture of a change and not worry about trivial errors. As we created more libraries and projects we recognized that sharing our pre commit hooks across projects is painful. We copied and pasted bash scripts from project to project and had to manually change the hooks to work for different project structures. We believe that you should always use the best industry standard linters. Some of the best linters are written in languages that you do not use in your project or have installed on your machine. For example scss-lint is a linter for SCSS written in Ruby. If you’re writing a project in node you should be able to use scss-lint as a pre-commit hook without adding a Gemfile to your project or understanding how to get scss-lint installed. We built pre-commit to solve our hook issues. It is a multi-language package manager for pre-commit hooks. You specify a list of hooks you want and pre-commit manages the installation and execution of any hook written in any language before every commit. pre-commit is specifically designed to not require root access. If one of your developers doesn’t have node installed but modifies a JavaScript file, pre-commit automatically handles downloading and building node to run jshint without root. Read more about pre-commit at: http://pre-commit.com Tweet Back to blog", "date": "2014-08-27"}, {"website": "Yelp", "title": "Maker’s Day – Or How Thursdays Became Every Yelp Engineer’s Dream", "author": ["\n        \n  Yoann R., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/09/makers-day-or-how-thursdays-became-every-yelp-engineers-dream.html", "abstract": "It’s been a busy past few years here at Yelp Engineering. With our 10th anniversary this year and our recent launch in Chile , we think it’s safe to say we’re on to something. But it would be foolish of us to stop here. At the end of the day, we’re engineers: we live for the fact that there are still so many challenging problems to solve, features to improve, and datasets to explore. With the size of the projects we’re tackling nowadays, our Engineering and Product Management teams need to be in constant contact to coordinate development, testing, and release. We fully embrace the rapid iterative process customary of Agile, Scrum, and XP programming, so you’ll often see a product manager and engineer hashing an idea out at one of our large built-in whiteboards or in the team’s pod. Soon enough, though, we found ourselves with schedules like this: Coordination is incredibly important, but we also need time to actually build all those cool features we come up with. That’s why, about a year ago, we introduced Maker’s Day here at Yelp. So what is Maker’s Day? The concept is pretty simple: meetings, interviews, and general interruptions aren’t allowed for engineers on Thursdays. Some teams even cancel standups on those days while others use them as a quick way to unblock folks so that there are fewer disruptions later on. If any questions come up, we use email instead of showing up at a person’s desk or pinging them over IM. Outside of those general guidelines, how engineers use Maker’s Day is really up to them: some make it into a long, uninterrupted coding period, others prefer it for reviewing designs and diving deep into a topic. And by the way, for those engineering managers out there, this applies to us, too. We’re certainly not the first to come up with this idea. Back in 2009, Paul Graham, in his “Maker’s Schedule, Manager’s Schedule” post , wrote how the partners at YC Combinator were implementing the idea: “You can’t write or program well in units of an hour. That’s barely enough time to get started.” Craig Kerstiens of Heroku mentioned, as part of his How Heroku Works series , how the value of Maker’s Day had increased exponentially as the company had grown. Intel even jumped into the discussion with hard facts from their “Quiet Time” pilot. Closer to the Python Community, Daniel Greenfeld tweeted what everyone was thinking back in 2012: Developers should have 4-6 hours of uninterrupted activity each day. Each 3-5 minute interruption costs more than you can imagine. — Daniel Roy Greenfeld (@pydanny) December 3, 2012 So how has Maker’s Day done here at Yelp? We don’t have spreadsheets of numbers to prove its success. However, on Thursdays, you’ll notice the engineering floors are a tad quieter, and folks are eager to get to their desks and jump into whatever task they’ve lined up for that day. That’s enough for us to stick with it. In the end, Maker’s Day was a good step, but we don’t think it’s the be-all end-all solution. Similar to our software development strategy, we’re also constantly iterating with our processes within Engineering. If you love thinking about these kinds of problems, we’re always looking for great Engineering Managers to help grow our talented team of engineers. Tweet Back to blog", "date": "2014-09-04"}, {"website": "Yelp", "title": "Making Sense of CSP Reports @ Scale", "author": ["\n        \n  Ivan L., Engineering Manager - Security\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/09/csp_reports_at_scale.html", "abstract": "CSP is Awesome Content Security Policy isn’t new, but it is so powerful that it still feels like the new hotness. The ability to add a header to HTTP responses that tightens user-agent security rules and reports on violations is really powerful. Don’t want to load scripts from third party domain? Set a CSP and don’t.  Trouble with mixed content warnings on your HTTPS domain? Set a CSP and let it warn you when users are seeing mixed content. Realistically, adding new security controls to a website and a codebase as large as Yelp needs to be a gradual process. If we apply the new controls all at once, we’ll end up breaking our site in unexpected ways and that’s just not cool. Fortunately, CSP includes a reporting feature - a “lemme know what would happen, but don’t actually do it” mode. By using CSP reporting, Yelp is able to find and fix problems related to new CSP controls before they break our site. Reading Sample CSP Report CSP reports are JSON documents POSTed from a user’s browser to Yelp. An example report might look like: {\n   \"csp-report\": {\n     \"document_uri\": \"https://biz.yelp.com/foo\",\n     \"blocked_uri\": \"http://www.cooladvertisement.bro/hmm?x=asdfnone\",\n     \"referrer\": \"https://biz.yelp.com\",\n     \"source_file\": \"https://biz.yelp.com/foo\",\n     \"violated_directive\": \"script-src https:\",\n     \"original_policy\": \"report-uri https://biz.yelp.com/csp_report; default-src https:; script-src https:; style-src https:\"\n   }\n} This report says, “I went to https://biz.yelp.com/foo but it loaded some stuff from cooladvertisement.bro over HTTP and I showed a mix content warning.” Looks like www.cooladvertisement.bro needs to get loaded over HTTPS and then all will be good. Making Sense of CSP Reports @ Scale It’s easy to read a single CSP report but what if you’re getting thousands of reports a minute? At that point you need to use some smart tools and work with the data to make sense of everything coming in. We wanted to reduce noise as much as possible so had to take a few steps to do that. Get rid of malformed or malicious reports Not all reports are created equally.  Some are missing required fields and some aren’t even JSON. If you have an endpoint on your website where users can POST arbitrary data, there will  be a lot of noise mixed with the signal.  The first thing we do is discard any reports that aren’t well formed JSON and don’t contain the necessary keys. Massage the reports to make them easier to aggregate It was helpful to group similar reports and apply the Pareto principle to guide our efforts at addressing CSP reports. We take any URI in the report and chop it down to it’s domain, getting rid of the uniqueness of nonces, query params, and unique IDs, making it easier to group {\n  \"csp-report\": {\n    \"document_uri\": \"https://biz.yelp.com/foo\",\n    \"document_uri_domain\": \"biz.yelp.com\"\n    \"blocked_uri\": \"http://www.cooladvertisement.bro/hmm?x=asdfnone\",\n    \"blocked_uri_domain\": \"www.cooladvertisement.bro\",\n    ...\n  }\n} Discard unhelpful reports Surprisingly, you’ll see a whole lot of stuff that’s not really about your website when you start collecting reports. We found some good rules to discard the unhelpful data. blocked_uri and source_file must start with http We see loads of reports with browser specific URI schemes, stuff related to extensions or the inner workings of a browser like chromeinvoke:// or safari-extension://.  Since we can’t fix these, we ignore them. source_file is an optional field in a CSP report, so we apply this rule to source_file only when it has a value. document_uri must match the subdomain the report was sent to If we’re looking at CSP reports that were sent to biz.yelp.com then we’re only interested in reports about documents on biz.yelp.com.  All sorts of strange proxy services or client side ad injectors will land up serving a modified copy of your page and generate reports for things you can’t fix. Retain some useful data We don’t want to lose useful data that came in as part of the POST request, so we tack it onto the report. Info like user-agent can be super helpful in tracking down a “Oh… that’s only happening on the iPhone” issue. {\n  \"csp-report\": {\n    \"document_uri\": \"https://biz.yelp.com/foo\",\n    \"document_uri_domain\": \"biz.yelp.com\"\n    ...\n  },\n  \"request_metadata\": {\n    \"server\\_time\": 1408481038,\n    \"yelp\\_site\": \"biz\",\n    \"user\\_agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 7\\_1\\_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\",\n    \"remote\\_ip\": \"127.0.0.1\"\n  }\n} Throw this all in a JSON log Once we’ve got a nice, well formed JSON report with some helpful extras, we throw it into a log. Logs aggregate from our various datacenters and make themselves available as a stream for analysis tools. Visualize, monitor, and alert for the win The Yelp security team is a huge fan of E lasticsearch/ L ogstash/ K ibana . Like we do with pretty much any log, we throw these CSP reports into our ELK cluster and visualize the results. This image shows a rapid decrease in incoming CSP report volume after fixing a page that caused mixed content warnings From there it’s easy for our engineers to view trends, drill into specific reports, and make sense of reports at scale. We’re also adding monitoring and alerting to the reports in our Elasticsearch cluster so it can let us know if report volumes rise or new issues crop up. Give it a try We’re making sense of CSP reports at scale and that’s super useful in monitoring and increasing web application security. We’d love to hear from you about how you’re using CSP. Let us know at opensource@yelp.com. Tweet Back to blog", "date": "2014-09-08"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Round 2 Winner and New Data", "author": ["\n        \n  Dr. Scott Clark, Ad Targeting Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/02/yelp-dataset-challenge-round-2-winner-and-new-data.html", "abstract": "The Challenge The second round of the Yelp Dataset Challenge opened in May 2013, giving students access to our massive Phoenix Academic Dataset, with reviews and businesses from the greater Phoenix metro area. The Yelp team is very excited to provide the academic community with a rich dataset over which to train and extend their models and research. We encourage students to take advantage of this wealth of data to develop and extend their own research in data analysis and machine learning. Students who submit their research are eligible for cash awards and incentives for publishing and presenting their findings. The dataset was downloaded by thousands of students around the world. From the completed entries we have selected David W. Vinson of University of California, Merced as the Round 2 winner with his submission “Valence Constrains the Information Density of Messages.” Updating and Extending the Dataset We are excited to announce that we have updated and extended the original Phoenix Academic Dataset! The original dataset, released in March 2013, has been well-received by the academic community, and has already been cited in papers and included in presentations around the world. For more information on past winners and their papers, please check out the Yelp Dataset Challenge site. The new dataset builds upon this foundation by not only refreshing it with new content created over the past year but also including new data like business attributes, the social graph and tips. The new Phoenix Academic Dataset incorporates the following updates and new data types: Businesses - 15,585 (+4,048 new businesses!) Business Attributes - 111,561 (new!) Check-in Sets - 11,434 (+3,152 new check-in sets!) Tips - 113,993 (new!) Users - 70,817 (+26,944 new users!) User Connections - 151,516 (new!) Reviews - 335,022 (+105,115 new reviews!)\nThis new data is available for immediate download at www.yelp.com/dataset_challenge and replaces the previous Phoenix Academic Dataset. We are eagerly anticipating seeing the projects and research that will be built using this data. We are especially excited to see the research related to the new content: from micropost analysis on tips to inferring business attributes from reviews to mining the rich social graph for insights. We look forward to what you come up with! Round 3 is Now Live Along with the updated dataset, we’re also happy to announce the next iteration of the Yelp Dataset Challenge . The challenge will be open to students in the US and Canada and will run from February 11th, 2014 to July 31, 2014. See the website for the full terms and conditions . This data can be used to train a myriad of models and extend research in many fields. So download the dataset now and start using our data right away! Tweet Back to blog", "date": "2014-02-20"}, {"website": "Yelp", "title": "Introducing RequestBucketer: A system for putting HTTP requests in named buckets", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/02/introducing-requestbucketer-a-system-for-putting-http-requests-in-named-buckets.html", "abstract": "JR Heard has many big projects under his belt, and this week we’ll get to learn about one of the most recent. Yelp pushes new code almost every day, so it’s no surprise we get new features every week. But how do we make sure they’re working as intended? JR describes one element of our solution below! Let’s talk about features. Building new features is super fun. Improving pre-existing ones is fantastic, too. What would be less fantastic would be if the your new feature turned out to crumble under production load, or if your untested-gut-feeling improvement to an old feature ended up causing people to use it less. Here at Yelp, we don’t have to worry about that too often, thanks to a system we use both for rolling out new features and for allocating percentages of traffic into the different branches of our A/B tests. Let me tell you about it! Context Over the years, we’ve found that the best way to build a big new feature is to break it into small pieces and push each bit to production as it’s completed. There are about a thousand reasons for this, most of which will be familiar to those who’ve worked on a large, long-lived software project ( Facebook and The Guardian know what I’m talking about). Fast iteration cycles mean that we get to see how our feature works in the wild much more quickly; on top of that, no matter how well-tested your code is, there’s just no substitute for the peace of mind you get from seeing it run on live traffic. Of course, when we’re working on a giant new feature that completely replaces an existing page (e.g. our homepage redesign a year and a half ago, not to mention our recent business page redesign! ), we can’t just suddenly replace the old page with a blank “Hello world!” page and ask that our users bear with us for a few months. Instead, for each big feature like this, we used to end up writing a function that looked something like: def should_show_new_homepage(self, request):\n    if internal_ip(request.remote_addr):\n        return True\n    if self.user_id in config.homepage_rollout_user_ids:\n        return True\n    if self.device_id in config.homepage_rollout_device_ids:\n        return True\n    if self.user.is_elite and config.homepage_rollout_is_active_for_elite_users:\n        return True\n    return False This function lets us control who gets to see our new feature-in-progress; essentially, it implements the logic that lets us whitelist a request into seeing our new feature. So this is great - the only people who get to see our feature-in-development are the people who are supposed to be seeing it, and our users don’t have to put up with an unfinished feature while we implement a redesign. The catch here is that we’ve got a lot of people working on lots of features. Writing one of these functions from scratch for each feature was a clear violation of the DRY principle . Worse: even though we code-review every line of code we write before shipping it, none of us was comfortable with the possibility of accidentally launching an incomplete feature due to mistakenly including a not in the wrong place the fiftieth time we wrote one of these functions. We decided to build a tool to solve this problem once and for all. Design Constraints Our ideal tool would be something that took in a string like ‘foo_shiny_feature’ and returned a string like ‘enabled’, ‘disabled’, or possibly some other string(s) depending on the semantics of the feature being gated. Our solution would have to satisfy the following requirements: Traffic allocation We should be able to say that, for instance, 5% of traffic gets to see our new feature and 95% of traffic doesn't. Extensible whitelisting We should be able to whitelist users into (or out of!) a particular feature in a number of ways (more on this below), and it should be very simple for maintainers to add new ways to whitelist requests. Speed We should be able to quickly ask about the status of hundreds of features/experiments over the course of serving a Web request. Idiot-proof One of the main motivations behind building this tool was to minimize the chance of accidentally launching an in-progress feature, so it should have as little room for operator error as possible. Multi-Purpose We would want to use this tool for other things besides feature rollouts: for instance, we would also like to use it to distribute traffic among cohorts in A/B tests. We came up with a solution we call RequestBucketer, and we’ve been using it in production for about a year now. You interact with it like this: request_bucketer.get_bucket('my_shiny_button_experiment')\n# => 'bright_red'\nrequest_bucketer.is_feature_enabled('new_biz_page')\n# => True\nrequest_bucketer.is_feature_disabled('a_service_being_load_tested')\n# => False RequestBucketer RequestBucketer gets its name because it lets you say: “My feature has these four buckets; these two buckets have special whitelisting behavior; and here are all four buckets’ traffic percentages. Here’s an HTTP request: what bucket does it fall into?” Let’s be more specific about what I mean when I talk about how buckets can have “special whitelisting behavior.” Toward the start of a new feature’s life, we want to make sure that the only people who actually see that new feature are the engineers working on it. We can do this in a couple of ways: We can whitelist access to the feature based on the ID of a request’s logged-in user, so that engineers can see the feature from their home computer if they’re logged into yelp.com. That doesn’t let our engineers test out how the feature behaves for logged-out users - to cover that case, we can whitelist access to the feature based on a device-specific ID.\nLater on, once the feature’s working well enough that it can be beta-tested by other folks, we have a couple of other whitelisting tools at our disposal: We can say that any request that originates from within our internal corporate network gets to see our new feature, but usually we won’t want to do this until the feature is pretty fully-functional, so that other departments don’t have to deal with our feature-in-progress. We also like to roll out features to certain types of logged-in users. For instance, when we added the ability for users to write reviews from their mobile devices, our Elites got to play with that feature weeks before anyone else. We also have a team of Community Managers in cities across the globe, and we love to collect early feedback on new features by giving our CMs early access.\nRequestBucketer is backed by a simple YAML file with a bunch of entries (we call them BucketSets) that look like this: foo_shiny_feature:\n    type: *FEATURE_RELEASE # as opposed to, for instance, *EXPERIMENT\n    buckets:\n        disabled:\n            percentage: 90\n        whitelist:\n            user_ids:\n                - *JRHEARD_USER_ID # jrheard is a curmudgeon, doesn't want the new version until it's done\n        dark_launch:\n            percentage: 0\n        enabled:\n            percentage: 10\n        whitelist:\n            user_types:\n                - *ELITE\n                - *COMMUNITY_MANAGER\n            ip_ranges:\n                - *ALL_INTERNAL_IPS\n            yuvs:\n                - *CONSUMER_YUVS\n                - *PM_YUVS\n            user_ids:\n                - *MOBILE_TEAM_USER_IDS\n                - *WING_USER_ID\n                - *A_HOUSECAT_USER_ID In the example above, when we check what bucket a given request falls into for foo_shiny_feature , we’ll first check the buckets’ whitelists. For instance, if my boss Wing is logged in, he’ll be in the ‘enabled’ bucket, guaranteed. If a request isn’t whitelisted into any buckets at all (e.g. it’s made from an IP outside of the Yelp corporate network and doesn’t have a whitelisted user-id or device ID), we’ll fall back to the buckets’ traffic percentages. As you’d expect, 10% of those requests will be assigned to the ‘enabled’ bucket, and the other 90% will be assigned to the ‘disabled’ bucket. “Hold on a second,” astute readers say - “what happens if jrheard is logged in and is making a request from an internal IP?” Great question! To deal with situations like this, RequestBucketer has a simple concept of a “whitelist match specificity.” Simply put: some types of whitelisting are more specific than others - a device ID is more specific than a logged-in user ID, and a logged-in user ID is more specific than an IP range. If a request has a whitelist match in multiple buckets, the bucket with the most specific match wins. This is all easily configurable, and as you teach RequestBucketer about new ways to whitelist requests, it’s super-simple to teach it how specific these new whitelist matches are - it looks a lot like this: WHITELIST_MATCH_SPECIFICITY_ORDERING = [\n    WhitelistMatchSpecificity.SPECIFIC_DEVICE,\n    WhitelistMatchSpecificity.SPECIFIC_USER,\n    WhitelistMatchSpecificity.TYPE_OF_USER,\n    WhitelistMatchSpecificity.IP_RANGE,\n] Questions? RequestBucketer’s a simple system, and we use it so frequently that I launched a feature with it halfway through writing this blog post. We use it to power our experiments system, too - but that’s a discussion for another post. Have any questions about how we use RequestBucketer in production or comments on its design? Let us know in the HN discussion thread ! Tweet Back to blog", "date": "2014-02-27"}, {"website": "Yelp", "title": "March Events at Yelp HQ: Python Madness & More!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/03/march-events-at-yelp-hq-python-madness-more.html", "abstract": "Love Python but can’t make it all the way to Montreal for PyCon this year? Never fear, Yelp is hosting a sneak peak of many of the talks! Instead of buying plane tickets, a hotel room, and a conference pass, you can kick back, drink beer, and have pizza with us! Next week’s Python meetups, both PyLadies and SF Python, will be hosting practice talks for the conference. Let’s dive into two of the talks: a novice session on machine learning, and an intermediate talk on working with external languages. At a high level, Melanie Warrick will get us started on machine learning. “Big Data” and analytics don’t have to be just scary buzzwords. Python is a great language to start tinkering in this area, and Melanie will show us how! Join us March 11th at the PyLadies event to learn more. Note, this event is open to both PyLadies and PyGents! On the low level side of things, Christine Spang will describe best practices in invoking subprocesses and wrapping C code. Subprocesses can be used to run other binaries, shell processes, or even other Python processes you’d like to keep in a separate namespace.  But understanding all the options is important to using that power effectively and correctly.  “Dropping down” to C code can be an effective way to speed up a Python program and Python provides a way idiomatically to interact with highly optimized libraries. SF Python will host this talk and others on March 12th. And there’s plenty more where that came from! Lada Adamic, a data scientist at Facebook, will model information spread at the Products that Count meetup, and at Designers + Geeks we’ll hear about cutting edge techniques to create narrative-driven experiences in the world around us. See you at Yelp! Tuesday, March 11, 2014 - 6:30PM - PyCon 2014 Preview ( PyLadies ) Wednesday, March 12, 2014 - 6:15PM - PyCon Practice Night ( San Francisco Python Meetup Group ) Wednesday, March 19, 2014 - 7:00PM - How does information spread on Facebook? ( Products That Count ) Thursday, March 20, 2014 - 6:30PM - Situational Design & The Art of Nonchalance ( Designers + Geeks ) Thursday, March 27, 2014 - 6:30PM - TBD ( SF JavaScript ) Tweet Back to blog", "date": "2014-03-06"}, {"website": "Yelp", "title": "MySQL replication, error 1159, and why you might want to upgrade", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/03/mysql-replication-network-issues-and-why-you-might-want-to-upgrade.html", "abstract": "pre.mysql-block { overflow-x: auto; margin: 0 0 1em 1em; } At Yelp, we use MySQL replication to provide the performance we need to serve our 120 million monthly unique visitors. In this post Josh Snyder walks us through some of the internals of MySQL’s replication system, and discusses an issue that we saw in production. MySQL replication and the slave I/O thread’s purpose On a MySQL slave, the task of replication is split between two threads: the I/O and SQL threads. The I/O thread’s job is to download the log of events executed on the master and write them to disk in the relay log. The SQL thread then reads and executes the downloaded events on the slave. For the purposes of this post we’ll discuss the I/O thread. The slave I/O thread connects to a master server using the MySQL protocol, just like any other client. It executes certain housekeeping queries on the master, and immediately afterward asks for a dump of the binary log starting at a specific log file and position. Once the slave I/O thread requests the binlog dump, the connection will be devoted entirely to the binlog stream until the TCP stream is closed. This means that the results of the housekeeping queries mentioned above will need to be cached for use during the entire lifetime of the TCP stream. Practically speaking this is not a problem, because it’s very cheap to break and reestablish the connection. What happens when the connection breaks? Nothing right away. The MySQL slave can’t recognize the difference between silence due to a lack of events in the binary log and silence due to connection issues. The slave I/O thread can take a guess, though, which is where slave_net_timeout and MASTER_HEARTBEAT_PERIOD come in. The manual describes slave_net_timeout as, “the number of seconds to wait for more data from the master before the slave considers the connection broken, aborts the read, and tries to reconnect.” This is correct, but the actual behavior is somewhat more subtle. As implemented, slave_net_timeout is the number of seconds that any read on the slave I/O socket will block before timing out. It’s passed directly to setsockopt(2) as SO_RCVTIMEO . Therefore, if the I/O thread ever spends more than slave_net_timeout seconds reading from its socket, the read will time out and an error will be returned. MySQL handles this error internally as ER_NET_READ_INTERRUPTED , and its ultimate reaction is to terminate the connection and initiate a new one. A small value of slave_net_timeout could then pose a problem. If slave_net_timeout is two seconds, MySQL will terminate and reconnect the replication stream whenever the master spends more than two seconds without publishing an event to its binary log. To remedy this, the slave can request that the master send periodic heartbeats using the MASTER_HEARTBEAT_PERIOD replication setting (which defaults to half of slave_net_timeout ). If the master heartbeat period elapses and the master has no events to send, it will insert a Heartbeat_log_event into the binary log stream. These events go over the wire, but are never written to disk; they don’t even get recorded in the slave’s relay log. Heartbeat events serve to “fill the silence” created by an idle master. Busy masters (i.e. those that always have an event to send within one heartbeat period) will never send heartbeat events. This can be verified on a slave by checking that the Slave_received_heartbeats status variable is never incremented when the master is busily accepting writes. What does a slave reconnection look like? I set up a master-slave MySQL pair within Docker containers, with an active binary log stream coming down from the master. I set slave_net_timeout to 30 seconds and then broke replication by removing the replication TCP flow from the tables of my stateful firewall. Using iptables/conntrack to break a replication connection First, set up a firewall rule that uses conntrack to disallow all invalid TCP sessions: root@mysql_master # iptables -A INPUT -p tcp –dport 3306 -m state –state INVALID -j LOG root@mysql_master # iptables -A INPUT -p tcp –dport 3306 -m state –state INVALID -j DROP Be sure to disable conntrack’s TCP loose mode, which will otherwise allow TCP ACKs to establish valid connections: root@mysql_master # echo 0 > /proc/sys/net/netfilter/nf_conntrack_tcp_loose Then simply delete the conntrack entry for MySQL replication, and our stateful firewall will block any MySQL tcp sessions that were in flight: root@mysql_master # conntrack -D -p tcp –dport 3306 I set my slave_net_timeout to 30 seconds and then broke replication. As expected, the slave I/O thread stopped advancing. Seconds_Behind_Master displays zero, because the slave SQL thread is not “ actively processing updates ”. Nothing of consequence happens until slave_net_timeout elapses. Once that happens, MySQL reconnects and Seconds_Behind_Master spikes to…wait what? Seconds_Behind_Master: 1634 … Last_IO_Errno: 0 How did that happen? We spent only the thirty second slave_net_timeout disconnected from the master host, but the slave SQL thread thinks it’s a whopping 1634 seconds behind. This is due to MySQL bug #66921 , discussed by Domas Mituzas here. Succinctly, when the connection is reestablished, the slave SQL thread re-executes the format description event from the master’s binary log, noting its timestamp as the latest executed event. Seconds_Behind_Master becomes equal to the age of the format description event in the master’s binary log file, which in this case was 1634 seconds. MySQL doesn’t bother to print to the error log that the I/O thread recovered from a connection failure. It considers these kinds of issues routine, so it gracefully recovers without comment. This presents an issue for a database administrator who would like to know how frequently reconnects are occurring. So far the only way I’ve found to gather this information is to interrogate the master about the age of its ‘Binlog Dump’ threads. [1] select id, host, time_ms, date_sub(now(), interval time second) as started from information_schema.processlist where command = ‘Binlog Dump’ order by time_ms; In some cases, the I/O thread’s attempt to reconnect will fail. This will cause a cheery error message to appear in the error log: [ERROR] Slave I/O: error reconnecting to master ‘repl@169.254.0.5:3306’ - retry-time: 60 retries: 86400, Error_code: 2003 Error code 2003 is the familiar “ Cannot connect to MySQL server ” error. Replication isn’t currently happening, but MySQL will gracefully handle the situation by continuing its attempts to reconnect to the master [2]. The error message means that the slave I/O thread will reconnect at 60 second ( MASTER_CONNECT_RETRY ) intervals, and will persist in doing so 86400 ( MASTER_RETRY_COUNT ) times. An annoying outage On one occasion, packet loss due to a network provider issue caused some of Yelp’s MySQL 5.5 series databases to print different and scarier messages: [ERROR] Slave I/O: The slave I/O thread stops because a fatal error is encountered when it try to get the value of SERVER_ID variable from master. Error: , Error_code: 1159 [ERROR] The slave I/O thread stops because SET @master_heartbeat_period on master failed. Error: Error_code: 1593 perror informs us that these errors are network timeout and fatal slave errors, respectively. $ perror 1159 1593 MySQL error code 1159 (ER_NET_READ_INTERRUPTED): Got timeout reading communication packets MySQL error code 1593 (ER_SLAVE_FATAL_ERROR): Fatal error: %s In both cases, MySQL did not gracefully handle the situation. The slave I/O thread did not attempt to reconnect: it was stone dead. A human had to intervene to restart replication. These problems arose because the I/O thread wasn’t correctly handling errors when doing its start-of-connection housekeeping queries. Our investigation revealed that each error message implicates a different code site. The first error, ER_NET_READ_INTERRUPTED , is already familiar to us. It’s the error that occurs when slave_net_timeout elapses without receiving any data from the master. In this case, the replication client executed SHOW VARIABLES LIKE 'SERVER_ID' , and did not receive a response in time. As explained above, MySQL drops the connection and reconnects when it encounters this error during a binlog dump. At this specific code site, MySQL handles errors a little differently. Instead of catching and handling ERR_NET_READ_INTERRUPTED the code checks for any network error. The code that matches the error codes looks like: bool is_network_error(uint errorno) { if (errorno == CR_CONNECTION_ERROR errorno == CR_CONN_HOST_ERROR errorno == CR_SERVER_GONE_ERROR errorno == CR_SERVER_LOST errorno == ER_CON_COUNT_ERROR errorno == ER_SERVER_SHUTDOWN) return TRUE; return FALSE; } As you can see, ER_NET_READ_INTERRUPTED is missing from this list. Because of this, MySQL decided to terminate the slave I/O thread instead of gracefully reconnecting. The second error was similar. Setting the heartbeat period on the master is one of the housekeeping queries done by the slave before requesting a binlog dump. The query looks like SET @master_heartbeat_period=%s . Due to the way this query’s result was handled, any error at all would cause death of the slave I/O thread. We worked with Percona to get fixes for these two issues included in the Percona Server 5.5.36 release. The patch , written by Vlad Lesin, also includes numerous test cases to exercise this behavior. Rubber Meets Road I wanted to see Vlad’s change in practice, so I constructed a testbed to mimic packet loss. I chose some very aggressive settings in an attempt to get a high frequency of connection breakage. On the master, I left my existing stateful firewall rules in place. I then emulated 60% packet loss: tc qdisc add dev eth0 root netem drop 60% On the slave, I set SET GLOBAL slave_net_timeout=2; CHANGE MASTER TO master_connect_retry=1; I continuously wrote into the master at a rate of 5 queries/second, with each event being 217 bytes. This translates to 1085 bytes/second of binlog traffic. Using Percona Server 5.5.36 as a baseline, I used tcpdump to determine that these network conditions caused MySQL to reconnect 74 times over a five minute period. At no point did the I/O thread fail entirely. Percona Server 5.5.35 is a different story. Over a five minute period under the same conditions, MySQL reconnected 69 times. On five occasions the I/O thread failed completely: 3x [ERROR] Slave I/O: The slave I/O thread stops because SET @master_heartbeat_period on master failed. Error: , Error_code: 1593 2x [ERROR] Slave I/O: The slave I/O thread stops because a fatal error is encountered when it try to get the value of SERVER_ID variable from master. Error: , Error_code: 1159 Based on these results, I’d say the fix is effective. Moreover, it seems unlikely that there are any further bugs of this kind in the reconnection code. In conclusion I was very happy to have a checkout of MySQL handy while debugging this. In general, I’ve found the MySQL replication source code to be readable enough (if not simple), especially when you’re searching for a specific error code. Most importantly, though, I wouldn’t be able to live without the ability to spin-up a throwaway MySQL instance for testing. Footnotes [1] well, I lied. I know of one other way. You can configure replication in a way that causes the slave I/O thread to print a warning when it connects. The warning will be printed to the slave’s error log. I’ve seen this, for instance, when a MySQL 5.6 replica connects to a MySQL 5.5 master. [2] whether your application will gracefully handle the resulting replication delay is a different story entirely Tweet Back to blog", "date": "2014-03-31"}, {"website": "Yelp", "title": "April Showers Bring More Talks to Yelp", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/04/april-showers-bring-more-talks-to-yelp.html", "abstract": "Yelp is hosting many exciting talks this month! Let’s take a closer look at two, but be sure to check out all of the links below to see what’s in the pipeline. What does it take to monitor an architecture that is continually in flux and being changed by hundreds of developers? A seriously flexible monitoring framework: Sensu! A resident Yelp Site Reliability Engineer, Kyle Anderson, will be giving a tour of how Sensu works at Yelp. He’ll be explaining how Sensu hooks into the Yelp SOA architecture to empower individual development teams to monitor their own services in real time. He’ll also be discussing the difficulty of monitoring servers in AWS, and how Sensu can be used to track them. Yelp hosts this talk, put on by San Francisco Dev Ops , tomorrow April 8th! Next up is a talk about RxJava. Functional reactive programming is an interesting, relatively new paradigm that models application state as a set of dependencies updated via a stream of data. As a concrete example, one option of building a UI that depends on mouse clicks is to write a callback function. When the mouse is clicked, the function is called to do some processing and optionally update the UI. However, callbacks can quickly get complicated, especially when they involve multiple threads that may interact with each other. The reactive alternative treats all mouse clicks as a iterable stream of events (Observable, in Rx jargon). An Observer can apply functional constructs – such as filter, map, or zip – to the stream of clicks and can in turn update other Observable objects. For examples involving real code, check out the blog post by Netflix . Sound cool? Then come check out Greg Benson, a software engineer on the Android @ Netflix team, as he presents on incorporating reactive programming into your Android apps with RxJava . Sign up for this April 10th presentation at San Francisco Android Livecode ! Tuesday, April 8, 2014 - 6:00PM - Sensu @ Yelp: A Guided Tour ( San Francisco Dev Ops ) Wednesday, April 9, 2014 - 6:00PM - Java 8 Lambda Expressions & Streams ( Java User Group ) Thursday, April 10, 2014 - 7:00PM - Reactive Programming on Android Using RxJava with Greg Benson! ( San Francisco Android Livecode ) Thursday, April 17, 2014 - 6:30PM - Working Hard, or Artily Twerking Kongsciousness ( Designers + Geeks ) Tuesday, April 22, 2014 - 6:30PM - Women Who Code: Lightning Talks ( Women Who Code ) Wednesday, April 23, 2014 - 6:45PM - Best Practices For Lean User Acquisition and Engagement ( Products That Count ) Thursday, April 24, 2014 - Large-Scale Machine Learning with Apache Spark ( SF Machine Learning ) Wednesday, April 30, 2014 - 6:15PM- Django 1.7 and You ( Django Meetup Group ) Tweet Back to blog", "date": "2014-04-07"}, {"website": "Yelp", "title": "More Yelp in your Ruby", "author": ["\n        \n  Tomer E., Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/04/more-yelp-in-your-ruby.html", "abstract": "What’s that you say? Your Ruby applications are feeling a little empty and meaningless without Yelp data to make them shine? Well, consider your prayers answered because today we’re launching our official Ruby Gem for interfacing with the Yelp API! Find the gem online now, and start using Yelp data in your Ruby applications with ease. Install the gem with bundle by adding ‘yelp’ to your Gemfile, or without a Gemfile by running gem install yelp. Your first step on the road to Yelp data awesomeness is to register and get API keys from our developer site. Next, create a new client with your API keys and use that to make requests against the API. client = Yelp::Client.new({ consumer_key: YOUR_CONSUMER_KEY, consumer_secret: YOUR_CONSUMER_SECRET, token: YOUR_TOKEN, token_secret: YOUR_TOKEN_SECRET }) Integrated into the gem is our Search API , which allows you to to find businesses based on the location provided and any search terms given: results = client.search(‘San Francisco’, { term: ‘restaurants’ }) results.businesses # => [<…, name = ‘Gary Danko’, …>, <…, name = ‘Little Delhi’, …>, …] Additionally, the Business API allows for retrieval of more information on specific businesses using the business id returned from the search: business = client.business(‘gary-danko-san-francisco’) business.name # => “Gary Danko” business.rating # => 4.5 business.review_count # => 3701 Yelp + Rails If you’re still fairly new to Ruby, want to work on a Rails app, and still want to use the gem then we’ve got a solution for you! We’ve created a small sample application to demo the gem and we’ve open sourced that as well. All you need to do is create a file inside of config/initializers with a configuration block that sets your API keys: inside config/initializers/yelp.rb Yelp.client.configure do config config.consumer_key = YOUR_CONSUMER_KEY config.consumer_secret = YOUR_CONSUMER_SECRET config.token = YOUR_TOKEN config.token_secret = YOUR_TOKEN_SECRET end Here, we’re using a configuration block to tell Yelp what your API keys is. This will automatically create a client instance for you that lives inside of Yelp.client, allowing you to use your client instance over and over again in different places throughout your application. Inside of your controller you can call Yelp.client with one of the API methods inside app/controllers/home_controller.rb class HomeController < ApplicationController # … def search parameters = { term: params[:term], limit: 16 } render json: Yelp.client.search(‘San Francisco’, parameters) end end And that’s it! You’re now able to use the Yelp gem throughout your Rails application. Using the gem should work similarly in other Ruby applications. Head over to GitHub to check out the source code for the gem and the example app , and find more information on how to use the gem with the readme and documentation . We can’t wait to see what you create. Learn more about the Yelp API at yelp.com/developers and make sure to share your apps with us ! Please remember to read and follow the Terms of Use and display requirements before creating your applications. If you want to work with our API team (and why wouldn’t you?!), check out yelp.com/careers for available positions. Tweet Back to blog", "date": "2014-04-29"}, {"website": "Yelp", "title": "May the Yelps be with You", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/05/may-the-yelps-be-with-you.html", "abstract": "May brings us talks from the Python meetup, another mind-blowing talk from Designers + Geeks, and a talk from Products That Count on brand naming. I’m also excited to give you a sneak peak into June: we’re hosting our second annual WWDC after party . We promise this after party will be so good, you won’t want to leave for the hotel lobby (even if R. Kelly himself invites you). For our Pythonista readers, let’s take a deeper look into the the upcoming Python meetup. Packaging turns out to be an important part of any language. Great packaging encourages language adoption, focuses the community on 1-2 of the best solutions to a problem, and encourages modular design. But it’s also a surprisingly tricky problem: support for a variety of OS, interactions with compiled libraries, organization of namespaces, programmatic specification of dependencies, and discovery and documentation are just some of the problems that need to be tackled. We haven’t even covered the difference between installing packages “locally” vs system wide, and the implications for deploying a set of packages! Luckily, next week Noah Kantrowitz is going to help us sort through these issues with two presentations covering Python packaging and deployments. In between the main talks, we’ll see lightning talks and get a chance to mingle and ask questions. Hope you can join us! Mark your calendars now for next month’s WWDC: Yelp is opening our doors for an after party to top them all! Meet some of the cool cats who work on our award-winning iPhone app and get an inside look at Yelp life. There will be plenty of 5-star hors d’oeuvres and wine served from our own customized barrels! Please RSVP here and don’t forget to bring your conference badge. Wednesday, May 14, 2014 - 6:15PM - May Presentation Night ( San Francisco Python Meetup Group ) Thursday, May 15, 2014 - 6:30PM - A City of Intelligent Machines ( Designers + Geeks ) Wednesday, May 28, 2014 - 6:45PM - Creating Brand Names With Buzz ( Products That Count ) Tuesday, June 3, 2014 - 6:30PM - Yelp WWDC 2014 After Party ( Yelp Engineering ) Tweet Back to blog", "date": "2014-05-12"}, {"website": "Yelp", "title": "Introducing MOE: Metric Optimization Engine; a new open source, machine learning service for optimal experiment design", "author": ["\n        \n  Dr. Scott Clark, Ad Targeting Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/07/introducing-moe-metric-optimization-engine-a-new-open-source-machine-learning-service-for-optimal-ex.html", "abstract": "At Yelp we run a lot of A/B tests. By constantly trying new features and testing their impact, we are able to continue evolving our products and make them as useful as possible. However, running online A/B tests can be expensive (in opportunity cost, user experience, or revenue) and time consuming (to achieve statistical significance). Furthermore, many A/B tests boil down to parameter selection (more of an A/A’ test, where a feature stays the same, and only the parameters change). Given a feature, we want to find the optimal configuration values for the constants and hyperparameters of the feature as quickly as possible. This can be analytically impossible for many systems. We need to treat these systems like black boxes where we can observe only the input and output. We want some combination of metrics (the objective function) to go up or down, but we need to run expensive, time consuming experiments to sample this function for each set of parameters. MOE, the Metric Optimization Engine , is an open source, machine learning tool for solving these global, black box optimization problems in an optimal way. MOE implements several algorithms from the field of Bayesian Global Optimization. It solves the problem of finding optimal parameters by building and fitting a model of the objective function given historical information using Gaussian Processes. MOE then finds and returns the point(s) of highest expected improvement. These are the points that will have the highest expected gain over the best historical samples seen so far. For more information see the documentation and examples . Here are some examples of when you could use MOE : Optimizing a system’s click-through rate (CTR). MOE is useful when evaluating CTR requires running an A/B test on real user traffic, and getting statistically significant results requires running this test for a substantial amount of time (hours, days, or even weeks). Examples include setting distance thresholds , ad unit properties, or internal configuration values. Optimizing tunable parameters of a machine-learning prediction method. MOE can be used when calculating the prediction error for one choice of the parameters takes a long time, which might happen because the prediction method is complex and takes a long time to train, or because the data used to evaluate the error is huge. Examples include deep learning methods or hyperparameters of features in logistic regression. Optimizing the design of an engineering system. MOE helps when evaluating a design requires running a complex physics-based numerical simulation on a supercomputer. Examples include designing and modeling airplanes, the traffic network of a city, a combustion engine, or a hospital. Optimizing the parameters of a real-world experiment. MOE can help guide design when every experiment needs to be physically created in a lab or very few experiments can be run in parallel. Examples include chemistry, biology, or physics experiments or a drug trial. We want to collect information about the system as efficiently as possible, while finding the optimal set of parameters in as few attempts as possible. We want to find the best trade-off between gaining new information about the problem (exploration) and using the information we already have (exploitation). This is an application of optimal learning. MOE uses techniques from this field to solve this problem in an optimal way. MOE provides REST , Python and C++ interfaces. A MOE server can be spun up within a Docker container in minutes. The black box nature of MOE allows it to optimize any number of systems, requiring no internal knowledge or access. By using MOE to inform parameter exploration of a time consuming process like running A/B tests, performing expensive batch simulations, or tuning costly models, you can optimally find the next best set of parameters to sample, given any objective function. MOE can also help find optimal parameters for heuristic thresholds and configuration values in any system. See the examples for more information. MOE is available on GitHub . Try it out and let us know what you think at opensource+moe@yelp.com. If you have any issues please tell us about them along with any cool applications you find for it! Tweet Back to blog", "date": "2014-07-24"}, {"website": "Yelp", "title": "Guido is coming to Yelp -- to talk about Tulip!", "author": ["\n        \n  Michael, VP of Engineering\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/10/guido-is-coming-to-yelp-to-talk-about-tulip.html", "abstract": "We’ve been brewing up some cool events for October! We took a break in September to move into our new digs, so please come to the right building :) Yelp is primarily a Python shop, so we are especially excited to be hosting the language’s creator, also known as the Benevolent Dictator for Life, Guido van Rossum , for a talk on his work replacing Python’s async I/O libraries with something called Tulip . We’ve also got a bunch of cool talks on Venture Capital brought to you by Women Who Code to a SFHTML event where Chris Wilson is talking about WebRTC . Full list of Upcoming October Events (RSVP by clicking on the links): -\nTuesday, October 8, 2013- 6:30PM - Venture Capital Investor Panel ( Women Who Code SF ) -\nWednesday, October 9, 2013- 5:30PM - All About WebRTC with Chris Wilson, Ben Strong and Dan Ristic ( SFHTML5 ) -\nWednesday, October 16, 2013- 6:15PM - Hear What Guido van Rossum Has to Say About Tulip ( San Francisco Python Meetup Group ) -\nThursday, October 17, 2013- 6:30PM - Find Your Must ( Designers + Geeks ) -\nThursday, October 24, 2013-  6:00PM - CSS3 Workshop ( Girls Develop It ) -\nTuesday, October 29, 2013- 6:45PM - Functional Monthly- Session 5 ( The SF JavaScript Meetup ) Tweet Back to blog", "date": "2013-10-03"}, {"website": "Yelp", "title": "Data Quality: How Yelp stacks up to the competition", "author": ["\n        \n  Jason Director of Search\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/11/data-quality-how-yelp-stacks-up-to-the-competition.html", "abstract": "Here at Yelp we take pride in providing accurate business listings. This is a harder problem than you might think. There are about 50 million businesses listed on Yelp, businesses come and go at an astonishing rate, and reasonable people can — and frequently do — disagree about whether a piece of information is accurate or not (e.g., does a doctor qualify for his own listing or just to be listed under her medical practice?). We’ve always thought we did a pretty good job, but we thought it was worth benchmarking ourselves against the competition. Though there is always room to improve, it turns out we are one of the best. For our study we collected “Best Of” lists from independent local publications like 7x7 in San Francisco, Time Out London, Esquire, and LA Weekly. We assumed that these lists will provide a rough sample of important local businesses as well as correct listing data (backed up by the business’s own website). We pulled together these lists until we had approximately 1,000 businesses distributed across a variety of categories and geographies — from ski resorts in Colorado to pubs in London — giving us a broad range of businesses for our test group. Clearly this approach has flaws. Neither the relative weighting of lists nor our assumptions are perfect — we have 100 gas stations but no French business listings — but it does give us a rough idea of where things stand. Once we had this list of businesses, our team went through the listings one by one verifying name, address, phone number, website, presence of photos, lack of duplicates, and location accuracy on Yelp as well as some of our competitors. Roughly, we gave each data provider 1 point for each correct datum and 0 points for each incorrect or missing datum (you can see a more detailed rubric at the end of the article). Here is an example of a Yelp listing that misses a few points: You may be wondering why we’re looking at photos and not at reviews. Not all of the sites in our study have the same concept of reviews, and we needed a measure of content that is comparable across sites. We also like photos as a data point since they validate listings data: folks posting photos usually have physically visited the business. With all this data collected, we looked to see what percentage of the possible points each listing site received in each category. Without further ado, lets see the results! Entries that are significantly better than the other entries in a given row at a 95% confidence level are bolded. There are a few things of note. First, TripAdvisor and Yellowbook do not have listings in some categories/geographies so they have a smaller set of samples. Second, fewer listings had websites than other types of data. This is at least partly because not all businesses have websites, so the maximum is less than 100%. Finally, one downside of our approach to scoring is that a missing listing gives you credit for not having a duplicate. This flaw in scoring combined with a fairly large number of listings outside of TripAdvisor’s main area of focus — 79 Shopping businesses in Dublin, Ireland for example — likely inflate TripAdvisor’s “No Dupes” score. The high-level story is that in terms of listing data Yelp and Google are closely matched and ahead of the other competition. Google wins out on finding business websites, which isn’t surprising given that crawling the web is part of its core business. On the other hand, Yelp is well ahead of Google in terms of photo content. This is just a rough early step toward understanding the quality of our data. Fortunately, we have a Data team to take on this very challenge. They are applying machine learning and data mining techniques to improve our listings both automatically and by better leveraging the Yelp community. If you think this sounds cool, check out our Data Mining or Data Infrastructure jobs. If you want to dive deeper into these differences across the market and publish your results then ping us at dataset@yelp.com and we will be happy to get you a closer look at the raw data. Scoring Rubric Business Name - Is the name recognizable? -\n1 - Correct within bounds of simple rewrites. “st” vs “street” gets a point, but “Fifth Avenue Barbershop” vs “5th Avenue Barber & Salon” is much harder to match and thus gets no point. -\n0 - Having a name that is either very wrong or unmatchable, not having any listing at all, or being closed and not marked as such Address -\n1 - Correct address -\n0 - Incorrect or missing address data Phone -\n1 - Correct phone number -\n0 - Incorrect or missing phone number Has Photos - A rough cross-site measure of content quality -\n1 - Has any photos -\n0 - Has no photos Website -\n1 - Correct website link -\n0 - Incorrect or missing link No Dupes - Are there any duplicate listings of the business? -\n1 - There is at most one listing of the business -\n0 - There are multiple listings for the business Location accuracy - Can you find the business from the map pin? -\n1 - Location is close enough to easily find the business. Fifteen yards down the street is OK, 3 blocks away is not. 0 - Location is clearly different than other sites, like not on the same block. Spot-verified by Google Street View Tweet Back to blog", "date": "2013-11-07"}, {"website": "Yelp", "title": "Whoa! That Embedded Web View Looks Hot in Your iOS App!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/11/whoa-that-embedded-web-view-looks-hot-in-your-ios-app.html", "abstract": "This post comes to us from Allen C., an engineer on our mobile team. The mobile team has dozens of innovations under their belt, and today Allen explains how the iOS team uses HTML views to quickly roll out features that originate on the web. In the third quarter of 2013, the Yelp mobile app was used on more than 11 million unique mobile devices on a monthly average basis. We’re continuously pushing the envelope to make the app user experience as great as possible. A common requirement in our app is displaying embedded web content for a variety of different features. One such feature is our new Yelp Platform, which allows users to order food from participating businesses directly from our site and mobile apps. In this blog post we’re going to walk you through building a seamless embedded web content experience for your native app on iOS. Why do you need to embed web content? Well, sometimes it makes sense in order to take advantage of the great mobile website you’ve already built. Other times, the content you want may only exist on the web. Here are some techniques that we use at Yelp to display gorgeous web content, while preserving the great experience our users have come to expect from the app. The typical method for displaying web content in an iOS app is to create a UIWebView and pass it a URL to load. If you only do that, you might end up with something that looks like this: This works, but it can be a jarring experience for users, depending on what type of web content you are showing. It feels as though they’ve left your app and entered a scaled-down version of Safari. One striking example, highlighted in the screen capture above, is the lack of a dedicated loading graphic - your user will see a blank screen while waiting for the first page to load. There are several things that are not optimal about this experience: The look and feel of the web view may be entirely different than the rest of the app. Navigation and page transitions within the web view are most likely different than the rest of the app. There is no way for the user’s interaction with web content to directly affect native views in the app. Since problem 1 is dependent to a large part on the specific app look and feel, this post will focus on overcoming problems 2 and 3. To tame UIWebView we need to give it a controller that implements UIWebViewDelegate . UIWebViewDelegate is a protocol that defines a set of methods which give the view controller significantly expanded control over its web view. We’ll be focusing only on one of those methods: - (BOOL)webView:(UIWebView *)webView shouldStartLoadWithRequest:(NSURLRequest *)request navigationType:(UIWebViewNavigationType)navigationType; UIWebView calls the method from its delegate (delegation is how Apple’s UIKit framework implements the Model View Controller pattern; a view’s delegate is typically its controller) whenever it is about to load a URL request, either as a result of a user’s action, or loaded programmatically from the app. It passes 3 arguments: itself, the URL request about to load, and a navigation type. The return value is where things get interesting: it’s a flag that tells the UIWebView whether or not to actually load the URL request. If you return NO, the web view will simply not load the request and do nothing. However, that doesn’t mean our app will do nothing - we will add our own code in this method which will specify the app’s response to this URL request. Native Looking Transitions The first thing we can do is create native looking animations when loading web pages. Typically, when a user clicks a link in a web view, the web view displays a loading screen and then displays the new page in place once it is finished loading, just like on a mobile browser, and this is what will happen if we return YES in the delegate method. Instead of doing that, we will return NO as discussed above, and then we can take the URL request that the web view wanted to load, and open it in another web view! The new web view can animate into the screen in whatever way best matches the look and feel of our app. The pseudo-code to do this looks roughly like this: - (BOOL)webView:(UIWebView *)webView shouldStartLoadWithRequest:(NSURLRequest *)request navigationType:(UIWebViewNavigationType)navigationType {\n  // Load the first request in place, because there is no web view currently showing\n  if (self.makingFirstRequest) {\n    self.makingFirstRequest = NO;\n    return YES;\n  }\n  \n  // The web view that is currently showing originated the request\n  if (webView == self.visibleWebView) {\n    [self.hiddenWebView loadRequest:request];\n    [UIView animateWithDuration:duration animations:^{\n      // Some desired animation here\n    } completion:^(BOOL finished) {\n      UIWebView *oldVisibleWebView = self.visibleWebView;\n      self.visibleWebView = self.hiddenWebView;\n      self.hiddenWebView = oldVisibleWebView;\n    }\n    return NO;\n  }\n  return YES;\n} Voila! We’ve just roughly implemented native looking animations between web page transitions. This isn’t complete yet, but the basic idea is here. One thing we learned while implementing this is that not every new URL request should be loaded in a new web view and animated in. For example a site might load an iframe which relies on being part of the original page. In this case, just opening the iframe URL in a new web view would be incorrect. Web View Events We can extend the same concept in order to implement dynamic interactions between web content and the native app. In this case, we simply define a new URL scheme: for example, ‘mobile-event’. When web content needs to interact with the native app, it can simply tell the browser to open a URL with this scheme. At Yelp, we do this by having the mobile site load an iframe with this custom URL and immediately close it. The app will detect this URL being opened, and must respond appropriately to the “web view event” in the delegate method. Here is some pseudo code: - (BOOL)webView:(UIWebView *)webView shouldStartLoadWithRequest:(NSURLRequest *)request navigationType:(UIWebViewNavigationType)navigationType {\n  // Detect a web view event\n  if ([request.URL.scheme isEqualToString:@\"mobile-event\"]) {\n    // Execute code here for the event\n    // Make sure to return NO or the web view will try to load a fake URL\n    return NO;\n  }\n  // Execute normal URL request handling logic\n} What the application does in response to a web view event is context dependent, but one example is to either load a new non-web view, or to pop back to an existing view on the navigation stack. This allows the flow on our web content to integrate seamlessly with the native app. Putting it All Together - Yelp Platform Let’s look at an example from Yelp’s new Platform feature, which allows users to order food from participating businesses directly from the Yelp iOS app (this is available on web and Android too, but let’s focus on iOS right now). The Yelp Platform flow is currently implemented through the mobile site and displayed in the iOS app on a web view. From the Yelp business page, the user can tap the Order Pickup or Delivery button, which loads a web view starting the platform flow on the order form. From there the web view controller uses native looking transitions to animate the menu onto the screen. Once the user reaches the checkout page and completes the purchase, our mobile site sends a web view event, notifying the iOS app that the purchase is complete. The iOS app then pops back to the business view, now with a nice little alert that the order has been placed and an email confirmation has been sent. What’s Next We’ve currently got several new features in the works for integrating web content into the Yelp mobile apps and making our user experience that much better. Hopefully, this post will also give you a few ideas for how your own iOS apps can integrate dynamic native looking web content. Tweet Back to blog", "date": "2013-11-07"}, {"website": "Yelp", "title": "Cool New Space, Cool New Tech", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/12/cool-new-space-cool-new-tech.html", "abstract": "On Wednesday, November 20, Yelp opened up the doors of our new space and invited tech industry friends to come by and see what we’re up to. Scott Clark and John B., two of our engineers, gave presentations about current technology challenges we’re working on here at Yelp. Search Engineering Manager Chris T. gives us a play-by-play below! The crowd started arriving at our new building in San Francisco even before the official start time of 6pm. I ended up chatting with some folks in front of the building and did not get up to the party on the 8th floor until around 6:30. By then it was packed! The bar was serving up drinks as fast as they could and hors d’oeuvres were brought to attendees by servers walking through the crowd. Normally, our coffee-bar. Tonight, our bar-bar. Only empty at this moment because tech talks were going on. Another shot of the new space, a bit earlier in the day. After enjoying some food, drinks and conversation, our tech talks started. First up was Scott, who gave a great talk about how to apply techniques from optimal learning - such as bandit algorithms and bayesian global optimization - to automatically improve the performance of our experiment framework. These techniques are already being applied in production here at Yelp. Next up was John, who described how we’ve integrated ElasticSearch at Yelp. Prior to working with ElasticSearch, we had built out search using custom services. John’s talk gives a great overview of some of the reasons we chose ES for our future development, as well as general tips for folks building out search on their site. Video and slides of the tech talks: “Optimal Learning for Fun and Profit” by Scott Clark (Presented at The Yelp Engineering Open House 11/20/13) from Yelp Engineering “Using ElasticSearch to Scale Near Real-Time Search” by John Billings (Presented at The Yelp Engineering Open House 11/20/13) from Yelp Engineering After the tech talks concluded, everyone mingled on our 8th Floor. We marked off various corners with topical discussion, such as Data Mining, Backend, Ops and so on. A lot of people ran into folks they crossed paths with previously and were happy to both reminisce and discuss the new things they’re working on. Keep following this blog for more updates about the exciting meetups and events we host, like the upcoming Intern Networking event, Yelp NITE , Python meetups and more. Tweet Back to blog", "date": "2013-12-06"}, {"website": "Yelp", "title": "Yelp Internship Program Summer 2013", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/12/yelp-internship-program-summer-2013.html", "abstract": "This summer, our interns spent their weekdays in downtown San Francisco alongside full-time engineers developing some of Yelp’s latest releases: mobile reviews , Yelp’s launch in Brazil , and others still in the works. Mark M. and Olivia G., interns from our mobile and community teams, shared a little about their projects. Though rivals on their college campuses, our Cal and Stanford interns put aside their differences to engineer some really neat stuff. Mark M. (Mobile Team - UC Berkeley) “I spent the summer creating an entirely new framework built around data analytics. My tool allows us to visualize and analyze critical user flows within our mobile apps and perform and monitor A/B tests. The most exciting part of this project was that I was allowed to work on the full stack from start to finish. From using mrjob (Yelp’s open-source map reduce framework on Hadoop), to collecting and parsing the important information, to building an entirely new front-end interface using javascript frameworks such as Angular.js, and D3, my project definitely taught me a lot about different technologies! One of my favorite parts about Yelp is that the interns don’t feel like interns, they feel like full-time engineers. I felt like a full-time engineer because of the large scope of my project and because of how autonomous I was allowed to be in deciding the technical design and implementation of the tool. After I gave a quick technical talk about my project to the entire engineering department, there was interest from many of the different teams at Yelp to integrate their metrics projects with mine. I could immediately tell that the project I had been working on was extremely useful and will continue to make an impact in the future.” Olivia G. (Community Team - Stanford) “I joined the Yelp team on April Fool’s Day 2013 and was, respectfully, only mildly pranked on that first day. Luckily, my internship offer was not a joke, and I quickly settled into our team project to revamp The Weekly Yelp . The Weekly Yelp is a fun newsletter that gets sent to over 100 distinct markets, nearly half of which are international, and reaches millions of subscribers., Our project sought to improve every step of providing the content, from the way our data was structured and stored, to how featured content was selected and presented, to the process by which the emails were queued, rendered, and sent. In my six months on the team, I have been able to help on this project, from creating a new repository for our code right at the outset, to one of the final essential tasks of wiring up the email template and queueing them to send to our subscribers. A few takeaways from this project: deciding on naming conventions is hard, but worth it; fit the use cases of the past, but don’t let old code restrict your thinking; and designing a flexible structure will not only help future users but may also help you during development, as decisions are made and changed. I have loved making concrete progress on The Weekly Yelp project every day and have loved working with such a supportive team and company. But watch out for darts!” Tweet Back to blog", "date": "2013-12-19"}, {"website": "Yelp", "title": "3D Printing at Yelp: Space, Drones and Monopoly", "author": ["\n        \n  Yoni D., Product Designer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/02/3d-printing-at-yelp-space-drones-and-monopoly.html", "abstract": "The desktop 3D printing evolution has taken the world by storm. For me, it all started a little over two years ago when Yelp bought its engineers a MakerBot Thing-O-Matic 3D Printer to play around with. The sounds and smells that accommodate desktop 3D printing have since become second nature to our corner of the office. While we haven’t yet found a way for 3D printing to help our users connect with great local businesses, it has been a great driver of innovation and fun in our engineering team. What started out as a few engineers printing toys has since spiralled into super creative projects that have stirred up great support from everyone here at Yelp. It was a group of mobile developers that assembled Yelp’s first 3D printer. I was quickly drawn to the machine after seeing it print a ridiculously bad looking (but functional) whistle. Soon after, a small group of engineers and myself rallied behind printing our own copy of the infamous Turtle Shell Racer . The large parts of the Turtle Shell Racer pushed the Thing-O-Matic to its limits and forced us to tweak and modify the printer constantly. The result however was amazing! We turned a spool of plastic and a few lunch breaks and evenings into a cute little R/C car. Seeing how far we had come empowered us to see what designs we could come up with ourselves. We wouldn’t have to wait long to put our imagination to the test. A few times a year, Yelp has a two-day period called Hackathon where our engineers set aside their everyday responsibilities to work on projects that are purely innovative and fun. At the next Hackathon, my team and I decided to try and send an iPhone to Space via weather balloon. The idea was to Check-In from Space using Yelp’s mobile app. The weather balloon’s tracking service that was shared with our Yelp colleagues proved very popular, as the 3rd party service soon went down due to an overload of traffic. With the help of 3D printed parts of our own design and a lot of tenacity, our iPhone did reach the edge of Space. However, on reentry we lost our balloon somewhere in the hills near Pyramid Lake in Reno Nevada. It wasn’t until the next Hackathon a few months later, when we returned with an autonomous reconnaissance drone loaded with custom 3D printed parts, that we were able to locate and recover our balloon payload. Our drone consisted of a Bixler airframe, an Ardupilot system, a GoPro camera and a custom short range “First Person View” system. Amazed by the drone we had managed to build, we decided to build a bigger and better drone to explore and share the possibilities of this new technology. We would also open source all our code, parts and 3D models. To our amazement the “ Burrito Bomber ” concept drone we made and its accompanied video went viral. Before long our drone and it’s Thing-O-Matic 3D printed parts had a quarter of a million views on YouTube and had been covered by CNN , The Huffington Post , Forbes and more ! They seem to really like the idea over at Amazon too. You’re welcome Jeff. Our next project would truly test if the sky’s the limit for 3D printing. We decided to try and print in the Earth’s Stratosphere at 100,000 feet altitude. Making a printer that’s light enough to be carried by a weather balloon but able to 3D print in an icy -50° Fahrenheit would be our biggest challenge yet. Using a modified Printrbot Simple , custom g-code and a lipo battery we decided to give it a shot. After only one failed attempt , we succeeded in printing a small Yelp / Printrbot logo at no less than 111,159 feet. That’s a full year before NASA’s 3D printer will join the 20-mile-high club. Not all 3D printed projects here at Yelp involve boy toy themes like space or drones though. Currently we’re experimenting with 3D scanning using a Makerbot Digitizer and printing on the more reliable Makerbot Replicator 2. At our most recent Hackathon we clay sculpted and 3D scanned our Yelp mascots before 3D printing them at “Honey, I Shrunk the Kids” size. Using a resolution of 100 microns and spray painting the models revealed the perfect game pieces to accommodate a custom Yelp Monopoly board. Oh, and obviously we also open sourced them (we even added a free bonus goat ). Now, in an effort to start cloning talented engineers, I’ve been using a Microsoft Kinect to scan engineers and print their busts . So far the clones have been lacking in the productivity department though. If you’re interested in building a great product, playing around with fun technology and want to save me the trouble of trying to perfect 3D printing cloning technology, you should think about applying to Yelp. Tweet Back to blog", "date": "2014-02-03"}, {"website": "Yelp", "title": "Fab February Events @Yelp", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/02/fab-february-events-yelp.html", "abstract": "It’s a big month for awesome events at Yelp HQ!  Of special interest to those wondering how Yelp uses Hadoop is the Hadoop User Group on the 19th.  I’ll be presenting on where Hadoop fits into our Big Data stack.  Unfortunately, Hadoop isn’t quite pixie dust you can sprinkle on data and transmorph it into insights.  To get the most of the system, we’re careful what we feed in, how we schedule jobs, and what we do with the results.  Please RSVP to join me for a great discussion! Other meetups this month include the always diverse and interesting Designers + Geeks, Python, and Data Science groups.  For those of you working hard at Developer Week , take a load off and refuel at Yelp’s free official after party - food, drinks, and shenanigans guaranteed. See you at Yelp! Wednesday, February 12, 2014- 6:15PM- Web, Data and Best Practices ( SF Python Meetup Group ) Tuesday, February 18, 2014- 6:30PM- Reboot at Yelp ( Developer Week After Party ) Wednesday, February 19, 2014- 6:30PM- Oh Hadoop, Where Art Thou? ( San Francisco Hadoop User Group ) Thursday, February 20, 2014- 6:30PM- Music’s Visual Future ( Designers + Geeks ) Wednesday, February 26, 2014- 6:30PM- Open Source Data Science Education with Open DST, MatterMark, and Udacity ( SF Data Science Meetup ) Tweet Back to blog", "date": "2014-02-07"}, {"website": "Yelp", "title": "Yelp’s got style (and the guide to back it up)", "author": ["\n        \n  Molly F. Front-End Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2014/02/yelps-got-style-and-the-guide-to-back-it-up.html", "abstract": "We’re excited to publicly share Yelp’s styleguide — a living document that we’ve used internally since May 2013 to create visual consistency across Yelp and reduce technical debt with modular, reusable markup and styles. To get this project off the ground, our front-end and design teams worked together more closely than ever before. We’re very pleased to be able to share it with anyone interested in hearing how we moved to a fast-paced UI design and development model. This kind of design and development requires a lot of discipline across product and engineering teams. “There are no special cases” has become our mantra. When working on a new feature, we hold fast to these rules: Use the pre-established patterns. No, really , please use the pre-established patterns. If the pre-established patterns do not solve your design problem, you have two options: a. Alter a pre-existing pattern to solve your problem, and implement that change across all of Yelp via a change to the styleguide. b. Establish a new pattern, and integrate it into the canon of Yelp’s UI patterns for future use. Now, onto the whys and hows: Yelp has been evolving at a rapid pace over the last nearly 10 years. Each new feature has brought improvements to the product, but also introduced more and more markup and styles. Our front-end code base was getting out of control and while the Photoshop mock-up to code workflow wasn’t exactly broken, it wasn’t as efficient as it could be either. As designs for our new business listing page began to take shape, it became clear we were establishing the future of Yelp’s look and feel. This would be the last time we’d go from Photoshop mock-up to coding the UI from scratch. Even before development on the new page began, we started pulling components out of the design and building them in the styleguide. Using Sass mixins, we applied the new grid system to all of our existing layouts. With the components already built, it was easy to refresh the search page and homepage with the typography, forms and containers from the styleguide. Solve something once, why solve it again? The styleguide surfaces existing solutions for designers as well as developers. When working on a new feature, the designer doesn’t need to think about how to call out information on the page: the “island” pattern does just that. When implementing the front-end for the same feature, the developer doesn’t have to think about how to build that island from scratch. They can simply use the documented markup. No new css necessary. This saves a lot of time and frees up mind space so we can stop thinking about what certain tabs should look like and instead focus on designing and building engaging user interfaces. It’s alive, it’s alive! Our patterns are documented in live production code, so what you see in the styleguide is exactly what you’ll see on the site. This is great for cross-browser testing our components and, unlike static design documents, there’s no need to worry about it becoming out of date. We explored a number of options for live css documentation. An early version ran on PyKSS (the python flavor of Kyle Neath’s KSS ), a framework for generating live styleguides from descriptions and markup in css comments. We enjoyed working with PyKSS, but ultimately chose to develop a custom solution. This allowed us to make use of existing partial templates. It also made it easier to provide code snippets that developers should actually include in templates, rather than the markup that those snippets output. No component left behind The styleguide makes cross site changes a breeze. Since development on the business listing page began, we’ve made several visual tweaks to our patterns. Our grays got warmer. Our islands lost their embossed look. The style was updated in one file and the changes were reflected everywhere the component was used, as well as in the styleguide. See it in action References While creating our styleguide we took inspiration from a number of awesome sources: - Creating Living Style Guides (Nicole Sullivan) - Unifying Agile and UX with Live Style Guides (Grant Hutchins, Andrew Cramer, Steve Berry) - Compose to a Vertical Rhythm (Richard Rutter) - Front-end Style Guides (Anna Debenham) - Atomic Design (Brad Frost) - Github’s Styleguide - Mailchimp’s Pattern Library - Salesforce1 Styleguide - Starbucks’ Style Guide Want to work closely with top-notch designers to build engaging user interfaces for 120 million monthly visitors? We’re hiring! Tweet Back to blog", "date": "2014-02-11"}, {"website": "Yelp", "title": "GrossQueryChecker:  Don't get surprised by query performance in production!", "author": ["\n        \n  Jenni S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/05/grossquerychecker-dont-get-surprised-by-query-performance-in-production.html", "abstract": "Ever been woken up at 3am by a misbehaving database query? Jenni S. , Yelp’s MySQL DBA, walks us through Yelp’s system to prevent such rude awakenings. Here at Yelp, we push multiple times a day.  We do this safely and surely with our process of code review, integration, automated testing, and extensive monitoring. I think that we’re all used to the idea of automated testing for our code, but what about the queries generated by the application against the database? Catch potential problem queries in testing, not production In most engineering organizations, it’s common for the development, build, test, and/or stage environments to include only a subset of production data.  This can be for increased portability (a smaller data set is easier to remove/recreate), ease of verification, or because we don’t want to maintain a comparable set of powerful databases for a substantially smaller set of traffic.  In any case, a smaller data set and different traffic pattern can mean radically different performance profiles between queries in development and production.  To keep us from being surprised by queries only once we get to production, we created a tool called the GrossQueryChecker. How does it work? The GrossQueryChecker uses the MySQL EXPLAIN command to check the query plan for each query executed in our development environment and by our test suite, raising an exception when a query execution plan suggests it will perform poorly. This is done using a wrapper around our database connections.  In our development and test environment, it runs EXPLAIN to capture the query plan, analyze it, and, if any issues are found, logs the query and raises an exception. What makes a query “gross”? Each query is assigned a score, and if the score crosses a threshold we set, the query is logged and an error raised.  Scores are calculated by applying weights to one or more of the following potential issues: full table scans, detected by a type column having a row with ALL mitigated slightly if  the extra column contains “Using where” similarly, if no index is used, detected by the key column containing NULL subqueries, detected by examining the select_type column if Using temporary; Using filesort appears in extra, it indicates we’re creating a temporary table and doing an extra pass through the data proportionally high numbers in the rows column when compared to the number of rows in the table (obtained using SHOW TABLE STATUS LIKE ‘table_name’) high numbers in the rows column in general too many rows in the explain plan itself:  we use an ORM to generate many of our queries, but we don’t want/need overly complex queries\nThe GrossQueryChecker will also attempt to calculate the size of the data being read, sorted (if, Using temporary; Using filesort appears in extra), and returned by the database. If over a threshold, this can also trigger an exception to be raised. What do engineers do when a query triggers an error? Now that we have been using the GrossQueryChecker for a while, these exceptions come up infrequently.  However, when one does, the messaging around the exception raised indicates why the query was flagged, and we have internal documentation to help engineers resolve it.  Usually this involves changing the way tables are joined, adding an index, or consulting a friendly neighborhood DBA. Quick example:  using the the command line tool First, please note that this query and numbers are NOT representative of any real dataset. Here, I’ve submitted a sample query for evaluation, missing a WHERE clause: ince this query performs a full table scan and considers a LOT of rows, it’s score is very, very high.  This score would absolutely cause an exception to be raised, tests to fail, and a developer to take notice. A logical next iteration would be for the query to then be submitted with a WHERE clause that takes advantage of an index.  Here, we’re using an index on the business_id column, but still seeing a temporary table and filesort, but on a small enough number of that the score is 0.00. A final iteration of this query, eliminating the temporary table and filesort may involve adjusting the indexes so that a single index can cover both the WHERE clause, as well as the GROUP BY: The score remains 0.00, but you can see that the query time improves! What if a query can’t be improved? In some cases, we can’t improve a query’s performance and are willing to live with it (say, when a query is issued by a reporting process, and not against a production database.)  This is exceptionally rare, but for this purpose we maintain a whitelist of queries.  The wrapper that performs the query plan checking will first normalize each query it finds, and check it against this whitelist.  If whitelisted, the query will not be checked for performance. By catching these queries early in the development cycle, we reduce the chance of impacting production!  It’s always important to monitor your production system for poorly performing queries, and we use the GrossQueryChecker in conjunction with other tools to keep the data flowing.  I could see future iterations of the GrossQueryChecker taking advantage of the new JSON format option to the EXPLAIN command in MySQL 5.6 to more easily parse the query plan, and/or use Percona Toolkit’s pt-visual-explain to present the query plan for a query in an easier to read format. While we haven’t open-sourced this tool yet, you can check out Yelp’s github repo for other tools we have released. Tweet Back to blog", "date": "2013-05-14"}, {"website": "Yelp", "title": "Never Stop Learning: School’s Out, Yelp’s In!", "author": ["\n        \n  Julia N., Senior Events Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/06/never-stop-learning-schools-out-yelps-in.html", "abstract": "Machine Learning (ML) plays a critical role here at Yelp. It allows us to transform Big Data into usable information. A major area of research in ML is understanding how to apply the myriad of techniques available to an engineer. In addition to deciding on which algorithm to use, a practitioner must decide how to verify models, distribute computation, and monitor performance. Best practices exist in each of these areas, but novices may not be aware of them, and they can be tedious, even for experts. We know that many problems in computer science can be solved with another layer of abstraction, and the MLBase project is trying to do just that. The project hopes to automate the selection, implementation, and deployment of ML solutions by using “plugins” that implement different algorithms and carry meta-data about how they can be used. MLBase was the subject of our most recent meetup, organized by the SF ML meetup group . Ameet Talwalkar, a Computer Science fellow from UC Berkeley, joined us to share his research on MLBase and a specific algorithm implemented therein. His research connects many related fields of computer science together, from algorithms research, to distributed systems, to language and API design. In just a few short weeks, we have another mastermind joining us to speak about programming on the front end. SFHTML5 has invited Pamela Fox, the host of GirlsDevelopIt SF, another one of our monthly meetup partners, to present on how to break down HTML5 and its unequal feature distribution. We’re looking forward to seeing Pamela again and hope that you can join us as well! Upcoming June Events: -\nWednesday, June 12, 2013- 6:15PM- Learn How Disqus Does “It” When “It” Isn’t Django ( San Francisco Python Meetup Group ) -\nThursday, June 20, 2013- 6:30PM- Hardware by Design ( Designers + Geeks ) -\nThursday, June 27, 2013- 5:30PM- Feature Detection and Fallback with Pamela Fox ( SFHTML5 ) Tweet Back to blog", "date": "2013-06-12"}, {"website": "Yelp", "title": "Django in July", "author": ["\n        \n  Julia N., Senior Events Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/07/django.html", "abstract": "Something that we value at Yelp is keeping things new and exciting. A few days ago, we were able to host a brand new meetup group, the San Francisco Django Meetup Group ! They launched their summer speaker series by inviting Randall Degges to discuss how he grew his Python API company using Tastypie, Django, Flask and Amazon’s DynamoDB. He touched on the trade-offs of these technologies from a business perspective and, along the way, told some fun stories from his own company. One of the techniques he advocates is service-oriented architecture (SOA), something that we here at Yelp have been aggressively moving toward as well. Later this month, we’ll be hosting three other exciting meetups to kickstart the summer: SF Java User Group , SF Python and Designers + Geeks . We’ll be learning about a variety of topics ranging from IPv6 (the latest revision of Internet Protocol implemented to address IPv4 exhaustion), to understanding the importance of creating design for end-users. Josh Brewer, Tom Hatch and Owen DeLong will bestow their knowledge upon us, and pizza and drinks will fill our bellies while enjoying the sunset view from the Yelp HQ event floor. Hope to see you there! Upcoming July Events: - Tuesday, July 9, 2013, 5:30PM - IPv6 - An Introduction for Developers ( SF Java User Group ) - Wednesday, July 10, 2013, 6:15PM - Learn About Building System and Configuration Management Tools From Tom Hatch ( SF Python ) - Thursday, July 18, 2013, 6:30PM - Design is about Relationships ( Designers + Geeks ) Tweet Back to blog", "date": "2013-07-08"}, {"website": "Yelp", "title": "Building the Wordmap", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/07/building-the-wordmap.html", "abstract": "Mark W. and Chris C. are part of the team behind our recently launched Wordmap , and today they give us a peek behind the scenes. Read on to learn how we’re helping people with a critical need: avoiding hipsters (oh yea, and connecting with great local businesses!) We have a lot of data here at Yelp, and we’re always thinking about cool new ways to explore it. One recent example is the Wordmap , a visualization of geographical patterns of word usage in Yelp reviews across a city. We’ll step through how we built this feature: how we transformed our review data into heat maps of word activity, and how we store and display the data on the front end. mrjob How do we make sense of the over 39 million reviews Yelpers have contributed to the site? mrjob ! mrjob is an open source project released by Yelp built to provide a way for engineers to run log processing jobs on the hadoop cluster at Yelp while retaining all the benefits of working in Python. Yelp now uses mrjob exclusively with Amazon’s EMR framework to power all the large data processing batches. With mrjob, we were able to quickly tokenize reviews into individual words and then add them up for each business. In pseudo code: function map(review): tokenize into words discard stop_words for each word: emit tuple(business, word), 1 function reduce_word((business, word), partial_count): emit business, tuple(word, sum(partial_count)) function reduce_business(business, (word, count)): # emit business and a dictionary of word and counts emit business, dict(word, count) Data normalization We wanted to avoid the common pitfall of geographic heatmaps , so we couldn’t simply plot the raw word counts over a map. Instead, we calculated the number of times the word was mentioned above average at a business. This helps get at the core of the idea, which is to find the most hipster-y/bacon-y/etc neighborhoods. If a business had 1000 reviews, but 20 mentions of a word, that wasn’t as impressive as if a business had 100 reviews but 10 mentions of a word. Since we had the count of each word for every business, we could calculate the aggregate count for each word by city, and from there, the total word count for each city. This allowed us to find the average frequency of a word in each city. We then subtracted the number of mentions of a word from the expected number of mentions based on the city-wide frequency of the word and the number of words in reviews a business had. Since the word counts are consistent with a power-law distribution , to arrive at the final Wordmap we applied a numerical transformation to flatten the data. This places the word counts along a linear and more human relatable scale. Word selection We selected some of the words based on how visually interesting their patterns were across cities. For local knowledge we relied on our Community Managers in each city for cool and unique words (for example, “fish and chips” in London). We also discovered some words based on how much more frequently certain cities used a word over the rest of the world (for example, “espresso” in Seattle). Hosting and displaying the data Once we had our data formatted in terms of locations and weights, we used the Google Maps API and its Heatmap Layer functionality to render the word density plot. The more mentions above average a word had for a business, the greater the intensity of the heatmap at that location. Where businesses with high intensity densely cluster, the Wordmap is a deep red. We found that a gradient across opacity created a more compelling view of our data than one across color, since the default sharp boundary at zero makes less sense given how our data is normalized. We also had to choose how to make it available to the front-end. Because the data is large and rarely changes, it didn’t make sense to store it in an application database. Instead, we formatted the data as JSONP and hosted it in static JS files on our CDN via S3. The JavaScript powering the Wordmap exposes a callback which every script file references as soon as it is loaded. This approach allowed us to decouple the application from the data, so that either can be updated independently. What’s next We’ve got a lot more ideas brewing for finding new meaning in the rich content contributed by our passionate users. If building the next great way to explore our data sounds like a fun challenge, we’d love to have you here … Thanks to engineer Mark W., front end engineer Molly F., designer Stephen V., and data scientist Chris C.! Tweet Back to blog", "date": "2013-07-11"}, {"website": "Yelp", "title": "Hot August Events at Yelp", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/08/hot-august-events-at-yelp.html", "abstract": "We’re super excited to have three events happening at Yelp this month. The SF Java User Group returns to Yelp with a presentation on “The Eschatology of Java.” Abdelmonaim Remani, a platform architect at just.me answers the question “What is the future of Java?” by analyzing how different forces, such as mobile and the growth of functional languages, will affect the Java ecosystem.  Abdelmonaim promises an “emotionally-charged” session, so fuel up with drinks and pizza and get ready to geek out! For the designers and product folks reading, we have a talk for you on “The Real Me,” happening on August 22. Aarron Walter is the Director of User Experience at MailChimp and will share his insight on brand voice and creating a more honest relationship with your customers. Upcoming August Events: -\nTuesday, August 13, 2013, 5:30PM - The Eschatology of Java ( SF Java User Group ) -\nThursday, August 22, 2013, 6:30PM - The Real Me ( Designers + Geeks ) -\nWednesday, August 28, 2013, 6:15PM- Learning More About Postgres’ Performance ( SF Python ) Tweet Back to blog", "date": "2013-08-13"}, {"website": "Yelp", "title": "HTML5 Canvas Learning Group", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/09/html5-canvas-learning-group.html", "abstract": "Up today is another learning group! For those new to the concept, Yelp’s learning group happens every Friday: we serve lunch and listen to a tech talk about something cool, from learning a new language to strategies in Starcraft.  Last month we had an excellent talk by Cameron P., a web developer here at Yelp, on HTML5 Canvas.  He covered how to speed up 2D rendering in browsers and demoed his personal project “ Literally Canvas ,” a WYSIWYG drawing widget you can embed on any web page. With HTML5’s Canvas, you can create: graphs, animations, games, image compositions, and more. The talk covers the graphics pipeline and how it relates to high level drawing operations, then dives into various techniques for improving performance, including: -\nminimizing state changes and drawing operations -\noffscreen rendering -\nefficient animations taking advantage of canvas specific quirk Tweet Back to blog", "date": "2013-09-04"}, {"website": "Yelp", "title": "Yelp Engineering Opens an Office in Palo Alto: Come Help the World Find Great Local Businesses!", "author": ["\n        \n  Michael, VP of Engineering\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/09/yelp-engineering-opens-an-office-in-palo-alto-come-help-the-world-find-great-local-businesses.html", "abstract": "So you want to be part of the Yelp engineering team, but you just wish you didn’t have to commute all the way up the peninsula to San Francisco? Gosh, if only there were a way…hold on a minute, now there is! Yes, we have read your mind and are pretty psyched to announce the opening of a Yelp office in Palo Alto! Bonus: it’s conveniently located mere blocks from the Caltrain station at California Avenue and in the midst of tons of local businesses and lunch spots. As part of the Palo Alto team, your mission, should you choose to accept it, will be to support developers and Yelp’s move to service-oriented architectures. Yelp serves an average of more than 108 million unique visitors every month, an increasing number through mobile devices, and connects these users to local businesses through more than 42 million online reviews. Behind the scenes Yelp’s software engineers diligently create the frameworks and templates to display and promote all that content. The Palo Alto team will focus on making Yelp’s development efforts more productive, efficient and scalable as we expand around the globe. Get your hands dirty with projects geared at speeding up developer environments and allowing developers to effortlessly use public cloud resources to run those environments. We’re looking for folks that are passionate about making their fellow engineer’s lives as efficient as possible. Long story short: You: Possessing mad engineering skills, looking to be gainfully employed in Palo Alto, and interested in open source and technologies like SQLAlchemy, Vagrant, packer.io, Python, Puppet, Docker and other leading edge technologies. Us: Offering killer benefits, fun in-office perks, projects to challenge your wit and skill, and looking for the perfect match to grow our Yelp Engineering family. Sound like your dream job? Check out our current openings in Palo Alto , and frequent yelp.com/careers for all Engineering job openings at Yelp. Tweet Back to blog", "date": "2013-09-30"}, {"website": "Yelp", "title": "Yelp Dataset Challenge Winners & Round Two Now Live", "author": ["\n        \n  Dr. Scott Clark, Ad Targeting Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/10/yelp-dataset-challenge-winners-round-two-now-live.html", "abstract": "The Challenge The inaugural Yelp Dataset Challenge opened in March 2013 with the release of our latest academic dataset featuring reviews and businesses from the greater Phoenix metro area. The goal of the dataset was to encourage development of new techniques in data analysis and machine learning while providing the academic community with a rich dataset over which to train their models. Students who submitted their research related to the dataset were eligible for a cash reward and further incentives for publishing and presenting their findings. The Winners of the First Yelp Dataset Challenge The challenge was viewed by many thousands of people and thousands of qualified applicants participated by downloading the dataset. From the completed entries we received, a team of our data mining engineers have selected four entries as grand prize winners (in alphabetical order by entry name): “Clustered Layout Word Cloud for User Generated Review.” Ji Wang, Jian Zhao, Sheng Guo, Chris North. Virginia Tech and University of Toronto. “Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text.” Julian McAuley, Jure Leskovec. Stanford University. “Improving Restaurants by Extracting Subtopics from Yelp Reviews.” James Huang, Stephanie Rogers, Eunkwang Joo. UC Berkeley. “Inferring Future Business Attention.” Bryan Hood, Victor Hwang, Jennifer King. Carnegie Mellon University.\nWe were extremely pleased with the breadth and depth of the entries received. Universities have started incorporating the dataset into their machine learning and natural language processing courses and we saw many strong entries from class projects. We look forward to more academic integration at the course level and seeing more entries from such projects. Most of the entries used some aspect of machine learning; from inferring subtopics (Huang, et. al. and McAuley, et. al.) to predicting future reviews (Hood, et. al.) and many others. We also received many other entries including one of the winners, Wang, et. al., which applied word clouds to increase the utility of a large number of reviews. Opening the Next Yelp Dataset Challenge We are happy to announce the next iteration of the Yelp Dataset Challenge . The challenge will be open to students in the US and Canada and will run from September 26, 2013 to February 10, 2014. See the website for the full terms and conditions . This data can be used to train a myriad of models and extend research in many fields. We can’t wait to see what you come up with! Tweet Back to blog", "date": "2013-10-02"}, {"website": "Yelp", "title": "Bringing Health Inspection Scores to Yelp", "author": ["\n        \n  Yoann R., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/01/bringing-health-inspection-scores-to-yelp.html", "abstract": "Our post today is by Will L., an engineering intern on one of Yelp’s backend teams this past fall. Will walks us through the challenges of bringing restaurant health inspection scores to Yelp, a feature we announced today at the United States Conference of Mayors in Washington, DC. As you may have seen on our official blog , we are very excited about our initial release of the Local Inspection Value-Entry Specification (LIVES). LIVES is an open data standard crafted by Yelp in partnership with the cities of San Francisco and New York to allow municipalities to publish restaurant health inspection information in a machine-readable format. In this post, we want to take you behind the scenes and give you an overview of all the steps and actions that have happened from getting the standard off the ground to having all of the health inspection information showing up on Yelp. The Local Inspection Value-Entry Specification was first drafted by Yelp engineer John Boiles (also known at Yelp for his Kinect hacking and the legendary KegMate ) in mid-June 2012 and was followed by several collaborative revisions with key members of city health departments. Individuals within the cities of San Francisco, New York, and Philadelphia were instrumental to the process of refining the standard with their domain expertise and feedback. On January 9th, 2013, the latest version (1.0) was published. A LIVES feed contains several comma-separated value (CSV) files to encapsulate the feed data in easy-to-read textual representation: businesses (businesses.csv), inspections (inspections.csv), related violations (violations.csv), score mappings for municipality-specific conventions (legend.csv), and finally data about the feed itself (feed_info.csv). Once the specification began shaping up at the beginning of October, a team of engineers at Yelp started to build a system to process and display LIVES data on our site. The first step was to come up with a scalable and maintainable system based on the requirements and constraints of the standard. While the LIVES standard is currently in use by two cities, Yelp is calling upon municipalities all across the US to share their health inspection data. As such, scalability played a critical role in our design process. One of the more interesting and challenging aspects of the project centered around matching up a city’s record for a business to its equivalent on Yelp. While this may sound simple at first, it proves technically challenging when you realize cities are more interested in the legal representation of a business whereas Yelp focuses on what you would see if you were standing in front of the business on the street. For example, Starbucks may register itself as “Starbucks Coffee Company” with the City of San Francisco but will show up as just “Starbucks” on Yelp. Similar problems arise with addresses and phone numbers, all of which are attributes we use to help pinpoint the right business on Yelp (e.g., a chain might use a central number for registrations but have its individual numbers on Yelp). While matching a set of data to a business is something we do routinely here at Yelp (after all, a search on Yelp is a very similar problem), the stakes for this project were much higher, especially in regards to false positives when matching. Just imagine how a 5-star restaurant with a perfect health record would feel if we incorrectly associated a failing inspection with their profile on Yelp. To fine tune our matching, we ran several sample data sets from San Francisco and New York City through our tools and evaluated our results, paying particular attention to false negatives and false positives. Through a combination of normalization of the raw data from the municipalities and tweaks to how we weigh each piece of data (name, address, and phone number), we were able to dramatically minimize the number of false positives. Matching business records is never a completed project, however, so we’re constantly collecting metrics on how it’s performing with new data sets and tweaking its algorithms and weights appropriately. Once we had all of the various implementation pieces glued together, the last step was to implement a rollout strategy. At Yelp, we’ve developed several tools to assist in this process to limit the exposure of a new feature. We’re able to release a feature to our internal users only, expose it to only a certain portion of public traffic, or whitelist the feature for certain businesses only. By combining all of these, we’re able to iterate and deploy features quickly all while keeping risks low. We still have a lot of work to do with LIVES. Besides continuing our gradual rollout of the feature, our priority is to advocate for the adoption of the standard with municipalities so that more health inspection data is available publicly and can be displayed on Yelp. Since LIVES is an open standard, this not only benefits consumers wondering if that food stand on the corner of the street is a good choice; it also allows other organizations, such as research institutions, to use this data to spot trends and perhaps prevent future foodborne illness outbreaks. We’re equally interested in this data and plan on looking at how the average scores evolve across cities as we make this data more readily available to consumers like you. LIVES was one of Yelp’s first forays into developing an open standard. We’re definitely hooked and look forward to working with more local governments in the future to iterate on this standard and help share the wealth of information they have on local businesses. Tweet Back to blog", "date": "2013-01-17"}, {"website": "Yelp", "title": "Mission: Beyond The Mobile Browser", "author": ["\n        \n  Arnaud Brousseau, Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/01/mission-beyond-the-mobile-browser.html", "abstract": "Last year we published an article explaining the reasoning behind the birth of our mobile site, Mission: Mobile Makeover . Since then, we’ve been hard at work, dedicated to improving this new Yelp property and transform it into an app-like experience. Ideally? Forget about your browser, and keep on Yelpin’! Today, we’ll focus on some of the work we’ve accomplished during the past year or so. We’ll talk about two technical challenges faced lately: how to boost interaction speed on mobile, and how to make the most out of the screen sitting in your pocket. Boosting Interaction Speed Clicks are a core concept on the web because it drives most form interactions and navigation. To support double clicking, mobile browsers have no choice but to wait a short amount of time (typically, around 300ms) after your finger has lifted off the screen to fire a click event. Since Yelp’s mobile site doesn’t require double clicks this behavior is actually detrimental to us: it makes interactions and navigation feel slower. That’s why we decided to boost interaction speed with a global shim that mitigates the delay that a user typically experiences between the moment when her finger lifts off the screen and the moment when a click event fires in JavaScript. The general principle behind the change is: If a user triggers a touchstart event at point A, followed by a touchend event at point B and if the two points are not far away from each other (both in distance and time), then it’s safe to assume we have a click intent. Otherwise, the intent was a drag, or something entirely different…but definitely not a click!To accomplish this, we listen to touchstart , touchmove and touchend events. If we spot a click intent when a touchend event is fired, we immediately trigger a click event via JavaScript and prevent the default action. As a result, the click event fires ~300ms earlier than it would otherwise have. If we fail to spot a click intent (maybe touchstart and touchend events were too far away from each other in distance, which means we have a dragging movement), we simply let the browser handle things for us. Quick notes about this: This idea is not new. Google has a really good article about it, mobile HTML5Boilerplate has fast buttons in its JavaScript helpers , and the Financial times app recently open sourced their solution for faster clicks on touch-enabled devices. Not all devices have this problem. In our experiments, we noticed that the latest Android devices handle touch events and clicks much faster than they used to. If you want to implement fast buttons on your own, we highly suggest that you develop in Chrome Canary . This browser lets you emulate touch events, and also override a bunch of browser properties like the browser’s UA, width, height, etc. For example, here’s a screenshot of the setup used at Yelp to iterate on faster clicks: Above is a picture of search suggestions on Yelp’s mobile site. This is the quintessential example of a feature that needs fast interactions to exist. With delayed clicks (on form inputs and suggestions) the experience would feel sloppy and awkward. With faster clicks, we’re getting closer to an app-like experience on Yelp’s mobile site. Making The Most Out Of Small Screens Let’s take a look at our search map page: This page has several fixed height parts (3 action bars, search form) and one dynamic height part (the map). Since our mobile site is going to be displayed on a small screen, we have to take advantage of every pixel we can get. That yields two independent goals: Get as many available pixels as we can Use 100% of the available space to draw content\nThe first goal is difficult because the available space to display your Web app is typically dependent on the OS. When working inside a browser, there’s only one thing you can kick out of the way: the browser chrome (containing the URL bar). Getting rid of it is pretty simple: we have to tell the browser to scroll to the top of the page. It’s a well-known trick, and the code fits on one line: window.scrollTo(0, 0); There is a catch though: Android won’t work with the code above, and requires a slightly different version: // For Android\nwindow.scrollTo(0, 1); Now comes in the hard part: we have to draw our layout (containing both fixed and flexible height parts) on the available screen. The only flexible part is the map so it’s going to fill whatever space we have left after placing the fixed height elements: var mapHeight = screenHeight - totalFixedHeightsFromOtherElements; We know the value of  totalFixedHeightFromOtherElements. However we don’t know screenHeight . How tall is the screen exactly? How can we do that in JavaScript? Well, there are a lot of different APIs out there. See for yourself: window.innerHeight window.outerHeight screen.height screeen.availHeight pageYOffset document.body.clientHeight document.body.offsetHeight document.documentElement.offsetHeight document.documentElement.clientHeight Given the plethora of options available we took several devices, did some testing and iterated to come up with a solution. The code we are using right now looks something like this: var width = screen.width;\nvar height = screen.height;\n\nif (screen.width > window.innerWidth) {\n    height = document.documentElement.clientHeight;\n    width = document.documentElement.clientWidth;\n} This code is not pretty to look at but it solves the problem at hand — client-side screen size detection — pretty well for us. Let’s see why. First, we found that screen.width/height was the most consistent API out there considering the devices we support (iOS and Android > 2.2). If you want to scare yourself, please have a look at the massive testing table James Pearce assembled! You will get a sense of how fragmented mobile devices/browsers are these days. The code snippet above solves another problem: some devices report screen.width/height in physical pixels where others report it in CSS pixels. This matters for devices with double density display. For instance the Nexus 7 reports a screen.height of 1280px. However, document.documentElement.clientWidth always reports a value in CSS pixels, so we fall back to that API if we spot that screen.width / height is giving us a bigger value than expected. Why don’t we use document.documentElement.clientWidth all the time then? You guessed it already: it’s not accurate enough, for instance on iOS (it reports 356px instead of the expected 480px because of the browser and system bars). As you can see this complex problem doesn’t have a definite answer. We’re still iterating on our solution at the time of writing. By the way you’ll probably find surprising things if you fire up real devices. For instance did you know Nexus7’s reported devicePixelRatio was 1.3250000476837158? Yup, that’s insane. Going forward The mobile site is still a young Yelp property. Barely out of its infancy, actually. Since its introduction we’ve been busy working on delivering the best experience for small screens, trying to make you forget you are in a browser. Fostering faster touch interaction, giving better feedback, making the most out of every available pixel, taking advantage of new HTML5 APIs… this is barely scraping the surface of the ideas we have to improve our mobile site going forward. We are truly excited about what’s ahead. If you are too, you should think about joining us — we’re hiring ;) Tweet Back to blog", "date": "2013-01-22"}, {"website": "Yelp", "title": "February Events at Yelp!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/02/februrary-events-at-yelp.html", "abstract": "Two weekends ago, we participated in and sponsored the 2013 O4U (Out for Undergraduate Technology) Conference in Palo Alto. This conference is hosted in different cities around the nation and strives to inspire lesbian, gay, bisexual and transgender undergraduate students to be more involved in the tech industry. Students were able to spend some quality time with one of our engineers and recruiters and were also able to participate in panels, workshops and demos. Meetups SFHTML5 - Introduction to the Backbone.js, Ember.js, and AngularJS Frameworks Sidney, Anthony and Miško brought in their expertise to tackle everyday questions about Backbone.js, Ember.js and AngularJS. While Backbone.js helps to create more reusable code by exposing MVC design patterns in a lightweight library, Ember.js  provides developers  a full fledged framework for building better browser-based apps. Finally, AngularJS simplifies web app development by providing new HTML primitives to bind data to UI elements, reacting to changes automatically. If you missed our meetup events last month, don’t fret! You’ll have plenty of chances to come hang out with us again this month! Our monthly partner, Designers + Geeks , always impresses us with their speakers, but this month they’re really stepping it up! This time they’re bringing in Dan Whaley, the creator of the world’s first online travel reservation system which currently brings in over $9 billion a year. Quite impressive. ##\nUpcoming February Events Thursday, February 21, 2013 -6:30PM - Annotating Knowledge ( Designers + Geeks ) Monday, February 25, 2013- 6:00PM - Speaker Series: Eric Rodenbeck of Stamen and Santiago Ortiz ( Data Visualization Group in Bay Area ) Tuesday, February 26, 2013- 6:00PM - Hack Night!  ( Women Who Code ) Thursday, February 28, 2013- 6:00PM - Dart with Seth Ladd and John McCutchan  ( SFHTML5 ) Tweet Back to blog", "date": "2013-02-13"}, {"website": "Yelp", "title": "March Events at Yelp", "author": ["\n        \n  Julia N., Senior Events Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/03/march.html", "abstract": "Yelp’s weekly “Learning Group” is one of our favorite engineering events. People come for the free food but stay for the hour-long presentation, usually given by a fellow engineer. Past topics, both technical and non-technical, have included everything from cinematography to typography to steganography, and the presentations are lovingly put together. Since we’ve found these presentations fascinating we’ve decided to share them! This first one is a Learning Group about learning Chinese, by Mark W. He’s spent several years learning to speak Chinese, and in this presentation he introduces the Chinese language along with some tools and techniques for mastering a new skill. (Learning Chinese by Mark Wilson- Slides) In addition to hosting our own events or meetups, we like to travel to stay connected with the tech community outside of San Francisco. Last weekend, we attended a Hacker Fair in the South Bay hosted by Hacker Dojo. A few of our representatives were able to attend and met several individuals who shared with us their current projects and inventions. The best part? Attendees of the fair rode down to the South Bay and back via party bus, courtesy of one of Hacker Dojo’s sponsors! Work hard, play harder. This upcoming weekend, we’re sending Grace, one of our recruiters, to the Tech Career Expo in Austin, Texas. Lucky girl! The Tech Career Expo is hosted during the same weekend as SXSW but is open to all individuals interested in learning more about the tech industry. Come by our booth and grab some food, drinks and great giveaways! Our booth will be located on the corner of 5th Street and Trinity Street, 1 block away from the Austin Convention Center . Back at home, we have another great Designers + Geeks event to look forward to. This month, things are getting steamy with Ethan Imboden, the founder of Jimmyjane, a leader in the pleasure product industry. Ethan will be joining us on March 14 to reveal how he went about redesigning and imagining this taboo industry. Ethan has over 10 years of brand and product innovation experience and wants to let you in on his secrets. Upcoming March events: Thursday, March 14, 2013- 7:00PM- Sex, Design & Disruption ( Designers + Geeks ) Wednesday, March 20, 2013- TBD - ( San Francisco & Silicon Valley Web Performance Grou p) Tweet Back to blog", "date": "2013-03-06"}, {"website": "Yelp", "title": "mrjob Sprint at Pycon 2013!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/03/mrjob-sprint-at-pycon-2013.html", "abstract": "Sudarshan G., Yelp’s current leading engineer behind mrjob, writes this week about an exciting opportunity to contribute to one of our most popular open source libraries: mrjob. We are happy to announce that we are running a mrjob sprint at Pycon 2013. mrjob is an open source Python framework for running Python scripts to process large amounts of data either on your own Hadoop cluster or in AWS using EMR. The mrjob sprint is targeted at both developers who are familiar with mrjob as well as new users who want to come up to speed on it. The sprints are being held on March 18th and March 19th between 10 am and 6 pm at: Hyatt Regency Santa Clara 5101 Great America Parkway\nSanta Clara, California USA 95054 Most of the maintainers of mrjob will be available on both the days to help get new users and developers up to speed. The sprint is free to attend and there is no requirement in terms of registering for Pycon to be able to attend this sprint. We strongly encourage users with all levels of expertise with mrjob to register on the PyCon website and attend this sprint! An initial lists of tickets to be tackled at the sprint are tracked on Github . The mrjob project has been developed at Yelp for over 3 years now. mrjob was built to provide a way for Yelp engineers to run log processing jobs on the hadoop cluster at Yelp while retaining all the benefits of working in Python and reusing the Yelp code base. Yelp released mrjob as an open source project in Oct 2010. The open source release came with support for Amazon’s EMR framework. Yelp has since retired its local hadoop cluster and now uses mrjob exclusively with EMR to power all the large data processing batches. Over 200 batches at Yelp are powered by mrjob. mrjob is under active development and is at version 0.3.5 right now. In the last 2 years mrjob has been rapidly improved with features like support for reusing job flows appearing in 0.2.7 and spot instances appearing in 0.3.2. We are about to release mrjob 0.4 which will support jobs written in different languages such as Ruby and Javascript as well as a bunch of other cool features. Sign up and we’ll see you there! Tweet Back to blog", "date": "2013-03-11"}, {"website": "Yelp", "title": "Event Listing Classifier: an Intern Story", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/04/event-listing-classifier-an-intern-story.html", "abstract": "What projects do interns work on? What are some of the machine learning algorithms we use to solve real world problems? Do interns work in teams solving critical problems, or just do small research tasks? These are some of the questions I frequently get when talking with candidates, so I invited Shengwei L., former intern and now full-time employee, to talk about one of his projects. As you can see, interns at Yelp get to apply the principles they learned in school to make Yelp better for millions of people. They sit alongside full-time members and take responsibility for projects that can span multiple teams and systems. But you don’t have to take my word for it, let’s hear from Shengwei! Yelp events are awesome, and we enjoy throwing great events around the world . After events, which are usually free for attendees, Community Managers create a new event listing specifically for their party so that all the participants can write reviews of the event itself instead of posting their reviews of an event onto the business listing that is only for the venue. It would be misleading to see so many reviews on a business listing if they were for a party and not a standard consumer experience with the business. In Yelp’s system, those event listings are essentially the same as other real business listings, such as a restaurant or hair dresser. Because event listings are mainly about past events, they are not useful for ordinary users. Hence we want to remove them from search after events are over and people have had time to review them. However, no one’s perfect, and we may occasionally lose track of some event listings and they will show up in search results. In order to improve the information/noise ratio for our users, we have implemented a system to automatically detect such listings so that we may exclude as many event listings as possible from search results. It can be tedious for a human to pick out a few event listings we have missed from numerous real business listings, but computers are good at screening large datasets in a short amount of time. Before tracking dogs start looking for a suspect, they need to sniff some of the suspect’s belongings to learn about the smell. Similarly, a computer needs to learn what an event listing event looks like, as opposed to a real business listing, before it can differentiate them on its own. This is essentially a classification problem. Naive Bayes classifier is broadly used for classification, and it is quite effective for document classification, such as spam email detection. It is based on the Bayes’ theorem , in the form of , where presents different categories, and denotes distinctive features that could possibly appear. Translated into more readable English, that’s . In our case, classes are “business” and “event,” and the features are explained later in this post. Given an instance with a fixed set of features, the denominator is a constant for different categories. Therefore, is sufficient to categorize the instance based on certain features. This simplification can be exploited to reduce computation effort. Moreover, as a fundamental assumption, features should be independent of each other, and hence is equivalent to . As a consequence, the probability of the category of one instance based on certain features is assessed by , and we take the category for which the score is higher as the instance’s category. Our wonderful summer intern Te T. implemented a naive Bayes classifier using log-likelihood based on both multinomial and Bernoulli distributions (known as event model). This classifier has been an instrumental tool in the analysis of our massive data, such as check-in tips. I decided to also use it for listing classification, because event listings exhibit some obvious patterns (i.e., features) that ordinary business listings usually do not have. For example, the listing names typically contain certain keywords (such as “yelp,” “elite” and “event”), and the creators are normally Community Managers. The naive Bayes classifier uses these patterns like the tracking dog uses smells: it can now sniff out those event listings created for elite events. After scrutiny of many samples and thanks to suggestions from team members, especially Artem A. , Marty F. , and Zeke K. , we finalized about 10 features, and they worked well in experiments with Bernoulli distribution as the event model. After the system was deployed, we spotted a number of listings for events that were created in the early days and not correctly tracked. In the meantime, we regularly check all newly created business listings to see if we have missed classifying any listings for events. We are not blindly marking business listings relying on the automatic classification. Human verification is also involved to make sure we do not mistakenly exclude real business listings. Going forward, we will continue to monitor and analyze the classification result, particularly false positive and false negative cases, trying to find out more features we may exploit. In addition, since we utilize Bernoulli distribution as the event model for features, there are thresholds to determine true or false value of features based on experiments with samples. We may also tweak the thresholds here and there according to analysis of the classification result. This was just one of my intern projects. Without support from my team members and help from other groups, I could hardly figure out all the parts I would need to accomplish this. I really enjoyed the progress, interacting with experienced industry veterans and other awesome interns. It is a great approach to acquire hands-on experience and transfer the knowledge learned in class into a working piece in the real world. Tweet Back to blog", "date": "2013-04-04"}, {"website": "Yelp", "title": "April Events at Yelp!", "author": ["\n        \n  Julia N., Senior Events Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/04/april-events-at-yelp.html", "abstract": "Last Tuesday, we stepped out of our technical meetup norm and hosted an event group whose mission is to improve the lives of women around the world. Spark is an organization comprised of a network of young philanthropic entrepreneurs. Their members offer pro bono services to up-and-coming women’s organizations. Spark works on a global level to reach out to the next generation’s leaders and offers mentorship opportunities to young women. We love what they are doing and are happy to announce that we’ll be hosting them again next month! Come visit us on May 15th for a movie screening of Wonderwomen! The Untold Story of American Superheroines! Check back in next month’s blog for details about the event. We also hosted a special HTML5 student mixer that highlighted projects from Howest University, Belgium’s top web design school. Several current students presented their work and we were able to see how they applied their studies to today’s app market. Tim van Damme, Instagram’s Head of Design, also made a guest appearance and touched briefly on launching apps designed for millions of users. He spoke about what to expect from target audiences and what designers and developers should be able to deliver. He also reiterated something we value at Yelp: keeping engineering teams small. This week, we had an exciting presentation from another one of our regular meetup partners, the San Francisco Python Meetup Group . They invited Roy Hyunjin Han, a developer of priority risk models used to ensure that cities run effectively using spatio-temporal forecasting, to educate us on how to develop web applications using the newest release of the Pyramid framework. Pyramid is Python’s general, open source web application development framework. Upcoming April Events: -\nThursday, April 18, 2013- 6:30PM- The Future of Infographics ( Designers + Geeks ) -\nTuesday, April 23, 2013- Learning About Styles and Themes in Android ( The San Francisco Android User Group ) Tweet Back to blog", "date": "2013-04-12"}, {"website": "Yelp", "title": "Databases, Selenium, and MLBase - oh May!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/05/databases-selenium-and-mlbase-oh-may.html", "abstract": "In addition to the public events we hold at Yelp, every Friday an engineer at Yelp presents on a topic they’re passionate about. The talks range from topics far afield from computers , to technical deep dives on some of the core trade-offs we make at Yelp. I’m happy to share one of the latter today, as Garrett J. takes us through the history of databases. From Codd’s description of the relational model, through the challenges Cassandra was designed to solve, this video gives a great overview of current options in database technology. <iframe frameborder=\"0\" height=\"281\" src=\"http://player.vimeo.com/video/65601864\" width=\"500\"></iframe> Slides - Navigating the Shifting Database Landscape We’ve got an exciting month for public meetups! The SF Python group will kick things off this Wednesday by covering Selenium, a tool that we use extensively here at Yelp. If you’re developing a Python web app, how are you making sure it’s working for your users? Hear committer Santiago Suarez Ordoñez describe how to write solid functional tests. The very next day, we’ll be hearing about how to use next generation CSS development techniques from Nicole Sullivan. In the middle of the month, learn more about Scala, SuperHeroines, design, and Women Who Code.  Wrapping up the month will be Ameet Talwalkar from UC-Berkeley’s AMPLab introducing us to MLBase. This research project aims to make machine learning more accessible by providing high-level abstractions over algorithm and data management patterns. See you at Yelp! Upcoming May Events: -\nWednesday, May 8, 2013 - 6:15PM - Selenium for Pythonistas ( San Francisco Python Meetup Group ) -\nThursday, May 9, 2013 - 6:30PM - The Top 5 Performance Shenanigans of CSS Preprocessors - Nicole Sullivan ( San Francisco & Silicon Valley Web Performance Group ) -\nTuesday, May 14, 2013 - 5:30PM - Scala for the Intrigued ( The San Francisco Java User Group ) -\nWednesday, May 15, 2013 - 6:00PM - WonderWomen! The Untold Story of American SuperHeroines ( Spark ) -\nThursday, May 16, 2013 - 6:30PM - The Enchantment of Everyday Things ( Designers + Geeks ) -\nTuesday, May 21, 2013 - 6:00PM - Lightning Talks ( Women Who Code) -\nThursday, May 30, 2013- 6:00PM- MLBase: A User-friendly System for Distributed Machine Learning ( SF Bay Area Machine Learning ) Tweet Back to blog", "date": "2013-05-08"}, {"website": "Yelp", "title": "Datamining and Visualization with the Academic Dataset", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/06/datamining-and-visualization-with-the-academic-dataset.html", "abstract": "One of the qualities Yelp looks for in engineers is a passion for making sense of “big data.”  But as a student or newbie in the field of data science, how are you supposed to demonstrate that passion? Enter the Yelp Academic Dataset , a snapshot of review, user, and business data from the communities around 30 universities.  Last semester, I had a chance to see some of the amazing things that UC Berkeley students could do with the dataset. Visualizing Data Unfortunately, the raw dataset is only exciting to those who can skim JSON serialized datastructures (OK, OK, I admit to being one of those people).  To make the data valuable to everyone else, it needs to be aggregated and displayed in an engaging, dynamic way.  Enter stufte , a project which displays aggregate information about businesses organized by campus. stufte uses Bootstrap , leaflet , Typekit , and several other HTML5 technologies to build a compelling and informative view of Yelp’s data.  Visually rich, the site conveys a lot of information without being overwhelming by allowing you to narrow your focus with intuitive mouse hover actions, click through modes, and widgets. Data mining stufte surfaces some interesting stats, like review distribution, featured restaurants, and most prolific users.  What is the best way to discover these nuggets given mountains of data to comb through?  Students used Yelp’s open source library mrjob to discover patterns. mrjob is a Python package that helps you write and run Hadoop Streaming jobs. mrjob can use Amazon’s Elastic MapReduce service to easily run huge data processing jobs. Amazon AWS in Education provides several programs to let students use their services for free.  This makes the two a great combination for students learning to datamine on the cheap! Hadoop and mrjob have extensive documentation. If you are new to these technologies, I recommend skimming these first to get an idea of what’s possible.  Now, let’s jump into some of the interesting questions that were asked and answered by students. Measuring Originality After an intro to mrjob , students tackled the question: which review or reviews have the most number of unique words (i.e., words not seen in any other review)? It turns out answering this question requires several steps: extracting words from their reviews finding which words were only used in one review recombining a particular review’s unique words scoring the reviews by the count of their unique words finding the review with the highest score\nAfter wrapping one’s head around the MapReduce paradigm, the next big challenge one faces is what to do when your task can’t be answered with one Job (MapReduce iteration).  Manually writing and connecting multiple jobs is a hassle. How do you handle dependencies, storage for the data between jobs, or cleanup after success?  Luckily, mrjob handles all of this for you, leaving a clean interface for specifying how data will flow through multiple steps : def steps(self): return [self.mr(self.words_in_review, self.doc_frequency), self.mr(reducer=self.unique_words), self.mr(reducer=self.find_max_review)] I’ll leave the implementation as an exercise to the reader (and future students!), but give you the punchline: 6 year Yelp Elite Tom E. shows his nerdy side with his unique review of MIT’s Middlesex Lounge . Second place goes to another Elite, Jonathan T. for his slang slingin’ review of Matsuri near CalTech’s campus. Dangerous Patterns For Apoorva Sashdev and Alex Chung’s final project, the Yelp dataset was used to predict business closings.  Yelp doesn’t want to send users to businesses that have closed, so how can we quickly catch businesses that appear to be closed but have not been caught yet?  If we could predict business closings, the tool could also be used by business owners to spot dangerous trends early on. Using a Baysian classifier and features such as votes, review frequency, and category, they predicted which businesses were now closed, and used Yelp to check their findings.  As is frequently this case with datamining projects, sometimes the most important finding is that there’s not much to find. Overall accuracy was high, but many truely closed businesses were not found; a classic “precision vs recall” problem with skewed datasets.  While the recall of their classifier for closed businesses wasn’t quite accurate enough for production use, they did make some interesting discoveries along the way: “We found that certain categories had a high percentage of closed businesses namely Moroccan, Wineries, Afghan, Hawaiian, Adult Education and Arcades whereas other categories like Health and Medical, Coffee & Tea, Bars, Beauty and Spas seem to remain open more often.” Jump In! Set yourself apart from other candidates by trying your hand at datamining. It’s never been easier with data sources like Yelp’s and tools like mrjob.  If this sounds fun, take a look at the software engineering positions we have open (including fall interns !).  We’re looking forward to hearing about your projects! Tweet Back to blog", "date": "2012-06-04"}, {"website": "Yelp", "title": "Introducing EMRio: Optimize your AWS bills", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/07/introducing-emrio-optimize-your-aws-bills.html", "abstract": "This post comes to us from Sean, one of our Data Mining interns and a student at the University of Pittsburgh. Sean wrote production code his first week here, and hasn’t slowed down since. I’m happy to see his main project released today: it’s called EMRio (pronounced like the Italian plumber ). Yelp has already started using EMRio to start saving money on our Elastic MapReduce bills. Take it away, Sean! At Yelp, we like to use MapReduce a lot. We use it so much, in fact, that we made mrjob , a tool that allows easy creation of mapreduce jobs in Python. We use Amazon’s Elastic MapReduce (EMR) service that allows us to compute large amounts of data on their EC 2 cluster . The service allows for anyone to buy different types of machines by the hour, depending on the needs of the business. Typically, you are billed by the hour with no upfront cost, but with a large enough demand the hourly rates can be reduced by switching to reserved instances . To save some money on our EMR bill next year, we developed a tool which runs a cost optimizer over our historical EMR usage and spits out the optimal reserved instance pool for our usage. It suggests how many reserved instances, of what type, and of what size we should buy to minimize our costs. For reserved instances, you pay a yearly upfront cost for a machine and in return you pay a lowered hourly rate. Here is a graph showing the three tiers of reserved instance types you can buy, and the cost associated with them over time. At the extreme (Heavy Utility), you pay only the upfront cost. Heavy Utility is used for machines that are consistently running during the year.  For more intermitant usage, a smaller upfront cost makes more sense. This is where my project comes in: find the optimal number and type of instances to buy that would save Yelp the most amount of money in the long run. This is how EMR Instance Optimizer (EMRio) was born. EMRio works by looking at your EC2 usage and calculating the optimal number and type of reserved instances you should have purchased during that period. Since Amazon only keeps 2 months of job flow history, it normalizes the history as if it was a single year’s worth of data. After normalizing, it then starts optimizing. This works by swapping between two modules: the optimizer and simulator. The simulator is surprisingly simple. The main run method of it looks like this: # event_queue is an ordered list of events that happened in your AWS job history. for (time, event_type, job) in event_queue: if event_type is START: allocate_job(jobs_running, reserved_instance_pool_used, job) elif event_type is LOG: log_hours(logged_hours, jobs_running, job_id) rearrange_instances(jobs_running, reserved_instance_pool_used, job) elif event_type is END: log_hours(logged_hours, jobs_running, job_id) remove_job(jobs_running, reserved_instance_pool_used, job) return logged_hours All it does is break down the jobs into a bunch of event tuples containing the time and type of a job event. These event types are START, END, or LOG. START and END are when the job starts to spin up on EC2 and when the job is finished, respectively. LOG is for logging an hour that the job runs. LOG is also useful since each hour Amazon can reallocate reserved instances to jobs if they become available. The simulator also reallocates the instances being used if there are any available reserved instances that can be used. The Optimizer starts out with zero reserved instances for all types. It then simulates adding a single instance for each utilization type for a single instance type. It simulates all the utilization types, and then whichever one was best, it adds that instance to the “best pool.” If at any point the cost for buying the next instance is more costly than the previous instance, we stop because we have found the optimal reserve instance pool that minimizes cost. This technique is called Hill Climbing and it is useful for this domain because each reserved instance added will only use as many hours as the reserved instance before it. This is because if the instance before it was free to be used, then it would have been scheduled for the hours used instead of the newly added machine. This means that each instance added will have lower or equal hour usage, but never higher which makes hill climbing applicable. Once the Optimizer is done, EMRio runs two more simulations: one for the optimized pool and one for only on demand usage, so we can compare the costs of both. It then outputs the optimal instance pool and a cost analysis. The cost analysis describes how much money both are and the money saved if these instances were used for a year. If you want to give EMRio a spin, just head on over to: https :// github . com / Yelp / EMRio and give it a try! Tweet Back to blog", "date": "2012-07-23"}, {"website": "Yelp", "title": "GPS vs WiFi: The Battle for Location Accuracy Using Yelp Check-Ins", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/08/gps-vs-wifi-the-battle-for-location-accuracy-using-yelp-check-ins.html", "abstract": "Our next post comes from Mason G., superstar intern on our IOS team.  While not assembling a 20-monitor wide version of Mario that displays the entire level at once, Mason has been cranking out features for our shiny iPhone and iPad apps.  Yelp thrives on data, so it is no suprise that one of Mason’s projects was to evaluate the technologies we use for pinpointing users’ geolocation.  Read on to get the full scoop! “Checking in” to a business using Yelp’s mobile applications is a fun way for Yelpers to engage with businesses and others in the Yelp community, but we don’t allow users to check in from just anywhere – they have to earn those badges! In order to make sure everyone’s playing fair, we compare a user’s reported location with the location of the business they’re trying to check in to figure out how far away they are. We thought it would be interesting to figure out how Wi-Fi positioning and accuracy compared to GPS and, in order to do this, we had to take a look at Yelp check-ins from different mobile devices. First, a little background. A phone can figure out its location in several ways: by using GPS, cell tower triangulation or Wi-Fi positioning. GPS connects to satellites orbiting the Earth, and figures out where it is compared to the satellites. Cell tower triangulation connects to nearby cell towers, and performs a similar calculation. Wi-Fi positioning depends on companies like Skyhook , who record the locations of Wi-Fi networks. Your phone can then look at nearby Wi-Fi networks and figure out where it is. Using Yelp’s open-source map reduce framework, mrjob , we analyzed all Yelp check-in data from June 2012, breaking it out by device type. Android and iPhone can use GPS, cell tower triangulation and nearby Wi-Fi networks to get a precise fix. On the other hand, iPods can only use Wi-Fi positioning. By comparing iPods to Android and iPhones, we were able to see how accurate Wi-Fi positioning is in comparison to a mix of all three techniques. After gathering all the data from June, we graphed a probability distribution function of Yelp check-ins versus distance in order to see the distance between users and the businesses they’re checking in to. This graph shows the percentage of check-ins attempted (y-axis) at a given reported distance from a business (x-axis). As you can see, check-ins from iPhones tend to be closest to the business, followed by Android phones and iPods. For example, 40% of Android check-ins are less than 0.1 miles away from the business. Next we converted this graph into a cumulative distribution function, in order to see how many Yelp check-ins occurred within a specific distance from the business. iPhone fared better than Android, likely due to lower-quality GPS chips in some Android devices. For example, 7.5% of Yelp check-ins from Android, the phone thought that the user was actually more than two miles away from the business, while only 4.5% of iPhone check-ins were that far away. For example, 79% of iPod check-ins were less than one mile from the business. When checking in, we also record the reported location inaccuracy. This number represents the phone’s best guess at the number of miles it might be off by. We account for this inaccuracy by subtracting the number from the distance to the business. This way users don’t miss out on the chance to check in. The dot shows your reported location, and the larger circle represents the inaccuracy. Android devices gain the most from this comparison, although iPhones still have better location services overall. However, the iPod line barely moves – the reported accuracy is always close to 0. For example, 90% of Android check-ins were less than one mile from the business after adjusting for the accuracy radius. By analyzing millions of data points, we can easily see how, on average, different platforms perform. iPhones consistently have the most accurate positioning, with a fairly small accuracy radius. Android phones are often inaccurate, but reliably reported that inaccuracy. And finally, iPods using Wi-Fi positioning proved the least accurate and usually reported incorrect accuracy radii. We can use this analysis to make sure that users have a great experience checking in, no matter their device. Tweet Back to blog", "date": "2012-08-14"}, {"website": "Yelp", "title": "Firefly: Illuminate Your Website's Performance", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/08/firefly-illuminate-your-websites-performance.html", "abstract": "One of Yelp’s core values is “play well with others.”  So it’s no surprise that Yelp thrives with open source projects written by others, and gives back by sharing projects of our own.  That’s why I’m excited to share this post by the manager of our Infrastructure team, Oliver N. (or as he’s known around the office, “BigO”), which adds to our library of open source projects . How do you know if your website slows down as a result of a code push?  How do you keep tabs on the performance of your most important endpoints?  How do you know if your error rates spike, or what their baselines are?  If you’re not actively using it, how do you even know your website is serving traffic?  For Yelp’s Infrastructure team, the answer is an emphatic “GRAPHS”.  Tasked with keeping the site up and running smoothly, we rely heavily on graphing the data from a variety of real-time metric systems.  We keep these graphs open on our work computers as well as splashed across large LCDs in our office, and they communicate to us the heartbeat of a system that serves approximately 78 million uniques per month.  Today we are releasing the home-grown tool we use to navigate, explore, annotate and graph these time series metrics on github .  Meet Firefly: Firefly is not a metric collection system itself.  It is a pluggable front-end for exploring time series metrics stored anywhere, combining these metrics into graphs, combining those graphs into dashboards, and sharing those dashboards with colleagues.  It’s designed to scale across datacenters and to bring together data from disparate sources.  It supports automatic graph titles and legends, as well as highly configurable graph annotations (vertical bars to record particular events). It has a stateful UI based on HTML5 pushState to allow easy sharing of dashboards.  Its server is built in Python on top of the Tornado framework, and its front-end is built in JavaScript atop the fantastic D3 graphing engine. Firefly consists of a Data Server and a UI Server.  The Data Server can run in multiple datacenters and provides the primary data interface - responding to requests that essentially translate as “give me the time series data for these particular metrics over this particular time window.”  It is the abstraction between a unified UI and a potential myriad of backends, be they Ganglia RRDs, HyperTable, MySQL, Redis or any other data source.  Each distinct data source is described as a (surprise) DataSource subclass, which exposes the standard interface methods of list_path(path), data(sources, start, end), legend(sources), and title(sources).  The UI server configures and serves up the HTML and JavaScript base interface, and handles the URL minification/expansion that underlies the stateful pushState features. Firefly has a ton of features, and has been super useful to us.  We’re going to keep expanding those features and are also really interested in seeing what uses the community can find for this tool.  We’re only shipping it with a DataSource configured for Ganglia right now, but adding new sources is designed to be easy and over time we’ll be looking to release some more parts of this system.  Fork Firefly on github , give it and shot and let us know what you think! Tweet Back to blog", "date": "2012-08-17"}, {"website": "Yelp", "title": "Yelp re:Invents!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/12/yelp-reinvents.html", "abstract": "Last month, Yelp was asked to talk about our experiences with Amazon’s Web Services at their re:Invent conference in Las Vegas.  I was delighted to be able to attend, speak, and learn from other speakers. All videos from the conference have been published, and below I’ve highlighted the sections specifically about Yelp. Big Data with Elastic MapReduce The majority of our AWS usage is Simple Storage Service (S3) and Elastic MapReduce (EMR).  We use these technologies because we want every engineer to be extremely effective, to be able to command a cluster of machines that would normally take another entire team just to manage.  We want our engineers asking and answering questions like: “What category did the customer want when she searched for ‘Pool’?” “When will our mobile traffic eclipse our web traffic?” “Was this review written by someone with firsthand experience ?”\nAt Yelp, whether you’re a senior engineer or an intern, if you want to test the next great data driven product, you don’t have to cajole another team for resources, or sit around waiting for your job to be scheduled.  Deploying is the same way: no need to worry about interrupting someone else’s batch job, or filling out TPS reports estimating the time you need on the production cluster.  Just ship the code; the boxes will be available when you need them. Yelp AWS Optimization<iframe src=\"http://www.youtube.com/embed/oGh5Tr4f_kQ?start=2170&feature=oembed\" width=\"500\" height=\"281\" frameborder=\"0\"></iframe> Make no mistake: optimizing for developer time can mean trading-off potential cost savings.  While we believe the trade-off is worth it, that doesn’t mean we ignore our costs!  Two of the specific ways we save money on EMR are: Re-use of job flows Buying reserved instances\nWe try to find ways of saving money that are invisible to other developers, and base improvements on how developers want to use the resources available.  Contrast this with the philosophy of making every developer justify and micro-optimize their costs.  We build tools to multiply the effectiveness of fellow engineers, instead of having policies that divide their attention between business issues and implementation details.  By sharing tools such as mrjob and EMRio we’re not only letting Yelp developers better focus on business problems, hopefully we’re letting other companies do the same. I hope you enjoy the videos, and Happy Holidays! Tweet Back to blog", "date": "2012-12-27"}, {"website": "Yelp", "title": "January Events at Yelp!", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/01/january-events-at-yelp.html", "abstract": "Want to know what widows, rivers, and pig bristles have to do with typography?  Listen to Carolina de Bartolo ( @carodebartolo ) explain why Typography Matters in the video below! Designers + Geeks is an engaging meetup that covers everything from beautiful architecture to mobile development.  At an event hosted at Yelp in November, Carolina covered topics ranging from the history of typography to practical advice on choosing fonts for your project. Designers + Geeks is just one of the many community events that we host in our spacious 10th floor lounge.  But fair warning: once you attend one event here, with our view of MOMA, two kegs, drinks, and snacks,  you may end up wanting to attend all our events!  Here’s what’s in the pipeline for the first half of January: Tuesday, January 8, 2013- 5:30PM  - Learn about Java EE 7 Directly From the Source (Marakana) Wednesday, January 9, 2013- 6:00PM - mrjob & Hadoop by Yelp’s own Sudarshan G (SF Hadoop Users) Tuesday, January 15, 2013 - Intro on Samsung/Intel/HTC Open Source Tizen Project (co-coordinated by Game and App Devs , GamesJS , and Mobile App Devs ) Thursday, January 17th -6:30PM- Revitalizing Unloved Devices (Designers + Geeks)\nSee you at Yelp! Tweet Back to blog", "date": "2013-01-07"}, {"website": "Yelp", "title": "More January Events at Yelp", "author": ["\n        \n  Julia N., Senior Events Specialist\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2013/01/more-january-events-at-yelp.html", "abstract": "We host several external meetup groups every month, but sometimes, they want us to present, too! SF Hadoop Users invited Sudarshan Gaikaiwair, one of our Yelptastic engineers to speak about mrjob, our MapReduce framework. mrjob is an open source project that runs on top of Hadoop. While mrjob can also be used with your own hadoop cluster, it is great for getting started very quickly with Amazon’s EMR offering. mrjob allows programmers with very little Hadoop or EMR experience to quickly write scripts that can process terabytes of logs on hundreds of machines. This talk covers the basics of MapReduce and presents code examples of how to write mrjob programs based on the problems Yelp faces. Tech Talk: SF Hadoop Users Meetup with Sudarshan G. from Yelp Studios on Vimeo . Coming up in the next few days will be several exciting meetups, including a Python developer “speed dating” event! Don’t worry, it’s strictly professional, courtesy of HackerX . We work closely with a variety of meetup groups all with the intention of providing our guests with great learning experiences. Stay tuned to learn about TIZEN, Revitalizing Unloved Devices, Data Visualization and much more! -\nWednesday, January 16, 2013 -6:30PM- HackerX- Python Developers (HackerX) -\nThursday, January 17, 2013 -6:30PM- Revitalizing Unloved Devices (Designers & Geeks) -\nThursday, January 24, 2013- 6:30PM- Graphics in HTML (Girls Develop It SF) -\nTuesday, January 29, 2013- 6:00PM- Data Visualization (Data Visualization) -\nThursday, January 31, 2013- 6:00PM- Introduction to the Backbone.js, Ember.js, and AngularJS Frameworks (SFHTML5) Tweet Back to blog", "date": "2013-01-16"}, {"website": "Yelp", "title": "Calling All Data Miners!", "author": ["\n        \n  Matt J. Search and Data-Mining Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/09/calling-all-data-miners.html", "abstract": "Summers are tough here at Yelp’s HQ in San Francisco. It’s hard to keep up with all the hot new businesses , order the best food at every restaurant you visit, or even just find that fancy dive bar your friends have been chatting up. If you’re anything like me, you aren’t satisfied just using these features on Yelp - you want to tear them apart, see how they tick, and make them better. The trick? All these features are powered by our incredible review data. The data Yelp is providing the reviews for nearly seven thousand businesses at 30 universities for students and academics to research and explore, along with some examples to get things started. Check out the examples on our GitHub page , and get the data from our Academic Dataset page (you’ll need to be associated with an academic institution to qualify for access). Some numbers: 30 schools 3 data types (reviews, businesses, users) Over 150k reviews of nearly 7k businesses\nCool, huh? How about we try building something with the dataset? Positive shmositive First off, let’s define the problem we want to solve. Personally, I’ve always been interested in sentiment analysis, so the first thing I thought of was this: what are the most positive and negative words for each category? (The following results are generated by the positive_category_words example, in case you want to follow along.) The simplest means of accomplishing this is to find the average star rating of all the reviews each word shows up in. We’ll use mrjob for this, since we’re working with a fair amount of data. Here’s simplified version of the job: class PositiveWords(MRJob): def review_mapper(self, _, data): if data[‘type’] != ‘review’: return unique_words = set(words(data[‘text’])) for word in unique_words: yield word, data[‘stars’] def positivity_reducer(self, word, ratings): yield word, avg(ratings) The full solution is implemented in positive_category_words/simple_global_positivity.py , and produces the following results: The 10 most positive words in ranked order : gem, treasure, knowledgeable, impeccable, topnotch, incredible, talented, compliments, macadamia, perfection The 10 most negative words in ranked order : worst, tasteless, flavorless, awful, rude, disgusting, horrible, terrible, poorly, zero Making it better So what’s wrong? The first thing that stands out is that all these words are very general - ‘gem,’ in particular, shows up in over 17k reviews, and could be used to describe pretty much anything. Let’s break it down by category, instead. The code for this portion lives in positive_category_words/weighted_category_positivity.py , and closely mirrors simple_global_positivity , with the addition of a category term. So, for a category near-and-dear to my heart, Italian: The 10 most positive words : melts, naples, exquisite, vino, promise, hooked, ovens, heaven, char, succulent The 10 worst words : disgusting, worst, horrible, flavorless, terrible, tasteless, awful, rude, manager, subpar These are looking a lot better, but there’s still room for improvement. We could use NLTK to distinguish parts of speech, and break down positive adjectives versus positive nouns. Or we could use a stemmer, and just look at root words (dream / dreams / dreaming might be better if they were bucketed together). Curious about the results for over 400 other categories? Check out our full examples page. Going further We’ve provided two more examples in our github repo - a tool that guesses the category of a business given a review and a simple markov-chain review generator (a tool that, given a category and some starting text, fills in the rest of the review), but we’re most excited about what you come up with. Head over to http://www.yelp.com/academic_dataset for information on how to get started and a list of the schools we’re providing data for. Have any other questions? Shoot us an email at dataset@yelp.com . Tweet Back to blog", "date": "2011-09-23"}, {"website": "Yelp", "title": "Output Filtering Failure", "author": ["\n        \n  Michael, VP of Engineering\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/11/output-filtering-failure.html", "abstract": "About a month ago, we launched m.yelp.com specifically targeting iPhone, Android, and WebKit-based smart phones. Our engineering team pushes code live on average three times a day. Moving fast means we need to have sound engineering practices internally — such as code reviews by peers and automated testing tools such as unit-tests and static analysis — to catch mistakes before they happen. In this blog post we will detail a misstep that we made and the response that followed. On the morning of October 27, 2011, we were alerted by a team of researchers at Harvard, Yale, and Boston University that they had found a servlet on m.yelp.com that could expose private user information. Working jointly with this team, our engineers gained a full understanding of the exposure. The leak allowed clients to see a JSON dictionary with user-specific fields, including email address, birth date, gender, and full names. No financially sensitive information was exposed. Once we understood the risk to our users we immediately took the mobile site down. We resolved the issue within an hour, but kept the site down for 12 more while we double- and triple-checked for other issues (none were found). We analyzed the servlet’s access logs to see if anyone exploited the hole, but we did not find any evidence that user information had actually been collected. We also created an automated system to detect sensitively named fields (last_name, birthdate, etc..) being sent to clients. Following this work, we felt comfortable that the risk of a future exposure of this type had been mitigated; so we turned the mobile site back on. The servlet at issue was using an ORM to retrieve information from the database. An ORM (object-to-relational mapper) system that automatically creates objects containing database content may make it easy to access that data, but it also introduces risk if those objects are not sanitized before being passed across a trust boundary. In this case, we missed a sanitation step in a servlet when grabbing data from the ORM. Our python logic for the biz details servlet looks like the following: def reviews ( self , businesss_id ): “““ unsafe version of reviews () “““ reviews = self . get_business_reviews ( business_id ) return json . write ( # UNSAFE CODE: unsanitized data going to the # client DANGER! { 'reviews' : reviews } ) In the code above, the call get_business_reviews() under the covers returns data about reviews, including details of the users who wrote the reviews. This is where the private information on the user is requested. To illustrate, here is an example JSON response from the offending servlet: { \" reviews \" : [ \" user \" : { \" birthdate \" : \" 1971,01,01 \" , \" display_name \" : \" Art G. \" , \" first_name \" : \" Art \" , \" gender \" : \" m \" , \" last_name \" : \" Goldfin \" , ... }, ... ] } Note: birthdate, gender, and last_name are all private fields that shouldn’t have been returned in the JSON response. The above servlet function reviews() was rewritten to (approximately this): def reviews ( self , business_id ): “““ output filtered version of reviews () “““ reviews = self . get_business_reviews ( business_id ) # Filter reviews to contain only data necessary # for frontend safe_reviews = filter_for_frontend ( reviews ) return json . write ({ 'reviews' : safe_reviews }) In the second implementation of the reviews() function you can see that the reviews object is transformed by filter_for_frontend() before being written into the JSON response.  We’ve also modified json.write() to have a list of sensitive fields that will throw an exception if an engineer tries to pass a field with a restricted name like last_name, birthdate, etc.. With these protections in place, we’re well protected from this type of exposure in the future. We’d like to thank researchers at Harvard, Yale, and Boston University: Georgios Zervas (a postdoc at Yale/Harvard) Michael Mitzenmacher (Harvard) ( bio ) John Byers (Boston University) ( bio )\nWe appreciate the team’s diligence in finding and notifying us about this important problem; their thoughtful handling of a sensitive and tricky security situation is commendable. If you do find any security-related issues on Yelp, please send an email to security-abuse@yelp.com . Yelp’s Engineering team is committed to excellence in engineering and data security; this incident was responded to with full force. Keeping user data safe is a top priority for Yelp, and we’ve taken concrete steps in response to this incident to make sure that it will not happen again. Tweet Back to blog", "date": "2011-11-01"}, {"website": "Yelp", "title": "Building and Testing Yelp Mobile", "author": ["\n        \n  Steven S., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/12/building-and-testing-yelp-mobile.html", "abstract": "The mobile team at Yelp always strives to bring the best possible experience to your mobile device , but what does it take to accomplish this? For Yelp’s iOS app , in addition to tools used by many iOS developers — Xcode for a development environment, Git for version control — we have found other useful tools and libraries to improve our development experience. If you’ve ever appreciated our app and wondered what goes into its development, we’re happy to shed some light on what tools from the iOS development community we make use of, and we even have some of our own to share! The foundation upon which the Yelp app is built is YelpKit . YelpKit is an Objective-C library that provides reusable controls and helpful extensions for Apple’s UIKit framework. YelpKit can assist with tasks from making requests and caching images to using pull-to-refresh. In fact, we found YelpKit to be so useful that we open-sourced it so we could use it in our own personal projects. Check out YelpKit on GitHub for instructions on using YelpKit and to see what features would come in handy for you! Of course, with all the work you put into building your app, you want to make sure it works! Automated testing is an important tool used by many developers, and the iOS developers at Yelp are no exception. We use GHUnit for unit testing and also make heavy use of its view testing capabilities; tests in GHUnit are run on an iOS device or emulator, and the resulting view of your app can be saved to an image file and compared against previous runs. GHUnit’s view tests prove invaluable for catching inadvertent changes, and can even help detect minute changes that occur from updates to the iOS SDK. As useful as automated testing is, getting your app into the hands of as many people as possible is still one of the surest ways to find bugs. This is where TestFlight comes in: TestFlight is a free service that makes it easy to distribute your app to testers. Testers must join TestFlight and be included in your app’s ad hoc provisioning profile. When you want to release a new version of your app for testing, you just upload the compiled IPA file and TestFlight will notify your testers that it is available for download! Testers need only click a link in this email to begin installing the app. You can choose to which testers to distribute the file, and TestFlight provides analytics indicating, for example, which testers have opened the email or if any have had problems installing your app. With our sharp group of fellow iPhone-wielding engineers eager to try out new features, this convenience offered by TestFlight allows Yelp to rapidly distribute and receive feedback on beta builds of apps. With the help of GHUnit and TestFlight, we manage to keep the Yelp app in good shape! Inevitably, though, some bugs can slip through, so we use Crashlytics to help us get to the bottom of any crashes that crop up once the app is released to the App Store. Crashlytics is a crash reporting solution that supplements the crash logs available through iTunes Connect. After configuring Crashlytics, the debug symbols (dSYM) for your app are uploaded to Crashlytics during the build; if your app crashes, a crash log is sent to the Crashlytics servers and symbolicated. Crashes are ordered by their prevalence in Crashlytics’ web interface, so at a glance you can tell what issues are most pressing and then start digging through the stack trace in order to keep your app functional and your users happy. In the lively iOS development community, you don’t have to look very far to find all sorts of useful tools and libraries. In showing you those that helped Yelp reach its success, we hope you’re excited about these tools, and we can’t wait to see what you can make using them! Tweet Back to blog", "date": "2012-12-12"}, {"website": "Yelp", "title": "Mission: Mobile Makeover", "author": ["\n        \n  Jeff M. Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/11/mission-mobile-makeover.html", "abstract": "As previously reported, we recently released a brand new mobile site , hand made with love for our more than 8 million users with mobile Webkit browsers who visited www.yelp.com from mobile devices in August. Surprisingly, there are a lot of challenges when creating a whole new, albeit mini, website (who would have thought?). If you were to look online for guidance on designing a mobile website, you would come across lots of articles that cover three main considerations: The once precise cursor is now your chubby, fumbling finger You have to present a lot of data on a wee-small screen Wireless networks, no matter what commercials say, tend to be slow\nWe aren’t going to reiterate those challenges here today. Instead, we will discuss some lesser publicized challenges and opportunities that we at Yelp were presented with. For the first iteration of the mobile site, we wanted to focus on the ease of discovery. Users are goal oriented on mobile devices; they want to get from their home screen to their destination as fast as possible and with the least amount of distraction. So naturally, our big question was “how can we help our users find great local businesses as easily as possible?” Although Siri tries , there is no silver bullet. The answer is multifaceted and involves both technical and design decisions. Making the design unobtrusive and useful When you type “yelp.com” in your smart phone browser, you are instantly directed to our new mobile site and the first thing you see is a search bar. No frills, no meaningless decoration, no lengthy explanation of what Yelp does (because who doesn’t already know!). Directly underneath the search bar is a list of categories. No header, no extraneous explanation that these are categories, no distractions. Finally, just below the category list, we show off one of our newest features: Hot New Businesses . Without even scrolling the page we have covered three user profiles: the hurried user that knows exactly what he wants to find, the user that knows she wants to find something and the user that is just casually seeing what’s new around them. Once clicking through to a business page, you will immediately see a map of the business and calls to action for getting directions or calling the business . If you are still undecided about the business, scroll down to quickly consume short but helpful commentary using Photos, Review Highlights and Quick Tips. Meanwhile, users that have a bit more time on their hands (jealous) can keep scrolling and read full-length reviews about the business. Once again, we have catered to three user profiles (concentrated, curious and casual) in order of their immediacy. Leisurely users are likely sitting on a comfortable sofa and can afford to flick the page around and browse longer-form content. However, the on-the-go (read: mobile) user needs to immediately see the tools that they need to complete their goal with ease and without distractions. Making the design understandable Giving the user the appropriate content in the appropriate place is a huge step to our overall goal. However, if the user doesn’t know how to interact with the content, it is all for naught. Even though a huge win for mobile sites is the “code once, run anywhere” principle, there seems to be a knee-jerk reaction to make everything look and behave like the iPhone. This is wrong and something that we were guilty of. There are at least two reasons why that way of thinking is harmful: Our friends on Android, Blackberry or Windows 7 phones aren’t familiar with the iOS design patterns. While the “Save” button in the upper right of the title bar, the red notification indicator and the dock-style navigation bar are great conventions; they are foreign on non-iOS users. All those sexy animations and gradients that are laced throughout the iPhone’s interface are nearly impossible to render smoothly, if at all. Even on the newest iPhone, Javascript and CSS animations won’t feel snappy like users have grown accustom to. And when (inevitably) they don’t work like native interactions, the user will feel like something is wrong, or at the very least, janky.\nA common solution for this is to progressively enhance the site based on the device it’s running on. However, a much more efficient approach is to design a website that doesn’t strictly adhere to a certain device’s guidelines. For this we can draw inspiration from the Bauhaus movement of the 1930’s and what they called “International Style”; a style that was universally applicable, acceptable and welcoming. This sort of aesthetic is much more in-line with the “code once, run anywhere” principle. When creating this hybrid aesthetic, you have the opportunity to synthesize all the best parts of native interfaces, common conventions from web browsers and your brand’s visual language. In our case, we implemented iOS-style clickable list-items, standard plain-text styling for links that take you out of the Yelp flow and an Android style loading screen – all while maintaining a Yelpy feel! While tailoring the design to each device could potentially provide the best user experience, it is a slippery slope, as you may be supporting N devices in the future. From content organization to button styling, our new mobile site made us re-evaluate our product. We had to sit down and focus on what we do best, helping people find great local businesses. We still have a lot to learn and a lot to build, so stick around - we are just starting. And if you have feedback, let us know at mobile-feedback@yelp.com! Tweet Back to blog", "date": "2011-11-04"}, {"website": "Yelp", "title": "Lookin' Good: UI Verification in GHUnit", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/12/lookin-good-ui-verification-in-ghunit.html", "abstract": "Here at Yelp, we build some popular mobile apps. We have around 5 million monthly unique users and about 1/3 of all Yelp searches come from mobile. Our apps continue to grow in complexity as we add new features. We try our best to test before releases, however, sometimes UI breaks and we don’t notice. For example, in a recent release, we had the ‘Get Directions’ button disappear from our iPad app! [Screenshot] We finally noticed it missing when we got a report from a user. We were fairly embarrassed that we hadn’t caught the problem ourselves. This was a problem that we needed to catch. In the past we’ve done manual testing to catch these issues, but being programmers, we decided to solve the problem with programming . For our iOS apps we use gh-unit, a testing framework developed by one of our engineers, Gabriel Handford. Recently, we added functionality to let us know when our views have changed. It works by rendering tested views to images and comparing them agains previous runs. Anytime a view changes, the developer is presented the old image, the new rendered view, and an image diff of the two. The developer then has to sign off by clicking the ‘Approve This Change’ button in the test. The developer then grabs these new images using a script and commits them to version control. This allows us to know exactly what changed in our UI due to a code change. We think it will be pretty useful for preventing UI-based regressions in our codebase. Get the code here: https://github.com/gabriel/gh-unit Also check out a presentation I did on GHUnit UI Verification at the App Developer Conference. Tweet Back to blog", "date": "2011-12-15"}, {"website": "Yelp", "title": "Diving into Yelpy Insights", "author": ["\n        \n  Aditya M. Software Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/04/diving-into-yelpy-insights.html", "abstract": "As you may have seen on our Yelp Official Blog , we recently did a post on a new feature we are testing: Yelpy Insights. Here on the Engineering and Product Blog, we wanted to dive into the work our engineering team did to help discover and display Yelpy Insights. The Idea: Yelpy Insights was actually borne from feedback we had received from vegetarian yelpers (and in fact, some members of the engineering team!) that finding a good place to eat vegetarian food was tricky. For example, some things that are liked by vegetarians are also liked by omnivores, but some things are only liked by vegetarians. The search team set out to help create a better search experience for veggie users (and Yelp engineers), but that also got us thinking: could the same be said for other groups? Why yes, it can! We discovered many such use cases we’re excited to test, including connecting yelpers with businesses that best fit their different age groups – because I know my Dad doesn’t want to be seated next to a bachelorette party at dinner. Or actually, maybe he would. What we look for: By combing through our vast review data, we’re able to pick out folks who share a vegetarian perspective in their reviews and highlight the businesses that they like. It’s not without its flaws, but it’s a great way to crowd-source insights about vegetarian-friendly businesses. In the same way that we dealt with vegetarians, we’ve identified places that are great for 20-somethings, 30-somethings, and 40-somethings. For example, if we find a restaurant with both an abnormally high concentration of good 30-something reviews, and a large overall number of these reviews, we can be pretty sure that a lot of 30-somethings like that place. That said, we never presume that you are only interested in one age group and you’re free to use this filter as you see fit. How we show it: There are several ways for users to discover these results. Users can search the ‘Liked by XX-somethings’ or “ Liked by Vegetarians ” filter on many searches performed on the site. Additionally, these businesses will also have a special badge above their reviews and a link to nearby places like it. And don’t worry, we wouldn’t dare share anyone’s private information . What we can do is learn from signals in the reviews and user profiles how different groups like various businesses in aggregate and use that information to improve your search experience on Yelp. What’s next: Of course, vegetarians and different age groups aren’t the only ones asking questions like “I want a business that does X, but I want one that people like Y like”. Our large and diverse community of reviewers can help us answer these questions for a variety of other groups that have a unique perspective on the businesses they review. Got an idea for the next Yelpy Insight? Let us know via feedback@yelp.com! Tweet Back to blog", "date": "2012-04-10"}, {"website": "Yelp", "title": "Day in the Life of a Yelp Systems Engineer", "author": ["\n        \n  Joanne H. Recruiting\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/04/a-day-in-the-life-of-a-yelp-systems-engineer.html", "abstract": "Meet Julien R., Yelp systems engineer extraordinaire! When he isn’t keeping San Francisco green by biking to work or brewing his own beer at home, this well-rounded engineer keeps Yelp running day and night. So, totally not multi-talented at all. Julien recently took some time to share his favorite stories during his three plus years at Yelp. Check it out! What did you do before coming to Yelp? I studied electrical engineering at Lehigh University in Pennsylvania. During one of my summer vacations, I did an internship for a small network engineering company and got my CCNA . Once out of school, I got a job at Friendster as a systems administrator. How did you first hear about Yelp and the job opening? My high-school friend Jane K. interned and worked at Yelp and referred me to the job. She tooted the Yelp horn long before I was looking to make a change in my career. When I eventually started looking, I applied to many places, but what made Yelp special was that it was one of the few places that I could honestly express my excitement when asked the question: “Why would you want to work at Yelp?” What’s your title at Yelp and how long have you been with the company? I am a systems administrator and I have been with Yelp for 3½ years. In that time, there have been lots of changes, but I love that Yelp still has a spunky personality and a fun work environment. What comprises a typical day for you? Every day I bike to work, but the rest isn’t always so routine! My main goal is to keep the site up and running, but I also have the flexibility to work with my manager and my team to prioritize my tasks. Currently, my main projects have been to monitor and add new metrics for graphing performance over time (i.e. search service) and monitoring system health. Recently, I have also been involved in expanding our processing capacity. This has been exciting because I get the opportunity to be involved across our infrastructure, including hardware selection, networking, and datacenter work. What’s the best part of working for Yelp? It’s the people and the culture. It’s exceptional how many knowledgeable people work here. Also the management style is really cool. My boss is on my team, but also advocates for me and provides direction – while still leaving me with lots of freedom! It also helps that there is a culture here which creates knowledgeable managers. It is a huge advantage to have a manager who understands the technical side of what is feasible and how hard it is to accomplish tasks. What is your favorite perk at Yelp? Definitely the kegmates on the 10th floor. We have an unofficial team of beer enthusiasts who pick out rare beers for our kegmates. I may or may not be on this team (meaning potential candidates now know who to suck up to :). What has been your favorite memory at Yelp? Yelp offsites! Particularly one we had a few years ago. We rented out a meadow in Golden Gate Park and had a picnic with outdoor games, BBQs, beers, and a dance floor. What separates Yelp from other places you’ve worked? The leaders in the company are accessible and make time for their employees. While the company has grown, they still maintain a start-up mentality and take the time to get to know us. They also help make sure that the environment is fun and enjoyable for employees – which is certainly visible if you take a look at our 10th floor space complete with a kitchen with snacks, pool, eating areas, sofas, beer on tap and more. Finally, what would be your one piece of advice for someone interested in your role ? I really like how personable everyone is at Yelp and how everyone has a passion outside of just Yelp (though we’re passionate about that, too!). So, be honest about your interests, share your passions both in work and out of work and be willing to share your war stories during an interview. Tweet Back to blog", "date": "2012-04-17"}, {"website": "Yelp", "title": "Hack@thon 7: Are You a Nerd?", "author": ["\n        \n  Michael, VP of Engineering\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/04/hackthon-7-are-you-a-nerd.html", "abstract": "It’s that time of year again: when Yelp engineers huddle for 48 hours to dream up and build some trippy stuff – and Yelp Hack@thon 7 proved no different! There was everything from custom Magic: The Gathering cards personalized for Yelp, our own version of Hungry, Hungry Hippos with (you guessed it) Hungry, Hungry Yelpers made in HTML5 to LOLPython on Monorails that featured LOLcode instead of Python (with some funny web framework to go along with it!). That Magic: The Gathering card look familiar, Jeremy? In past years , we’ve provided a deeper look into some of the projects our team has created, but this time around, we wanted to give you more insight into the people and teams that hack at Yelp. Hungry, hungry Amir! Check out just what goes on at a Yelp Hack@thon below and let us know, what makes you a nerd?<iframe src=\"http://www.youtube.com/embed/cg3roD2YGjc\" width=\"470\" height=\"296\" frameborder=\"0\"></iframe> Tweet Back to blog", "date": "2012-04-23"}, {"website": "Yelp", "title": "After Hours Project: Simulating Flight with Kinect", "author": ["\n        \n  John B Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2012/04/after-hours-project-simulating-flight-with-kinect.html", "abstract": "If you could have any superpower, what would it be? (You can’t say the power to get more powers.) If you’re like me, it’d be the power to fly. Unfortunately, I’m not a Superman, Neo, nor a bird. I’m a software engineer at Yelp; I make things happen by writing code. So I started to write code that tricks me into feeling like I’m flying. What I ended up with was a Mac app that allows you to control a quadcopter by flapping your arms like wings. To do this I used the Microsoft Kinect , a Parrot ARDrone quadcopter, OpenNI , and the app I wrote, KinectWings. How Does it Work? KinectWings uses the Kinect and OpenNI to track your body position. It monitors your body for movements that look like flapping or tilting. When a flap or a tilt is recognized, KinectWings sends control messages over wifi to the ARDrone. Video from the quadcopter is transmitted over wifi back to KinectWings, where it’s displayed for the user. When used in front of a large screen, it creates the illusion that you are flying by flapping and tilting your arms. I Want to Fly! If you’ve got some code know-how, you too can build software to control hardware using your body! I’ve opensourced an example project for using OpenNI in Cocoa applications for OSX. This provides a good starting point for doing skeletal tracking using a Mac and Kinect. I’ve also opensourced the changes I made to the ARDrone SDK so that it works on Mac. Check them out on GitHub! http://github.com/johnboiles/ARDroneSDK http://github.com/johnboiles/CocoaOpenNI Newsflash R. Kelly ! I no longer believe I can fly, I can fly. Tweet Back to blog", "date": "2012-04-30"}, {"website": "Yelp", "title": "Upcoming Tech Events at Yelp", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/02/upcoming-tech-events-at-yelp.html", "abstract": "Over the next couple of weeks, Yelp is going to be hosting two open-to-the-public events for members of the software development community: PyPy Just-in-Time Interpreters March 3rd, 6pm Armin Rigo of the PyPy project will be giving a presentation on achievements made by PyPy, the “fastest, most compatible, and most stable ‘alternative’ Python interpreter.” Special attention will be given to advancements in the area of dynamic (JIT) interpreters. You can find more information on SFPython’s Meetup page . If you plan to come, make sure to RSVP at least a day in advance so that security will allow you into the building. San Francisco Hadoop User Meetup March 9th, 6pm The third SF Hadoop meetup will be taking place at Yelp! The meetup is discussion-based, using an “unconference” format. Agenda and topics are determined at the beginning of the meeting (and anyone may volunteer a topic), but a proposed theme for the upcoming session is “integration.” The session is expected to last approximately 2 hours, and more information is also available on the SF Hadoop Meetup page . Again, please make sure to RSVP at least a day in advance in order to be admitted past security. Tweet Back to blog", "date": "2011-02-24"}, {"website": "Yelp", "title": "After Hours Project: Kinect Hacking", "author": ["\n        \n  John B Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/03/after-hours-project-kinect-hacking.html", "abstract": "Here at Yelp, we’re passionate about building things; it’s at the core of our engineering philosophy. In fact, we enjoy it so much that many of us keep on building after we finish work. I recently found some spare time to work on an interesting project with the Microsoft Kinect. I think it’s a cool start and I’ve open sourced the code so that others can build something even cooler. Easy Skeletal Tracking If you’re reading this blog, you’re likely familiar with the Microsoft Kinect. It combines an RGB camera, an infrared laser projector, and an infrared camera to determine depth of objects in a scene. This makes problems for computer vision that were previously very difficult, like interpreting skeletal motion, much easier. Recently, PrimeSense , the company that built the core technology behind the Kinect, launched an open-source version of their natural interaction framework called OpenNI . OpenNI provides APIs for interacting with hardware as well as incorporating higher level computer vision modules that can recognize objects and gestures. This allows us to solve really complex problems (like tracking skeletons in 3d space) very simply, since the heavy lifting is already done for us in OpenNI. Fun and Games I thought it’d be neat to get Kinect data into a game engine that supported physics so I could interact with objects in a virtual world. I chose Garry’s Mod for Valve’s Source engine because it can be easily scripted using Lua . Theory of Operation My code consists of two major parts: a backend that interprets Kinect data and a Lua script that controls the game. The backend (based on one of the OpenNI example projects) reads data from the Kinect over USB, uses OpenNI to track the skeleton of the user, then sends UDP packets containing (x, y, z) coordinates corresponding to the joints in the user’s skeleton. The Lua script in Garry’s mod parses those UDP packets, then maps those coordinates to spheres that move themselves to positions corresponding to the coordinates (these are like Garry’s Mod hoverballs, but in 3d). By attaching these position-tracking balls to different entities or objects, the user can move objects in the game by moving around! Interfacing with the Kinect is super easy with OpenNI. Here’s an example of all it takes to get and send data about the player’s left hand: void SendUDPLeftHandData(XnUserID player) { // Make sure the player is being tracked if (!g_UserGenerator.GetSkeletonCap().IsTracking(player)) return; // Get the player’s left hand position XnSkeletonJointPosition leftHand; g_UserGenerator.GetSkeletonCap().GetSkeletonJointPosition(player, XN_SKEL_LEFT_HAND, leftHand); // Create a data packet with the hand coordinate char packet[40]; sprintf(packet, “lhx%0.3fy%0.3fz%0.3f”, leftHand.position.X, leftHand.position.Y, leftHand.position.Z); // Uses standard posix calls to send a UDP packet with the data SendUDPPacket(packet, strlen(packet)); } What Now? This project is definitely still in the proof-of-concept stage, but that’s where you come in. The code is available on github at github.com/johnboiles/JBKinectHacks . Feel free to branch it and hack it into your own, better, creation. Drop me a line to let me know how it goes: johnb at yelp dot com Tweet Back to blog", "date": "2011-03-09"}, {"website": "Yelp", "title": "MySQL Minutiae & InnoDB Internals", "author": ["\n        \n  Evan K. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/03/we-store-the-vast-majority-of-our-data-at-yelp-in-mysql-during-normal-traffic-hours-were-issuing-tens-of-thousands-of-state.html", "abstract": "At Yelp, we store nearly all of our data in MySQL. At any given time we’re issuing tens of thousands of SQL queries to our database cluster per second , with some individual servers going above the 10k qps mark. Our database cluster consists of billions of rows. In response to a lot of different problems we’ve had to optimize the snot out of our MySQL installation, and we’ve learned some interesting things along the way. A colleague and I recently gave a presentation to some of our coworkers, titled MySQL Minutiae & InnoDB Internals . The talk covered some good background knowledge that every developer should have about MySQL (transaction isolation levels, replication, etc.) and some more advanced topics such as InnoDB locking semantics. There’s some good stuff in the talk for people of almost all skill levels, but to whet your appetite I’m going to dive into an interesting deadlock example. A Mysterious Deadlock The following example is based on a real deadlock I was trying to debug recently. Some of our photo code was issuing queries like this: DELETE FROM biz_photo WHERE business_id = ...; INSERT INTO biz_photo (…) VALUES (…); When adding a new photo, the code wasn’t sure if a row would already exist in the table, so it would issue a DELETE just in case followed by an INSERT with the correct data. This code was deadlocking a lot. The deadlocks were really mysterious too—you’d see that totally different business ids were causing deadlocks, so when photos were simultaneously added to two different businesses with different ids, the queries would still cause a deadlock. There’s a bunch of different ways that you could rework the queries to work around the problem—e.g. by using REPLACE INTO , ON DUPLICATE KEY UPDATE , a SELECT followed by an INSERT or UPDATE —but some of these techniques may not be replication safe , and without understanding _why _this is deadlocking it’s not clear which of these techniques will avoid the problem. Diving a bit deeper, I was able to reproduce this deadlock with a very simple contrived example (the notation T1 and T2 depicts two different transactions): CREATE TABLE dreadlock (i INT PRIMARY KEY) ENGINE=InnoDB; – N.B. dreadlock is empty at this point T1: BEGIN; DELETE FROM dreadlock WHERE i = 1; T2: BEGIN; DELETE FROM dreadlock WHERE i = 2; T1: INSERT INTO dreadlock (i) VALUES (1); – blocks T2: INSERT INTO dreadlock (i) VALUES (2); – deadlock here What’s going on? As it turns out the DELETE statement in T1 grabs an IX (intent exclusive) table lock on dreadlock , and an X (exclusive) gap lock on the dreadlock primary key index. Since the table is empty, the gap lock will cover the entire index (i.e. negative infinity to positive infinity). T2 does the same thing. Wait, what? X-locks are supposed to be exclusive, right? So how can T1 and T2 both hold an X-lock on the same part of the index? Shouldn’t T2’s DELETE statement block after T1 acquires its gap X-lock, since X-locks are incompatible ? As it turns out there are multiple types of gap locks, and the gap locks acquired by DELETE statements are of the purely “inhibitive” variety. The DELETE gap lock blocks INSERT statements (which acquire “insert intention” locks), but do not block other DELETE X-locks. T1’s INSERT statement tries to upgrade its X-lock to an insert intention lock which blocks on T2’s gap X-lock. When T2 tries to insert, InnoDB detects the deadlock and kills T2 (because it holds the fewest number of locks). Note that in this example, using MySQL’s SELECT FOR UPDATE syntax would not have avoided the problem, since that would have acquired exactly the same type of lock as the DELETE . In this case the best solution is to retry the transaction after a deadlock. If you found this interesting, or you want some coverage of more of the basics (including a full introduction to different InnoDB lock types) I encourage you to check out the slides . Tweet Back to blog", "date": "2011-03-21"}, {"website": "Yelp", "title": "Gotta Bounce... to an Engineering Offsite", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/04/gotta-bounce-to-an-engineering-offsite.html", "abstract": "Here in Yelp Engineering, we spend lots of time working on useful and cool stuff for Yelp users of all sorts—consumers, business owners, and even our own Community Managers. Sometimes though, it’s nice to take a break and go hang out with your coworkers for a bit. What better place to “hang out” than a few feet up in the air? Our most recent offsite took us to House of Air , a giant indoor trampoline park. After getting a brief safety introduction from HoA’s “Flight Crew” the team hit the trampolines, and well… I think these say it all: Of course, why stop at just trampolines, when you can add big red dodgeballs? That’s right - not only does House of Air have a giant trampoline grid, they also have a trampoline dodgeball colliseum. After some quick team organization, the dodgeball matches began: More cool pictures are after the “jump”… though if you’re on a mobile device, you might want to refrain! Final verdict? Would bounce again! Tweet Back to blog", "date": "2011-04-06"}, {"website": "Yelp", "title": "Parcelgen: A Code Generator for Android Data Objects", "author": ["\n        \n  Alex P. Mobile Team\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/04/parcelgen-a-code-generator-for-android-data-objects.html", "abstract": "When I switched from working on Yelp’s iPhone app to our Android app, one of the first things I encountered was the radical difference between the equivalent classes to handle what I normally consider a “screen” or “page” of an app. On Android, an Activity handles what on iOS you use a UIViewController for, but they work in fundamentally different ways. One of the biggest differences is that on Android you can’t just instantiate a new Activity and display it like you can with a UIViewController . Instead you create an Intent and tell the system to start an Activity based on that Intent. This works fine for simple Activities, but things get complicated when you have a complex data object which you want to display in a new Activity. Android doesn’t let you pass arbitrary objects between Activities. As this Android google group discussion points out, data passed to Activities must be placed in globally-accessible state, stored on the device’s flash memory, or passed inside an Intent . A static singleton works for a few objects, but it requires having a place to store all the objects being passed and is generally considered poor design . If you use a static collection to pass multiple objects, the receiver must make sure to remove the objects lest they leak memory. Writing an object to flash is expensive, slow, and requires having a reliable way to write and read an object (such as sqlite or Serializable ). Intent seems like it should be great, except all objects written to an Intent must be of a limited set of Java types and classes, or know how to read and write themselves to a Parcel by implementing the Parcelable protocol. After trying a number of different techniques to pass objects between Activities, we began making the basic objects in our application conform to the Parcelable protocol. This greatly simplified the process of launching new Activities, and allowed our application to save all its objects when suspended via onSaveInstanceState() so recovering from being stopped by the Android system was much easier to handle. As our application grew and we added more and more data objects, I tired of writing similar Parcel- and JSON-related code for each class. Enter Parcelgen Inspired by Android’s existing code generation in aapt , instead of manually writing classes that implement Parcelable , I wrote a Parcelable code generator in Python . All it needs to know is the class name, instance variables and types, and a little metadata about each object to create. I realized that I needed the exact same information to read an object from a Parcel that I needed to read an object from a JSONObject , so I enhanced my script to generate code to do that too. How it Works For each object to generate, write a small json description of the object’s members and their types: { “do_json”: true, “package”: “com.yelp.parcelgen”, “props”: { “String”: [ “id”, “name”, “imageUrl”, “url”, “mobileUrl”, “phone”, “displayPhone”, “ratingImageUrl”, “ratingImageUrlSmall”, “snippetText”, “snippetImageUrl” ], “int”: [“reviewCount”], “double”: [“distance”], “Location”: [“location”] }, “json_map”: { “ratingImageUrl”: “rating_img_url”, “ratingImageUrlSmall”: “rating_img_url_small” } } This is the parcelable description for (a subset of) a Business returned by the Yelp API as used in a sample app I wrote for parcelgen. Then execute the parcelgen python script to generate the java code for the object: $ python ~/parcelgen/parcelgen.py parcelables/Business.json src/ This creates two files: _Business.java and Business.java . _Business contains the parcel and json reading/writing logic, and Business contains a CREATOR static variable as required by Parcel . Business doesn’t have any dependencies on the object’s properties. If the json description changes and you re-run parcelgen _Business.java will be overwritten, but not Business.java . This lets you add data and application logic to Business without losing the flexibility to modify the parcel description later (any members added to Business won’t automatically get saved to a Parcel). Creating and Passing Parcelgen(erated) Objects Want to pass an object to a new Activity in an Intent? Just use Intent.putExtra() ( BusinessesActivity.java ): intent.putExtra(“business”, mBusiness); Then, in your Activity’s onCreate() : Business business = getIntent().getParcelableExtra(“business”); Want to create a list of objects from a JSON array of dictionaries ? Check out how the sample app does it : JSONObject response = new JSONObject(result); List businesses = JsonUtil.parseJsonList( response.getJSONArray(“businesses”), Business.CREATOR); Using parcelgen saves a lot of repetitive code and developer time in applications that are based around a web API. In Yelp for Android we use parcelgen to handle businesses, users, reviews, and other basic data objects. Whenever you tap a business from the search results list, it’s passed to the Activity to display the business through a Parcel with the help of parcelgen. Read Up Detailed instructions on how to use parcelgen in your project and some more advanced features are outlined in parcelgen’s readme on github . There is a working sample Android application on github here you can use as a reference for how to use parcelgen. Fork me on github Parcelgen is open source under the Apache 2.0 license and available for anyone’s use and modification on github . Please feel free to submit patches, feature requests, and such on github. If you use parcelgen in a project, please drop us a line. We’d love to know about other people using it! You can contact me directly through Yelp or through github . Tweet Back to blog", "date": "2011-04-19"}, {"website": "Yelp", "title": "Understanding Git", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/06/understanding-git.html", "abstract": "Yelp converted from using Subversion for source control over to Git over a year ago. As it turns out, however, Git (and distributed version control systems in general) can sometimes be daunting for some developers to understand, especially if they’re used to more traditional centralized versioning solutions. Git can also be a bit daunting for new developers just starting to use source control - it tends to assume that everyone is a power user, offering a high amount of potential, but sometimes at the cost of user-friendliness. In an effort to help more people get a deeper understanding of just what exactly is going on underneath the hood when it comes to Git, one of our recent weekly learning groups here at Yelp focused on Git fundamentals. Since we believe that others outside of Yelp might also benefit from a better understanding of one of the most powerful and yet free version control systems out there, we’re making the video and slides of the session available for anyone who wishes to peruse them. Enjoy! Tweet Back to blog", "date": "2011-06-15"}, {"website": "Yelp", "title": "Day in the Life of a Yelp Engineer", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/07/day-in-the-life-of-a-yelp-engineer.html", "abstract": "Check out the latest post on the main Yelp blog about a day in the life of one of our engineers, JR Heard! Tweet Back to blog", "date": "2011-07-09"}, {"website": "Yelp", "title": "Yelp Hackathon #5 Brings You KegTime and More", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/08/yelp-hackathon-5-brings-you-kegtime-and-more.html", "abstract": "If you read this blog regularly, you’re probably already familiar with Yelp Hackathons. They’re 48 hour periods of time we devote to anyone in the company to crank on projects that they’re excited about. In the past, our Hackathons have included special guests , pinatas and, oh yeah, hacking ! This year’s didn’t disappoint, with 28 total projects built, a petting zoo in the office (What? Your company doesn’t have those?) and a High School Science Fair type set-up so our hundreds of Yelp employees in San Francisco could wander, sip some Belgian ale and take a gander at what their fellow colleagues dreamed up. We wanted to showcase some of our favorite projects, so once again we created a video that takes a humorous look at just a handful of our insanely creative employees. While these projects aren’t (yet) live on Yelp, they may be something that we incorporate shortly! Take a look at Review Low-Lights, Side-by-Side and Kegmate 2.0 featuring KegTime. Let us know what you think, and if you’re interested in joining our ranks, check out our job openings at http://www.yelp.com/careers . Tweet Back to blog", "date": "2011-08-22"}, {"website": "Yelp", "title": "What the Engineers-in-Training Work on at Yelp", "author": ["\n        \n  Susan Yelp Recruiter\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2010/08/what-the-underage-engineers-work-on-at-yelp.html", "abstract": "This summer, we were fortunate to have 7 awesome interns join us for a 12-week internship at Yelp HQ. Coming from elite schools across the nation, these interns weren’t fetching us coffee or making copies. They were working side by side with our best and brightest on some of the most important and critical pieces of Yelp’s business. We had them coding away on everything from search infrastructure to user visible features. We couldn’t be prouder of the work they did and now we’d like to share their accomplishments with you! Aditya M.: Bachelors student at Carnegie Mellon Aditya tackled the huge undertaking of converting our Yelp Review Highlights over to MapReduce . He also improved search for yelp.fr and yelp.de despite the many nerf darts whizzing by his head every day. Garrick C.: Bachelors student at Carnegie Mellon Garrick worked tirelessly on our API version 2.0. He also got to toy around with some fancy (but secret) new features for our iPhone app. Garrick most enjoyed our small team sizes, Friday learning groups, and company offsites. Abhinav S.: Bachelors student at Carnegie Mellon Thanks to Abhinav, we now have a brand new photo uploader. Within seconds, you can upload as many photos as you want of your favorite local business. Go ahead - try it out! Abhinav’s favorite part about Yelp? His fellow Yelp Engineers (awww…). Evrhet M.: Masters student at Brigham Young University Evrhet re-built our email delivery tool for Yelp Elite Squad members and added a reporting system for all email correspondence to users and business owners. He was most impressed with Yelp Engineering’s speed, code quality, and work environment! Daniel L.: Bachelors student at Harvey Mudd Daniel spent his summer digging deep into Yelp data to improve business categorization. Searching for that auto repair shop in Wapakoneta, OH will now be a whole lot easier as we roll this change out. Daniel loved participating in the Summer Hackathon and hanging out with his co-workers in our brand new Yelp Kitchen . James O.: Masters student at MIT James worked with our business team on credit card compliance and improving business owner comments feature. Right off the bat, he felt part of the team and was most impressed with our code review process and development environment. Matt T.: Bachelors student at CalPoly Matt joined the infrastructure team and took over the Tron project, our centralized scheduler similar to fcron . Tron will load balance batch jobs across multiple machines, support jobs with multiple dependencies, and much much more. Matt most appreciated our small company culture, speedy development cycle, and passion for what we do here. We’re sad to see the summer come to an end but we look forward to a busy college recruiting season this fall! We’ll be making stops at Stanford, CMU, MIT, Waterloo, and many others so come find us to learn about full-time and internship opportunities . Tweet Back to blog", "date": "2010-08-24"}, {"website": "Yelp", "title": "Push it!", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2010/09/push-it.html", "abstract": "At Yelp, we push new code live every day. Pushing daily allows us to quickly prototype new features and squash bugs in a proactive manner. Because we aim to deploy new code so often, we’re always looking for ways to make the process efficient and painless. There are four main stages to the Yelp push process: code review, integration, testing, and finally, live deployment. Each step is important, and there are ways to maximize the efficiency of all of them. Code Review This first stage of the push process happens before a push request is even made: all code destined for the live site is subject to a full review by at least one other developer. For more expansive changes, reviewers with relevant experience or responsibility are added to the review as well. Code review gives us a place to spot silly mistakes, suggest alternative methods of accomplishing certain goals, and ensure that we maintain uniform code standards. By involving engineers who maintain relevant portions of the code base, it also keeps them informed of what’s going on with areas of the code for which they are responsible. Furthermore, we’ve found that code review helps drive a sense of community as we discover and share tips, tricks, and lessons that we’ve encountered throughout the development process. We use the excellent open-source Review Board to simplify the process. With features like the ability to add comments inline on diffs and compare different versions of diffs themselves, Review Board has proven itself a very helpful tool. Integration Yelp now uses Git for version control - we transitioned from Subversion a few months ago, but that’s a story for another blog post. Developers work in feature branches, pulling in changes from master at their own leisure. Before their code is added to a push, however, we always rebase the individual feature branches with all of the latest changes in master, to ensure that nothing conflicts with code that previously went live. In addition, each feature branch is squashed down to a single atomic commit, to make any necessary reverts easier. When we were smaller, coordinating requests for pushing changes was handled via email. As our team grew, however, it became apparent that we needed a more central and consistent way of tracking the push process. We didn’t find any existing solutions that really fit our needs, so we developed an in-house Google App Engine app to track pending requests and the status of the current push stages - we’ve open sourced it as PushmasterApp on GitHub. The app also provides reporting functionality to make it easier for us to go back and review who pushed what, when. Testing Yelp doesn’t employ a dedicated QA team. As a result, we rely heavily on a combination of automated testing and engineer-prepared test plans for new features. Turnaround time for squashing bugs on code that we’re trying to push is significantly shorter, since the engineers who write the tests are the same engineers who write the code. Automated Regression Testing With hundreds of thousands of lines of code in the Yelp repository, trying to test for regressions by hand wouldn’t just be arduous, it’d be impossible. While new features and bug fixes are verified by the engineer who wrote them, we also employ a suite of over 10,000 test cases (built on Testify , the open-source testing framework we developed in-house) to check for potential issues with a build. We also run a number of Selenium tests to test client-side functionality on the front end. While engineers often run individual test cases or test suites in their own development environment, running the entire battery of tests would be taxing on any individual system. Instead, we distribute the process to a number of machines using BuildBot . Staging Environment We maintain a set of staging servers that mirror the configuration of the live servers as much as possible. Each candidate build is deployed to the stage machines for manual verification by the developers who have changes in that push. Before we push to production, each developer and product manager must verify that their changes are present and working properly. Only after everyone involved has signed off on the build does it go live. Handling Problems If a changeset causes issues, the pushmaster has two options: Get a fix from the developer on the spot. Pull the changeset out of the current push, to be re-pushed later.\nIn practice, unless a given changeset is high-priority (a fix for a critical bug, or a new feature that has a deadline), we’ve found that pulling the changeset is generally the more efficient option . Holding up the overall push process while a single engineer works out a fix for an issue wastes the time of many engineers. Opting to just pull out problematic changesets allows us to decrease push times, and thus be able to push more often . This in turn decreases the time before the next push, so even if a changeset is pulled it can be re-pushed again soon after if the fix was simple. Going Live Once all of the involved parties have verified their changes and all our regression tests pass, the chosen build is sent from the staging servers directly to production. A key aspect of this process is that it enforces stage testing before code is pushed live - it’s impossible to push code directly to production. This protects us from accidentally pushing unknown code to production. Once a build is deployed to the production servers, the active version is switched over to the new build. The pushmaster and engineers involved in the push continue to monitor our error logs to make sure the build is stable. After the build has continued to appear stable for a reasonable period of time, the deployment branch is added back into the master branch, marking the new changes as the base for all future development. While the overall process doesn’t iterate quite as quickly as, say, continuous deployment , we’ve found that it offers a good balance between build iteration time and developer involvement in testing. What we given up in speed we gain back again in being able to quickly check the kinds of changes that can be hard to write comprehensive test cases for but are trivial for a developer to verify. Tweet Back to blog", "date": "2010-09-08"}, {"website": "Yelp", "title": "tron - Centralized Scheduling", "author": ["\n        \n  Matthew Infrastructure Team\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2010/09/tron.html", "abstract": "As Yelp has grown over the years, we’ve amassed huge collections of data - the collective output of the actions of tens of millions of users. Analyzing this data helps us improve the user experience across the site, from ranking businesses and extracting “review highlights” to fighting spam and keeping the site secure. These tasks involve long-running batch processes that analyze large logs and database tables, with workflows sometimes composed of five or more dependent processes. Managing these workflows with traditional scheduling tools - most notably the cron family - eventually caused us to breach the “complexity comfort zone” that surrounds any large engineering project. But then we had a vision, and it is that vision we are releasing today as tron . Tron Jobs A tron “Job” is a coherent goal, composed of one or more processing steps. Collecting business page metrics on Yelp is a Job and within this Job we aggregate business data, data mine the information, and send an email notification with the metrics. A Job is not a simple atomic command but rather several steps we call Actions . Dividing our Jobs into Actions allows us to pinpoint failures and restart them accordingly. Additionally, we may uniquely configure each Action. Only when all the Actions complete successfully is the Job instance marked a success. Dependencies A common need when configuring a Job is managing the prerequisites for certain Actions within it. For example, before you can decide which business pages were the most viewed this month, you must must first tally up how many times each business page was viewed. These dependencies are strict in that if tallying up the business monthly views fails, it is useless (and possibly dangerous) to compute top viewed business pages. To handle this problem we say “Get Top Viewed Businesses” Action requires “Tally Business Views” Action. An Action can require any number of other Actions, and what results is a DAG of dependencies shown in the example below. The diagram above depicts the full Job of “Collecting Business Metrics”. When one of the Job instances is kicked off, the Action that starts is “Tallying Monthly Business Views” because it is the only Action that has no dependencies. Once it finishes successfully, both “Getting Top 10 Businesses Viewed” and “Getting High Fluctuation Businesses” can start. And you can only “Send Email with Metrics” if the “Top 10” and “High Fluctuation” Actions complete successfully. If any Action fails along the way, no dependent Actions will start. Note on circular dependencies: Circular dependencies (if allowed) would cause some Actions to never run. This is solved in configuration by only allowing you to reference Actions defined previously in the file. Centralization tron runs on a Master/Slave configuration by running on the Master and sending commands to run on Slaves through SSH. This allows us to: keep all of our configuration in one place apply load balancing techniques easily schedule jobs that span multiple machines\nThe figure below shows the Master/Slave relationship. The Master runs the tron daemon and the daemon opens SSH connections to the Slaves when it’s time to run an Action instance. tron then executes the Action’s command via SSH and collects stdout , stderr and the exit status. The results are viewable through tronview (one of our tron tools). Note on Nodes: We added features into tron that allow you to customize which Actions are run on which Nodes. The default is that all Actions within a Job instance run on the same Node. Scheduling A large part of a batch process scheduler is, well, the scheduling. Similiar to cron , tron is able to run Jobs on different time intervals as well as more interesting schedules such as every Monday and Friday at 3p.m. We have also added tron variables you can add to an Action’s command that interpolate the scheduled date or time. This is especially useful if a script requires a date or time argument. For instance, if you want to tally all business views for a given date, you can use a tron variable to pass the script that date. tron variables are also persistent for a given Job instance. So if you want to cancel a Job instance and restart it days later, it passes the same date or time to the script as originally planned. Viewing tronview (a tron tool) displays tron status on four levels: All Jobs:  Displays all Jobs, their status and last success Job Runs:  Displays the Job history for a given Job Action Runs:  Displays all Action instances for a given Job instance The Action:  Displays information on the Action instance including stdout / stderr tron also has a web interface under active development. Controlling When something goes terribly wrong in the execution of an Action, it goes into a failed state along with the Job containing it. Sometimes this is fine with the user and tron starts the next Job instance when it is scheduled, but if the user wants to resolve this Action they can use tronctl . tronctl allows the user to restart the failed Action and continue the Job. Alternatively the user can restart the entire Job instance (and reprocess succeeded Actions). If the failure is worse than terrible, tronctl also allows users to disable an entire Job, then re-enable it once everything is resolved. Development tron is written in python and is available on our GitHub account . tron depends on the following open source projects: Twisted - Networking PyYaml - Markup language (it ain’t) Future Plans We have many Jobs running on tron and we are in the process of converting more of our cron-scheduled batch processes. tron is still in active development and more documentation and features are on the way. In particular, we plan to add: Improved web interface Handling multiple configuration files Monitoring Tools (email, IRC) Smarter Load Balancing More Info If you want to learn more about tron , read the tron wiki . n Tweet Back to blog", "date": "2010-09-14"}, {"website": "Yelp", "title": "Now Testify!", "author": ["\n        \n  Rhett G. Infrastructure Team\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2010/10/now-testify.html", "abstract": "The Yelp code base has been under development for over six years. We push multiple times a day. We don’t have a dedicated QA team. Yelpers get very angry when their detailed account of how that waitress was totally into them doesn’t get saved. Effective automated testing is the only way we can stay sane. One of the great benefits of Python is its “batteries included” philosophy which gives us access to lots of great libraries including the built-in unittest module. However, as our code base grows in size and complexity, so do our testing needs. There are plenty of open source libraries that have been developed to help augment Python testing. Notably: Nose which provides a more advanced test runner (and automated test discovery) unittest2 (or just unittest in python 2.7) provides some enhancements to unittest including class and module level setup/teardown, discovery and decorator-based test skipping.\nHowever none of these projects really met all our needs and more importantly didn’t prove to be very easily extended. So we wrote our own test framework: Testify . Testify provides: PEP8 naming conventions. No more setUp() Less java-like dependencies on class methods for things like assertions. Down with self.assertEqual() Enhanced test fixture setup. Multiple setup/teardown methods. Class and module level fixtures. Test discovery Flexible, decorator based suite system. Fancy color test runner with lots of logging / reporting options (JSON anyone?) Split test suites into buckets for easy parallelization Built-in utilities for common operations like building mocks, profiling, and measuring code coverage. Plugin system for hooking in whatever extra features you like. Mostly backwards compatible with unittest test cases. A cool name. A few words on how we actually use testify Testify is pretty flexible so it may help to give some examples of how it works for us. Our test framework is required to meet a few different workflows: Test Driven Development Verifying your change isn’t going to break the whole site prior to code review and staging Verification that a code push isn’t going to break the site.\nFor TDD, you often need to re-run the same test or tests from a single module over and over again. Selecting exactly what test method to run is important: testify yelp.tests.search SpellCheck.test_pizza_search For running a larger set of tests that might be affected by your change you might organize your tests into one or more suites . For example, tests that impact search will put together in the search suite. If you make a change that might impact search, we can just run: testify yelp.tests -i search For pre-deployment testing we need to run everything. Everything is a lot. KegMate or no, who wants to wait hours to find out UFC is really FUC’d. Thankfully we can split tests up: testify yelp.tests –buckets=10 –bucket=0 This allows us to split all our tests into 10 equal-ish sized chunks. We use Buildbot , another open source project, to run them across a cluster of machines. We are not using testify only for unit testing (Despite the name, most users of unittest probably aren’t either). We have a mix of unit, functional and integration tests which means we have a variety of environmental requirements. Sometimes we need a snapshot of a real database to test on (scrubbed of those illicit PMs to your favorite reviewer of course). Sometimes we need a full search index to search against. Sometimes we need an empty database where we can create exactly what data a test will see. This means complex test fixtures with slow startup times. Since different tests require different resources to run, we can associate these tests with suites to isolate them. So if your test can run in a simulated environment with minimal data, it can go in a sandbox suite. If it requires external services like search index, you can add it a search-index-required suite. This allows us to further collect Buildbot slaves together to reduce the number of tests running in more expensive environments. Of course when you have complex tests with complex external dependencies, eventually some of them will become unreliable. To help control for that we use Testify’s reporting capabilities to store a history of our test runs. When was the last time this test failed ? Is it me? Is it you? We can take all the JSON output from prior runs and put it all in a database making such analysis much easier. If the test is just fundamentally flawed, it’s easy to disable it. Just add it to the disabled suite, file a ticket, and move on. Try it out Testify has been great for us. We hope others find it useful as well. May your testing infrastructure be forever pain free. Testify is released under the Apache 2.0 license and hosted on GitHub. For more information check out our QuickStart guide. Tweet Back to blog", "date": "2010-10-07"}, {"website": "Yelp", "title": "mrjob: Distributed Computing for Everybody", "author": ["\n        \n  Dave M Search and Data-Mining Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2010/10/mrjob-distributed-computing-for-everybody.html", "abstract": "Ever wonder how we power the People Who Viewed this Also Viewed… feature? How does Yelp know that people viewing Coit Tower might also be interested in the Filbert Steps and the Parrots of Telegraph Hill? It’s pretty much what you’d expect: we look at a few months of access logs, find sessions where people viewed more than one business, and collect statistics about pairs of businesses that were viewed in the same session. Now here’s the kicker: we generate on the order of 100GB of log data every day. How do we deal with terabytes of data in a robust and efficient manner? Distributed computing! Today we want to share that power with you. As you may have guessed, we use MapReduce . MapReduce is about the simplest way you can break down a big job into little pieces. Basically, mappers read lines of input, and spit out tuples of (key, value) . Each key and all of its corresponding values are sent to a reducer. Here’s a simple MapReduce job that does a word frequency count, written in our mrjob Python framework: from mrjob.job import MRJob import re WORD_RE = re.compile(r”[w’]+”) class MRWordFreqCount(MRJob): def mapper(self, _, line): for word in WORD_RE.findall(line): yield (word.lower(), 1) def reducer(self, word, counts): yield (word, sum(counts)) if __name__ == ‘__main__’: MRWordFreqCount().run() In goes text, out come tuples of (word, count) . You can give it your README file, or all of Project Gutenberg , and it’ll scale gracefully. (More complex examples: PageRank , Text Classifier ) We used to do what a lot of companies do, which is run a Hadoop cluster. We had a dozen or so machines that we otherwise would have gotten rid of, and whenever we pushed our code to our webservers, we’d push it to the Hadoop machines. This was kind of cool, in that our jobs could reference any other code in our code base. It was also not so cool. You couldn’t really tell if a job was going to work at all until you pushed it to production. But the worst part was, most of the time our cluster would sit idle, and then every once in a while, a really beefy job would come along and tie up all of our nodes, and all the other jobs would have to wait. About a year ago we noticed that Amazon had a service called Elastic MapReduce (EMR for short) where you could essentially spin up virtual Hadoop clusters with as few or as many machines as you needed, and pay for it by the hour. I was tasked with migrating our code over to EMR. Not just getting Python scripts to run on EMR, but getting our code base to run more or less as-is, with all of its 40+ library dependencies, C and Cython code that compiles into Python modules, and files that are different from development to production. It took me a while, but on May 18, 2010, we retired our Hadoop cluster for good, and switched all of our production jobs over to EMR. Our framework, mrjob , is finally mature enough that we’re releasing it as an open source library. If you’re new to MapReduce and want to learn about it, mrjob is for you. If you want to run a huge machine learning algorithm, or do some serious log processing, but don’t want to set up a Hadoop cluster, mrjob is for you. If you have a Hadoop cluster already and want to run Python scripts on it, mrjob is for you. If you want to migrate your Python code base off your Hadoop cluster to EMR, mrjob is for you. (If you don’t want to write Python, mrjob is not so much for you. But we can fix that .)\nSo try it out and let us know what you think! And send us cool examples so we can feature them on our blog. :) To install: sudo easy_install mrjob Documentation PyPI profile page GitHub If you have questions or encounter issues, feel free to contact me through Yelp or through GitHub . I really want using mrjob to be a smooth and productive experience for everyone. By the way, here are some other features on our site that are powered by mrjob : Review highlights Autocomplete as you type on search Search spelling suggestions Top searches Ads Tweet Back to blog", "date": "2010-10-29"}, {"website": "Yelp", "title": "Yelp's Third Hackathon", "author": ["\n        \n  Amber Y. Infrastructure Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2010/11/yelps-third-hackathon.html", "abstract": "Click your heels together and say the magic words three times: “there’s no place like Yelp…there’s no place like Yelp…there’s no place like Yelp.” Then perhaps you’ll be whisked away to the land of Yelp Engineering where our third Hackathon just wrapped up. Like the previous Hackathon and the one before that , this 48-hour period where engineers were set loose to work on “anything you wouldn’t normally be able to work on that might be useful, funny, or cool” (and ostensibly, related at least indirectly to Yelp) resulted in some awesome creations. One of the biggest milestones for our third Hackathon was the inclusion of the first non-engineer participants! For instance, Alex O. from our Sales Ops team joined forces with engineer Aaron V. from our Systems team and A/V tech Patrick D. to produce an unofficial Yelp song: </embed> ( HD version available on YouTube .) Of course, there were plenty of software-oriented projects as well. One team created what they dubbed “MRPig” - inspired by the concept of Apache Pig , they created a general data query language on top of Yelp’s open source MRJob framework that allows easier creation of MapReduce jobs for data analysis, with less need to involve an engineer in the process. Our product managers were excited about the possibility of having faster turnaround times on questions like, “is there a correlation between ad category determination accuracy and overall click-through rates?” Data query languages are great, but they’re not exactly shiny . A different team of engineers created a visualization system that displayed what parts of the Yelp codebase were being actively worked in real time, by hooking into system-level events via inotify on development machines, and then piping that through a Tornado-based server to clients rendering HTML5 <canvas> -based displays. Check out some of the results: (Directory names have been converted to hashes for security reasons, and usernames have been pixelated.) Of course, our Hackathon wouldn’t be complete without some friendly competition. In addition to the Useful, Funny, and Cool awards from previous Hackathons, this time we added a fourth category, “Bling”, for the project that had the potential to generate the most money. The awards were earned by the following projects: Useful MRPig MapReduce-based query system for data Julian K., Sudarshan G., Bill H. Funny Yelp Came Knockin’ On My Door The unofficial Yelp theme song Aaron V., Alex O., Patrick D. Cool Awesome Badge Improvements New, more interactive check-in badges Alex D., Matt J., Eric S., Wing Y. Bling Key Account Sign-Up Building more efficient ways to deal with large advertisers Matt T., James D., Ryan D. We had a total of 20 teams for this Hackathon, the most we’ve ever had. Other projects included a system to defeat the paradox of choice by helping you decide where to go to lunch, a self-balancing robot built around an iPhone (that adorably cried “Malfunction!” if it fell over), and many more. While no project is guaranteed to make it all the way to the live site, don’t be surprised if some of them show up as new features in the future! Tweet Back to blog", "date": "2010-11-29"}, {"website": "Yelp", "title": "Towards Building a High-Quality Workforce with Mechanical Turk", "author": ["\n        \n  Paul W. Search Data-Mining Engineer\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/02/towards-building-a-high-quality-workforce-with-mechanical-turk.html", "abstract": "In addition to having written over 15 million reviews , Yelpers also contribute hundreds of thousands of business listing corrections each year.  Not all of these corrections are accurate, though, and there are quite a few jokers out there (e.g. suggesting the aquariums category for popular seafood restaurants… very funny!).  Yelp is serious about the correctness of business listings, so in order to efficiently validate each and every change, we’ve turned to Amazon’s Mechanical Turk (AMT) as well as other automated methods.  We recently published a research paper [1] at the NIPS 2010 Workshop on Computational Social Science and the Wisdom of Crowds reporting on our experiences. Vetting Workers On Test Tasks Our experiences agree with several other studies in finding that the AMT workforce has many high-quality workers but also many spammers who don’t perform tasks reliably.  In particular, only 35.6% of workers passed our basic multiple-choice pre-screening test.  We used expert-labeled corrections in order to test worker performance and found that the variance of worker accuracies was very high: Please see our paper for a full discussion of our observations.  Previous studies have proposed mechanisms to correct for the sort of worker biases we observed.  However, these mechanisms correct results as a post-processing step after workers have been paid for completing all tasks .  Given our experiences and financial goals, we find that a useful mechanism must vet workers online as they complete tasks . Naïve Bayes Classifier Bests Categorization Task As highlighted in a recent MIT Technology Review article, one of the notable results of our study is that a Naïve Bayes classifier trained on Yelp reviews outperformed AMT workers on a business categorization task. Naïve Bayes classifiers have been highly successful in various domains from email spam detection to predicting gene expression .  We trained and evaluated our classifier on over 12 million reviews and business names using our open source mrjob framework and compared its prediction performance to that of our workforce. Using a combination of the classifier and simple heuristics to clean up common misclassifications, the classifier outperformed workers for both accepting and rejecting category changes. While we might expect traditional machine learning techniques to perform well on our particular document categorization task, other tasks such as phone number verification are more readily crowdsourced.  Furthermore, in the future we may build upon techniques pioneered in other studies that use crowdsourced labels to train classifiers to perform well on hard problems. Lessons Learned We learned a ton at the NIPS Workshop and are very thankful for a number of very helpful comments.  Much of the crowdsourcing research presented at the workshop illustrates that very careful attention to the incentive structure of AMT tasks is critical to attracting and retaining the best workers.  In particular: John J. Horton shows that task pricing is critical to attracting high-output workers .  Some AMT workers have wage targets, and AMT Requesters who only offer low wages may have trouble retaining the best workers. Winter Mason shows how, for small tasks similar to those in our study, quota systems elicit more effort than piece-rate pay . Workers put forth more effort when small tasks are grouped together and require workers to form a plan of attack to complete work on time. Furthermore, making tasks collaborative (e.g. where workers can work together on a result or communicate in a chat room) may also improve worker performance. Some Workers Take Longer Than Others We would like to expand on a fact mentioned in the paper that has led to some misconceptions about the rates at which we paid workers.  In the paper, we report that the median worker task completion time was 5 minutes and that workers were paid about USD$0.05 per task.  First, the reported completion time statistic is improperly represented: we observed a median HIT completion of 5 minutes, where each HIT included between two to four tasks .  Second, the modal HIT completion time was 60 seconds overall and 90 seconds for workers with low completion time variance.  Given our HIT pricing, there were many workers who earned between USD$6 and USD$9 per hour for their work.  We chose our pricing in part based upon the prices of other HITs posted on AMT. Though many workers completed HITs quickly, we question the accuracy and usefulness of AMT’s recorded completion times because we observed a very high variance in this data.  The histogram below shows that some workers accepted tasks hours before submitting answers. Workers tended to complete batches of test tasks within a day or two after we posted them on AMT.  We rarely rejected work, so many workers may have seen our tasks as a guaranteed source of income.  Assuming that AMT’s completion time records are accurate, we hypothesize that some workers hoard batches of tasks far in advance of actually completing them. Acknowledgements We would like to again thank the workshop attendees and the anonymous reviewers for their helpful comments.  We would also like to thank Professor Jenn Wortman Vaughan for her feedback on a preliminary draft of this work.  Please send any questions or comments to the corresponding author . [1] Wais, P.,  Lingamneni, S., Cook, D., Fennell, J., Goldenberg, B., Lubarov, D., Marin, D., Simons, H. “Towards Building a High-Quality Workforce with Mechanical Turk.”  In NIPS Workshop on Computational Social Science and the Wisdom of Crowds, Whistler, BC. 2010.  [ citeulike/bibtex ] Tweet Back to blog", "date": "2011-02-18"}, {"website": "Yelp", "title": "Weird iPhone Compiler/Architecture Bug", "author": ["\n        \n  Jim B., Engineering Manager\n\n", "\n      "], "link": "https://engineeringblog.yelp.com/2011/02/the-problem-on-the-device-but-not-the-sim-the-following-code-would-crash-on-views-that-implemented-drawinrect-methods-tha.html", "abstract": "Alan on our iPhone team recently encountered a tricky bug that had to do with the ObjectiveC compiler and differences in architecture between ARM and x86. The Problem On the device (but not the sim), the following code would crash on views that implemented drawInRect methods that returned a CGPoint (instead of having a void return type like other drawInRect methods). for (id view in _subviews) { [view drawInRect:[view frame]]; } To make matters weirder, the pointer ‘view’ before stepping into drawInRect was not the same as the pointer ‘self’ after stepping into drawInRect! However, by typecasting view to the correct class before calling drawInRect, you could avoid the crash. for (id view in _subviews) { if ([view isKindOfClass:[YPUserBadgeView class]]) [(YPUserBadgeView *)view drawInRect:[view frame]]; else [view drawInRect:[view frame]]; } The skinny This bug occurred because the compiler doesn’t know for sure the return type of drawInRect. It made a guess and it guessed incorrectly. This caused problems on ARM and not x86 because of differing calling conventions. Why is the compiler dumb? Because Objective-C is dynamically typed, it can’t be sure of the return type. According to Mike Ash “[the compiler] makes a guess at the method prototype based on the methods it can see from the declarations that it has parsed so far. If it can’t find one, or there’s a mismatch between the declarations it sees and the method that will actually be executed at runtime, Bad Things Happen.” Because the compiler found other methods named ` - (void)drawInRect:(CGRect)rect`, it made a guess to expect drawInRect to return void. Bad things happened. Normally the compiler warns us of this, but in this case, it didn’t know about our implementation of drawInRect that returned CGPoint. If you put #import \"YPUserBadgeView.h\" above the position where drawInRect: was called, we get a warning that reads “ Multiple methods named ‘-drawInRect:’ found”. So at least in most cases, the compiler will tell us when it’s taking a guess. Why did bad things happen on the device and not the sim? This caused problems on the device and not the simulator because of difference in calling conventions between x86 and ARM (see http://en.wikipedia.org/wiki/Calling_convention ). x86 stores the return value (or a pointer to it), in the eAX register. It therefore doesn’t care if there was or was not a return value. You may get garbage in your return value if you try to read a function with no return value, but it shouldn’t have a serious impact. ARM however is different. It uses the first four registers for parameters and the return value. According to the aforementioned wikipedia article, “if the type of value returned is too large to fit in r0 to r3, or whose size cannot be determined statically at compile time, then the caller must allocate space for that value at run time, and pass a pointer to that space in r0.” TODO(johnb): reference http://infocenter.arm.com/help/topic/com.arm.doc.ihi0042d/IHI0042D_aapcs.pdf http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.subset.swdev.abi/index.html instead of wikipedia. This means that depending on the the return type, r0 may contain a pointer to the return value, or it may contain the parameter self. TODO(johnb): Figure out why CGPoint wasn’t stored in registers and whether it is if you turn on optimizations. As an example the registers when a function like - (void)drawInRect:(CGRect)rect is called might look like r0 = self r1 = _cmd (@selector(drawInRect:)) r2 = &rect while the registers when a function like - (CGPoint)drawInRect:(CGRect)rect is called might look like r0 = &retVal // the memory location at which to store the returned CGPoint struct r1 = self r2 = _cmd (@selector(drawInRect:)) r3 = &rect This causes obvious problems if a method is expecting the registers to look like the second example, but actually gets registers that look like the first example. Self is then read as the SEL value @selector(drawInRect:)! What we should do about it? According to “http://developer.apple.com/library/mac/#documentation/Cocoa/Conceptual/ObjectiveC/Articles/ocStaticBehavior.html#//apple_ref/doc/uid/TP30001163-CH16-TPXREF161”>this Apple reference page: “In general, methods in different classes that have the same selector (the same name) must also share the same return and argument types.” Therefore we should avoid implementing methods that have the same name, but different argument types. Because our naming scheme usually mentions argument types, this is probably most likely to occur from different return types. In the case of this bug, we shouldn’t have methods like - (void)drawInRect:(CGRect)rect; and - (CGPoint)drawInRect:(CGRect)rect; See Also http://stackoverflow.com/questions/2442278/question-about-objective-c-calling-convention-and-argument-passing-on-arm Tweet Back to blog", "date": "2011-02-23"}]