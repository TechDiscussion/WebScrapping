[{"website": "Slack", "title": "Creating a React Analytics Logging Library", "author": ["Konstantin Savransky", "Justin Huddleston", "Fabio Canache"], "link": "https://slack.engineering/creating-a-react-analytics-logging-library-2/", "abstract": "  In the    first installment of the article   , we examined why we built a React analytics library. We also looked at how we use the library to share data efficiently, log smarter impressions, and simplify event logging.  . In the  . first installment of the article . , we examined why we built a React analytics library. We also looked at how we use the library to share data efficiently, log smarter impressions, and simplify event logging. .  In this second part of the article, we will focus on how we abstracted the library for use by any team at Slack, along with powerful history tracking and live data viewing features.   . In this second part of the article, we will focus on how we abstracted the library for use by any team at Slack, along with powerful history tracking and live data viewing features.  .  In the following    Abstract PropType Type-Checking    and    Custom Payload Data Shape    sections, we demonstrate how to create customizable data objects and payloads with built-in validation and static typing. In    Keeping a History   , we look at how we record logs as a user navigates through the app, improving our data context. In    Building a Viewer   , we examine how we created a live log-viewer to gain visibility into the data. Finally, in the    Impact    section, we look at the adoption and potential effect the library has had at Slack. Let’s go!  . In the following  . Abstract PropType Type-Checking .  and  . Custom Payload Data Shape .  sections, we demonstrate how to create customizable data objects and payloads with built-in validation and static typing. In  . Keeping a History . , we look at how we record logs as a user navigates through the app, improving our data context. In  . Building a Viewer . , we examine how we created a live log-viewer to gain visibility into the data. Finally, in the  . Impact .  section, we look at the adoption and potential effect the library has had at Slack. Let’s go! .  You may have noticed in our previous installment that we ignored    PropTypes    in our example code. The reason for the evasion is that there is a bit of complexity, so let’s tackle it now.   . You may have noticed in our previous installment that we ignored  . PropTypes .  in our example code. The reason for the evasion is that there is a bit of complexity, so let’s tackle it now.  .  If you’re not familiar with using React PropTypes, you can    read up on them here   . But basically they provide type-checking for information passed between React components.  . If you’re not familiar with using React PropTypes, you can  . read up on them here . . But basically they provide type-checking for information passed between React components. .  We need our library to be flexible enough to allow the different teams using it to define their own data tag props (i.e. key-values that will provide log data, e.g. page: ‘home’) and payload structure. So we can’t “hardcode” our props. Instead, let’s create a configuration to allow teams to define their own data tag props. We want the prop configuration to look like this:  . We need our library to be flexible enough to allow the different teams using it to define their own data tag props (i.e. key-values that will provide log data, e.g. page: ‘home’) and payload structure. So we can’t “hardcode” our props. Instead, let’s create a configuration to allow teams to define their own data tag props. We want the prop configuration to look like this: .  Now that we have a configuration for our data props, let’s use it in our Log component:  . Now that we have a configuration for our data props, let’s use it in our Log component: .  What?! We’ve abstracted proptype-checking? Yup, React lets us do that with    PropTypes.checkPropTypes   ! We import our    dataTagPropTypes    configuration into Log and, in Log’s    propTypes   , run it through    checkPropTypes    (   validateTagPropTypes    is called automatically). Now anyone who uses our library can customize their data tag keys and types however they see fit. Awesome! By type-checking our log data props, we greatly reduce data errors. More awesome!  . What?! We’ve abstracted proptype-checking? Yup, React lets us do that with  . PropTypes.checkPropTypes . ! We import our  . dataTagPropTypes .  configuration into Log and, in Log’s  . propTypes . , run it through  . checkPropTypes .  ( . validateTagPropTypes .  is called automatically). Now anyone who uses our library can customize their data tag keys and types however they see fit. Awesome! By type-checking our log data props, we greatly reduce data errors. More awesome! .  So far, we’ve assumed that the data payload    sendLog    sends to the backend is flat. But what if a team needs to nest its data tags in a custom shape? Continuing our example, say we want to nest our    action   ,    elementType   , and    elementName    data tags under a key named    ui_properties   . We can add a    shape    object, which will provide the payload structure for our log. We can have any key-value structure we want:  . So far, we’ve assumed that the data payload  . sendLog .  sends to the backend is flat. But what if a team needs to nest its data tags in a custom shape? Continuing our example, say we want to nest our  . action . ,  . elementType . , and  . elementName .  data tags under a key named  . ui_properties . . We can add a  . shape .  object, which will provide the payload structure for our log. We can have any key-value structure we want: .  In our    shape    configuration, the values are the string names of data tag props.   . In our  . shape .  configuration, the values are the string names of data tag props.  .  Now that we have a shape, we need to flatten it into paths so we can programmatically create our payload. We’ll run it through a flattening function (   example here   ), to get this:  . Now that we have a shape, we need to flatten it into paths so we can programmatically create our payload. We’ll run it through a flattening function ( . example here . ), to get this: .  After we have finished building our flattened shape paths, we are ready to recreate the custom payload shape. To do that we need to write some code. Let’s wrap our original    sendLog    function with that code:  . After we have finished building our flattened shape paths, we are ready to recreate the custom payload shape. To do that we need to write some code. Let’s wrap our original  . sendLog .  function with that code: .  We iterate through the data tag props passed into    sendLog   . For each prop, we use the flattened    shape    object to get the path for the tag. With the path, we use lodash’s     set     function to recreate the desired payload structure.   . We iterate through the data tag props passed into  . sendLog . . For each prop, we use the flattened  . shape .  object to get the path for the tag. With the path, we use lodash’s  . set .  function to recreate the desired payload structure.  .  By having a configured payload shape, we guarantee that data keys are named and nested correctly and sent to the data warehouse in a consistent, structured manner!    . By having a configured payload shape, we guarantee that data keys are named and nested correctly and sent to the data warehouse in a consistent, structured manner!   .  In the previous sections, we learned how to fire a basic log event. Now let’s cover another useful case: Log history. Imagine we log a page impression, but we also want to know    how    the user arrived on this page. Impressions are useful, but having this extra information can help drive a deeper understanding of how users are navigating within our app. Maybe one particular button on a page is clicked more often than another button that leads to the same page. Sure, we could examine the click event logs, but unless we    also    add the button’s intended    href    to the log’s data payload, this information might not be easily accessible.  . In the previous sections, we learned how to fire a basic log event. Now let’s cover another useful case: Log history. Imagine we log a page impression, but we also want to know  . how .  the user arrived on this page. Impressions are useful, but having this extra information can help drive a deeper understanding of how users are navigating within our app. Maybe one particular button on a page is clicked more often than another button that leads to the same page. Sure, we could examine the click event logs, but unless we  . also .  add the button’s intended  . href .  to the log’s data payload, this information might not be easily accessible. .  What if we store the most recent log event as it occurs? That way, we can enrich the next impression we log with data from the referring event:  . What if we store the most recent log event as it occurs? That way, we can enrich the next impression we log with data from the referring event: .  In the example above, the log data contains a key called    referring_data   . This object contains all the previous event’s data. Now when we analyze our data later, it is immediately obvious how the user landed on our page: from the header section of our settings page, as well as which button!  . In the example above, the log data contains a key called  . referring_data . . This object contains all the previous event’s data. Now when we analyze our data later, it is immediately obvious how the user landed on our page: from the header section of our settings page, as well as which button! .  So how can we get this data without lifting a finger? Simple: let’s use the browser’s     window.sessionStorage     to store the last event log. You might be thinking: why can’t we simply cache the last log event locally and read it on the following event? While this will work for single-page applications, it will not work when navigating between multiple applications that have their own internal storage (this may be specific to Slack’s use-case and will not apply to everyone). Using    window.sessionStorage    allows the data to be retained in browser memory, so long as the same session is used. When the current tab or window is closed, any stored data is cleared.  . So how can we get this data without lifting a finger? Simple: let’s use the browser’s  . window.sessionStorage .  to store the last event log. You might be thinking: why can’t we simply cache the last log event locally and read it on the following event? While this will work for single-page applications, it will not work when navigating between multiple applications that have their own internal storage (this may be specific to Slack’s use-case and will not apply to everyone). Using  . window.sessionStorage .  allows the data to be retained in browser memory, so long as the same session is used. When the current tab or window is closed, any stored data is cleared. .  Up to this point, we have shown how the library is capable of tracking different types of user events and how easy and flexible it is to configure these events. Now let’s take a look at how we expose these events so anyone can see the logged data.  . Up to this point, we have shown how the library is capable of tracking different types of user events and how easy and flexible it is to configure these events. Now let’s take a look at how we expose these events so anyone can see the logged data. .  When we started defining the library, we knew it had to offer a way to visualize the different log events configured on a given page or component. We wanted to allow anyone to easily access these events without requiring any specific knowledge of or access to the actual code. So, we implemented a simple log-viewer tool to allow visualizing this data.  . When we started defining the library, we knew it had to offer a way to visualize the different log events configured on a given page or component. We wanted to allow anyone to easily access these events without requiring any specific knowledge of or access to the actual code. So, we implemented a simple log-viewer tool to allow visualizing this data. .  The log-viewer tool is included in the library and automatically generates a visual representation of not only the configured log events but also the events as they get triggered by the user.  . The log-viewer tool is included in the library and automatically generates a visual representation of not only the configured log events but also the events as they get triggered by the user. .  The log-viewer can be enabled on any page on which this logging library is being used. Once enabled, the viewer will render a series of pulsing dots next to each component which has been configured to log an event, allowing you to easily visualize which events will be logged. (To make accessing the log-viewer easy, we created a simple keyboard shortcut to activate it.) Clicking on one of these dots will show the information tracked by that specific component. Here’s how this looks in some of our menu options within Slack:  . The log-viewer can be enabled on any page on which this logging library is being used. Once enabled, the viewer will render a series of pulsing dots next to each component which has been configured to log an event, allowing you to easily visualize which events will be logged. (To make accessing the log-viewer easy, we created a simple keyboard shortcut to activate it.) Clicking on one of these dots will show the information tracked by that specific component. Here’s how this looks in some of our menu options within Slack: .  On the image above, you can see the pulsing dots indicating the presence of a configured log event on those menu options. Let’s now view the information we get when we click on one of them:  . On the image above, you can see the pulsing dots indicating the presence of a configured log event on those menu options. Let’s now view the information we get when we click on one of them: .  By clicking on one of the pulsing dots, you view the specific data logged by that particular event. Moreover, the log-viewer will also render a list with all the events that have been triggered so far on that page. This list will show new events as they continue to occur on that specific instance. That provides us with a real-time view of all events as they are triggered. Here’s how it looks when we react to a message:  . By clicking on one of the pulsing dots, you view the specific data logged by that particular event. Moreover, the log-viewer will also render a list with all the events that have been triggered so far on that page. This list will show new events as they continue to occur on that specific instance. That provides us with a real-time view of all events as they are triggered. Here’s how it looks when we react to a message: .  As we interact with these components, the library tracks all triggered events and shows them in the UI, making it explicit which log events are being tracked.  . As we interact with these components, the library tracks all triggered events and shows them in the UI, making it explicit which log events are being tracked. .  Creating these visuals was actually not that complicated. The library knows the exact location of each configured log event, so it is able to render an additional log-viewer component (pulsing dot) on each of these components to indicate the specific log event data. Moreover, the library is able to display the live event list by keeping track of every log event that has been triggered. It does this by storing these events in a single array shared among all components. This is possible thanks to the same    React Context API    instance (e.g.    LogContext   ) that we used to share library data props as we describe in the previous    blog post   .  . Creating these visuals was actually not that complicated. The library knows the exact location of each configured log event, so it is able to render an additional log-viewer component (pulsing dot) on each of these components to indicate the specific log event data. Moreover, the library is able to display the live event list by keeping track of every log event that has been triggered. It does this by storing these events in a single array shared among all components. This is possible thanks to the same  . React Context API .  instance (e.g.  . LogContext . ) that we used to share library data props as we describe in the previous .  blog post . . .  This library has been a tremendous help in the way we log user events. We are now able to track detailed and high-quality data with minimal effort. Let’s review some of these benefits in more detail:  . This library has been a tremendous help in the way we log user events. We are now able to track detailed and high-quality data with minimal effort. Let’s review some of these benefits in more detail: .  Because the library abstracts a lot of the complexity involved in logging user events, developers are only required to provide some minimal configuration to set up logs for a given component.  . Because the library abstracts a lot of the complexity involved in logging user events, developers are only required to provide some minimal configuration to set up logs for a given component. .  Thanks to this ease-of-use, we’ve seen a   66% decrease in the amount of lines of code   required to implement these logs. Similarly, we have seen a   75% decrease in the amount of development hours   spent setting up these logs.    These are huge savings!   . Thanks to this ease-of-use, we’ve seen a  .  required to implement these logs. Similarly, we have seen a  .  spent setting up these logs.  . These are huge savings! .  Because the library programmatically generates and validates the event data it generates, it guarantees that we collect and persist consistent and more accurate data. This should allow for easier and more detailed data analysis.  . Because the library programmatically generates and validates the event data it generates, it guarantees that we collect and persist consistent and more accurate data. This should allow for easier and more detailed data analysis. .    .  The built-in log-viewer allows almost anyone to quickly access the log events that are configured in our components. This quick and reliable access to log events should facilitate more interactions between teams that want to check exactly what data is getting logged.  . The built-in log-viewer allows almost anyone to quickly access the log events that are configured in our components. This quick and reliable access to log events should facilitate more interactions between teams that want to check exactly what data is getting logged. .  We hope you enjoyed this two part series about how we built a   React Analytics Logging Library  . We will continue to improve this library to cover more use cases around logging user events at Slack. And we’ll make sure that we share more details and lessons as we invest more in this area. If this is a problem that you are interested in helping us solve, please    join us!   . We hope you enjoyed this two part series about how we built a  . . We will continue to improve this library to cover more use cases around logging user events at Slack. And we’ll make sure that we share more details and lessons as we invest more in this area. If this is a problem that you are interested in helping us solve, please  . join us! ", "date": "2020-12-16"}, {"website": "Slack", "title": "A Day in the Life of a Backend Platform Engineer at Slack Pune", "author": ["Kalpak Pingle"], "link": "https://slack.engineering/a-day-in-the-life-of-a-backend-platform-engineer-at-slack-pune/", "abstract": "  First alarm rings. *Snooze.*  . First alarm rings. *Snooze.* .  The final alarm rings and I know this is the last one, so I get up and immediately start making my bed. I open the curtains to take a look at the weather and the early morning light. I walk to my balcony and stand there to breathe some fresh air. This awakens me and makes sure I do not doze off again.      . The final alarm rings and I know this is the last one, so I get up and immediately start making my bed. I open the curtains to take a look at the weather and the early morning light. I walk to my balcony and stand there to breathe some fresh air. This awakens me and makes sure I do not doze off again.  .   .  I am very obsessed with having a neat and clean bed.  It gives me satisfaction and starts a day with positivity and the sense that I can complete the tasks lined up for the day.   . I am very obsessed with having a neat and clean bed.  It gives me satisfaction and starts a day with positivity and the sense that I can complete the tasks lined up for the day.  .  Now is the time for some yoga and Surya Namaskar. This is a habit I have picked up from my parents (my mom mostly). Then I have a shower and get dressed. 45 minutes of yoga is so refreshing, and makes you feel light and happy.  . Now is the time for some yoga and Surya Namaskar. This is a habit I have picked up from my parents (my mom mostly). Then I have a shower and get dressed. 45 minutes of yoga is so refreshing, and makes you feel light and happy. .  Once I’m done, I’m ready for my kids. I have 2 kids — my son is 4 years old and my daughter is just 7 months. I help my son with his morning routine and prepare a quick breakfast for us.  I put on some instrumental songs to lighten up the mood. Lastly, I set up everything for my son so he is all ready for his online school. The pandemic has affected everyone, but he has now trained himself to navigate through his online classes.   . Once I’m done, I’m ready for my kids. I have 2 kids — my son is 4 years old and my daughter is just 7 months. I help my son with his morning routine and prepare a quick breakfast for us.  I put on some instrumental songs to lighten up the mood. Lastly, I set up everything for my son so he is all ready for his online school. The pandemic has affected everyone, but he has now trained himself to navigate through his online classes.  .  The past few months were both challenging and exciting for our family.  The COVID-19 pandemic has impacted us all. Suddenly we were working from home. My wife went into labor during lockdown.  We rushed to the hospital and out came our little princess. Slack supported me throughout this phase like a lifelong friend. They announced “Friyay” (one Friday off a month) to re-energize and focus. I have 12 weeks of paternity leave spread across a year from the birth of our daughter. The company and my manager have allowed flexible timing for work when needed. Our team has adjusted meetings accordingly to maximize both productivity and rest.    . The past few months were both challenging and exciting for our family.  The COVID-19 pandemic has impacted us all. Suddenly we were working from home. My wife went into labor during lockdown.  We rushed to the hospital and out came our little princess. Slack supported me throughout this phase like a lifelong friend. They announced “Friyay” (one Friday off a month) to re-energize and focus. I have 12 weeks of paternity leave spread across a year from the birth of our daughter. The company and my manager have allowed flexible timing for work when needed. Our team has adjusted meetings accordingly to maximize both productivity and rest.   .  I’m finally in my home office with my cup of tea. The first thing I do is check my notes from last evening and my calendar to see how my day is planned. Then I start going through my unread Slack messages. They are a combination of team activities, tasks, jokes, announcements, and some fun from around the globe. I generally use this morning time with the San Francisco team to gather for tea, drinks, or    code kata   . Often we just come together and talk about any topic under the sun and just connect with each other. There are many famous code kata problems on the internet; I usually practice one of them but my favorite is    Romans to decimal    and vice versa. I spice it up by challenging myself either by having a time limit or following TDD.  . I’m finally in my home office with my cup of tea. The first thing I do is check my notes from last evening and my calendar to see how my day is planned. Then I start going through my unread Slack messages. They are a combination of team activities, tasks, jokes, announcements, and some fun from around the globe. I generally use this morning time with the San Francisco team to gather for tea, drinks, or  . code kata . . Often we just come together and talk about any topic under the sun and just connect with each other. There are many famous code kata problems on the internet; I usually practice one of them but my favorite is  . Romans to decimal .  and vice versa. I spice it up by challenging myself either by having a time limit or following TDD. .  I start with any pending code reviews. This is a good time to look at some code — at the start of the day when you are fresh. I also take a look at any review comments pushed for my code.   . I start with any pending code reviews. This is a good time to look at some code — at the start of the day when you are fresh. I also take a look at any review comments pushed for my code.  .  Everyone on our team encourages each other to spend time on code review. This not only improves code quality but also acts as a great knowledge sharing session. I personally love the constructive and (innovative 😅 ) review comments from developers here at Slack.      example:          Let’s replace this with the more standard     &lt;&lt;library_function&gt;&gt;     for uniformity.        This change looks good, “May the force be with you” Let’s launch 🚀   . Everyone on our team encourages each other to spend time on code review. This not only improves code quality but also acts as a great knowledge sharing session. I personally love the constructive and (innovative 😅 ) review comments from developers here at Slack. .    . example:   .    . Let’s replace this with the more standard  . &lt;&lt;library_function&gt;&gt;  . for uniformity. .    . This change looks good, “May the force be with you” Let’s launch 🚀 .  We also use this time for celebration(s) of our recent releases where all team members from Pune and SF join.   . We also use this time for celebration(s) of our recent releases where all team members from Pune and SF join.  .  Slack has introduced the concept of Maker Time, where we have an uninterrupted three hours, three days a week to focus. Meetings are not allowed to be scheduled during this time across our product, design, and engineering organizations. This has been the best time for me to be productive and organize my thoughts.   . Slack has introduced the concept of Maker Time, where we have an uninterrupted three hours, three days a week to focus. Meetings are not allowed to be scheduled during this time across our product, design, and engineering organizations. This has been the best time for me to be productive and organize my thoughts.  .  I’ve been working on a couple of things. One is Org-Apps, enabling scalable distribution of Slack apps across Enterprise Workspaces. The second is refactoring a huge piece of code related to OAuth flows during Slack app installation. My first priority has been Org-Apps, which we just launched to everyone. This is an important feature large enterprise customers have been waiting for and they have loved the beta version.   . I’ve been working on a couple of things. One is Org-Apps, enabling scalable distribution of Slack apps across Enterprise Workspaces. The second is refactoring a huge piece of code related to OAuth flows during Slack app installation. My first priority has been Org-Apps, which we just launched to everyone. This is an important feature large enterprise customers have been waiting for and they have loved the beta version.  .  I find a task on my plate where we want to fix the user experience when a Slack Enterprise Workspace Administrator is granting or revoking Slack app access to teams in their org. We want to show which teams are eligible to grant or revoke apps, while adding the ability to select all or paginate. I see there is an issue with the way the backend API is set up to pull indexed results. I remember pairing with my SF colleagues on this piece of code in the past and I start to propose a solution for this.   . I find a task on my plate where we want to fix the user experience when a Slack Enterprise Workspace Administrator is granting or revoking Slack app access to teams in their org. We want to show which teams are eligible to grant or revoke apps, while adding the ability to select all or paginate. I see there is an issue with the way the backend API is set up to pull indexed results. I remember pairing with my SF colleagues on this piece of code in the past and I start to propose a solution for this.  .  My elder kid has informed me that his online lesson is over and he is ready for games or another activity.  . My elder kid has informed me that his online lesson is over and he is ready for games or another activity. .  Time to grab lunch! This is the time I play with my toddler and my elder kid walks me through his drawings or other activities. During the pandemic while working from home I have taken longer lunches. With my wife, kids, and parents all at home we divide tasks amongst ourselves and take turns to take care of the toddler and also to have lunch, do the dishes, and clean up.   . Time to grab lunch! This is the time I play with my toddler and my elder kid walks me through his drawings or other activities. During the pandemic while working from home I have taken longer lunches. With my wife, kids, and parents all at home we divide tasks amongst ourselves and take turns to take care of the toddler and also to have lunch, do the dishes, and clean up.  .  Yes! I’m back at my desk. I open up VSCode to start investigating a piece of code which is breaking granting and revoking app access to teams. I ask my teammates Dinesh and Neha to pair with me on this, and they come up with different edge cases which would appear if we added pagination. We follow the same path and it looks like we find the issue: A change in the backend API is returning an extra count. I apply the fix and update test cases for the code path. This fix corrects the pagination cursors as well as resolving the user experience issue. I quickly raise a PR for the issue.   . Yes! I’m back at my desk. I open up VSCode to start investigating a piece of code which is breaking granting and revoking app access to teams. I ask my teammates Dinesh and Neha to pair with me on this, and they come up with different edge cases which would appear if we added pagination. We follow the same path and it looks like we find the issue: A change in the backend API is returning an extra count. I apply the fix and update test cases for the code path. This fix corrects the pagination cursors as well as resolving the user experience issue. I quickly raise a PR for the issue.  .  It is Wednesday which means Chai Time.  This is the most engaging time for our Pune office. We all join a video call and discuss random happenings and fun facts. It helps us connect with each other here with other Pune team members across different departments. We used to have a physical chai time when we were all in the office and it has been great to continue the same via video.    . It is Wednesday which means Chai Time.  This is the most engaging time for our Pune office. We all join a video call and discuss random happenings and fun facts. It helps us connect with each other here with other Pune team members across different departments. We used to have a physical chai time when we were all in the office and it has been great to continue the same via video.   .  I sync up with my team member Dinesh and discuss planning, designing, and breaking up stories regarding refactoring the OAuth flow. This is a very important flow and we want to ensure a correct implementation as it is the starting point of app installation with Slack. We already have a good understanding of the problems, but want to break down the work. This takes about an hour and a half.   . I sync up with my team member Dinesh and discuss planning, designing, and breaking up stories regarding refactoring the OAuth flow. This is a very important flow and we want to ensure a correct implementation as it is the starting point of app installation with Slack. We already have a good understanding of the problems, but want to break down the work. This takes about an hour and a half.  .  I go through my notes and add anything which I need to remind myself for the next day. I post my daily standup details in our team Slack channel. I then cut myself off from the office and spend time with my family, play with the kids, and have a fun evening followed by dinner.   . I go through my notes and add anything which I need to remind myself for the next day. I post my daily standup details in our team Slack channel. I then cut myself off from the office and spend time with my family, play with the kids, and have a fun evening followed by dinner.  .  We have a sync up with our entire engineering team.  We make sure not to overshoot the time to be mindful of each other’s personal time throughout the globe. This is the time of the day we sync between Pune and the SF team. We will typically use this time at the beginning of the week to plan or discuss project status. Once a month we will hold a retrospective of how things are working or how we need to change. Most Wednesdays are reserved for specific technical discussions.   . We have a sync up with our entire engineering team.  We make sure not to overshoot the time to be mindful of each other’s personal time throughout the globe. This is the time of the day we sync between Pune and the SF team. We will typically use this time at the beginning of the week to plan or discuss project status. Once a month we will hold a retrospective of how things are working or how we need to change. Most Wednesdays are reserved for specific technical discussions.  .  It is story time for the kids and I put them to sleep (if they are not already   ). I have a list of good books to read or just have random discussions with family about anything we learned during the day. This helps me relax and prepare for a sound sleep.  Then when I see my bed all ready (which I made up first thing this morning) it reminds me of my accomplishments and, with a happy mind, I can rest.  . It is story time for the kids and I put them to sleep (if they are not already  . ). I have a list of good books to read or just have random discussions with family about anything we learned during the day. This helps me relax and prepare for a sound sleep.  Then when I see my bed all ready (which I made up first thing this morning) it reminds me of my accomplishments and, with a happy mind, I can rest. .  Kalpak is a Staff Engineer at Slack. When Kalpak joined Slack, he worked on features in Email Bridge. More recently, he joined the Platform Admin team and works on the backend to build and support admin APIs to make life for Slack Enterprise App administrators easier.   . Kalpak is a Staff Engineer at Slack. When Kalpak joined Slack, he worked on features in Email Bridge. More recently, he joined the Platform Admin team and works on the backend to build and support admin APIs to make life for Slack Enterprise App administrators easier.  . In our “Day in the Life” series, we hear from Slack Engineers in different offices and specializations to learn more about what a typical day looks like. We previously heard from a  Frontend Foundations engineer , a  Mobile Product engineer , a  Backend Product engineer , a  Frontend Product engineer , and a  Backend Foundation engineer . ", "date": "2021-01-13"}, {"website": "Slack", "title": "Women in Security at Slack", "author": ["Suzanna Khatchatrian", "Nikki Brandt", "Lauren Rubin", "Vivienne Pustell", "Carly Robinson"], "link": "https://slack.engineering/women-in-security-at-slack/", "abstract": "  Since its inception, Slack has fostered a culture of inclusion and diversity. The Security organization at Slack is a prime example of how women can thrive in the security space, transitioning to security from different backgrounds and expertises. With Slack’s strong commitment to diversity, it should not be a surprise that nearly a third of the security employees at Slack identify as women and more than third of Security leadership consists of women. We are excited to share with you some of the stories of the women of Slack’s Security team.  . Since its inception, Slack has fostered a culture of inclusion and diversity. The Security organization at Slack is a prime example of how women can thrive in the security space, transitioning to security from different backgrounds and expertises. With Slack’s strong commitment to diversity, it should not be a surprise that nearly a third of the security employees at Slack identify as women and more than third of Security leadership consists of women. We are excited to share with you some of the stories of the women of Slack’s Security team. . Senior Engineering Manager, Product Security Foundations .  Our customers depend on us to keep their data safe — the measure of our success will be the value we create for them when work systems aren’t getting hacked, and when workers aren’t worrying about the confidentiality of their data.    Our mission, in Slack’s Product Security team, is to make people’s working lives more secure.   . Our customers depend on us to keep their data safe — the measure of our success will be the value we create for them when work systems aren’t getting hacked, and when workers aren’t worrying about the confidentiality of their data.  .  Slack’s Product Security team works with development teams across Slack to enable adoption of best practices for building secure products. The team pursues this goal by participating in the product development lifecycle, building components to solve harder security problems, deploying tools to detect coding and configuration failures, and creating and delivering customized developer training.    . Slack’s Product Security team works with development teams across Slack to enable adoption of best practices for building secure products. The team pursues this goal by participating in the product development lifecycle, building components to solve harder security problems, deploying tools to detect coding and configuration failures, and creating and delivering customized developer training.   .  Two years ago the Product Security team was split into two subteams:  . Two years ago the Product Security team was split into two subteams: . ProdSec Classic (PSC): This team is focused on traditional application security aspects to ensure Slack developers ship secure products. PSC engineers perform security reviews of new features, run static and dynamic scanning, coordinate penetration tests, manage the bug bounty program, and provide testing frameworks to improve vulnerability detection and the secure development lifecycle. . ProdSec Foundations (PSF): As the founding manager of this team, I was tasked with  building a new and innovative team with backend engineering capabilities  . and .   security expertise. The team’s mission is to “future proof” Slack against classes of vulnerabilities by building secure-by-default services, libraries, and tools.  .  Here are some examples of work done by the Product Security Foundations team in the past couple of years. This work has had a big impact in hardening our product, reducing the volume of security incidents, and fostering better engineering practices.  . Here are some examples of work done by the Product Security Foundations team in the past couple of years. This work has had a big impact in hardening our product, reducing the volume of security incidents, and fostering better engineering practices. .  Image Processing Service –   A new service that insulates production data and infrastructure from vulnerabilities in image processing libraries, and improves overall image upload performance, reliability, and security.   . A new service that insulates production data and infrastructure from vulnerabilities in image processing libraries, and improves overall image upload performance, reliability, and security.  .  Crypto Library –   Cryptography is really hard to get right, and subtle errors can have big consequences.   PHP doesn’t help — it  exposes low-level interfaces that require the caller to specify confusing security parameters and is extremely permissive about input and output formats.   lib_crypto   is designed to be    misuse resistant   ,   with strictly typed input and output objects, that can be used for all of Slack’s backend cryptographic needs.  . Cryptography is really hard to get right, and subtle errors can have big consequences. . PHP doesn’t help — it  exposes low-level interfaces that require the caller to specify confusing security parameters and is extremely permissive about input and output formats.  . is designed to be  . misuse resistant . ,  . with strictly typed input and output objects, that can be used for all of Slack’s backend cryptographic needs. .  HTML Sanitizer   – This library protects against cross-site scripting attacks by sanitizing HTML for unfurled links posted in Slack. Our library is open source, available on    the public SlackHQ repo    since August 7th, so anyone who uses Hacklang can also have an HTML sanitizer and contribute!  .  – This library protects against cross-site scripting attacks by sanitizing HTML for unfurled links posted in Slack. Our library is open source, available on . the public SlackHQ repo . since August 7th, so anyone who uses Hacklang can also have an HTML sanitizer and contribute! .  Log Canary   – This    tool    automatically detects and alerts accidental logging of tokens and other sensitive data (e.g. message data, channel names, or file names).  .  – This  . tool .  automatically detects and alerts accidental logging of tokens and other sensitive data (e.g. message data, channel names, or file names). .  The team is currently working to solve other complex problems including authentication hardening and simplifications, malware detection and prevention, and kubernetes hardening.    . The team is currently working to solve other complex problems including authentication hardening and simplifications, malware detection and prevention, and kubernetes hardening.   . Engineering Manager, Product Security Classic .  Originally I found myself on Slack’s doorstep because of an opening for a manager in Product Security. Prior to Slack, I had been working as a consultant, doing technical work while also managing other consultants (and some interns!). This gave me a taste of people management and a hint that I might enjoy doing it full-time, but then-CSO Geoff Belknap told me directly after I interviewed: I didn’t have the experience they were looking for in a manager for Product Security, but Slack would love to have me join the team as an engineer.  . Originally I found myself on Slack’s doorstep because of an opening for a manager in Product Security. Prior to Slack, I had been working as a consultant, doing technical work while also managing other consultants (and some interns!). This gave me a taste of people management and a hint that I might enjoy doing it full-time, but then-CSO Geoff Belknap told me directly after I interviewed: I didn’t have the experience they were looking for in a manager for Product Security, but Slack would love to have me join the team as an engineer. .  As a consultant, I was going in between companies every few weeks, many in the tech space. Think of a tech company in the Bay Area: I’ve probably consulted for them! I had seen so many engineering organizations and cultures in my work that when I interviewed with Slack, I knew it was a special place. It wasn’t just my interviewers saying that what they loved most at Slack was “the people” or the diversity of my hiring panel. I was really drawn to how transparent the organization was, and how little siloing there was between different parts of the engineering organization. I now know that this transparency, openness, and culture of information sharing is driven by the very product we make. But back then, I was just like: Whoa! This organization really understands collaboration! Where do I sign?!  . As a consultant, I was going in between companies every few weeks, many in the tech space. Think of a tech company in the Bay Area: I’ve probably consulted for them! I had seen so many engineering organizations and cultures in my work that when I interviewed with Slack, I knew it was a special place. It wasn’t just my interviewers saying that what they loved most at Slack was “the people” or the diversity of my hiring panel. I was really drawn to how transparent the organization was, and how little siloing there was between different parts of the engineering organization. I now know that this transparency, openness, and culture of information sharing is driven by the very product we make. But back then, I was just like: Whoa! This organization really understands collaboration! Where do I sign?! .  And so I joined Slack as an engineer on the Product Security team and dove into all the technical work that was offered to me. I loved the breadth of the work and the degree of responsibility I was given right away. In less than a year, I took ownership of our security review process, among other things. Then the unexpected happened: I was given the opportunity to move into an incredible role taking on people management responsibilities while still being able to contribute technically. I jumped at it!  . And so I joined Slack as an engineer on the Product Security team and dove into all the technical work that was offered to me. I loved the breadth of the work and the degree of responsibility I was given right away. In less than a year, I took ownership of our security review process, among other things. Then the unexpected happened: I was given the opportunity to move into an incredible role taking on people management responsibilities while still being able to contribute technically. I jumped at it! .  In this role, I was able to continue to utilize my technical background while also growing and exploring people management at Slack. I worked on technical efforts like standardizing our security review process and securing Slack’s App Directory, while also growing my team from 3 to 6 engineers and helping several of my people through difficult phases of their career development. After about 18 months in this position, I transitioned into full-time management at the beginning of 2020.  . In this role, I was able to continue to utilize my technical background while also growing and exploring people management at Slack. I worked on technical efforts like standardizing our security review process and securing Slack’s App Directory, while also growing my team from 3 to 6 engineers and helping several of my people through difficult phases of their career development. After about 18 months in this position, I transitioned into full-time management at the beginning of 2020. .  In many companies, I would not have gotten the chance to move from engineering into leadership. At Slack, I not only had the support of my leadership (including the inestimable Larkin Ryder) and peers in security management (including Suzanna Khatchatrian, who is also in this blog – hi Suzanna!), I also saw women like me successfully make the transition from being amazing engineers into being amazing engineering managers at Slack, again and again.  I could fill a whole page with the names of inspirational female leaders at Slack, but I don’t need to — just look back through our engineering blog and you’ll find them.  :)  . In many companies, I would not have gotten the chance to move from engineering into leadership. At Slack, I not only had the support of my leadership (including the inestimable Larkin Ryder) and peers in security management (including Suzanna Khatchatrian, who is also in this blog – hi Suzanna!), I also saw women like me successfully make the transition from being amazing engineers into being amazing engineering managers at Slack, again and again.  I could fill a whole page with the names of inspirational female leaders at Slack, but I don’t need to — just look back through our engineering blog and you’ll find them.  :) . Senior Technical Program Manager .  In 2016, I joined Slack as a Senior Technical Recruiter hiring across various Engineering teams, one of which was Security. Fast forward 18 months and I was Slack’s first Technical Program Manager for Security.  . In 2016, I joined Slack as a Senior Technical Recruiter hiring across various Engineering teams, one of which was Security. Fast forward 18 months and I was Slack’s first Technical Program Manager for Security. .  Wait, what?  . Wait, what? .  I know… sometimes I still can’t believe it either. There I was, promoting internal mobility to prospective new hires to join Slack, and then Slack put that exact opportunity to make an incredible career change right in front of me. Slack and its Security leadership team are, without a doubt, the driving forces that made my move a possibility. In a typically misogynistic industry, Slack’s Security team has fantastic women leaders and individual contributors who welcomed me with open arms and reassured me that I would be successful in my new role. When I was pregnant with my first daughter, Slack encouraged and allowed me to step away from my day-to-day job in order to participate in a security bootcamp course to expand my domain knowledge to better support the team. Throughout my tenure as a Senior Technical Program Manager, I’ve had the privilege of helping define and implement programs that support Slack’s broader business strategy and operational excellence including incident management streamlining, technology hygiene and end of life technologies, QBR vendor reviews, Hacktober security awareness, bug bounty, vulnerability patching, JIRA optimization, security tooling proof of concepts, and other security engineering objectives.  . I know… sometimes I still can’t believe it either. There I was, promoting internal mobility to prospective new hires to join Slack, and then Slack put that exact opportunity to make an incredible career change right in front of me. Slack and its Security leadership team are, without a doubt, the driving forces that made my move a possibility. In a typically misogynistic industry, Slack’s Security team has fantastic women leaders and individual contributors who welcomed me with open arms and reassured me that I would be successful in my new role. When I was pregnant with my first daughter, Slack encouraged and allowed me to step away from my day-to-day job in order to participate in a security bootcamp course to expand my domain knowledge to better support the team. Throughout my tenure as a Senior Technical Program Manager, I’ve had the privilege of helping define and implement programs that support Slack’s broader business strategy and operational excellence including incident management streamlining, technology hygiene and end of life technologies, QBR vendor reviews, Hacktober security awareness, bug bounty, vulnerability patching, JIRA optimization, security tooling proof of concepts, and other security engineering objectives. .  I am honored and proud to be a member of Slack’s Security team working alongside wonderful and talented teammates.  . I am honored and proud to be a member of Slack’s Security team working alongside wonderful and talented teammates. . Senior Risk &amp; Compliance Engineer .  As a former teacher, one of the things I love most in life is learning! In my time at Slack, I have not only gotten the *opportunity* to learn, I’ve been provided support and encouragement and guidance to learn more, faster, and better.  . As a former teacher, one of the things I love most in life is learning! In my time at Slack, I have not only gotten the *opportunity* to learn, I’ve been provided support and encouragement and guidance to learn more, faster, and better. .  Internally, I’ve had the opportunity to take classes that aren’t necessarily directly related to my day-to-day work, but my manager has supported me in using my time to learn.  . Internally, I’ve had the opportunity to take classes that aren’t necessarily directly related to my day-to-day work, but my manager has supported me in using my time to learn. .  As I moved deeper into developing technical skills, I was able to pursue more in-depth learning outside of Slack. As my knowledge grew, I found myself working with more varied teams across the company, and opportunities for informal learning and skill development through collaboration began pouring in.  . As I moved deeper into developing technical skills, I was able to pursue more in-depth learning outside of Slack. As my knowledge grew, I found myself working with more varied teams across the company, and opportunities for informal learning and skill development through collaboration began pouring in. .  My manager made sure that I had the chance to use what I had learned on the job whenever possible. I got to start providing more technical leadership in the team, and took over new responsibilities. Finally, in 2019, I was able to enroll in a part-time software engineering bootcamp. My whole team had my back through my entire bootcamp journey, and I got to start using bootcamp skills at work long before I finished the program. Other engineers across the company were encouraging and supportive as well, and I know I have a great network I can reach out to whenever I hit a wall.  . My manager made sure that I had the chance to use what I had learned on the job whenever possible. I got to start providing more technical leadership in the team, and took over new responsibilities. Finally, in 2019, I was able to enroll in a part-time software engineering bootcamp. My whole team had my back through my entire bootcamp journey, and I got to start using bootcamp skills at work long before I finished the program. Other engineers across the company were encouraging and supportive as well, and I know I have a great network I can reach out to whenever I hit a wall. .  For me, the most important thing to have in a career is the chance to grow, and with all the opportunities I have at Slack to learn in the classroom, on the job, and through outside trainings, along with the support of my manager and my colleagues, I know that I’ll never be stuck stagnating.   . For me, the most important thing to have in a career is the chance to grow, and with all the opportunities I have at Slack to learn in the classroom, on the job, and through outside trainings, along with the support of my manager and my colleagues, I know that I’ll never be stuck stagnating.  . Senior Security Software Engineer .  Working at Slack is a distinctive and positive experience. In my five years at the company, I’ve found that the most unique thing about it is the ease with which we’re able to work on, and transfer between, multiple teams. This allows us to develop holistic skillsets, which, in my view, makes us better engineers.  . Working at Slack is a distinctive and positive experience. In my five years at the company, I’ve found that the most unique thing about it is the ease with which we’re able to work on, and transfer between, multiple teams. This allows us to develop holistic skillsets, which, in my view, makes us better engineers. .  When I graduated from Hackbright in 2015, I was fascinated by hacking and cybersecurity, but I felt it was important to be in a role that saw me writing code. In other words, I wanted to learn how to build things before I learned how to break them.  . When I graduated from Hackbright in 2015, I was fascinated by hacking and cybersecurity, but I felt it was important to be in a role that saw me writing code. In other words, I wanted to learn how to build things before I learned how to break them. .  So that’s where I started, as a backend application engineer, where I would spend four amazing years. I grew from an associate to a senior engineer and worked with a number of teams, across the Enterprise, Platform, Operations, and Infrastructure engineering pillars. I wrote code in PHP/Hack, Javascript/Typescript, and Go, designed backend billing systems and public-facing admin APIs, wrestled with Terraform and bash scripts to provision AWS infrastructure, and helped configure Thanos for scaling Prometheus metrics across a fleet of several thousand servers. In a nutshell, I built a lot.  . So that’s where I started, as a backend application engineer, where I would spend four amazing years. I grew from an associate to a senior engineer and worked with a number of teams, across the Enterprise, Platform, Operations, and Infrastructure engineering pillars. I wrote code in PHP/Hack, Javascript/Typescript, and Go, designed backend billing systems and public-facing admin APIs, wrestled with Terraform and bash scripts to provision AWS infrastructure, and helped configure Thanos for scaling Prometheus metrics across a fleet of several thousand servers. In a nutshell, I built a lot. .  Then, in October of last year, a coworker forwarded me a link to a job posting on the Product Security Foundations team — a new, experimental group, whose mission is to build secure-by-default internal tools to support secure development across the engineering organization. A job where I could build things, while also learning how to break them like the hackers do? My heart skipped a beat. I was drawn to PSF’s challenging, high-impact projects, its technical leadership opportunities, and its completely female management team. “Sign me up,” I said.  . Then, in October of last year, a coworker forwarded me a link to a job posting on the Product Security Foundations team — a new, experimental group, whose mission is to build secure-by-default internal tools to support secure development across the engineering organization. A job where I could build things, while also learning how to break them like the hackers do? My heart skipped a beat. I was drawn to PSF’s challenging, high-impact projects, its technical leadership opportunities, and its completely female management team. “Sign me up,” I said. .  I’ve worked in security now for ten months. Everyone has been so humble, down-to-earth and eager to share their knowledge. Many of us have taken an unconventional journey into security, and that diversity of experience is one of our greatest assets. In our time together, we’ve refactored a tool for monitoring Slack’s AWS infrastructure operations, launched an education program for software developers, developed testing for Slack Authentication systems and led an initiative to drive adoption of our secure-by-default libraries. Our work touches every part of the organization and our impact is undeniable.  . I’ve worked in security now for ten months. Everyone has been so humble, down-to-earth and eager to share their knowledge. Many of us have taken an unconventional journey into security, and that diversity of experience is one of our greatest assets. In our time together, we’ve refactored a tool for monitoring Slack’s AWS infrastructure operations, launched an education program for software developers, developed testing for Slack Authentication systems and led an initiative to drive adoption of our secure-by-default libraries. Our work touches every part of the organization and our impact is undeniable. .  I am grateful to Slack for continuing to nurture my curiosity and encourage my eclectic skillset across numerous teams.  . I am grateful to Slack for continuing to nurture my curiosity and encourage my eclectic skillset across numerous teams. .  We hope these personal stories were insightful and inspirational. If you are interested in joining Slack’s security team, we are hiring! We have multiple opportunities across Product Security, Security Operations, and Risk and Compliance. We’d love to hear your story, so please check out  slack.com/careers  and let’s chat!  . We hope these personal stories were insightful and inspirational. If you are interested in joining Slack’s security team, we are hiring! We have multiple opportunities across Product Security, Security Operations, and Risk and Compliance. We’d love to hear your story, so please check out  slack.com/careers  and let’s chat! ", "date": "2021-01-20"}, {"website": "Slack", "title": "Slack’s Outage on January 4th 2021", "author": ["Laura Nolan"], "link": "https://slack.engineering/slacks-outage-on-january-4th-2021/", "abstract": "   And now we welcome the new year. Full of things that have never been.   . And now we welcome the new year. Full of things that have never been. .    — Rainer Maria Rilke   .  — Rainer Maria Rilke .  January 4th 2021 was the first working day of the year for many around the globe, and for most of us at Slack too (except of course for our on-callers and our customer experience team, who never sleep). The day in APAC and the morning in EMEA went by quietly. During the Americas’ morning we got paged by an external monitoring service: Error rates were creeping up. We began to investigate. As initial triage showed the errors getting worse, we started our incident process (see Ryan Katkov’s article    All Hands on Deck    for more about how we manage incidents).  . January 4th 2021 was the first working day of the year for many around the globe, and for most of us at Slack too (except of course for our on-callers and our customer experience team, who never sleep). The day in APAC and the morning in EMEA went by quietly. During the Americas’ morning we got paged by an external monitoring service: Error rates were creeping up. We began to investigate. As initial triage showed the errors getting worse, we started our incident process (see Ryan Katkov’s article  . All Hands on Deck .  for more about how we manage incidents). .  As if this was not already an inauspicious start to the New Year, while we were in the early stages of investigating, our dashboarding and alerting service became unavailable. We immediately paged in our monitoring team to try and get our dashboard and alerting service back up.   . As if this was not already an inauspicious start to the New Year, while we were in the early stages of investigating, our dashboarding and alerting service became unavailable. We immediately paged in our monitoring team to try and get our dashboard and alerting service back up.  .  To narrow down the list of possible causes we quickly rolled back some changes that had been pushed out that day (turned out they weren’t the issue). We pulled in several more people from our infrastructure teams because all debugging and investigation was now hampered by the lack of our usual dashboards and alerts. We still had various internal consoles and status pages available, some command line tools, and our logging infrastructure. Our metrics backends were still up, meaning that we were able to query them directly — however this is nowhere near as efficient as using our dashboards with their pre-built queries. While our infrastructure seemed to generally be up and running, we observed signs that we were seeing widespread network degradation, which we escalated to AWS, our main cloud provider. At this point Slack itself was still up — at 6.57am PST 99% of Slack messages were being sent successfully (but our success rate for message sending is usually over 99.999%, so this was not normal).    . To narrow down the list of possible causes we quickly rolled back some changes that had been pushed out that day (turned out they weren’t the issue). We pulled in several more people from our infrastructure teams because all debugging and investigation was now hampered by the lack of our usual dashboards and alerts. We still had various internal consoles and status pages available, some command line tools, and our logging infrastructure. Our metrics backends were still up, meaning that we were able to query them directly — however this is nowhere near as efficient as using our dashboards with their pre-built queries. While our infrastructure seemed to generally be up and running, we observed signs that we were seeing widespread network degradation, which we escalated to AWS, our main cloud provider. At this point Slack itself was still up — at 6.57am PST 99% of Slack messages were being sent successfully (but our success rate for message sending is usually over 99.999%, so this was not normal).   .  Slack has a traffic pattern of mini-peaks at the top of each hour and half hour, as reminders and other kinds of automation trigger and send messages (much of this is external — cronjobs from all over the world). We manage the scaling of our web tier and backends to accommodate these mini-peaks. However, the mini-peak at 7am PST — combined with the underlying network problems — led to saturation of our web tier. As load increased so did the widespread packet loss. The increased packet loss led to much higher latency for calls from the web tier to its backends, which saturated system resources in our web tier. Slack became unavailable.   . Slack has a traffic pattern of mini-peaks at the top of each hour and half hour, as reminders and other kinds of automation trigger and send messages (much of this is external — cronjobs from all over the world). We manage the scaling of our web tier and backends to accommodate these mini-peaks. However, the mini-peak at 7am PST — combined with the underlying network problems — led to saturation of our web tier. As load increased so did the widespread packet loss. The increased packet loss led to much higher latency for calls from the web tier to its backends, which saturated system resources in our web tier. Slack became unavailable.  .  Around this time two things happened independently. Some of our instances were marked unhealthy by our automation because they couldn’t reach the backends that they depended on. Our systems attempted to replace these unhealthy instances with new instances. Secondly, our autoscaling system downscaled our web tier. Because we were working without our monitoring dashboards, several engineers were logged into production instances investigating problems at this point. Many of the incident responders on our call had their SSH sessions ended abruptly as the instances they were working on were deprovisioned. This made investigating the widespread production issues even more difficult. We disabled the downscaling to facilitate investigation and to preserve our serving capacity.   . Around this time two things happened independently. Some of our instances were marked unhealthy by our automation because they couldn’t reach the backends that they depended on. Our systems attempted to replace these unhealthy instances with new instances. Secondly, our autoscaling system downscaled our web tier. Because we were working without our monitoring dashboards, several engineers were logged into production instances investigating problems at this point. Many of the incident responders on our call had their SSH sessions ended abruptly as the instances they were working on were deprovisioned. This made investigating the widespread production issues even more difficult. We disabled the downscaling to facilitate investigation and to preserve our serving capacity.  .  We scale our web tier based on two signals. One is CPU utilization (which is    almost universally a useful scaling metric   ) and the other is utilization of available Apache worker threads. The network problems prior to 7:00am PST meant that the threads were spending more time waiting, which caused CPU utilization to drop. This drop in CPU utilization initially triggered some automated downscaling. However, this was very quickly followed by significant automated upscaling as a result of increased utilization of threads as network conditions worsened and the web tier waited longer for responses from its backends. We attempted to add 1,200 servers to our web tier between 7:01am PST and 7:15am PST.  . We scale our web tier based on two signals. One is CPU utilization (which is  . almost universally a useful scaling metric . ) and the other is utilization of available Apache worker threads. The network problems prior to 7:00am PST meant that the threads were spending more time waiting, which caused CPU utilization to drop. This drop in CPU utilization initially triggered some automated downscaling. However, this was very quickly followed by significant automated upscaling as a result of increased utilization of threads as network conditions worsened and the web tier waited longer for responses from its backends. We attempted to add 1,200 servers to our web tier between 7:01am PST and 7:15am PST. .  Unfortunately, our scale-up did not work as intended. We run a service, aptly named ‘provision-service’, which    does exactly what it says on the tin   . It is responsible for configuring and testing new instances, and performing various infrastructural housekeeping tasks. Provision-service needs to talk to other internal Slack systems and to some AWS APIs. It was communicating with those dependencies over the same degraded network, and like most of Slack’s systems at the time, it was seeing longer connection and response times, and therefore was using more system resources than usual. The spike of load from the simultaneous provisioning of so many instances under suboptimal network conditions meant that provision-service hit two separate resource bottlenecks (the most significant one was the Linux open files limit, but we also exceeded an AWS quota limit).  . Unfortunately, our scale-up did not work as intended. We run a service, aptly named ‘provision-service’, which  . does exactly what it says on the tin . . It is responsible for configuring and testing new instances, and performing various infrastructural housekeeping tasks. Provision-service needs to talk to other internal Slack systems and to some AWS APIs. It was communicating with those dependencies over the same degraded network, and like most of Slack’s systems at the time, it was seeing longer connection and response times, and therefore was using more system resources than usual. The spike of load from the simultaneous provisioning of so many instances under suboptimal network conditions meant that provision-service hit two separate resource bottlenecks (the most significant one was the Linux open files limit, but we also exceeded an AWS quota limit). .  While we were repairing provision-service, we were still under-capacity for our web tier because the scale-up was not working as expected. We had created a large number of instances, but most of them were not fully provisioned and were not serving. The large number of broken instances caused us to hit our pre-configured autoscaling-group size limits, which determine the maximum number of instances in our web tier. These size limits are multiples of the number of instances that we normally require to serve our peak traffic. Some responders cleared these broken instances up while others continued to investigate the widespread connectivity problems. Throughout all this, we were still without our monitoring dashboards — we couldn’t provision new dashboard service instances because provision-service was overloaded.  . While we were repairing provision-service, we were still under-capacity for our web tier because the scale-up was not working as expected. We had created a large number of instances, but most of them were not fully provisioned and were not serving. The large number of broken instances caused us to hit our pre-configured autoscaling-group size limits, which determine the maximum number of instances in our web tier. These size limits are multiples of the number of instances that we normally require to serve our peak traffic. Some responders cleared these broken instances up while others continued to investigate the widespread connectivity problems. Throughout all this, we were still without our monitoring dashboards — we couldn’t provision new dashboard service instances because provision-service was overloaded. .  Once provision-service was back in operation (around 8:15am PST) we began to see an improvement as healthy instances entered service. We still had some less-critical production issues which were mitigated or being worked on, and we still had increased packet loss in our network. However, by 9:15am PST our web tier had a sufficient number of functioning hosts to serve traffic. Our load balancing tier was still showing an extremely high rate of health check failures to our web application instances due to network problems, but luckily, our load balancers have a feature called ‘   panic mode’    which balances requests across all instances when many are failing health checks. This — plus retries and    circuit breaking    — got us back to serving. Slack was slower than normal and error rates were higher, but by around 9:15am PST Slack was degraded, not down. It took an hour to provision enough instances to reduce our error rates to a low level for two reasons. Firstly, because the network still wasn’t completely healthy, we needed more instances than we normally do to serve traffic. Secondly, it took longer than usual to complete that provisioning process, again because the network was not healthy.  . Once provision-service was back in operation (around 8:15am PST) we began to see an improvement as healthy instances entered service. We still had some less-critical production issues which were mitigated or being worked on, and we still had increased packet loss in our network. However, by 9:15am PST our web tier had a sufficient number of functioning hosts to serve traffic. Our load balancing tier was still showing an extremely high rate of health check failures to our web application instances due to network problems, but luckily, our load balancers have a feature called ‘ . panic mode’ .  which balances requests across all instances when many are failing health checks. This — plus retries and  . circuit breaking .  — got us back to serving. Slack was slower than normal and error rates were higher, but by around 9:15am PST Slack was degraded, not down. It took an hour to provision enough instances to reduce our error rates to a low level for two reasons. Firstly, because the network still wasn’t completely healthy, we needed more instances than we normally do to serve traffic. Secondly, it took longer than usual to complete that provisioning process, again because the network was not healthy. .  By the time Slack had recovered, engineers at AWS had found the trigger for our problems: Part of our AWS networking infrastructure had indeed become saturated and was dropping packets.   . By the time Slack had recovered, engineers at AWS had found the trigger for our problems: Part of our AWS networking infrastructure had indeed become saturated and was dropping packets.  .  A little architectural background is needed here: Slack started life, not so many years ago, with everything running in one AWS account. As we’ve grown in size, complexity, and number of engineers we’ve moved away from that    architecture    towards    running services in separate accounts    and in dedicated    Virtual Private Clouds    (VPCs). This gives us more isolation between different services, and means that we can control operator privileges with much more granularity. We use    AWS Transit Gateways    (TGWs) as hubs to link our VPCs.   . A little architectural background is needed here: Slack started life, not so many years ago, with everything running in one AWS account. As we’ve grown in size, complexity, and number of engineers we’ve moved away from that  . architecture .  towards .  running services in separate accounts .  and in dedicated  . Virtual Private Clouds .  (VPCs). This gives us more isolation between different services, and means that we can control operator privileges with much more granularity. We use  . AWS Transit Gateways .  (TGWs) as hubs to link our VPCs.  .  On January 4th, one of our Transit Gateways became overloaded. The TGWs are managed by AWS and are intended to scale transparently to us. However, Slack’s annual traffic pattern is a little unusual: Traffic is lower over the holidays, as everyone disconnects from work (good job on the work-life balance, Slack users!). On the first Monday back, client caches are cold and clients pull down more data than usual on their first connection to Slack. We go from our quietest time of the whole year to one of our biggest days quite literally overnight.  . On January 4th, one of our Transit Gateways became overloaded. The TGWs are managed by AWS and are intended to scale transparently to us. However, Slack’s annual traffic pattern is a little unusual: Traffic is lower over the holidays, as everyone disconnects from work (good job on the work-life balance, Slack users!). On the first Monday back, client caches are cold and clients pull down more data than usual on their first connection to Slack. We go from our quietest time of the whole year to one of our biggest days quite literally overnight. .  Our own serving systems scale quickly to meet these kinds of peaks in demand (and have always done so successfully after the holidays in previous years). However, our TGWs did not scale fast enough. During the incident, AWS engineers were alerted to our packet drops by their own internal monitoring, and increased our TGW capacity manually. By 10:40am PST that change had rolled out across all Availability Zones and our network returned to normal, as did our error rates and latency.   . Our own serving systems scale quickly to meet these kinds of peaks in demand (and have always done so successfully after the holidays in previous years). However, our TGWs did not scale fast enough. During the incident, AWS engineers were alerted to our packet drops by their own internal monitoring, and increased our TGW capacity manually. By 10:40am PST that change had rolled out across all Availability Zones and our network returned to normal, as did our error rates and latency.  .  AWS assures us that they are reviewing the TGW scaling algorithms for large packet-per-second increases as part of their post-incident process. We’ve also set ourselves a reminder (a    Slack reminder   , of course) to request a preemptive upscaling of our TGWs at the end of the next holiday season.  . AWS assures us that they are reviewing the TGW scaling algorithms for large packet-per-second increases as part of their post-incident process. We’ve also set ourselves a reminder (a  . Slack reminder . , of course) to request a preemptive upscaling of our TGWs at the end of the next holiday season. .     Monitoring is one of our most critical services — it’s how we know whether our user-facing services are healthy or not — and it is one of our most important tools for diagnosis of problems. We make efforts to keep our monitoring tools as independent of Slack’s infrastructure as possible, so that they don’t fail us when we need them the most. In this case, our dashboard and alerting services failed because they were running in a different VPC from their backend databases, creating a dependency on the TGWs. Running the dashboard service instances in the same VPC as their database will remove this dependency.  .   . Monitoring is one of our most critical services — it’s how we know whether our user-facing services are healthy or not — and it is one of our most important tools for diagnosis of problems. We make efforts to keep our monitoring tools as independent of Slack’s infrastructure as possible, so that they don’t fail us when we need them the most. In this case, our dashboard and alerting services failed because they were running in a different VPC from their backend databases, creating a dependency on the TGWs. Running the dashboard service instances in the same VPC as their database will remove this dependency. .  Finally, we’ll make sure to regularly load test provision-service to make sure there aren’t any more scaling problems in store (we’ve load tested it in the past but this event exceeded our previous scale-ups). We will also reevaluate our health-checking and autoscaling configurations to prevent inadvertently overloading provision-service again, even in extreme circumstances such as this significant network disruption.  . Finally, we’ll make sure to regularly load test provision-service to make sure there aren’t any more scaling problems in store (we’ve load tested it in the past but this event exceeded our previous scale-ups). We will also reevaluate our health-checking and autoscaling configurations to prevent inadvertently overloading provision-service again, even in extreme circumstances such as this significant network disruption. .  We deeply regret the disruption in service. Every incident is an opportunity to learn, and an unplanned investment in future reliability. We learned a lot from this incident and, as always, we intend to make the most of this unplanned investment to make our infrastructure better in 2021 and beyond.  . We deeply regret the disruption in service. Every incident is an opportunity to learn, and an unplanned investment in future reliability. We learned a lot from this incident and, as always, we intend to make the most of this unplanned investment to make our infrastructure better in 2021 and beyond. ", "date": "2021-02-01"}, {"website": "Slack", "title": "Shadow Jobs", "author": ["Zac Sweers"], "link": "https://slack.engineering/shadow-jobs/", "abstract": "  We take developer productivity pretty seriously at Slack. It’s multi-faceted, spreading across build speed, reliability, modernity, and more. One thing we really pride ourselves in is using the latest build tools when possible. Not necessarily bleeding edge (alphas, betas, RC, etc.), but the latest stable versions. Aside from the new features and improvements they bring, we found developers are generally happier feeling like they are using the latest and greatest. Life’s too short to not use modern tools.  . We take developer productivity pretty seriously at Slack. It’s multi-faceted, spreading across build speed, reliability, modernity, and more. One thing we really pride ourselves in is using the latest build tools when possible. Not necessarily bleeding edge (alphas, betas, RC, etc.), but the latest stable versions. Aside from the new features and improvements they bring, we found developers are generally happier feeling like they are using the latest and greatest. Life’s too short to not use modern tools. .  Updates come with inherent risk though, especially when some build tools have long release cycles like the Android Gradle Plugin (AGP) or Kotlin. Updating on day one can impose a risk you’re not willing to take, as you’re trusting these tools’ tests to ensure they have no regressions. There’s no test as thorough as running it in your own codebase, but that’s not something library and tool maintainers can do.  . Updates come with inherent risk though, especially when some build tools have long release cycles like the Android Gradle Plugin (AGP) or Kotlin. Updating on day one can impose a risk you’re not willing to take, as you’re trusting these tools’ tests to ensure they have no regressions. There’s no test as thorough as running it in your own codebase, but that’s not something library and tool maintainers can do. .  Or can they?  . Or can they? .  Consider this: most major build tools have some degree of public alpha, beta, RC, or snapshot channel to allow developers to try out new versions and features before they’re stabilized. You wouldn’t check this in to your code base immediately, but you could test this! Many shops do but often wait until late in the development cycle to test something out, if ever.  . Consider this: most major build tools have some degree of public alpha, beta, RC, or snapshot channel to allow developers to try out new versions and features before they’re stabilized. You wouldn’t check this in to your code base immediately, but you could test this! Many shops do but often wait until late in the development cycle to test something out, if ever. .  At Slack, we test a number of core build tools    continuously    to catch regressions early and give feedback to maintainers as soon as possible and to give ourselves peace of mind updating to stable when it’s released. These are run as custom Continuous Integration (CI) jobs on    GitHub Actions    that build our repo against new, pre-release versions of the tools we use. These are called   Shadow Jobs,   because they    shadow    the main branch and just apply a set of changes on top of it to build it against these newer tools. In this post, we’ll cover how to design a simple shadow job and how to integrate it in your build. It’s oriented toward an Android/Kotlin/Gradle codebase, but the concepts should be pretty transferable!  . At Slack, we test a number of core build tools  . continuously .  to catch regressions early and give feedback to maintainers as soon as possible and to give ourselves peace of mind updating to stable when it’s released. These are run as custom Continuous Integration (CI) jobs on  . GitHub Actions .  that build our repo against new, pre-release versions of the tools we use. These are called  . because they  . shadow .  the main branch and just apply a set of changes on top of it to build it against these newer tools. In this post, we’ll cover how to design a simple shadow job and how to integrate it in your build. It’s oriented toward an Android/Kotlin/Gradle codebase, but the concepts should be pretty transferable! .  In a GitHub Actions workflow, this is easy to set up! You don’t have to use Actions, but we’ve found it very amenable to this case. Most modern CI systems should be able to facilitate this pattern.  . In a GitHub Actions workflow, this is easy to set up! You don’t have to use Actions, but we’ve found it very amenable to this case. Most modern CI systems should be able to facilitate this pattern. .  Our workflows are pretty complex, but let’s look at a simple example workflow for testing newer Java versions, AGP versions, and    Kotlin IR   .  . Our workflows are pretty complex, but let’s look at a simple example workflow for testing newer Java versions, AGP versions, and  . Kotlin IR . . .  Let’s break this down section by section.  . Let’s break this down section by section. .  First, we schedule this to run on a nightly basis via the    cron    scheduling event. We could run it more often if we wanted, but found this cadence to be enough for us to check in on tools. In this case, it runs once a day at 8am UTC (giving us a “nightly” run for our largely-North-American-based team).  . First, we schedule this to run on a nightly basis via the  . cron .  scheduling event. We could run it more often if we wanted, but found this cadence to be enough for us to check in on tools. In this case, it runs once a day at 8am UTC (giving us a “nightly” run for our largely-North-American-based team). .  We set   fail-fast:   false   because we want all the runs of our matrix to complete since each tests a different combination. By default, Actions will cancel all other pending jobs upon the first failure.  . We set  .  false .  because we want all the runs of our matrix to complete since each tests a different combination. By default, Actions will cancel all other pending jobs upon the first failure. .  We define a matrix via the    matrix    strategy section with all the versions we want to test.  . We define a matrix via the  . matrix .  strategy section with all the versions we want to test. .  It’s fairly straightforward, but a couple things warrant added context:  . It’s fairly straightforward, but a couple things warrant added context: . Even though we can only target JDK 8 for Android binaries, we want developers to be able to use modern tools and plan to move up to a higher version soon. AGP 7.0 will also require a minimum of JDK 11. We’re not sure which version we’ll move up to formally, so we test both! . Since AGP 7 requires JDK 11, we don’t want to run a build with AGP 7 and Java 8. We use  . exclude: .  here to exclude all combinations that include AGP 7 and Java 8. This is a powerful way to cut down on redundant or otherwise not-usable combinations. .  The first place we use a matrix input is in the    setup-java    action  . The first place we use a matrix input is in the  . setup-java .  action .  This is pretty simple and handled right in the workflow. The    setup-java    action will use the version we define here.  . This is pretty simple and handled right in the workflow. The  . setup-java .  action will use the version we define here. .  The next section is a bit more complex. This is where we actually run our build (unit tests, lint, and assembling the production app).   . The next section is a bit more complex. This is where we actually run our build (unit tests, lint, and assembling the production app).  .  In order to pass our matrix values into the build, we expose them via     Gradle Properties     and pass them via    -Pproperty=value    syntax in the command.  . In order to pass our matrix values into the build, we expose them via  . Gradle Properties .  and pass them via  . -Pproperty=value .  syntax in the command. .  To pick these up in our build, we simply look for these properties. Here is an example build file for a simple project that reads the above properties and uses them for setting the AGP version and Kotlin IR.  . To pick these up in our build, we simply look for these properties. Here is an example build file for a simple project that reads the above properties and uses them for setting the AGP version and Kotlin IR. .  Now we’ll have a matrix of builds that cover all possible combinations. Running these nightly allow us to get early insight into how our build reacts to these types. Running them as a matrix allows us to also test their interaction with newer versions of other tools, too!  . Now we’ll have a matrix of builds that cover all possible combinations. Running these nightly allow us to get early insight into how our build reacts to these types. Running them as a matrix allows us to also test their interaction with newer versions of other tools, too! .  You can reuse the above patterns to configure a number of other things as well. Here are some examples of other things we currently test, or have tested in the past:  . You can reuse the above patterns to configure a number of other things as well. Here are some examples of other things we currently test, or have tested in the past: . Kotlin milestone/RC versions via Gradle property . New Android SDK versions via Gradle property . Upcoming Gradle RCs and nightlies via the . eskatos/gradle-command-action .  action, which supports different configurations as arguments. See its  . README . . . Snapshots/RCs of regular app library dependencies and building against them early too. This includes libraries in Sonatype’s snapshots repo, Kotlin EAP snapshots, and AndroidX’s snapshots. . Internal changes even! You don’t have to just limit this pattern to external dependencies, another great use case is for incubating an invasive internal change that you want to give extra time to bake. . One example of this for us was an RxJava 3 migration script that we wanted to test for a couple of weeks before committing to applying it in the codebase. This allowed us to migrate to RxJava 3 with virtually no issues. .  We’ve been using shadow jobs for about a year now and in that time we’ve caught dozens of issues early. I highly recommend doing the same in your own project! Especially if you use Kotlin, Jetbrains is    actively asking developers to try out the new IR backend    and this is an excellent way to do that.  . We’ve been using shadow jobs for about a year now and in that time we’ve caught dozens of issues early. I highly recommend doing the same in your own project! Especially if you use Kotlin, Jetbrains is . actively asking developers to try out the new IR backend .  and this is an excellent way to do that. .  Not only do you give your team confidence in updating to new dependencies, but you also give valuable early feedback to developers of these tools. On top of all that, these tools often introduce improvements that find existing issues in your codebase (particularly Kotlin updates), and you can usually backport fixes for these issues into your codebase today.  . Not only do you give your team confidence in updating to new dependencies, but you also give valuable early feedback to developers of these tools. On top of all that, these tools often introduce improvements that find existing issues in your codebase (particularly Kotlin updates), and you can usually backport fixes for these issues into your codebase today. .  Lastly – by staying on top of these we also give team members the ability to try new tools that depend on these, like Android Studio Arctic Fox or Jetpack Compose. Even if it’s only local usage, we better empower them to forge ahead.  . Lastly – by staying on top of these we also give team members the ability to try new tools that depend on these, like Android Studio Arctic Fox or Jetpack Compose. Even if it’s only local usage, we better empower them to forge ahead. .  If you find this interesting and like using modern tools, come join us!    https://slack.com/careers   . If you find this interesting and like using modern tools, come join us!  . https://slack.com/careers .  To really drive home the value we’ve gotten out of this, here is a comprehensive list of every issue we’ve filed as a result of these jobs over the past year:  . To really drive home the value we’ve gotten out of this, here is a comprehensive list of every issue we’ve filed as a result of these jobs over the past year: . Kotlin: .  KT-44420  . , .  KT-43538  . , .  KT-44045  . , .  KT-43242  . , .  KT-43241  . , .  KT-42915  . , .  KT-41493  . , .  KT-42257  . , .  KT-42752  . , .  KT-41484  . , .  KT-41494  . , .  KT-41702  . , .  KT-41732  . , .  KT-41486  . , .  KT-40351  . , .  KT-40485  . , .  KT-40091  . , .  KT-37944  . , .  KT-36478  . , .  KT-33052  .  b/161920328 ,  .  b/161727305 ,  .  b/159516509 ,  .  b/157723604 ,  .  b/157681341 ,  .  b/156657359 ,  .  b/148284064 ,  .  b/148284065 ,  .  b/162528480 ,  .  b/162446295 ,  .  b/162259266 ,  .  b/161381697 ,  .  b/160815393 ,  .  b/160714171 ,  .  b/158597249 ,  .  b/154315491  . ,  .  b/148929520  .  #15674 ,  .  #13333 ,  .  #13302 ,  .  #13811 ,  .  #12997 ,  .  #12452 ,  ", "date": "2021-02-18"}, {"website": "Slack", "title": "Migrating Millions of Concurrent Websockets to Envoy", "author": ["Ariane van der Steldt", "Radha Kumari"], "link": "https://slack.engineering/migrating-millions-of-concurrent-websockets-to-envoy/", "abstract": " Slack has a global customer base, with millions of simultaneously connected users at peak times. Most of the communication between users involves sending lots of tiny messages to each other. For much of Slack’s history, we’ve used  HAProxy  as a load balancer for all incoming traffic. Today, we’ll talk about problems we faced with HAProxy, how we solved them with  Envoy Proxy , the steps involved in the migration, and what the outcome was. Let’s dive in! . To deliver messages instantly, we use a  websocket connection , a bidirectional communications link which is responsible for you seeing “Several people are typing…” and then the thing they typed, nearly as fast as the speed of light permits. The websocket connections are ingested into a system called “wss” (WebSocket Service) and accessible from the internet using wss-primary.slack.com and wss-backup.slack.com (it’s not a website, you just get a HTTP 404 if you go there). .   . Websocket connections start out as regular HTTPS connections, and then the client issues a protocol switch request to upgrade the connection to a websocket. At Slack, we have different websocket services dedicated to messages, to presence (listing which contacts are online), and to other services. One of the websocket endpoints is specifically made for apps that need to interact with Slack (because apps want real-time communication too). .   . In the past, we had a set of HAProxy instances specifically dedicated to websockets in multiple  AWS  regions to terminate websocket connections close to the user and forward the request to corresponding backend services. . While we have been using HAproxy since the beginning of Slack and knew how to operate it at scale, there were some operational challenges that made us consider alternatives, like Envoy Proxy. . At Slack, it is a common event for backend service endpoint lists to change (due to instances being added or cycled away). HAProxy provides two ways to update its configuration to accommodate changes in endpoint lists. One is to use the HAProxy Runtime API. We used this approach with one of our sets of HAProxy instances, and our experience is described in another blog post —  A Terrible, Horrible, No-Good, Very Bad Day at Slack . The other approach, which we used for the websockets load balancer (LB), is to render the backends into the HAProxy configuration file and reload HAProxy. . With every HAProxy reload, a new set of processes is created to handle the new incoming connections. We’d keep running the old process for many hours to allow long-lived websocket connections to drain and avoid frequent disconnections of users. However, we can’t have too many HAProxy processes each running with it’s own “at the time” copy of the configuration — we wanted instances to converge on the new version of the configuration faster. We had to periodically reap old HAProxy processes, and restrict how often HAProxy could reload in case there was a churn in underlying backends. . Whichever approach we used, it needed some extra infrastructure in place for managing HAProxy reloads. . Envoy allows us to use  dynamically configured clusters and endpoints , which means it doesn’t need to be reloaded if the endpoint list changes. If code or configuration do change, Envoy has the ability to  hot restart  itself without dropping any connections. Envoy watches filesystem configurations with  inotify  for updates. Envoy also copies statistics from the parent process to the child process during a hot restart, so gauges and counters don’t get reset. . This all adds up to a significant reduction in operational overhead with Envoy, and no additional services needed to manage configuration changes or restarts. . Envoy provides several advanced load-balancing features, such as: . Because of the above reasons, in 2019, we decided to migrate our ingress load balancing tier from HAproxy to Envoy Proxy, starting with the websockets stack. The major goals of the migration were improved operability, access to new features that Envoy provides, and more standardization. By moving from HAProxy to Envoy across all of Slack, we would eliminate the need for our team to know the quirks of two pieces of software, to maintain two different kinds of configuration, to manage two build and release pipelines, and so on. By then, we were already using Envoy Proxy as the  data plane  in our service mesh. We also have experienced Envoy developers in-house, so we have ready access to Envoy expertise. . The first step in this migration was to review our existing websocket tier configuration and generate an equivalent Envoy configuration. Managing Envoy configuration was one of our biggest challenges during the migration. Envoy has a rich feature set, and its configurations are quite different to those of HAProxy. Envoy configuration deals with four main concepts: . Configuration management at Slack is primarily done via  Chef . When we started with Envoy, we deployed envoy configuration as a  chef template file , but it became cumbersome and error-prone to manage. To solve this problem, we built chef libraries and  custom resources  for generating Envoy configurations.    Inside Chef, the configuration is a  Singleton , modelling the fact that there is only one Envoy configuration per host. All Chef resources operate on that singleton, adding the listeners, routes, or clusters. At the end of the chef run, the envoy.yaml gets generated, validated, and then installed — we never write intermediate configurations, because these could be invalid. . This example shows how we can create one HTTP listener with two routes that routes traffic to two  dynamic  clusters.  . It took some effort to replicate our complicated HAProxy configuration in Envoy. Most of the features needed were already available in Envoy so it was just a matter of adding the support for it to the chef library and voila! We implemented a few missing Envoy features (some were contributed  upstream  and some are maintained in-house as extensions). . Testing the new Envoy websockets tier was an iterative process. We often prototyped with hand-coded Envoy configurations and tested it locally on a development machine with one listener, route, and cluster each. Hand-coded changes, once they worked, would be moved into the chef libraries. . HTTP routing was tested with  curl : . We used Envoy debug logging locally on the machine when things didn’t work as expected: Debug logging explains clearly why Envoy chose to route a specific request to a specific cluster. Envoy debug log is very helpful but also verbose and expensive (you really don’t want to enable that in your production environment). Debug logging can be enabled via Curl as shown below: .  curl -X POST http://localhost:&lt;envoy_admin_port&gt;/logging?level=debug  . The Envoy admin interface is also useful in initial debugging, particularly these endpoints: . Our Chef libraries run Envoy with the `–mode validate` command-line option as a validation step, in order to prevent installation of invalid configurations. This can also be done manually: .  sudo /path/to/envoy/binary -c &lt;/path/to/envoy.yaml&gt; --mode validate  . Envoy provides JSON formatted listener logs. We ingest those logs into our logging pipeline (after sanitizing the logs for  PII , of course) and this has often been helpful for debugging. . Once confident with config in our development environment, we were ready to do some more testing — in production! . In order to minimize risk during the migration, we built a new Envoy websocket stack with an equivalent configuration to the existing HAProxy tier. This meant that we could do a gradual, controlled shift of traffic to the new Envoy stack, and that we could quickly switch back to HAProxy if necessary. The downside was our AWS cost — we were using double the resources during the migration but we were willing to spend the time and resources to do this migration transparently for our customers. . We manage our DNS records   wss-primary.slack.com   and   wss-backup.slack.com   via  NS1 . We used weighted routing to shift traffic from  haproxy-wss  to  envoy-wss   NLB  DNS names. The first regions were rolled out individually in steps of 10%, 25%, 50%, 75%, and 100%. The final regions were done faster (25%, 50%, 75%, 100% in just two days compared to prior one region over the course of a week), as we had confidence in the new Envoy tier and in the rollout process. . Even though the migration was smooth and outage-free, there were a number of minor problems that cropped up, such as differences in timeout values and headers. We reverted, fixed, and rolled out again multiple times during the migration. .   .   . After a very long and exciting 6 months, the migration was complete and the entire HAProxy websocket stack was replaced with Envoy Proxy globally with  zero customer impact . . The migration itself was relatively uneventful and boring.  Boring is a good thing : exciting means things break, boring means things keep working. . We found that the old HAProxy config had grown organically over time. It was largely shaped by the model that HAProxy uses — one large configuration that includes all listeners. Envoy’s configuration model uses much more defined scopes than HAProxy’s model. Once a listener is entered, only the rules inside that listener apply to the requests. Once a route is entered, only rules on that route apply. This makes it much easier to associate rules with the relevant requests. . It took a lot of time to extract what was important in the old HAProxy configuration from what was effectively technical debt. It was often difficult to figure out why a certain rule was in place, what was intentional as opposed to unintentional, and what behavior other services relied on. For example, some services were supposed to only be under one of two virtual hosts (vhosts), but were actually available under both vhosts in HAProxy. We had to replicate this mistake, because existing code relied on that behavior. . We missed a few subtle things in the HAProxy stack. Sometimes these were important — we broke Slack’s Daily Active User (DAU) metric for a bit (oops!). There were a lot of minor issues to fix as well. Load balancer behavior is complex and there was no real way around this problem other than time and debugging. . We started the migration without a testing framework for the load balancer configurations. Instead of having automated tests that validated that test URLs routed to the correct endpoint and behaviors related to request and response headers we had… a HAProxy config. Tests are helpful during migrations because they can provide a lot of context about the reasons for expected behaviors. Because we lacked tests, we often had to check in with service owners instead to find out what behavior they relied on. . The Chef resources that we built intentionally supported only a subset of Envoy functionality. This kept our libraries simpler — we only had to consider the features we actually used. The drawback was that each time we wanted to use new Envoy features, we had to add support for them in our Chef libraries. For example,  SNI  (https) listeners were written part-way through development, when we decided it would be simpler than adding support to the existing listeners. However, when it came to vhost support, we had so much code developed and in-use already that refactoring resources that were in use elsewhere throughout the company would have taken a long time. The vhost support in our Chef library is a hack (and one day soon we will fix it). . To make it safer to change the Envoy resource Chef libraries — in other words, to ensure we didn’t break other teams that were using our libraries — we introduced a comprehensive set of tests that generated those teams’ entire configurations. This made it easy to tell exactly how all our generated Envoy configurations would (or wouldn’t) be impacted when we updated the Envoy Chef resources. . One of the key things in this migration (like any other) is communication. We worked hard to keep everyone informed and aligned with the changes we were making. Our Customer Experience (CE) team was a great partner — they were able to monitor incoming tickets for anything that might indicate users had been impacted as a result of this migration. . Despite occasional minor setbacks, the envoy websocket migration was a great success. We’ve followed up by migrating another critical Slack service, our software client metrics ingestion pipeline — which is isolated from our other ingress load balancers — to Envoy Proxy. We’re nearly done with migrating the internal load balancers for our web and API traffic to Envoy. The final part of this epic migration is to move our (regular, non-websocket) HTTP stack which terminates incoming traffic at our edge, from HAProxy to Envoy; this is also underway. . We are now within sight of our final goal of standardization on Envoy Proxy for both our ingress load balancers and our service mesh data plane, which will significantly reduce cognitive load and operational complexity for the team, as well as making Envoy’s advanced features available throughout our load balancing infrastructure. Since migrating to Envoy, we’ve exceeded our previous peak load significantly with no issues. .  Huge shoutout to the team  —  Laura Nolan, Stephan Zuercher, Matthew Girard, David Stern, Mark McBride, John On, Cooper Bethea, V Brennan, Ann Paul, Pramilla Singh, Rafael Elvira and Stuart Williams for all the support and contributions to the project.  . Want to help Slack solve tough problems and join our growing team? Check out all our  open positions  and apply today. ", "date": "2021-03-15"}, {"website": "Slack", "title": "Client Tracing: Understanding Mobile and Desktop Application Performance at Scale", "author": ["Justin Rushing"], "link": "https://slack.engineering/client-tracing-understanding-mobile-and-desktop-application-performance-at-scale/", "abstract": "  A customer writes in and says the dreaded words: “My app is slow”. Here we go…   . A customer writes in and says the dreaded words: “My app is slow”. Here we go…  .  Performance problems can be a real struggle to track down, especially if they aren’t easily reproducible. Looking at the customer’s logs, you see that it takes over 1.5 seconds to switch between channels on their Android client! That must be what they’re talking about, but what’s causing the slowdown? Nothing else pops out to you in the logs and the trail’s gone cold.  . Performance problems can be a real struggle to track down, especially if they aren’t easily reproducible. Looking at the customer’s logs, you see that it takes over 1.5 seconds to switch between channels on their Android client! That must be what they’re talking about, but what’s causing the slowdown? Nothing else pops out to you in the logs and the trail’s gone cold. .  Slack client engineers were running into situations like this too frequently, where they knew a performance issue existed but couldn’t find the underlying cause. We were tired of letting our users down by not being able to diagnose their issues. Our mobile and desktop client infrastructure team did a survey of available technologies and, you guessed it, tracing was the perfect tool for the job!   . Slack client engineers were running into situations like this too frequently, where they knew a performance issue existed but couldn’t find the underlying cause. We were tired of letting our users down by not being able to diagnose their issues. Our mobile and desktop client infrastructure team did a survey of available technologies and, you guessed it, tracing was the perfect tool for the job!  .  Distributed tracing is a widely-used technology for understanding latency in a multi-service web application, but we found it opens up a new world of possibilities for understanding performance on the client as well. In this post we’re going to explain how we shipped our tracing stack to our iOS, Android, and JavaScript clients, and what we learned along the way.  . Distributed tracing is a widely-used technology for understanding latency in a multi-service web application, but we found it opens up a new world of possibilities for understanding performance on the client as well. In this post we’re going to explain how we shipped our tracing stack to our iOS, Android, and JavaScript clients, and what we learned along the way. .  Distributed tracing is a common technology for managing distributed services, but it’s still quite new for client instrumentation. Tracing allows you to instrument how long an action takes (called a    span   ) and break it down into smaller sub-actions (called    child spans   ). This allows you to not only tell when something gets slower, but also pinpoint    where and why    it got slower. The “distributed” in distributed tracing comes from its ability to combine spans from separate services, on separate machines, into a single trace. This gives you a single picture describing latency in a multi-service architecture.  . Distributed tracing is a common technology for managing distributed services, but it’s still quite new for client instrumentation. Tracing allows you to instrument how long an action takes (called a  . span . ) and break it down into smaller sub-actions (called  . child spans . ). This allows you to not only tell when something gets slower, but also pinpoint  . where and why .  it got slower. The “distributed” in distributed tracing comes from its ability to combine spans from separate services, on separate machines, into a single trace. This gives you a single picture describing latency in a multi-service architecture. .  Every span in the trace can be thought of as a structured log. They all have the same    trace_id   , which allows our tooling to group them together. (This will come in handy later)  . Every span in the trace can be thought of as a structured log. They all have the same  . trace_id . , which allows our tooling to group them together. (This will come in handy later) .   .  For more information on what tracing is and our infrastructure that supports it, see our previous blog post,    Tracing at Slack      . For more information on what tracing is and our infrastructure that supports it, see our previous blog post,  . Tracing at Slack .   .   Tracing at Slack    outlines how we implemented tracing across our API layer. With that in place, we could look at any endpoint to see how long our backend spent servicing requests and which services were slowing us down. There was only one problem — while our backend might think a request took 80ms to service, the network delay meant that our clients were seeing something totally different! At the end of the day user perception is what we really care about, which meant it was time to take request tracing to the client.  . Tracing at Slack .  outlines how we implemented tracing across our API layer. With that in place, we could look at any endpoint to see how long our backend spent servicing requests and which services were slowing us down. There was only one problem — while our backend might think a request took 80ms to service, the network delay meant that our clients were seeing something totally different! At the end of the day user perception is what we really care about, which meant it was time to take request tracing to the client. .  A reasonable start was to trace how long it takes to run the request from the client’s perspective. We called the span    api:request    . That gave us something that looked like this:  . A reasonable start was to trace how long it takes to run the request from the client’s perspective. We called the span  . api:request .  . That gave us something that looked like this: .     .   .  Perfect, now we understand how long it takes to make an API call on the client. This isn’t the full picture though. We’ve been fixing performance issues for a long time and we know that there are various things that can go wrong:  . Perfect, now we understand how long it takes to make an API call on the client. This isn’t the full picture though. We’ve been fixing performance issues for a long time and we know that there are various things that can go wrong: . Clients can only make a limited number of concurrent requests. If you’re making a lot of API calls, you need to put them in a queue. Our desktop app has priority queueing for requests, but our mobile clients don’t. Does that make a big difference? . We use strongly typed API responses on mobile. Inefficient parsers can take a long time, especially on older devices. Are there inefficient API calls that we don’t even know about? . We attempt to retry API calls internally. How often does that happen? Does it behave as expected? .   .  Since this is tracing, we’re able to consider all of these things together instead of as separate metrics reported from the client. We decided to break    api:request    down into three phases:  . Since this is tracing, we’re able to consider all of these things together instead of as separate metrics reported from the client. We decided to break  . api:request .  down into three phases: . api:queue_time . : The time we spend sitting in an API queue before executing our request . api:http_request . : The time it takes to make a single HTTP request and get a raw response payload back from the server . api:parse . : The time it takes to parse the data in the response into an object we return to the caller. This could be a dictionary, a strongly typed object, or whatever makes sense for the platform. .  When each phase is complete, we also include metadata like what endpoint the log was for, HTTP status code, or how many bytes were returned from the HTTP request.  . When each phase is complete, we also include metadata like what endpoint the log was for, HTTP status code, or how many bytes were returned from the HTTP request. .   .  Clearly distinguishing between the client-side processing and network latency makes it clear to developers if their API call is slow due to the system being overloaded, or slow due to API response parsing (Hello Java reflection, my old friend). There’s one thing missing — was my HTTP request slow because of network issues, or did our backend take a long time to service the request?   . Clearly distinguishing between the client-side processing and network latency makes it clear to developers if their API call is slow due to the system being overloaded, or slow due to API response parsing (Hello Java reflection, my old friend). There’s one thing missing — was my HTTP request slow because of network issues, or did our backend take a long time to service the request?  .  Because distributed tracing was    built    for measuring latency across systems, the solution was simple. All we had to do was plumb the trace identifiers we described earlier through our HTTP request header so that we could have client and server logs together in the   same trace  .  . Because distributed tracing was  . built .  for measuring latency across systems, the solution was simple. All we had to do was plumb the trace identifiers we described earlier through our HTTP request header so that we could have client and server logs together in the  . . .   .  And it worked! In a single chart we’re able to see:  . And it worked! In a single chart we’re able to see: . What the impact of a busy system is on our API layer . Which endpoints are the slowest to parse on the client . How our network latency impacts request latency .  Above is an example trace from one of our production clients. This screenshot and others throughout the post come from a product called    Honeycomb   , which we use to interact with traces.  . Above is an example trace from one of our production clients. This screenshot and others throughout the post come from a product called  . Honeycomb . , which we use to interact with traces. .  We decided to match our existing backend sampling strategy and roll this out to 1% of all API requests, across all clients. That proved to be enough to give us a clear signal and also guarantee that when teams working on new features add a new API call, we have their back.  . We decided to match our existing backend sampling strategy and roll this out to 1% of all API requests, across all clients. That proved to be enough to give us a clear signal and also guarantee that when teams working on new features add a new API call, we have their back. .  While network speed and API performance are often regarded as the biggest bottleneck on the client, modern applications have become much more complex than simple request/response web pages. Our customers expect our apps to cache information they’ve already seen and to support seamless interactions even when they’re offline. To accomplish this we’ve built our mobile apps on top of fully-featured SQLite databases, which play a large role in overall application performance. Understanding database performance in the wild was notoriously difficult with previous tools, and we knew we could do it better with tracing.  . While network speed and API performance are often regarded as the biggest bottleneck on the client, modern applications have become much more complex than simple request/response web pages. Our customers expect our apps to cache information they’ve already seen and to support seamless interactions even when they’re offline. To accomplish this we’ve built our mobile apps on top of fully-featured SQLite databases, which play a large role in overall application performance. Understanding database performance in the wild was notoriously difficult with previous tools, and we knew we could do it better with tracing. .  When working with a database, most systems follow a similar flow:  . When working with a database, most systems follow a similar flow: . Begin a transaction. If the database is already being used this may require us to wait until the previous transactions are completed. . Create, read, update, and delete records. . Close the transaction, atomically committing all changes to the database. .  Our iOS application implements this pattern like so: The provided block is executed once a database transaction has been opened, and when the block returns all changes made during the transaction are committed to the database. If an exception is thrown, the transaction will be rolled back and no changes will be committed.  . Our iOS application implements this pattern like so: The provided block is executed once a database transaction has been opened, and when the block returns all changes made during the transaction are committed to the database. If an exception is thrown, the transaction will be rolled back and no changes will be committed. .  The question is, what information about the transaction should we include in the trace? We ended up taking a lot of inspiration from how we structured    api:request   . The question is, what information about the transaction should we include in the trace? We ended up taking a lot of inspiration from how we structured  . api:request . db:wait . : We are limited on the number of concurrent database transactions, so when the system is under load we can expect to  .  a while until the transaction is opened. . db:execute . : The time it takes to  .  the transaction will tell us how long it takes to read &amp; write from the DB. Oftentimes this is when business logic that has nothing to do with accessing the database can sneak in and back up our DB queue. . db:save . : The time it takes to  .  the dirty changes to disk reflects how many changes were created during the transaction. . db:perform_transaction . : The total amount of time it takes to wait, execute, and save the transaction. .  When we put it all together, it comes out looking something like this:  . When we put it all together, it comes out looking something like this: .   .  So what does the code look like? When figuring out how to implement tracing we considered a couple of options.    OpenTracing    is a widely-used standard and has a general-purpose API for creating traces in code. While it’s a great option, with our    simplified span event    format we saw an opportunity to make a more opinionated API, designed to work specifically for the needs of clients. For example, a common pattern used in OpenTracing is leveraging thread local storage to hold context about the current trace to easily append child spans. This breaks on mobile clients where concurrency is the default, and a given operation will inevitably pass through many different threads.  . So what does the code look like? When figuring out how to implement tracing we considered a couple of options. . OpenTracing .  is a widely-used standard and has a general-purpose API for creating traces in code. While it’s a great option, with our . simplified span event .  format we saw an opportunity to make a more opinionated API, designed to work specifically for the needs of clients. For example, a common pattern used in OpenTracing is leveraging thread local storage to hold context about the current trace to easily append child spans. This breaks on mobile clients where concurrency is the default, and a given operation will inevitably pass through many different threads. .  By writing our own API, we were also able to take full advantage of the language features available on each platform. While the details of the API vary between languages, the interfaces are roughly the same. We’ll be showing our    Swift    code in this post.  . By writing our own API, we were also able to take full advantage of the language features available on each platform. While the details of the API vary between languages, the interfaces are roughly the same. We’ll be showing our  . Swift .  code in this post. .  To begin a trace, we use an object called a    Tracer   . This creates a new trace and marks the time at which the trace was started. The return type adheres to a protocol we call    Spannable    .    Spannable    is quite simple — it defines an object that can be started, completed, or cancelled. To maximize the surface area that we trace without overloading our backend with logs, we use sampling to limit how many logs are uploaded by clients. Sampling is done in the    Tracer   , and the resulting object is    nil    if we aren’t going to perform the trace. Once the action being measured is complete, we simply call    complete()    on the trace with any metadata we’d like to upload along with it.  . To begin a trace, we use an object called a  . Tracer . . This creates a new trace and marks the time at which the trace was started. The return type adheres to a protocol we call  . Spannable .  .  . Spannable .  is quite simple — it defines an object that can be started, completed, or cancelled. To maximize the surface area that we trace without overloading our backend with logs, we use sampling to limit how many logs are uploaded by clients. Sampling is done in the  . Tracer . , and the resulting object is  . nil .  if we aren’t going to perform the trace. Once the action being measured is complete, we simply call  . complete() .  on the trace with any metadata we’d like to upload along with it. .  One of the challenges we faced on mobile was that at any time, the application could be sent to the background, which means that the process will be put to sleep (and eventually killed). We aren’t informed by the operating system when this happens, which leads to crazy outliers. To fix this issue, internal to our tracing system we simply complete all open traces when the system informs us the app is sent background and mark them with a    _background_flush : true    to indicate to our tooling that the trace was incomplete.  . One of the challenges we faced on mobile was that at any time, the application could be sent to the background, which means that the process will be put to sleep (and eventually killed). We aren’t informed by the operating system when this happens, which leads to crazy outliers. To fix this issue, internal to our tracing system we simply complete all open traces when the system informs us the app is sent background and mark them with a  . _background_flush : true  . to indicate to our tooling that the trace was incomplete. .  Every    Spannable    instance has something called a    TraceContext   .    TraceContext    lets us insert child spans into our trace. The benefit of having this be in a separate object is that it lets us limit the behavior possible in the function. APIs that take    TraceContext    are able to add subspans to the trace, but they cannot complete the span without us knowing. Furthermore, by injecting the context explicitly instead of implicitly reading it from thread local storage, we can safely pass it across thread boundaries.  . Every  . Spannable .  instance has something called a  . TraceContext . .  . TraceContext .  lets us insert child spans into our trace. The benefit of having this be in a separate object is that it lets us limit the behavior possible in the function. APIs that take  . TraceContext .  are able to add subspans to the trace, but they cannot complete the span without us knowing. Furthermore, by injecting the context explicitly instead of implicitly reading it from thread local storage, we can safely pass it across thread boundaries. .  The less work application developers have to do to get helpful trace information, the better. To make it easy we added    TraceContext    support to our most commonly used infrastructure libraries like networking and persistence.  . The less work application developers have to do to get helpful trace information, the better. To make it easy we added  . TraceContext .  support to our most commonly used infrastructure libraries like networking and persistence. .  Here is a real code snippet from our iOS codebase for a trace that measures how long it takes to update which channels in a workspace are unread and contain mentions. It’s a simple example that makes an API call and inserts the results into our database, and it leverages all of the concepts outlined above.  . Here is a real code snippet from our iOS codebase for a trace that measures how long it takes to update which channels in a workspace are unread and contain mentions. It’s a simple example that makes an API call and inserts the results into our database, and it leverages all of the concepts outlined above. .  API latency and database performance are critical building blocks for understanding the low-level performance of any app. What these metrics lack, however, is context on how operations impact   user experience.   First and foremost we invest in performance to make the product better for our users, so it’s time to tie everything together by modeling user interactions with tracing.  . API latency and database performance are critical building blocks for understanding the low-level performance of any app. What these metrics lack, however, is context on how operations impact  .  First and foremost we invest in performance to make the product better for our users, so it’s time to tie everything together by modeling user interactions with tracing. .  The most common interaction in any user facing app is loading views, and it should be    fast   . Being a messaging app, we decided to start with our most important screen — the channel view. We’ve been optimizing the performance of opening a channel for a long time, and we make heavy use of our database to do so. Our goal is to always show the user whatever we have in our cache as quickly as possible, and asynchronously fetch the latest content from the server. We’ve spoken about this approach previously in our    unified cross-platform performance metrics    post. With tracing, we can do better.   The flow looks something like this:  . The most common interaction in any user facing app is loading views, and it should be  . fast . . Being a messaging app, we decided to start with our most important screen — the channel view. We’ve been optimizing the performance of opening a channel for a long time, and we make heavy use of our database to do so. Our goal is to always show the user whatever we have in our cache as quickly as possible, and asynchronously fetch the latest content from the server. We’ve spoken about this approach previously in our  . unified cross-platform performance metrics .  post. With tracing, we can do better.  . The flow looks something like this: .   .  As it turns out, this pattern of “render what’s in the cache while we fetch the latest from the server” is generally applicable to most of the screens in the app. We decided to make a general-purpose schema for tracing views that we call a    View Load Trace   . View load traces are simple and require three different spans:  . As it turns out, this pattern of “render what’s in the cache while we fetch the latest from the server” is generally applicable to most of the screens in the app. We decided to make a general-purpose schema for tracing views that we call a  . View Load Trace . . View load traces are simple and require three different spans: . : The root of the trace. This is completed when both  . visible .  and  . up_to_date .  are completed. Examples include  . view_load:chat .  (our internal name for the channel view),  . view_load:all_threads . , etc. . : Time until meaningful content is rendered in the view. “Meaningful” is the important word to focus on here and changes from screen to screen. For  . view_load:chat . we decided to consider  . visible .  complete once we can see messages on the screen. Any spans related to querying our local cache or rendering the UI should go here. . : Time until view has up-to-date content. Any spans related to fetching information from the server should go here. .   .  With this schema in place we created a new kind of tracer (   ViewLoadTracer   ) to implement the spec, and used it to measure the performance of many different screens in the app. By injecting the    TraceContext    into our existing services (persistence, networking, etc.) these traces proved to be incredibly useful for identifying bottlenecks in these complex systems where we’re fetching data from multiple places and rendering it on the screen.  . With this schema in place we created a new kind of tracer ( . ViewLoadTracer . ) to implement the spec, and used it to measure the performance of many different screens in the app. By injecting the  . TraceContext .  into our existing services (persistence, networking, etc.) these traces proved to be incredibly useful for identifying bottlenecks in these complex systems where we’re fetching data from multiple places and rendering it on the screen. .  The next kind of interaction we wanted to measure was actions taken inside of Slack, like sending a message or adding an emoji reaction. From a tracing perspective, it proved to be pretty easy! For message sending, we started the trace when the user hit “Send” and completed the trace once we finished making the API call that posts the message. By leveraging database and API tracing, we were able to get a full picture of the interaction:  . The next kind of interaction we wanted to measure was actions taken inside of Slack, like sending a message or adding an emoji reaction. From a tracing perspective, it proved to be pretty easy! For message sending, we started the trace when the user hit “Send” and completed the trace once we finished making the API call that posts the message. By leveraging database and API tracing, we were able to get a full picture of the interaction: .   .  We suffix all of these types of traces with    _attempt    because we recognized this as a great opportunity to understand the reliability of the action — how often does message sending succeed or fail? By adding a    success    tag to every trace we can create client-side metrics about reliability, which prove to be a useful tool for assessing the impact of production incidents on our clients. Most incident metrics observe API failure rate, but what’s missing from them is the visible impact on the user. An API may be failing 50% of the time, but with proper retry logic on the client the user may never notice. Tracking reliability as the user sees it gives us a much more complete picture of how the product is behaving for our customers.   . We suffix all of these types of traces with  . _attempt .  because we recognized this as a great opportunity to understand the reliability of the action — how often does message sending succeed or fail? By adding a  . success .  tag to every trace we can create client-side metrics about reliability, which prove to be a useful tool for assessing the impact of production incidents on our clients. Most incident metrics observe API failure rate, but what’s missing from them is the visible impact on the user. An API may be failing 50% of the time, but with proper retry logic on the client the user may never notice. Tracking reliability as the user sees it gives us a much more complete picture of how the product is behaving for our customers.  .  Below you can see an example from an incident simulation exercise that helped us understand its impact on message send reliability. The chart shows three lines —- one for each client — where the value of the series is the success rate for the    message_send_attempt    trace. The main thing to observe here is that we can see precisely when the outage began impacting clients, how long they were impacted for, and the shape of the recovery. Incident response at Slack relies on accurate timelines for assessing what happened, when mitigation is working, and when we can consider the incident complete. Accurate client logs are often the best way to tell that story.  . Below you can see an example from an incident simulation exercise that helped us understand its impact on message send reliability. The chart shows three lines —- one for each client — where the value of the series is the success rate for the  . message_send_attempt .  trace. The main thing to observe here is that we can see precisely when the outage began impacting clients, how long they were impacted for, and the shape of the recovery. Incident response at Slack relies on accurate timelines for assessing what happened, when mitigation is working, and when we can consider the incident complete. Accurate client logs are often the best way to tell that story. .   .  Tracing user interactions also gives us a much more complete picture of how our applications handle themselves in the case of an incident. What do our clients do when our servers are down? Do they retry sending? How many times? Do we use exponential backoff?   . Tracing user interactions also gives us a much more complete picture of how our applications handle themselves in the case of an incident. What do our clients do when our servers are down? Do they retry sending? How many times? Do we use exponential backoff?  .  Here is a trace that was collected during the above incident simulation. You can see that our client made 10 API calls during a 40 second time window, without much of a break between calls. This isn’t what we want to happen when our servers are already overloaded, which we can tell by the 503 status code we’re receiving. After seeing this information we’ve updated our logic to use exponential backoff so that next time we’ll be better prepared.  . Here is a trace that was collected during the above incident simulation. You can see that our client made 10 API calls during a 40 second time window, without much of a break between calls. This isn’t what we want to happen when our servers are already overloaded, which we can tell by the 503 status code we’re receiving. After seeing this information we’ve updated our logic to use exponential backoff so that next time we’ll be better prepared. .   .  Sometimes spans are slow and it’s hard to understand why. One approach may be to add more child spans, but at a certain point too many spans are hard to reason through and it makes the trace more expensive for the client. Another approach is to get more out of the spans we already have by attaching useful metadata.  . Sometimes spans are slow and it’s hard to understand why. One approach may be to add more child spans, but at a certain point too many spans are hard to reason through and it makes the trace more expensive for the client. Another approach is to get more out of the spans we already have by attaching useful metadata. .  Sometimes we would come across traces with an unexplained gap between spans, and assume it was due to too much work happening on the main (UI) thread. For situations where we’re main thread bound, the best course of action is to run the feature through a local profiler (like Instruments, Android Profiler, or Chrome Dev Tools) and either move work off of the main thread or make it more efficient. We found ourselves making the    assumption    that this was the issue quite often, but we weren’t always right. To add extra clarity to the traces, we made it so that every trace is automatically decorated with the total number of dropped frames that occur during the trace. Dropped frames happen when the main thread is busy for longer than 16ms and are most often associated with choppy UI. They’re also a great indicator that we’re doing too much on the main thread.   . Sometimes we would come across traces with an unexplained gap between spans, and assume it was due to too much work happening on the main (UI) thread. For situations where we’re main thread bound, the best course of action is to run the feature through a local profiler (like Instruments, Android Profiler, or Chrome Dev Tools) and either move work off of the main thread or make it more efficient. We found ourselves making the  . assumption .  that this was the issue quite often, but we weren’t always right. To add extra clarity to the traces, we made it so that every trace is automatically decorated with the total number of dropped frames that occur during the trace. Dropped frames happen when the main thread is busy for longer than 16ms and are most often associated with choppy UI. They’re also a great indicator that we’re doing too much on the main thread.  .   .  Because all span tags can be independently queried to form their own metrics, we’re also able to aggregate dropped frame percentiles across our different traces. We often use this information to determine which of our view loads are bound by main thread performance so we can better prioritize projects that improve user-perceived latency.  . Because all span tags can be independently queried to form their own metrics, we’re also able to aggregate dropped frame percentiles across our different traces. We often use this information to determine which of our view loads are bound by main thread performance so we can better prioritize projects that improve user-perceived latency. .  Knowing how database access contributed to performance was eye-opening for a lot of the interactions in the client. We realized quickly though that having no context into what was happening in the database transaction made it hard to know why things were slower for some users than others. Being able to record individual SQL calls would have been really powerful, but we found it to be unrealistic for operations that made potentially    hundreds    of calls to the database. Instead we leveraged tags to decorate database transactions with the number of times different tables were read from or written to.  . Knowing how database access contributed to performance was eye-opening for a lot of the interactions in the client. We realized quickly though that having no context into what was happening in the database transaction made it hard to know why things were slower for some users than others. Being able to record individual SQL calls would have been really powerful, but we found it to be unrealistic for operations that made potentially  . hundreds .  of calls to the database. Instead we leveraged tags to decorate database transactions with the number of times different tables were read from or written to. .     .   .  Concretely, we saw that some users took way longer than we expected for their channel list to update. Above you can see a trace for this operation. It’s a simple one that we described earlier in this blog post —- we make an API call and update our database with the results. To take our new    db:perform_transaction    metadata for a spin, we pulled up one of those traces. The first thing we noticed is that the user is a member of 129 channels, and all 129 get updated — looks good. The next thing we noticed is that even though we only updated 47 direct messages, we fetched almost 600 of them from the DB! Even though we only show a handful of direct messages in the channel list, the app was loading every conversation the user had ever had. After resolving this issue, we saw the performance for these p95 cases drop by almost 40%.  . Concretely, we saw that some users took way longer than we expected for their channel list to update. Above you can see a trace for this operation. It’s a simple one that we described earlier in this blog post —- we make an API call and update our database with the results. To take our new  . db:perform_transaction .  metadata for a spin, we pulled up one of those traces. The first thing we noticed is that the user is a member of 129 channels, and all 129 get updated — looks good. The next thing we noticed is that even though we only updated 47 direct messages, we fetched almost 600 of them from the DB! Even though we only show a handful of direct messages in the channel list, the app was loading every conversation the user had ever had. After resolving this issue, we saw the performance for these p95 cases drop by almost 40%. .  Most of our above examples are small, simple interactions in the app. Tracing thrives just as well in more complex situations, where lots of different things are happening at the same time. Below is an example that’s near and dear to every application — our launch trace! We leveraged our existing view-load tracing to define launch as complete once the view load times for all views on the screen have completed. Classic performance tooling allows you see which functions are taking up the most CPU time during something like launch, but what can be exceedingly difficult is understanding how different concurrent operations interact with each other. It’s been fascinating to see how things like pre-main time compare with the rest of launch, how many different views are created, and even how API queue contention can cause performance issues on boot.   . Most of our above examples are small, simple interactions in the app. Tracing thrives just as well in more complex situations, where lots of different things are happening at the same time. Below is an example that’s near and dear to every application — our launch trace! We leveraged our existing view-load tracing to define launch as complete once the view load times for all views on the screen have completed. Classic performance tooling allows you see which functions are taking up the most CPU time during something like launch, but what can be exceedingly difficult is understanding how different concurrent operations interact with each other. It’s been fascinating to see how things like pre-main time compare with the rest of launch, how many different views are created, and even how API queue contention can cause performance issues on boot.  .  In this post we outlined how we used the existing distributed tracing technology at Slack to solve real problems on our iOS, Android, and desktop clients. Tracing adds causality to our logs, making them inherently actionable in a way that they never were with simple events. Explicitly modeling common actions on the client like making an API call, writing to a database, or loading a view brings consistency to our traces so that developers are able to easily interpret them. With over a billion logs per day across almost 100 distinct traces our developers are already able to have a deep understanding of application performance in the wild using tracing. We’re just getting started here but we’re excited to see what we’ll be able to do next! If you love solving these kinds of problems,    come join us   !   . In this post we outlined how we used the existing distributed tracing technology at Slack to solve real problems on our iOS, Android, and desktop clients. Tracing adds causality to our logs, making them inherently actionable in a way that they never were with simple events. Explicitly modeling common actions on the client like making an API call, writing to a database, or loading a view brings consistency to our traces so that developers are able to easily interpret them. With over a billion logs per day across almost 100 distinct traces our developers are already able to have a deep understanding of application performance in the wild using tracing. We’re just getting started here but we’re excited to see what we’ll be able to do next! If you love solving these kinds of problems,  . come join us . !  .   ", "date": "2021-03-24"}, {"website": "Slack", "title": "Tracing at Slack: Thinking in Causal Graphs", "author": ["Suman Karumuri"], "link": "https://slack.engineering/tracing-at-slack-thinking-in-causal-graphs/", "abstract": "  “Why is it slow?” is the hardest problem to debug in a complex distributed system like Slack. To diagnose a slow-loading channel with over a hundred thousand users, we’d need to look at client-side metrics, server-side metrics, and logs. It could be a client-side issue: a slow network connection or hardware. On the other hand, it could be a server-side issue: a request execution error or an edge case in code. Triaging and fixing these issues is a tedious process when we have to manually correlate all the logs and metrics for a single request across client devices, network devices, backend services,    and    databases.  . “Why is it slow?” is the hardest problem to debug in a complex distributed system like Slack. To diagnose a slow-loading channel with over a hundred thousand users, we’d need to look at client-side metrics, server-side metrics, and logs. It could be a client-side issue: a slow network connection or hardware. On the other hand, it could be a server-side issue: a request execution error or an edge case in code. Triaging and fixing these issues is a tedious process when we have to manually correlate all the logs and metrics for a single request across client devices, network devices, backend services,  . and .  databases. .  Slack employs tools like    Prometheus    (metrics) and    Elasticsearch   /   Presto    (log querying) to provide visibility into these issues. And while metrics provide an aggregated view of the performance of a request, we don’t have granular information about why a    specific    request was slow. Our logs do contain the granular context needed to track how a specific request is processed, but they can’t provide additional context on what happened before or after that event.  . Slack employs tools like  . Prometheus .  (metrics) and  . Elasticsearch . / . Presto .  (log querying) to provide visibility into these issues. And while metrics provide an aggregated view of the performance of a request, we don’t have granular information about why a  . specific .  request was slow. Our logs do contain the granular context needed to track how a specific request is processed, but they can’t provide additional context on what happened before or after that event. .  Distributed tracing is a commonly used solution to stitch together requests across services. However, our prior experience with tracing uncovered two limitations that made us hesitant to adopt existing solutions. As documented in    this post by Cindy Sridharan   , using existing tracing tools and consuming traces via a trace UI may not justify the investment in building a solution. In addition, current tracing frameworks are primarily designed for backend services and we found them unsuitable for our client applications and shell scripts.  . Distributed tracing is a commonly used solution to stitch together requests across services. However, our prior experience with tracing uncovered two limitations that made us hesitant to adopt existing solutions. As documented in  . this post by Cindy Sridharan . , using existing tracing tools and consuming traces via a trace UI may not justify the investment in building a solution. In addition, current tracing frameworks are primarily designed for backend services and we found them unsuitable for our client applications and shell scripts. .  We also wanted to get more mileage out of our trace data by running ad hoc analysis using SQL queries in addition to viewing the spans in a trace UI. This insight mirrors Twitter’s experience with querying raw trace data for insights, as Dan Luu documented in    this post   .  . We also wanted to get more mileage out of our trace data by running ad hoc analysis using SQL queries in addition to viewing the spans in a trace UI. This insight mirrors Twitter’s experience with querying raw trace data for insights, as Dan Luu documented in  . this post . . .  To address these limitations and to easily enable querying raw trace data, we model our traces at Slack as    Causal Graphs   , which is a Directed Acyclic graph of a new data structure called a    SpanEvent   .  This post is the first in a series where we talk about the motivations and architecture of our tracing system at Slack.  . To address these limitations and to easily enable querying raw trace data, we model our traces at Slack as  . Causal Graphs . , which is a Directed Acyclic graph of a new data structure called a  . SpanEvent . .  This post is the first in a series where we talk about the motivations and architecture of our tracing system at Slack. .  To triage elevated error rates or performance regressions using logs and metrics, we have to mentally reconstruct a chain of events that happened as part of a request. This requires the relevant metrics and logs from a chain of different services. Once we have a representative request and its chain of events, the cause of the issue is usually obvious.  . To triage elevated error rates or performance regressions using logs and metrics, we have to mentally reconstruct a chain of events that happened as part of a request. This requires the relevant metrics and logs from a chain of different services. Once we have a representative request and its chain of events, the cause of the issue is usually obvious. .  Distributed tracing automates the manual correlation described above, tracking a request across multiple services to create correlated logs. A typical distributed tracing setup involves the following components:  . Distributed tracing automates the manual correlation described above, tracking a request across multiple services to create correlated logs. A typical distributed tracing setup involves the following components: .   Instrumentation   : An application is typically instrumented using an open source tracing library. It consists of 3 components: the sampler, the trace context propagation library, and the span reporter. A sampler samples the incoming request to see if the request should be picked for tracing. The trace context propagation library is responsible for tracking a request and propagating the trace context across threads (intra-process) in a process and also across various services (inter-process). The span reporter is responsible for collecting the produced spans and reporting them to a trace ingestion pipeline or a backend storage system.  . Instrumentation . : An application is typically instrumented using an open source tracing library. It consists of 3 components: the sampler, the trace context propagation library, and the span reporter. A sampler samples the incoming request to see if the request should be picked for tracing. The trace context propagation library is responsible for tracking a request and propagating the trace context across threads (intra-process) in a process and also across various services (inter-process). The span reporter is responsible for collecting the produced spans and reporting them to a trace ingestion pipeline or a backend storage system. .   Ingestion:    The trace ingestion pipeline captures the spans emitted by the application, performs any additional sampling (tail-sampling) if needed, and writes the span data to a trace storage system.  . Ingestion: .  The trace ingestion pipeline captures the spans emitted by the application, performs any additional sampling (tail-sampling) if needed, and writes the span data to a trace storage system. .   Storage and Visualization:    A trace storage component stores the traces for a set number of days, and allows searching and visualizing the trace data. The most common visualization layer for traces is a waterfall view.  . Storage and Visualization: .  A trace storage component stores the traces for a set number of days, and allows searching and visualizing the trace data. The most common visualization layer for traces is a waterfall view. .   Zipkin    and    Jaegar    are two of the most popular open source tracing projects that follow the above model.  . Zipkin .  and  . Jaegar .  are two of the most popular open source tracing projects that follow the above model. .  We found the traditional approach to distributed tracing didn’t suit us well.   Starting with systems like    Dapper   , the distributed tracing community has traditionally modelled sequences of events spread across systems as    distributed traces   . Each segment in a trace is called a span. While the core idea of a trace as a sequence of causally related events (a causal graph) is not new, the core idea is often obscured by higher level APIs and internal span formats. The goal of our tracing system is to build and operate on a simplified form of a trace called a causal graph. In this section, we’ll discuss why we need to explicitly call out thinking in causal graphs as more fundamental than thinking in traces.  . We found the traditional approach to distributed tracing didn’t suit us well.  . Starting with systems like  . Dapper . , the distributed tracing community has traditionally modelled sequences of events spread across systems as  . distributed traces . . Each segment in a trace is called a span. While the core idea of a trace as a sequence of causally related events (a causal graph) is not new, the core idea is often obscured by higher level APIs and internal span formats. The goal of our tracing system is to build and operate on a simplified form of a trace called a causal graph. In this section, we’ll discuss why we need to explicitly call out thinking in causal graphs as more fundamental than thinking in traces. .  Currently, tracing APIs like    OpenTracing    are designed to track a request —an operation with a clear start and an end — between backend services written in high-level multithreaded languages. In addition to the simple API to create Spans and report them to a tracing backend, the instrumentation libraries also track a request as it’s processed by various threads and thread pools inside an application.   . Currently, tracing APIs like  . OpenTracing .  are designed to track a request —an operation with a clear start and an end — between backend services written in high-level multithreaded languages. In addition to the simple API to create Spans and report them to a tracing backend, the instrumentation libraries also track a request as it’s processed by various threads and thread pools inside an application.  .  While the current APIs work very well for their intended use cases, using those APIs in contexts where there is no clear start or end for an operation can be confusing or not possible. To trace mobile applications or JavaScript frameworks, whose event loops call into application code (   inversion of control   ), we need complex workarounds in existing APIs that often break the abstractions provided by these libraries.  . While the current APIs work very well for their intended use cases, using those APIs in contexts where there is no clear start or end for an operation can be confusing or not possible. To trace mobile applications or JavaScript frameworks, whose event loops call into application code ( . inversion of control . ), we need complex workarounds in existing APIs that often break the abstractions provided by these libraries. .  In theory, we can use traces to represent a dependency graph by a build tool like npm or Maven, a CI/CD flow in Jenkins, or a complex shell script execution. While these processes don’t follow the same execution model as a grpc application, large parts of their execution can be represented as traces and can benefit from trace analysis tools. Further, we also found that in those use cases there may be multiple request flows (producer-consumer patterns via kafka, goroutines, streaming systems) happening at the same time or we may want to track a single event across multiple request flows. In either case, having a single tracer for the execution of the entire application can be a limiting factor.  . In theory, we can use traces to represent a dependency graph by a build tool like npm or Maven, a CI/CD flow in Jenkins, or a complex shell script execution. While these processes don’t follow the same execution model as a grpc application, large parts of their execution can be represented as traces and can benefit from trace analysis tools. Further, we also found that in those use cases there may be multiple request flows (producer-consumer patterns via kafka, goroutines, streaming systems) happening at the same time or we may want to track a single event across multiple request flows. In either case, having a single tracer for the execution of the entire application can be a limiting factor. .  For use cases where there is no clear notion of a request, a lightweight API is preferable. While a simpler API like the one below pushes the burden of intra/inter-process context propagation to the application developer, it’s easier to use and allows for a more gradual addition of tracing to a non-traditional application.  . For use cases where there is no clear notion of a request, a lightweight API is preferable. While a simpler API like the one below pushes the burden of intra/inter-process context propagation to the application developer, it’s easier to use and allows for a more gradual addition of tracing to a non-traditional application. .  Each tracing system has its own span format (Zipkin’s    span thrift schema   , Jaegar’s    span thrift schema   ). Since they aren’t designed to be consumed by humans directly, they’re usually designed as internal formats and contain a lot of additional metadata in deeply nested structures. These internal implementation details are hidden from users, who interact with these spans via an instrumentation API at creation time and via the trace UI at consumption time.   . Each tracing system has its own span format (Zipkin’s  . span thrift schema . , Jaegar’s  . span thrift schema . ). Since they aren’t designed to be consumed by humans directly, they’re usually designed as internal formats and contain a lot of additional metadata in deeply nested structures. These internal implementation details are hidden from users, who interact with these spans via an instrumentation API at creation time and via the trace UI at consumption time.  .  If we were to produce spans directly from an application, we’d expose our developers to several confusing fields. For example:  . If we were to produce spans directly from an application, we’d expose our developers to several confusing fields. For example: . Both Zipkin and Jaeger spans have annotations/logs that declare zero-duration events that happened during the duration of a span. However, if these events had a duration attached to them, they’d be modeled as child spans. . Jaegar spans don’t have a field for storing the service name. The service name for a span is defined in a process object. . The trace_id on both Zipkin and Jaeger spans are represented as 2 64bit integers, a storage optimization. . These spans are network centric. Zipkin has an endpoint as a field on both Annotations and BinaryAnnotations to capture the network information of the process. Jaegar spans are encapsulated in a Process struct. .  Traces capture very rich causal and contextual data. However, given the complexity of their span formats, these tracing systems don’t allow us to run complex analysis at interactive speeds on raw span data. For example, I’d like to be able to ask a question like “Am I missing my SLA because of slow DB queries on a specific http endpoint?” Even though traces contain this information, users often have to write complex Java programs, a multi-day effort, to answer their questions. Without an easy way to ask powerful analytical questions, users have to settle for the insights that can be gleaned from the trace view. This drastically limits the ability to utilize trace data for triage.  . Traces capture very rich causal and contextual data. However, given the complexity of their span formats, these tracing systems don’t allow us to run complex analysis at interactive speeds on raw span data. For example, I’d like to be able to ask a question like “Am I missing my SLA because of slow DB queries on a specific http endpoint?” Even though traces contain this information, users often have to write complex Java programs, a multi-day effort, to answer their questions. Without an easy way to ask powerful analytical questions, users have to settle for the insights that can be gleaned from the trace view. This drastically limits the ability to utilize trace data for triage. .  Our experience matches    this article    in that the constraints and limitations of tracing systems often don’t justify the effort of implementing tracing in an organization. Even when a tracing system is implemented, it falls by the wayside once the initial excitement wanes. Thus, it’s not surprising that tracing has seen anemic adoption in the industry when compared to technologies like containers, micro services, centralized logging, and service meshes.  . Our experience matches  . this article .  in that the constraints and limitations of tracing systems often don’t justify the effort of implementing tracing in an organization. Even when a tracing system is implemented, it falls by the wayside once the initial excitement wanes. Thus, it’s not surprising that tracing has seen anemic adoption in the industry when compared to technologies like containers, micro services, centralized logging, and service meshes. .  Given the limitations of current tracing systems and our prior experience, we set the following goals to make our effort a success:  . Given the limitations of current tracing systems and our prior experience, we set the following goals to make our effort a success: .  Our tracing system should be useful outside of our backend systems (ex: mobile, desktop clients, and async services).  . Our tracing system should be useful outside of our backend systems (ex: mobile, desktop clients, and async services). .  Simple things should be easy. Hard things should be possible. Provide a simple, lightweight API for non-backend use cases. High-level APIs to capture complex request interactions within and across backend services written in multiple languages.  . Simple things should be easy. Hard things should be possible. Provide a simple, lightweight API for non-backend use cases. High-level APIs to capture complex request interactions within and across backend services written in multiple languages. .  Traces should be useful for triaging incidents and performance regressions in real-time. We felt this would justify the return on investment in a tracing system.  . Traces should be useful for triaging incidents and performance regressions in real-time. We felt this would justify the return on investment in a tracing system. .  Users of the system should be able to query raw span data and run analytics on spans belonging to different traces, or spans within a trace.  . Users of the system should be able to query raw span data and run analytics on spans belonging to different traces, or spans within a trace. .  The tracing system should provide a visual query language for querying traces and an intuitive UI to visualize our traces and analytics.  . The tracing system should provide a visual query language for querying traces and an intuitive UI to visualize our traces and analytics. .  The core idea of a trace is a directed acyclic graph of linked events called spans. However, this is often wrapped in a higher level tracing API and exposed via a trace UI. This hidden structure prevents us from meeting our goals of querying raw trace data and making traces suitable for a wide variety of use cases.   . The core idea of a trace is a directed acyclic graph of linked events called spans. However, this is often wrapped in a higher level tracing API and exposed via a trace UI. This hidden structure prevents us from meeting our goals of querying raw trace data and making traces suitable for a wide variety of use cases.  .  Like a trace, a causal graph is a directed acyclic graph of SpanEvents. A causal graph and a SpanEvent capture the essential aspects of a trace and a span, designed to meet the goals we defined in the previous section. A high-level overview of a causal graph is shown in the image below:  . Like a trace, a causal graph is a directed acyclic graph of SpanEvents. A causal graph and a SpanEvent capture the essential aspects of a trace and a span, designed to meet the goals we defined in the previous section. A high-level overview of a causal graph is shown in the image below: .   .  Figure 1:   A sample causal graph is a graph of SpanEvents. In the above image, each horizontal rectangle represents a span event. The first SpanEvent in a causal graph that has no parent is referred to as the root. A SpanEvent can be a child of another SpanEvent. Each SpanEvent can optionally have Tags attached to it. A child SpanEvent can extend beyond the duration of the parent SpanEvent.  .  A sample causal graph is a graph of SpanEvents. In the above image, each horizontal rectangle represents a span event. The first SpanEvent in a causal graph that has no parent is referred to as the root. A SpanEvent can be a child of another SpanEvent. Each SpanEvent can optionally have Tags attached to it. A child SpanEvent can extend beyond the duration of the parent SpanEvent. .   Facebook’s Canopy    also defines a custom event format to represent spans and traces. We see the causal graph and SpanEvent as an evolution of that format which eliminates the need for the model reconstruction phase of their ingestion pipeline.  . Facebook’s Canopy .  also defines a custom event format to represent spans and traces. We see the causal graph and SpanEvent as an evolution of that format which eliminates the need for the model reconstruction phase of their ingestion pipeline. .  A    Causal Graph    is a graph of    SpanEvent    objects. A    SpanEvent    contains the following fields:  . A  . Causal Graph .  is a graph of  . SpanEvent .  objects. A  . SpanEvent .  contains the following fields: . Id – string –  A unique id representing the event. . Timestamp – long – Start time of operation. . Duration – long – Time to perform an operation. . Parent Id – string –  An id of the parent event. . Trace Id – string –  An id for a trace (for deeply nested span events). . Name – string – Name of the event . Type – string – Type of event usually service name.  . Tags  – Set&lt;String, TypedValue&gt; – Set of key value pairs.  . Span type – string – Type of span. (client, server, producer, consumer, annotation, etc..) .  We think of SpanEvents as a lower-level object than a span in a distributed tracing system since we don’t place any restrictions on the data it contains. Data correctness is ensured by a higher-level tracing API or by user applications. As a result, a SpanEvent can represent any graph of related events. A SpanEvent is to tracing systems what an llvm IR is to a programming language. You can produce raw SpanEvents directly or use higher-level APIs to create them.  . We think of SpanEvents as a lower-level object than a span in a distributed tracing system since we don’t place any restrictions on the data it contains. Data correctness is ensured by a higher-level tracing API or by user applications. As a result, a SpanEvent can represent any graph of related events. A SpanEvent is to tracing systems what an llvm IR is to a programming language. You can produce raw SpanEvents directly or use higher-level APIs to create them. .  The SpanEvent format is designed for human consumption.  . The SpanEvent format is designed for human consumption. .  We embrace a flat structure and eschew deeply nested lists on the SpanEvent itself. Nested lists like annotations are represented as child spans with a specialized span-type tag. This makes filtering out the annotations very easy at query time if needed, without requiring complex SQL queries. While this makes annotations slightly less efficient over the wire, we felt that the ease of production and consumption justified the trade-off.  . We embrace a flat structure and eschew deeply nested lists on the SpanEvent itself. Nested lists like annotations are represented as child spans with a specialized span-type tag. This makes filtering out the annotations very easy at query time if needed, without requiring complex SQL queries. While this makes annotations slightly less efficient over the wire, we felt that the ease of production and consumption justified the trade-off. .  We also use a duration field instead of an end time on spans so we don’t have to subtract the start and end time fields when consuming them.  . We also use a duration field instead of an end time on spans so we don’t have to subtract the start and end time fields when consuming them. .  We allow spans with zero durations and empty IDs. A root span may or may not match the duration of the entire trace. We also encourage traces with only one span since they can be joined with other span events at query time. This also allows the gradual addition of more child spans to those events without any modification to the parent span. In addition, this ensures that queries we write on older data don’t need to be changed when child spans are added.  . We allow spans with zero durations and empty IDs. A root span may or may not match the duration of the entire trace. We also encourage traces with only one span since they can be joined with other span events at query time. This also allows the gradual addition of more child spans to those events without any modification to the parent span. In addition, this ensures that queries we write on older data don’t need to be changed when child spans are added. .  Every piece of metadata on the span is a tag.   . Every piece of metadata on the span is a tag.  .  Instead of optional fields on a span, we opt for tags with reserved keys for well known fields and typed values. This vastly simplifies both span creation and querying. Any information that’s useful for machine consumption, like references to other spans, is defined as a tag or specialized child span.  . Instead of optional fields on a span, we opt for tags with reserved keys for well known fields and typed values. This vastly simplifies both span creation and querying. Any information that’s useful for machine consumption, like references to other spans, is defined as a tag or specialized child span. .  Generating a span event  : SpanEvents can be generated in several ways. The simplest way is to generate a protobuf or JSON object from code and POST it to our backend using a simple function call like    createAndReportSpan   . The same thing can be achieved by invoking a curl command as shown below. This makes adding traces as easy as adding a log statement to our code or invoking an additional command in a shell script. Our mobile apps and shell scripts often use this approach.  . : SpanEvents can be generated in several ways. The simplest way is to generate a protobuf or JSON object from code and POST it to our backend using a simple function call like  . createAndReportSpan . . The same thing can be achieved by invoking a curl command as shown below. This makes adding traces as easy as adding a log statement to our code or invoking an additional command in a shell script. Our mobile apps and shell scripts often use this approach. .  For Hack and JavaScript applications, we’ve implemented OpenTracing compatible tracers, which generate spans in our SpanEvent format. For Go and Java applications, we use the amazing open source instrumentation libraries from    Jaegar go    and    Zipkin brave    respectively. We’ve written adapters to convert spans generated by these libraries into our SpanEvent format.  . For Hack and JavaScript applications, we’ve implemented OpenTracing compatible tracers, which generate spans in our SpanEvent format. For Go and Java applications, we use the amazing open source instrumentation libraries from  . Jaegar go .  and  . Zipkin brave .  respectively. We’ve written adapters to convert spans generated by these libraries into our SpanEvent format. .  Querying Spans:   We chose SQL as the query language for our span data because it’s widely used, flexible, easy to get started with, and supported by a wide variety of storage backends. Further, for users not comfortable with SQL there are several well known visual interfaces that can formulate SQL queries. While verbose, we found SQL to be well suited for expressing both simple and complex analyses of trace data very easily.   .  We chose SQL as the query language for our span data because it’s widely used, flexible, easy to get started with, and supported by a wide variety of storage backends. Further, for users not comfortable with SQL there are several well known visual interfaces that can formulate SQL queries. While verbose, we found SQL to be well suited for expressing both simple and complex analyses of trace data very easily.  .  The architecture of our tracing system is shown in the image below. It consists of the following components:  . The architecture of our tracing system is shown in the image below. It consists of the following components: .   .  Our Hack applications are instrumented with an    OpenTracing    compatible tracer. Our mobile and desktop clients trace their code and emit SpanEvents using either a high-level tracer or a low-level span-creation API. These generated SpanEvents are sent to Wallace over HTTP as JSON or protobuf encoded events.  . Our Hack applications are instrumented with an  . OpenTracing .  compatible tracer. Our mobile and desktop clients trace their code and emit SpanEvents using either a high-level tracer or a low-level span-creation API. These generated SpanEvents are sent to Wallace over HTTP as JSON or protobuf encoded events. .  Wallace is an HTTP server application written in Go that runs independent of the rest of our infrastructure, so we can capture errors even when Slack is down. Wallace validates the span data it receives and forwards those events to Kafka, via Murron. Murron is our inhouse event bus that routes logs, events, and metrics from our applications to various storage backends in our infrastructure.   . Wallace is an HTTP server application written in Go that runs independent of the rest of our infrastructure, so we can capture errors even when Slack is down. Wallace validates the span data it receives and forwards those events to Kafka, via Murron. Murron is our inhouse event bus that routes logs, events, and metrics from our applications to various storage backends in our infrastructure.  .  Our internal Java and Go applications use the open source instrumentation libraries from Zipkin and Jaegar respectively. To capture the spans from these applications, Wallace exposes reporting endpoints for both projects. These endpoints, called trace adapters, translate the reported spans into our SpanEvent format and write them to Wallace.  . Our internal Java and Go applications use the open source instrumentation libraries from Zipkin and Jaegar respectively. To capture the spans from these applications, Wallace exposes reporting endpoints for both projects. These endpoints, called trace adapters, translate the reported spans into our SpanEvent format and write them to Wallace. .  The trace data written to Kafka is processed using a lambda architecture. Murron Consumer is an internal Go application that reads our SpanEvents from Kafka and sends them to both a real-time store and our data warehouse.   . The trace data written to Kafka is processed using a lambda architecture. Murron Consumer is an internal Go application that reads our SpanEvents from Kafka and sends them to both a real-time store and our data warehouse.  .  The real-time store provides almost immediate access to our trace data with an end-to-end latency of less than 5 seconds. We use Honeycomb to visualize this data and run simple analytics, and it plays an important role in making our traces useful for triage.  . The real-time store provides almost immediate access to our trace data with an end-to-end latency of less than 5 seconds. We use Honeycomb to visualize this data and run simple analytics, and it plays an important role in making our traces useful for triage. .  By comparison, our data warehouse lags by 2 hours. Here we use Presto, which supports complex analytical queries over longer time ranges via SQL. Our engineers use this to better understand long-term trends and to answer questions they can’t easily solve with Honeycomb.  . By comparison, our data warehouse lags by 2 hours. Here we use Presto, which supports complex analytical queries over longer time ranges via SQL. Our engineers use this to better understand long-term trends and to answer questions they can’t easily solve with Honeycomb. .  Currently, our tracing pipeline traces a request end to end across our stack, from our clients to our backend services and on to our storage layer. We also have several novel applications of traces like tracking websocket states in our clients and monitoring our Jenkins CI/CD pipeline executions. We’ve even traced our build dependency graphs to understand where our builds are slow.   . Currently, our tracing pipeline traces a request end to end across our stack, from our clients to our backend services and on to our storage layer. We also have several novel applications of traces like tracking websocket states in our clients and monitoring our Jenkins CI/CD pipeline executions. We’ve even traced our build dependency graphs to understand where our builds are slow.  .  Like other services at Slack, the adoption of tracing has followed the    sigmoid curve   . Our tracing pipeline has been in production for over a year now and we trace 1% of all requests from our clients. For some low volume services, we trace 100%. Our current pipeline processes ~310M traces/day and about ~8.5B spans per day, producing around 2Tb of trace data every day.   . Like other services at Slack, the adoption of tracing has followed the  . sigmoid curve . . Our tracing pipeline has been in production for over a year now and we trace 1% of all requests from our clients. For some low volume services, we trace 100%. Our current pipeline processes ~310M traces/day and about ~8.5B spans per day, producing around 2Tb of trace data every day.  .  The ability to query raw traces has unlocked some very powerful insights into our services and customer experience at Slack. Traces have become one of the primary ways to triage and fix performance issues at Slack. We have also made use of trace data to successfully triage production incidents and to estimate the end-user impact of a deployment or incident.   . The ability to query raw traces has unlocked some very powerful insights into our services and customer experience at Slack. Traces have become one of the primary ways to triage and fix performance issues at Slack. We have also made use of trace data to successfully triage production incidents and to estimate the end-user impact of a deployment or incident.  .  With tracing data we can also answer very complex questions about our application that weren’t possible before like:  . With tracing data we can also answer very complex questions about our application that weren’t possible before like: . What is the effect of a given improvement on user-perceived latencies and errors? . Is a particular client request slowdown because of a backend delay?  . Are cross-region queries increasing user perceived latency? By how much?  . What is a representative trace for a given endpoint? . What are the hotkeys for memcache? What endpoints are generating those hot keys? . What  . async tasks .  are run for a given request? How long did those tasks take? .  The causal graph model and the SpanEvent format have allowed us to represent each span as a single row in a table. This simplicity has made querying trace data easy and the value proposition of tracing straightforward. As a result, tracing has seen rapid adoption across the organization. By comparison, Twitter’s trace data was split into six tables which, while probably more efficient, would make querying much harder.  . The causal graph model and the SpanEvent format have allowed us to represent each span as a single row in a table. This simplicity has made querying trace data easy and the value proposition of tracing straightforward. As a result, tracing has seen rapid adoption across the organization. By comparison, Twitter’s trace data was split into six tables which, while probably more efficient, would make querying much harder. .  To conduct targeted investigations for specific customer issues reported to our CE team, the desktop client team added a    special slash command    to our client, which traces every request on the client for the next two minutes. Once this command is run, we trace all requests from the client without sampling them.   . To conduct targeted investigations for specific customer issues reported to our CE team, the desktop client team added a  . special slash command .  to our client, which traces every request on the client for the next two minutes. Once this command is run, we trace all requests from the client without sampling them.  .  Our client team has also started using SpanEvents as a generic event format to report client logs to the backend. Reusing the SpanEvent format has helped us leverage the same real-time ingestion pipeline for traces as well as logs. Using similar field names between the two has also made querying more intuitive.  . Our client team has also started using SpanEvents as a generic event format to report client logs to the backend. Reusing the SpanEvent format has helped us leverage the same real-time ingestion pipeline for traces as well as logs. Using similar field names between the two has also made querying more intuitive. .  After running the system in production for a year, we can confidently say that we’ve met all the goals we set out to achieve with our tracing pipeline.  . After running the system in production for a year, we can confidently say that we’ve met all the goals we set out to achieve with our tracing pipeline. .  In the future, we plan to build on this success by adding a more powerful query language for querying trace data.   . In the future, we plan to build on this success by adding a more powerful query language for querying trace data.  .  For large services, we currently sample 1-2% of requests. We’d like to provide a way to sample all the requests for a cohort of users (a specific team, device or version) over longer periods of time. We’d also like to explore adaptive sampling strategies for sampling spans in a request at different rates.  . For large services, we currently sample 1-2% of requests. We’d like to provide a way to sample all the requests for a cohort of users (a specific team, device or version) over longer periods of time. We’d also like to explore adaptive sampling strategies for sampling spans in a request at different rates. .  Today our traces, logs, and events are islands with a lot of duplication among them. Going forward, we aspire to consolidate this infrastructure into a more unified system based on tracing.  . Today our traces, logs, and events are islands with a lot of duplication among them. Going forward, we aspire to consolidate this infrastructure into a more unified system based on tracing. .  In this post, we’ve described the architecture of our tracing system. It’s designed around a new structure called a SpanEvent which is designed to be both easy to produce and simple to query without sacrificing the core idea of a span.   . In this post, we’ve described the architecture of our tracing system. It’s designed around a new structure called a SpanEvent which is designed to be both easy to produce and simple to query without sacrificing the core idea of a span.  .  Our tracing system has been deployed in production for over a year now and produces ~330M traces and ~8.5B spans per day. Our users find it very easy to produce and consume span data. As a result, we’re able to represent a broad range of application interactions as causal graphs: complex backend microservice interactions, mobile, and desktop client interactions and CI/CD flows in Jenkins jobs to name a few.  . Our tracing system has been deployed in production for over a year now and produces ~330M traces and ~8.5B spans per day. Our users find it very easy to produce and consume span data. As a result, we’re able to represent a broad range of application interactions as causal graphs: complex backend microservice interactions, mobile, and desktop client interactions and CI/CD flows in Jenkins jobs to name a few. .  Using SQL as our query language has given both novice and advanced users the ability to query our tracing data and get the insights they need. As a result, we’ve identified and fixed many performance and reliability problems in our product. Tracing has become one of the primary tools for debugging and fixing performance and reliability issues at Slack.   . Using SQL as our query language has given both novice and advanced users the ability to query our tracing data and get the insights they need. As a result, we’ve identified and fixed many performance and reliability problems in our product. Tracing has become one of the primary tools for debugging and fixing performance and reliability issues at Slack.  .  In the next post of this series, we’ll cover various use cases of tracing at Slack in greater depth.  . In the next post of this series, we’ll cover various use cases of tracing at Slack in greater depth. ", "date": "2020-08-28"}, {"website": "Slack", "title": "Starting an Initiative", "author": ["Josh Cartmell"], "link": "https://slack.engineering/starting-an-initiative/", "abstract": " Early in my career at Slack I was looking for ways to have an impact.  I asked my manager what I could work on to add value to the team.  She said that we wanted to catch bugs earlier in the development process and that Cypress testing might be one way to do this.  I began exploring Cypress and eventually this work developed into a reliability initiative. . Spinning up new initiatives is tricky, and I learned some lessons along the way that might be applicable to people in other organizations. In this post I’ll describe the process I used to find opportunities, build consensus, justify the cost, and ultimately gain support from stakeholders. . You may or may not get a straightforward answer about where to add value like I did.  To identify a place to add value think about a few questions: . You probably don’t need to focus on what the team is doing well.  Look at the remaining three questions to  find an intersection between where the team is struggling and what you are good at or interested in .  Run your ideas by a manager or another engineer to validate that they seem like a good place to start.  .   . In my case the team was looking to uncover bugs earlier; any bug we could proactively find was one a customer wouldn’t stumble on.  Having never done any automation testing before I was interested in learning Cypress.  I had already had some conversations with my manager about exploring Cypress as a way to address the reliability issues.  I now knew where there was potential to add value and began focusing extra cycles on learning about Cypress and how it was currently used. . The next step is gaining the support of stakeholders.  There are always costs to starting an initiative, whether that’s people taken away from other existing work or just the opportunity cost of choosing one direction instead of another. Making it easy for your stakeholders to justify this initiative to  their  own stakeholders makes it that much more likely you’ll get the green light.  Gaining support is not a competition  against  your stakeholders, it’s a collaboration  with  them. . To start, compile data to help explain what you want to do and to show why it’s more valuable than other ways that you could spend your time.  Involve other engineers early.  Reaching a critical mass of people ready and willing to work on something often contributes to making the value apparent.  Realize that change takes time, and you will likely have to persevere before things take off. . In the initiative I worked on I started a public Slack channel where I’d broadcast my findings about Cypress and document small successes along the way.  I would tag stakeholders on messages showing small improvements I made or data about how Cypress could have prevented incidents we encountered.  Over time this led to organic conversation about Cypress in the channel. .   . Documenting what I was doing in a public channel made the initiative possible.  This may be a bit counterintuitive; showing your process can be scary as it lets others see missteps that happen along the way.  That’s not bad though, as it humanizes the entire process and let’s people understand more fully where you or the organization are (and where it would be best to get it to).   A public forum provides a place for your stakeholders to really understand what you want to start , and eventually it becomes a place to drive it forward. . An initiative can’t just be you.  To have sustained impact on the business (and value for your stakeholders) it will need to grow.   Growth can be organic or intentional,  here are some ideas for both. . Organic growth comes from raising awareness: conversations with other engineers, demoing your work to your team, the public documentation of your process.  Any way that you can raise awareness about what you are doing provides an opportunity for organic growth.  In my case other engineers began posting in the public channel I had used to document the process.  They began to associate that channel with Cypress.  That channel provided a jumping in point for other engineers. . There is also a place to actively and intentionally grow.  If you have any sway over your team’s priorities you can begin to suggest or add things that further your goal.  Since you’ve gained the support of key stakeholders you may be able to get them to help you get things on the roadmap.  You can also ask other engineers if they’d like to get involved; if not, you can ask them what they think could be done better or what’s stopping them from getting involved.  By continuously raising awareness you will keep your ideas in the forefront of people’s minds.  One day at a meeting where we reviewed a production issue, we decided a Cypress test could have prevented it.  Another engineer in the meeting volunteered to write the test.  That engineer had been watching the channel, and so through a combination of organic and intentional growth the initiative had grown. . Growth is great, but stakeholders will not be satisfied with that; you also have to demonstrate that the value proposition you convinced them of is viable.  Showing metrics, data, and results sustains the initiative by maintaining the support of stakeholders.  If you’ve said that your idea will reduce bugs, demonstrate this.  Show the number of bugs before and show that now that number is in decline.  If your initiative is supposed to increase user engagement, show that engagement before and after.  Whether or not you know it’s working,  showing quantifiable value is imperative for the initiative to survive.  .   . Eventually an initiative can take on a life of its own; it won’t be “yours” anymore but will become a part of your team’s culture or process.  It can be hard to release control of something you’ve started but change of ownership is a great sign, and indicates that you’ve been successful.  This is a time to reflect on how to continue being involved and what may come next for you.  Can you make it even better?  Can you continue to enable more people to benefit from it?  Is there something else to start focusing on?  In my case I’m still very involved in Cypress testing, but I’ve more recently begun putting time and attention into Typescript.  It’s also something that will help with reliability but it’s a new adventure with new challenges and problems to solve. . There isn’t a formula for starting an initiative but I hope you leave this post with some ideas, some encouragement and maybe some new resolve to start whatever idea you’ve been thinking about.  You can. . Oh, and if you’re interested in starting an initiative at Slack, or working on one of the initiatives we already have going,  we’re hiring ! ", "date": "2020-09-03"}, {"website": "Slack", "title": "The Unsuspecting Beauty & Complexity of Web Forms", "author": ["Frances Coronel"], "link": "https://slack.engineering/the-unsuspecting-beauty-complexity-of-web-forms/", "abstract": "  A form on a website may seem like a pretty trifling thing. A couple of text fields, a submit button, and you’re good to go, right?     #lifeisgood    . A form on a website may seem like a pretty trifling thing. A couple of text fields, a submit button, and you’re good to go, right?  . #lifeisgood .  Wrong! Forms are beautiful and complex creatures.  .  You must dive beneath the surface and explore all the beautiful features that can help create a truly powerful form/lead generator/moneymaker. 📝 💰  . You must dive beneath the surface and explore all the beautiful features that can help create a truly powerful form/lead generator/moneymaker. 📝 💰 .  Together, we’ll explore the   must-add features   Slack uses to build web forms that generate leads for successful deals and reflect on the   impact   they’ve had.  . Together, we’ll explore the  .  Slack uses to build web forms that generate leads for successful deals and reflect on the  .  they’ve had. .  Forms are part of a bigger picture at Slack: a massive pipeline that brings in sales, marketing, customer support, and engineering all under one umbrella.  . Forms are part of a bigger picture at Slack: a massive pipeline that brings in sales, marketing, customer support, and engineering all under one umbrella. .  Hundreds of users submit our forms every week. Hundreds also try to spam us, but we’re pretty good at blocking them (more on security features later). 🙅🏽‍♀️ 🔐  . Hundreds of users submit our forms every week. Hundreds also try to spam us, but we’re pretty good at blocking them (more on security features later). 🙅🏽‍♀️ 🔐 .  After you submit a form on Slack.com, the submission gets routed to the proper channel — Salesforce or Zendesk — depending on the info you provided.  . After you submit a form on Slack.com, the submission gets routed to the proper channel — Salesforce or Zendesk — depending on the info you provided. .  If it arrives in Zendesk, it’s typically a customer support inquiry and our fantastic support team will work to make sure you get an answer ASAP.  . If it arrives in Zendesk, it’s typically a customer support inquiry and our fantastic support team will work to make sure you get an answer ASAP. .  If it’s a sales lead, it gets piped to Salesforce where it’s assigned to a relevant sales rep from across the world (we’re global after all!). We have a dedicated Salesforce team that set us up with custom workflows for these incoming leads. Our sales team will then work with you to discuss how we can make your working life simpler, more pleasant, and more productive. You’ll realize: Wow, Slack is fantastic! How did I live without it? And that’s when you’ll know you’re at the end of the “Contact Sales” funnel. 🎉  . If it’s a sales lead, it gets piped to Salesforce where it’s assigned to a relevant sales rep from across the world (we’re global after all!). We have a dedicated Salesforce team that set us up with custom workflows for these incoming leads. Our sales team will then work with you to discuss how we can make your working life simpler, more pleasant, and more productive. You’ll realize: Wow, Slack is fantastic! How did I live without it? And that’s when you’ll know you’re at the end of the “Contact Sales” funnel. 🎉 .  As part of the Customer Acquisition team, I’ve helped build out the “Contact Sales” form, which has accounted for almost half of all leads from conversational marketing efforts since its launch in December 2018. Similarly, I worked on the partner referral and contact forms, which have driven   millions in revenue   from enterprise clients.  . As part of the Customer Acquisition team, I’ve helped build out the “Contact Sales” form, which has accounted for almost half of all leads from conversational marketing efforts since its launch in December 2018. Similarly, I worked on the partner referral and contact forms, which have driven  . millions in revenue .  from enterprise clients. .  From these efforts, I’ve become the official form expert on my team and want to pay it forward by sharing my knowledge.  . From these efforts, I’ve become the official form expert on my team and want to pay it forward by sharing my knowledge. .  Let’s dive into some must-have features for any web form, especially those meant for lead generation:  . Let’s dive into some must-have features for any web form, especially those meant for lead generation: . HTML Attributes . Data Analytics . Success State . i18n &amp; Localization . Field Validation . Rate Limiting . Custom Routing .   According to MDN   , the &lt;input&gt; element is one of the most powerful and sophisticated in HTML due to the sheer number of input types and attributes. Much like Netflix, it looks simple but there’s a lot going on in the background. 🎥  . According to MDN . , the &lt;input&gt; element is one of the most powerful and sophisticated in HTML due to the sheer number of input types and attributes. Much like Netflix, it looks simple but there’s a lot going on in the background. 🎥 .  Here’s a sample input field we use to capture a potential customers email address in the Contact Sales form:  . Here’s a sample input field we use to capture a potential customers email address in the Contact Sales form: .  Let’s explore each of the ten attributes I’ve configured here to better understand them:  . Let’s explore each of the ten attributes I’ve configured here to better understand them: . class — We use  . Spacesuit as our design system .  on Slack.com, which uses classes to define how elements are presented. . required —  Some fields are always going to be needed, others not so much. If a field is marked as required, the user can’t submit the form unless they fill out the field. . type — This is the type of element to display and how the input behaves depends heavily on this. There are  . many options .  and it’s essential to choose the most relevant one based on the type of data you’re trying to accept. Only use text if it makes the most sense. . id —We use this to query the field with JavaScript. Remember, every field should have a unique id! . name — This attribute acts as the key for the field data we send to the server. Our  . Hack backend .  will use it to find and validate the field so it can be sent to the proper channel via the Salesforce &amp; Zendesk APIs. . value — This attribute lets us prefill input fields when we know something about the user already, like their email address if they’re already signed into Slack. . autocomplete — Okay, so maybe they’re not logged in. We can still help users fill out the form faster by using the autocomplete feature built into the browser. You can test this out in Chrome using  . . . placeholder — This is what we show by default to help the user understand what should be submitted, like  . example@acme.com . . . data-value-missing — This is where you can define a custom error message when the value provided is missing. That way, even if you don’t have any frontend validation setup with JS, you can customize your error messages through HTML5. If it’s not a required field, this error message won’t appear even if configured because it’s not a blocker to submitting the form. . data-value-invalid — Similar to the previous attribute, this customizes the error messaging if the input is invalid. You can do this via JS, but HTML5 automatically does some of this based on the type of attribute you defined earlier. Since we set the field type attribute as an email, any other kind of data like a URL will be marked as invalid. .  On your team, you may have someone whose job is to use data collected from your forms to ideate on strategies to better the user experience and investigate potential leaks in the sales funnel.  . On your team, you may have someone whose job is to use data collected from your forms to ideate on strategies to better the user experience and investigate potential leaks in the sales funnel. .  If you dive into a subset of form submissions, you can look at the data and better understand the current user experience. For example, let’s say that of every 100 users that submit your form: 80 succeed, 10 fail to fill out the company size field and 10 abandon the form mid-way.  . If you dive into a subset of form submissions, you can look at the data and better understand the current user experience. For example, let’s say that of every 100 users that submit your form: 80 succeed, 10 fail to fill out the company size field and 10 abandon the form mid-way. .  Of the 80 that succeed, only 20 submissions end up as qualified leads for sales, and 2 of those qualified leads become closed deals, indicating that the conversion rate from all submissions is about 2% or 10% if looking at just qualified leads.  . Of the 80 that succeed, only 20 submissions end up as qualified leads for sales, and 2 of those qualified leads become closed deals, indicating that the conversion rate from all submissions is about 2% or 10% if looking at just qualified leads. .  The only way to have a high-level overview of the pipeline is by configuring all your form fields with your company’s data analytics platform of choice. This will pipe information back to you as the customer is filling in the form, not just when they’ve submitted it. At Slack, we have our own data warehouse that contains a historical record of events like these and supplemental information we’ve optimized for complex analytics over large amounts of data.   . The only way to have a high-level overview of the pipeline is by configuring all your form fields with your company’s data analytics platform of choice. This will pipe information back to you as the customer is filling in the form, not just when they’ve submitted it. At Slack, we have our own data warehouse that contains a historical record of events like these and supplemental information we’ve optimized for complex analytics over large amounts of data.  .  The failed submissions can be analyzed, and we can draw conclusions to try and close the gap, such as:  . The failed submissions can be analyzed, and we can draw conclusions to try and close the gap, such as: . Having more intuitive error messages . Tweaking the design to make fields easier to fill out . Dropping a required field if many users struggle with it . Refining customer support or sales workflows for faster response times .  Imagine submitting a form but nothing happens. That would drive me crazy! Maybe you’d try spamming the submit button ten times before giving up and forgetting about it. That’s why it’s crucial to have a success state that makes it clear to the user that we got their submission and they can move on with their day.  . Imagine submitting a form but nothing happens. That would drive me crazy! Maybe you’d try spamming the submit button ten times before giving up and forgetting about it. That’s why it’s crucial to have a success state that makes it clear to the user that we got their submission and they can move on with their day. .  When someone fills out the sales form, we redirect them to a separate thank you page and even include some customized text if they have cookies enabled.  . When someone fills out the sales form, we redirect them to a separate thank you page and even include some customized text if they have cookies enabled. .  In the future, we plan to take this one step further and send customized emails to each lead after they submit.  . In the future, we plan to take this one step further and send customized emails to each lead after they submit. .  Slack is global, which means our forms need to reflect that with dedicated localization practices.   . Slack is global, which means our forms need to reflect that with dedicated localization practices.  .   Internationalization   , or “i18n”, is the process of building Slack.com to support multiple languages.    Localization    is the process of adapting Slack.com for a specific region or language by translating text or adding locale-specific content. Internationalization enables localization to happen.  . Internationalization . , or “i18n”, is the process of building Slack.com to support multiple languages.  . Localization .  is the process of adapting Slack.com for a specific region or language by translating text or adding locale-specific content. Internationalization enables localization to happen. .  Because i18n is built into our web application, I can create just one form that will then be translated into all nine languages we support. We translate the text across the various locales that Slack.com supports and adapt our imagery and layout for specific regions:  . Because i18n is built into our web application, I can create just one form that will then be translated into all nine languages we support. We translate the text across the various locales that Slack.com supports and adapt our imagery and layout for specific regions: . English (US):  . https://slack.com/contact-sales .   . Spanish (Latin America):  . https://slack.com/intl/es-la/contact-sales .   . Portuguese:  . https://slack.com/intl/pt-br/contact-sales .   . German:  . https://slack.com/intl/de-de/contact-sales .   . Spanish (Spain):  . https://slack.com/intl/es-es/contact-sales .   . French:  . https://slack.com/intl/fr-fr/contact-sales .   . English (UK):  . https://slack.com/intl/en-gb/contact-sales .   . Japanese:  . https://slack.com/intl/ja-jp/contact-sales .   . English (India):  . https://slack.com/intl/en-in/contact-sales .   .  For example, in Japan, we show our Japanese customer logos and also switch the first name and last name fields so that the last name or family name comes first, as is customary in Japan.  . For example, in Japan, we show our Japanese customer logos and also switch the first name and last name fields so that the last name or family name comes first, as is customary in Japan. .  You’ve probably interacted with at least one form today already. Maybe it was a simple form like a user login or a more complex form like “Become a Slack Partner,” which is the most extensive form on Slack.com with anywhere from 18 to 26 required fields. When you were filling out the form, maybe you got messages like this “This field is required,” “Please enter a valid email,” “Please select at least one region,” “Please agree to the Terms and Conditions”, etc.  . You’ve probably interacted with at least one form today already. Maybe it was a simple form like a user login or a more complex form like “Become a Slack Partner,” which is the most extensive form on Slack.com with anywhere from 18 to 26 required fields. When you were filling out the form, maybe you got messages like this “This field is required,” “Please enter a valid email,” “Please select at least one region,” “Please agree to the Terms and Conditions”, etc. .  Isn’t it frustrating when forms don’t give immediate feedback? We’ve all been through this: you carefully fill in every field and click submit only to be thrown back to the start. Thankfully, we can add instant field validation so users don’t have to submit before they see potential error messages.  . Isn’t it frustrating when forms don’t give immediate feedback? We’ve all been through this: you carefully fill in every field and click submit only to be thrown back to the start. Thankfully, we can add instant field validation so users don’t have to submit before they see potential error messages. .  All these forms pass data to a server, but before that data can be submitted it’s vital to ensure all the necessary fields are filled out and in the correct format. Client-side or frontend form validation comes in very handy here.  . All these forms pass data to a server, but before that data can be submitted it’s vital to ensure all the necessary fields are filled out and in the correct format. Client-side or frontend form validation comes in very handy here. .  By setting up form validation, your users will have a less stressful experience: the form will catch invalid data and let them know right then and there. If we wait until the data reaches our server before letting them know, there’s a considerable delay in that feedback. There’s the time it takes for a round trip to your server, and the possibility that your user has scrolled past the field with the problem.  . By setting up form validation, your users will have a less stressful experience: the form will catch invalid data and let them know right then and there. If we wait until the data reaches our server before letting them know, there’s a considerable delay in that feedback. There’s the time it takes for a round trip to your server, and the possibility that your user has scrolled past the field with the problem. .  The worst-case scenario is when you try to fill out a form with no validation, and there’s no error to tell you what you’ve done wrong. There are two different types of client-side validation you can set up: built-in form through HTML5 (like the kind we covered above) and JavaScript.  . The worst-case scenario is when you try to fill out a form with no validation, and there’s no error to tell you what you’ve done wrong. There are two different types of client-side validation you can set up: built-in form through HTML5 (like the kind we covered above) and JavaScript. .  For security measures, you shouldn’t rely on client-side validation alone. Server-side validation is important since malicious users can easily disable JavaScript and bypass the client.  . For security measures, you shouldn’t rely on client-side validation alone. Server-side validation is important since malicious users can easily disable JavaScript and bypass the client. .  Things get complicated when you have to set up conditional validation, meaning you configure the JavaScript for the form to require specific fields only when others are selected. To handle this gracefully it’s ideal to have clean, well documented, and    DRY    HTML code.   . Things get complicated when you have to set up conditional validation, meaning you configure the JavaScript for the form to require specific fields only when others are selected. To handle this gracefully it’s ideal to have clean, well documented, and  . DRY .  HTML code.  .  Rate limiting controls the amount of traffic to or from a network. We configure it so that any user who tries to fill out our forms more than a few times per minute from the same IP address will have to fill out a    reCAPTCHA   .   . Rate limiting controls the amount of traffic to or from a network. We configure it so that any user who tries to fill out our forms more than a few times per minute from the same IP address will have to fill out a  . reCAPTCHA . .  .  reCAPTCHA uses an advanced risk analysis engine and adaptive challenges to keep automated software from engaging in abusive activities.  . reCAPTCHA uses an advanced risk analysis engine and adaptive challenges to keep automated software from engaging in abusive activities. .  Of course, all our form submissions pipe directly into a Slack channel (because duh), but that’s just child’s play! With the Contact Sales form, we go a step further and pipe the submission to Zendesk or Salesforce too, depending on the input we’ve received from the user.  . Of course, all our form submissions pipe directly into a Slack channel (because duh), but that’s just child’s play! With the Contact Sales form, we go a step further and pipe the submission to Zendesk or Salesforce too, depending on the input we’ve received from the user. .  We also include relevant metadata such as their Slack user ID and team ID if a user is already logged in, Zendesk tags, the particular ad campaign they came from if applicable, and anything else we might find useful. This extra information helps our support team have better context as to where the user is coming from which leads to requests being solved faster.  . We also include relevant metadata such as their Slack user ID and team ID if a user is already logged in, Zendesk tags, the particular ad campaign they came from if applicable, and anything else we might find useful. This extra information helps our support team have better context as to where the user is coming from which leads to requests being solved faster. .  Okay, cool, so you’ve shipped a robust form. Now what? It turns out — a lot! This year, we had a large health company fighting COVID-19 upgrade to Slack’s plus plan in a multi-million dollar deal. Guess where they reached out first? The Contact Sales form!  . Okay, cool, so you’ve shipped a robust form. Now what? It turns out — a lot! This year, we had a large health company fighting COVID-19 upgrade to Slack’s plus plan in a multi-million dollar deal. Guess where they reached out first? The Contact Sales form! .  This form attracts ~20K visits in any given month and accounts for 25% to 35% of all our leads. Some months, it accounts for almost 50%!  . This form attracts ~20K visits in any given month and accounts for 25% to 35% of all our leads. Some months, it accounts for almost 50%! .  Of course, the numbers go both ways. It’s not all sunshine and rainbows. What happens if a user is struggling to submit the form or it’s broken even for a minute? How many hundreds of potential leads, and therefore thousands of potential dollars, have we lost? 😳  . Of course, the numbers go both ways. It’s not all sunshine and rainbows. What happens if a user is struggling to submit the form or it’s broken even for a minute? How many hundreds of potential leads, and therefore thousands of potential dollars, have we lost? 😳 .  Clearly we need to make sure our forms are bulletproof before they go live. How do we do this?  . Clearly we need to make sure our forms are bulletproof before they go live. How do we do this? .  The best way to quell anxiety as a developer is to test, test, and test some more. Become good friends with your QA team — shout out to Ilaria and Natalie! After all, they’re the ones with eagle eyes and they’ll be able to help you catch any nasty bugs before they hit production. Eat those bugs up! 🐛 🦅  . The best way to quell anxiety as a developer is to test, test, and test some more. Become good friends with your QA team — shout out to Ilaria and Natalie! After all, they’re the ones with eagle eyes and they’ll be able to help you catch any nasty bugs before they hit production. Eat those bugs up! 🐛 🦅 .  If you don’t have a dedicated QA team, you can create a checklist    like this one    to perform manual testing yourself.  . If you don’t have a dedicated QA team, you can create a checklist  . like this one .  to perform manual testing yourself. .  With great power comes great responsibility,   so after you launch your form it’s time to focus on maintaining it and making small improvements that can have a massive impact.  .  so after you launch your form it’s time to focus on maintaining it and making small improvements that can have a massive impact. .  Don’t just deploy your new form, celebrate, and be done with it. Pump your fist in the air, dance to Drake’s latest hit, then immediately set up a triage channel so relevant stakeholders from across support, sales, marketing, and other relevant departments can provide feedback, flag potential bugs, and ideate on how to make improvements.  . Don’t just deploy your new form, celebrate, and be done with it. Pump your fist in the air, dance to Drake’s latest hit, then immediately set up a triage channel so relevant stakeholders from across support, sales, marketing, and other relevant departments can provide feedback, flag potential bugs, and ideate on how to make improvements. .  For example, if a user is reaching out to us with a product question, we route them to our Customer Success (CS) team. This team handles hundreds of requests via Zendesk every day, so any useful info we can provide them about a form submission can significantly decrease back and forth as they follow up with a user.  We got a request to include a tag in the submission to Zendesk if the user didn’t provide enough additional details. This allowed the CS team to create an automated workflow to help get back to folks faster.  . For example, if a user is reaching out to us with a product question, we route them to our Customer Success (CS) team. This team handles hundreds of requests via Zendesk every day, so any useful info we can provide them about a form submission can significantly decrease back and forth as they follow up with a user.  We got a request to include a tag in the submission to Zendesk if the user didn’t provide enough additional details. This allowed the CS team to create an automated workflow to help get back to folks faster. .  As you maintain the form, you can also try out different things through A/B tests to see what yields the best result. After all, an improvement that makes it easier for 10% of users to submit the form faster can lead to a 10% increase in contract value generated.  . As you maintain the form, you can also try out different things through A/B tests to see what yields the best result. After all, an improvement that makes it easier for 10% of users to submit the form faster can lead to a 10% increase in contract value generated. .  Below is an experiment we ran in    Optimizely    (a leading progressive delivery &amp; experimentation platform) to test if calling out required fields with asterisks will help with the conversion rate. Turns out, it does so we’ll be making that change soon!  . Below is an experiment we ran in  . Optimizely .  (a leading progressive delivery &amp; experimentation platform) to test if calling out required fields with asterisks will help with the conversion rate. Turns out, it does so we’ll be making that change soon! .  So go forth and build your forms, friends.   . So go forth and build your forms, friends.  . In summary, when building million-dollar forms, you want to make sure you: .  After all, there is no gold pot at the end of the rainbow unless you made the rainbow.   📝 💰 🌈  . After all, there is no gold pot at the end of the rainbow unless you made the rainbow.  . 📝 💰 🌈 .   .  Kudos to Rowan Oulton for providing great editorial feedback, Terra Spitzner for her lovely design skills, and Natalie Stormann for her QA expertise. 💛  . Kudos to Rowan Oulton for providing great editorial feedback, Terra Spitzner for her lovely design skills, and Natalie Stormann for her QA expertise. 💛 . And as an FYI, September 15th marks the start of Latinx &amp; Hispanic Heritage Month in the US so feel free to follow Slack on social media to get updates from Fuego, Slack’s employee resource group that strives to provide visibility, community, and a sense of belonging for the Latinx and Hispanic employees and allies in and outside of Slack. 🔥 .  \t\t\t\tInterested in taking on interesting projects, making people’s work lives easier, or just building some pretty cool forms? We’re hiring! 💼\t\t\t\t Apply now  \t\t\t ", "date": "2020-09-15"}, {"website": "Slack", "title": "Building the Next Evolution of Cloud Networks at Slack", "author": ["Archie Gunasekara"], "link": "https://slack.engineering/building-the-next-evolution-of-cloud-networks-at-slack/", "abstract": " At Slack, we’ve gone through an evolution of our AWS infrastructure from the early days of running a few hand-built EC2 instances, all the way to provisioning thousands of EC2s instances across multiple AWS regions, using the latest AWS services to build reliable and scalable infrastructure. One of the pain points inherited from the early days caused us to spin up a brand-new network architecture redesign project, called Whitecastle. In this post, we’ll discuss our design decisions and our technological choices made along the way. .   . In recent times, Slack has become somewhat of a household name, though its origin is more humble: the messaging service began as the internal communication tool when Slack was Tiny Speck, a game company building Glitch. Once they realised the potential of this tool, it was officially launched as Slack in 2014. It was simpler back then and there were only a few customers using it. As time went by, Slack started to evolve, and so did its infrastructure, allowing us to scale from tens of users to millions. . If you ever wondered what the infrastructure behind the original Slack looked like, it was all in a single AWS account. There were very few AWS instances in it and they were all built manually. Eventually the manual processes were replaced by scripting, and then by a Chef configuration management system. Terraform was also introduced to manage our AWS infrastructure. . As our customer base grew and the tool evolved, we developed more services and built more infrastructure as needed. However, everything we built still lived in one big AWS account. This is when our troubles started. Having all our infrastructure in a single AWS account led to AWS rate-limiting issues, cost-separation issues, and general confusion for our internal engineering service teams. To overcome this hurdle, we introduced the concept of child accounts. Now the service teams could request their own AWS accounts and could even peer their VPCs with each other when services needed to talk to other services that lived in a different AWS account. This was great for a while, but as a company we continued to grow. Having hundreds of AWS accounts became a nightmare to manage when it came to CIDR ranges and IP spaces, because the mis-management of CIDR ranges meant that we couldn’t peer VPCs with overlapping CIDR ranges. This led to a lot of administrative overhead. .   . The Slack Cloud Engineering team had to devise ways to simplify this problem and make our lives easier. We looked at  AWS shared VPCs . This was a feature AWS introduced at the end of 2018, allowing one AWS account to create VPCs and subnets, and then share those VPC and subnets to other AWS accounts. . With this concept we could create VPCs in one AWS account and share them across multiple accounts. Other accounts are able to build resources on these shared VPCs, but these resources are not visible outside that specific child account. The rate limit of their host AWS account applies to these resources as well. This solved our earlier issue of constantly hitting AWS rate limits due to having all our resources in one AWS account. This approach seemed really attractive to our Cloud Engineering team, as we could manage the IP space, build VPCs, and share them with our child account owners. Then, without having to worry about managing any of the overhead of setting up VPCs, route tables, or network access lists, teams were able to utilize these VPCs and build their resources on top of them. .   . With all these benefits in mind, we kicked off a project to build this out. We decided to call this project Whitecastle, because the lead engineer of the project conceived the idea while visiting the White Castle burger chain. .   . So what does this all look like? We created three separate accounts, one for each development environment here at Slack (sandbox, dev and prod). Service teams use the sandbox environment mostly for experimentation purposes while they use the dev environment for all pre-production testing. The prod environment is used for all customer facing workloads. Therefore, the Cloud Engineering team must treat the three environments as production since we are bound by SLOs to our internal customers. We also built a separate sandbox network called Bellwether just for those of us in the Cloud Engineering team to use for experimentation. This environment is not accessible to Slack’s service teams so we can test our changes, experiment, or break things without affecting anyone outside the Cloud Engineering team. . In each of these networking accounts in the us-east-1 region, we created a VPC with two /16 CIDR ranges (the biggest CIDR range AWS allows) and shared these VPCs with all our child accounts. Two /16 CIDR ranges provide over 130,000 IP addresses per region – more than enough to serve our current capacity and projected capacity for the foreseeable future. As we approach capacity, we are easily able to add more /16 CIDRs to these VPCs. . Once the build-out was completed for the us-east-1 region, we repeated the build-out for all other regions that our current AWS infrastructure operates in. . Our next challenge was to get all these remote regions to communicate with each other successfully, since we run customer facing workloads such as image caching for fast response times in several regions. However, the regions need connectivity between each other and our main region us-east-1 for various operations such as service discovery. For this we looked into another AWS feature that was introduced in late 2019 called Transit Gateway  Inter-Region Peering . As a start, we created a Transit Gateway in each region and attached it to the region’s local VPC. Then a given Transit Gateway was peered with another Transit Gateway to build inter-region connectivity. .   . With AWS Transit Gateway Inter-Region Peering, one Transit Gateway must request the peering connection and the other Transit Gateway must accept it. Then routes must be added to each Transit Gateway’s route tables to be able to route traffic between each other. . The tricky part was figuring out how to choose which Transit Gateway to request the peering connection from, and which Transit Gateway to accept it, because we had to automate all this using Terraform. We decided to use the md5sum of the Transit Gateway ID given they are unique. During the Terraform run, we calculate the md5sum of all Transit Gateways. Then we take the last 16 characters of the md5sum and convert it into an integer. We called this number the Transit Gateway priority.  If a given Transit Gateway has a greater priority number to another Transit Gateway, we choose that to be the requester and the other to be the accepter. . At the same time, Terraform updates the route tables of each Transit Gateway to be able to route traffic between each other. When we allocated CIDR ranges to our regions, we ensured all CIDRs fall inside the summary route of 10.0.0.0/8. To make sure each VPC sends any non-local traffic to the Transit Gateway that matches this summary route, we created a default route 10.0.0.0/8 in each VPC’s route table pointing at its local Transit Gateway so as to keep the VPC route tables relatively simple. .   . There are many routes and many VPCs that need to talk to each other. A single bad route will render a service unable to talk to another service, and things can go bad very, very quickly. To remedy this, we decided to include real-time network testing. We built a very simple Go application called Whitecastle Network Tester, and then built an autoscaling group with a single instance running this application in each of our VPCs. This application registers its VPC ID, environment, and the IP of the instance in a DynamoDB table. We call this our Whitecastle Network Tester service discovery table. . The Whitecastle Network Tester application does two things: . We display this CloudWatch data source on a Grafana dashboard to monitor and alert based on any failing network test results. .   . Our Whitecastle Network Tester also validates the build-out of new regions by adding test cases pre-build-out and then creates connectivity between the new/old regions and the legacy VPCs. With this approach, we can see any tests failing to start, but then moving to a pass state as the build-out completes. . The current process of applying any changes to the Whitecastle network is to initially make changes and test them in the sandbox environment, then deploy the same changes to the dev environment. Once we are confident with our changes, we follow through to the prod environment. Having real-time network testing allows us to catch any issues with our configuration before these changes make it to the production environments. The Whitecastle Network Tester helped tremendously in developing a network mesh Terraform module, as we could validate the changes deployed by each Terraform apply command to network routes and resources. .   . Once we had this all built, the next step was to get our internal customer teams to migrate their services onto the Whitecastle network and into child AWS accounts. However, big-bang migrations are cumbersome and risky and it was difficult to convince Cloud Engineering’s internal customer service teams to take the leap initially. . If we offered incremental migration to the new network architecture, then there would be much more buy-in from service teams, so we engineered a simple solution for that, by sharing the new Transit Gateway to child accounts and attaching them to the region’s legacy VPC. With this approach, if a portion of the services moved onto the new VPCs, they’d still be able to communicate with parts of the system located in the legacy VPCs. .   . Although the sandbox environment is not shown in the above diagram, it works in a similar way to the dev environment. . All in all, we found a lot of success in building this new network architecture. There were many challenges along the way, but we managed to work through them and learn a great deal. Once we started running production workloads in Whitecastle, we stopped making ad hoc changes to all three environments that could impact our internal customers. It also meant we adopted a reliable and safe process to rollout changes. Having a sandbox environment dedicated to the Cloud Engineering team to test concepts and experiment helped catch any issues before pushing changes out to customer-facing environments. Tools like Whitecastle Network Tester give us confidence in rolling out our changes and also alerts us promptly if any problems slip through our testing. All these safety measures give us the ability to provide extremely satisfying SLAs such as high reliability and availability to the Cloud Engineering team’s internal customers. Having said all that, we are still looking for ways to improve our platform by adopting the latest technologies available to us. As an example, we will be looking at replacing all our inter-region route propagation logic once AWS releases BGP route propagation support for Transit Gateway peerings. . We are certain there will be many more challenges to overcome and lessons to learn as we move more services onto the new network architecture. If this work seems interesting to you, Slack is always on the lookout for new engineers to help us build great new things, and  you can join our ever growing team . ", "date": "2020-09-21"}, {"website": "Slack", "title": "Bridging the Gap Between Slack and Email Users", "author": ["Madhu Balakrishna"], "link": "https://slack.engineering/bridging-the-gap-between-slack-and-email-users/", "abstract": "  Slack brings the right people, information, and tools together to get work done. Normally, team members can invite one another to Slack via email invitation. Invited users can accept the invitation to activate their account, but if they don’t, then their team members can’t collaborate with them.  . Slack brings the right people, information, and tools together to get work done. Normally, team members can invite one another to Slack via email invitation. Invited users can accept the invitation to activate their account, but if they don’t, then their team members can’t collaborate with them. .  To solve this problem we created Email Bridge: with invited users showing up instantly in Slack to start getting notifications via email. They can even respond to Slack messages using email alone.  . To solve this problem we created Email Bridge: with invited users showing up instantly in Slack to start getting notifications via email. They can even respond to Slack messages using email alone. .  Today, Email Bridge helps some of our largest enterprise customers to expand and onboard all of their employees quickly and efficiently. With this feature, it’s now possible to add co-workers to appropriate channels and groups    before    they activate their Slack account. Some customers use apps within Slack to DM their new team members with onboarding information, which also helps them get up to speed more quickly. When invited people    do    join their workspace, they have all the information and conversations that happened via email immediately available to them in Slack.  . Today, Email Bridge helps some of our largest enterprise customers to expand and onboard all of their employees quickly and efficiently. With this feature, it’s now possible to add co-workers to appropriate channels and groups  . before .  they activate their Slack account. Some customers use apps within Slack to DM their new team members with onboarding information, which also helps them get up to speed more quickly. When invited people  . do .  join their workspace, they have all the information and conversations that happened via email immediately available to them in Slack. .  In this post we’ll explore the architecture of Email Bridge and how it works, along with lessons learned while shipping the feature.    . In this post we’ll explore the architecture of Email Bridge and how it works, along with lessons learned while shipping the feature.   .  There are two main ways to invite a user to a Slack workspace.  . There are two main ways to invite a user to a Slack workspace. .  1. Manual invitations:   Small companies tend to use    manual invitations    to add users to the workspace.  .  Small companies tend to use  . manual invitations .  to add users to the workspace. .  Behind the scenes, an   invitation   is created for the email address with   Name  ,   Team   to join and user   Role   (Full Member, Single-Channel Guest or Multi-Channel Guest) along with a list of   channels   for them to join. We send an email to the address immediately. We also send a reminder email after 24 hours if the invitation is not accepted.  . Behind the scenes, an  . invitation .  is created for the email address with  . Name . ,  . Team .  to join and user  . Role .  (Full Member, Single-Channel Guest or Multi-Channel Guest) along with a list of  . channels .  for them to join. We send an email to the address immediately. We also send a reminder email after 24 hours if the invitation is not accepted. .  Once the invitation is accepted. We then create a   User   entity under the   Team   with the specified   Role  . We also add the new user to all the specified   channels   and mark the invitation as accepted.  . Once the invitation is accepted. We then create a  . User .  entity under the  . Team .  with the specified  . Role . . We also add the new user to all the specified  . channels .  and mark the invitation as accepted. .  2. System for Cross-domain Identity Management (SCIM) protocol:   Most of our large customers use an identity management system (such as Okta) to automatically add users to the organization.  . Most of our large customers use an identity management system (such as Okta) to automatically add users to the organization. .  With this, a new   User   entity is created with every request and moves the user to a  pre-provisioned    state, which essentially means that these users can not be discovered in Slack. Since there is no invitation, no email is sent to the user.   . With this, a new  . User .  entity is created with every request and moves the user to a  pre-provisioned  . state, which essentially means that these users can not be discovered in Slack. Since there is no invitation, no email is sent to the user.  .  Once the user joins using their Identity provider, we mark the user as    active    (we remove them from the pre-provisioned state) and make them discoverable in Slack search.  . Once the user joins using their Identity provider, we mark the user as  . active  . (we remove them from the pre-provisioned state) and make them discoverable in Slack search. .   .  For Slack Email Bridge, we built an interoperability layer to exchange emails and Slack messages. This consisted of three modules.  . For Slack Email Bridge, we built an interoperability layer to exchange emails and Slack messages. This consisted of three modules. .  Since we know that the invitation will eventually be accepted, we created a   User   entity with a flag that indicates that the user has not accepted the invitation.  . Since we know that the invitation will eventually be accepted, we created a  . User .  entity with a flag that indicates that the user has not accepted the invitation. .  With this change, any new invitation to join a Slack workspace creates a new user with a type   Invited member  .  We also mark all the users added via the SCIM protocol as   Invited members  . Since these users are not active, we exclude them from    our fair billing    calculations. Once the user accepts the invitation, we simply unset the   Invited member   flag.  . With this change, any new invitation to join a Slack workspace creates a new user with a type  . Invited member . .  We also mark all the users added via the SCIM protocol as  . Invited members . . Since these users are not active, we exclude them from  . our fair billing .  calculations. Once the user accepts the invitation, we simply unset the  . Invited member .  flag. .  For the client side, we added banners and messages under user information to indicate that they are in an invited   state  .  . For the client side, we added banners and messages under user information to indicate that they are in an invited .  state . . .  We made   Invited member  s visible in search and during    @mention   , so they can be added to channels and discussions.  . We made  . Invited member . s visible in search and during  . @mention . , so they can be added to channels and discussions. .  The primary responsibility of this service is to send email notifications to the   Invited member  s, and when they reply to the email, route the message to the appropriate Slack channel.  . The primary responsibility of this service is to send email notifications to the  . Invited member . s, and when they reply to the email, route the message to the appropriate Slack channel. .   .  Since the conversations in Slack can flow quickly, we don’t want to overwhelm users with one email per message. So we wait for a few seconds and batch messages within a single email body. We add a unique    Reply-To    address to the email header in the format    &lt;unique-id&gt;@&lt;domain&gt;.slack-mail.com ;   which is used to trace back the source of the message on reply. We send the email to the user via AWS Simple Email Service (SES).  . Since the conversations in Slack can flow quickly, we don’t want to overwhelm users with one email per message. So we wait for a few seconds and batch messages within a single email body. We add a unique  . Reply-To .  address to the email header in the format  .  &lt;unique-id&gt;@&lt;domain&gt;.slack-mail.com ; .  which is used to trace back the source of the message on reply. We send the email to the user via AWS Simple Email Service (SES). .  Users can reply to each notification message using the   Reply   call to action button in the email or simply use their email client’s reply button. This will open an email compose window where users can type in their message.   . Users can reply to each notification message using the  . Reply .  call to action button in the email or simply use their email client’s reply button. This will open an email compose window where users can type in their message.  .   .  Sent messages are received by    &lt;unique-id&gt;@&lt;domain&gt;.slack-mail.com     address via AWS SES, which sends a notification to Slack via webhook using Simple Notification Service (SNS) with a payload to identify the message.  . Sent messages are received by  . &lt;unique-id&gt;@&lt;domain&gt;.slack-mail.com .   address via AWS SES, which sends a notification to Slack via webhook using Simple Notification Service (SNS) with a payload to identify the message. .  The email service securely validates the authenticity of every incoming email. It also sanitizes the email body and converts it to a Slack message, preserving most of the formatting. The email service then posts a message on behalf of the invited user to the correct channel or thread.  . The email service securely validates the authenticity of every incoming email. It also sanitizes the email body and converts it to a Slack message, preserving most of the formatting. The email service then posts a message on behalf of the invited user to the correct channel or thread. .  After completion, here is our architecture for invitation and email notification.   . After completion, here is our architecture for invitation and email notification.  .  Slack’s Grid user flow has more complex business logic, with different enterprise-level preferences. This required a careful and gradual rollout strategy to ensure we covered all the cases. We spent more than two months releasing the feature to our Free, Standard, and Plus teams, and once we knew that the feature worked well for those teams, we came up with a gradual rollout plan for Grid customers.  . Slack’s Grid user flow has more complex business logic, with different enterprise-level preferences. This required a careful and gradual rollout strategy to ensure we covered all the cases. We spent more than two months releasing the feature to our Free, Standard, and Plus teams, and once we knew that the feature worked well for those teams, we came up with a gradual rollout plan for Grid customers. .  Having a gradual rollout plan really helped us to catch bugs early and address them with minimal impact.  . Having a gradual rollout plan really helped us to catch bugs early and address them with minimal impact. .  Changing the core   user   data model had a major impact on our existing instrumentation. We realized this a little late — some of our data pipelines were not aware of the new type of user. Since we changed the definition of what a user is, it impacted internal metrics and dashboards of user count by a large margin.  . Changing the core  . user .  data model had a major impact on our existing instrumentation. We realized this a little late — some of our data pipelines were not aware of the new type of user. Since we changed the definition of what a user is, it impacted internal metrics and dashboards of user count by a large margin. .  Having the instrumentation discussions early in the product development cycle is critical to the success of any feature development.  . Having the instrumentation discussions early in the product development cycle is critical to the success of any feature development. .  When we decided to ship it, instead of holistically re-thinking the user onboarding flow we added    if the user is invited member → do something    statements! Though this made our user workflow very complex for a while, it was a choice we made to deliver the feature to early users as soon as possible. Once we reached a stable product development cycle, we did a rewrite with better understanding of features and their side effects.  . When we decided to ship it, instead of holistically re-thinking the user onboarding flow we added  . if the user is invited member → do something .  statements! Though this made our user workflow very complex for a while, it was a choice we made to deliver the feature to early users as soon as possible. Once we reached a stable product development cycle, we did a rewrite with better understanding of features and their side effects. .  Sometimes it is necessary to move faster with what works best at the moment. But always make sure to rethink the choices and clean up the technical debts after.  . Sometimes it is necessary to move faster with what works best at the moment. But always make sure to rethink the choices and clean up the technical debts after. .  Our customers are excited by Email Bridge, and we’ve gotten feedback that they also want the email reply feature for    existing    Slack users. We are currently rolling out this feature to all our existing users. With this release, any Slack notification email related to a user mention can be replied to directly from notification emails.  . Our customers are excited by Email Bridge, and we’ve gotten feedback that they also want the email reply feature for  . existing .  Slack users. We are currently rolling out this feature to all our existing users. With this release, any Slack notification email related to a user mention can be replied to directly from notification emails. .  We are also rolling out some exciting new features that will help us deliver relevant Slack messages and team updates to our invited users. These emails will contain the important messages, new members information, and a list of recommended channels to join. And if this work sounds interesting to you,    we’re hiring   .  . We are also rolling out some exciting new features that will help us deliver relevant Slack messages and team updates to our invited users. These emails will contain the important messages, new members information, and a list of recommended channels to join. And if this work sounds interesting to you,  . we’re hiring . . . Thanks to Saran Arul, Harshad Saykhedkar, Ninad Pundalik, Ross Harmes, Anubhav Aanand, Matthew Haughey, and Matt Wahl for their valuable and constructive suggestions. ", "date": "2020-10-14"}, {"website": "Slack", "title": "Creating a React Analytics Logging Library", "author": ["Konstantin Savransky", "Fabio Canache", "Justin Huddleston"], "link": "https://slack.engineering/creating-a-react-analytics-logging-library/", "abstract": "  Like many applications, the Slack desktop app logs how users interact with it. For example, it may log when a user views a screen or clicks on a button. Product Managers and Data Scientists analyze the logs, hoping to discover actionable insights to drive product refinements.  . Like many applications, the Slack desktop app logs how users interact with it. For example, it may log when a user views a screen or clicks on a button. Product Managers and Data Scientists analyze the logs, hoping to discover actionable insights to drive product refinements. .  Slack’s desktop application is written in    React   . We built a React analytics logging library to: 1) increase developer velocity by making it easier to write logging code, 2) reduce log data errors, and 3) create a viewer to get a real-time look at logging. In this first (of two parts) article, we examine how we reached these goals, reviewing key pieces of the library: data sharing, logging impressions, and other events.  . Slack’s desktop application is written in  . React . . We built a React analytics logging library to: 1) increase developer velocity by making it easier to write logging code, 2) reduce log data errors, and 3) create a viewer to get a real-time look at logging. In this first (of two parts) article, we examine how we reached these goals, reviewing key pieces of the library: data sharing, logging impressions, and other events. .  We’ve broken this post down into three sections:  . We’ve broken this post down into three sections: . Sharing Data Efficiently . : how to share log data between components without manual data copying and code duplication — thereby increasing velocity and reducing data errors . Smarter Impressions: .  how we made it easy to log impressions while improving impression accuracy . Easy Event Logging . : how we simplified logging events, like clicks, while bolstering data consistency .  Let’s dive in!  . Let’s dive in! .  Traditionally, analytics logging in React has been done by importing a logging function into a component file and calling it with specified data inside a    lifecycle    or    event handler    method. While this approach works, there is room for improvement.  . Traditionally, analytics logging in React has been done by importing a logging function into a component file and calling it with specified data inside a  . lifecycle .  or  . event handler .  method. While this approach works, there is room for improvement. .  To start, the approach leads to duplicate code. Say we want to log an impression when a user views a home page. We’ll import our logging function into our home page component file. Then, in  our  componentDidMount  or  useEffect-equivalent , we’ll call: . To start, the approach leads to duplicate code. Say we want to log an impression when a user views a home page. We’ll import our logging function into our home page component file. Then, in . sendLog . ({  . page: . 'home' . ,  . action: . 'impression' .  }); .  So far, so good. Next, say we want to log an impression of a section on the home page. We’ll find the section component file, import the    sendLog    function, and in    componentDidMount    call:  . So far, so good. Next, say we want to log an impression of a section on the home page. We’ll find the section component file, import the  . sendLog .  function, and in  . componentDidMount .  call: .  Alright, a bit of copying. Then, say we also want to log a banner that appears in the welcome section. Again, import    sendLog    into the banner component, and in its    componentDidMount    call:  . Alright, a bit of copying. Then, say we also want to log a banner that appears in the welcome section. Again, import  . sendLog .  into the banner component, and in its  . componentDidMount .  call: .  I’m sure you see the issue. We have to import the    sendLog    function into each file, repeatedly write code in    componentDidMount   , and add the same key-values as we work our way down the component tree. We may even incorrectly copy a key-value,    ruining data   .    . I’m sure you see the issue. We have to import the  . sendLog .  function into each file, repeatedly write code in  . componentDidMount . , and add the same key-values as we work our way down the component tree. We may even incorrectly copy a key-value,  . ruining data . .   .  Can we improve this? As a React dev, your first instinct may be to use props to pass down    sendLog    and our data object. That works. But what if the promo banner in our example is nested in a subsection of a subsection? Now you are “Prop-drilling”    sendLog    and the data props through components that aren’t even logging.    . Can we improve this? As a React dev, your first instinct may be to use props to pass down  . sendLog .  and our data object. That works. But what if the promo banner in our example is nested in a subsection of a subsection? Now you are “Prop-drilling”  . sendLog .  and the data props through components that aren’t even logging.   .   React’s Context API    allows us to share information between parent and child components no matter how far apart on the component tree     without Prop-drilling    . Let’s create a new Log component that uses the Context API to share our data props! The Log component also can contain our    sendLog    function and logging logic, thereby keeping our substantive code separate from our logging code. That’s    Separation of Concerns   !     . React’s Context API .  allows us to share information between parent and child components no matter how far apart on the component tree  . without Prop-drilling . . Let’s create a new Log component that uses the Context API to share our data props! The Log component also can contain our  . sendLog .  function and logging logic, thereby keeping our substantive code separate from our logging code. That’s  . Separation of Concerns . !    .  To correctly share data props, we need the Log component to be a    Provider    of its data props to other child Log components somewhere down the component tree. Also, we need Log to be a    Consumer    of parent Log component data props. Here’s a skeleton:  . To correctly share data props, we need the Log component to be a  . Provider .  of its data props to other child Log components somewhere down the component tree. Also, we need Log to be a  . Consumer .  of parent Log component data props. Here’s a skeleton: . import . React . , {  . Component .  }  . from . 'react' . ; . import .  {  . sendLog .  }  . from . './utils' . ; . const . Context .  =  . React . . . createContext . ( . '' . ); . export . const . LogContext .  =  . Context . ; . class . Log . extends . Component .  { .     . componentDidMount . () { .         . // some code that calls sendLog .    } .     . render . () { .         . const .  {  . children . , ... . directProps .  } =  . this . . . props . ; .         . return .  ( .             . &lt; . LogContext.Consumer . &gt; .                 . { . ( . consumedProps . )  . =&gt; .  { .                     . const . combinedProps .  = { ... . consumedProps . , ... . directProps .  }; .                     . return . &lt; . LogContext.Provider . value . = . { . combinedProps . } . &gt; . { . children . } . &lt;/ . LogContext.Provider . &gt; . ; .                } . } .             . &lt;/ . LogContext.Consumer . &gt; .        ); .    } . } . export . default . Log . ; .  We use    LogContext.Consumer    to consume data props from all parent Log components and then, after combining them with directly-passed data props, we pass the    combinedProps    down to other Log components through    LogContext.Provider   . No more Prop-drilling or manual data copying!    . We use  . LogContext.Consumer .  to consume data props from all parent Log components and then, after combining them with directly-passed data props, we pass the  . combinedProps .  down to other Log components through  . LogContext.Provider . . No more Prop-drilling or manual data copying!   .  Now that we can share data effectively, let’s add impression logging functionality. We want data sharing and impression logging to work with this example usage:  . Now that we can share data effectively, let’s add impression logging functionality. We want data sharing and impression logging to work with this example usage: . &lt; . Log . page . = . \"home\" . &gt; . &lt; . div . id . = . \"home\" . &gt; . &lt; . Log . logImpression . section . = . \"welcome\" . &gt; . &lt; . div . id . = . \"welcome-section\" . &gt; . Welcome . &lt;/ . div . &gt; .         &lt;/ . Log . &gt; . &lt;/ . div . &gt; . &lt;/ . Log . &gt; .  In the example code, we want to set a data tag on the home page,    page=\"home\"   . But we aren’t logging a page impression. We want to log an impression of the welcome section. To do so, we pass a    logImpression    prop into the Log component wrapping the welcome section (along with another data prop,    section=\"welcome\"   ).  . In the example code, we want to set a data tag on the home page,  . page=\"home\" . . But we aren’t logging a page impression. We want to log an impression of the welcome section. To do so, we pass a  . logImpression .  prop into the Log component wrapping the welcome section (along with another data prop,  . section=\"welcome\" . ). .  To make the Log component send an impression of the welcome section, we could check in Log’s    componentDidMount    whether    logImpression    exists and, if it does, call    sendLog   . That way, when Log and its wrapped child (the welcome section) mount, an impression is logged. Not bad. But what if the welcome section is mounted off the screen? Say at the bottom of a tall page where the user has to scroll down to actually see it:   . To make the Log component send an impression of the welcome section, we could check in Log’s  . componentDidMount .  whether  . logImpression .  exists and, if it does, call  . sendLog . . That way, when Log and its wrapped child (the welcome section) mount, an impression is logged. Not bad. But what if the welcome section is mounted off the screen? Say at the bottom of a tall page where the user has to scroll down to actually see it:  . &lt; . Log . page . = . \"home\" . &gt; .     &lt; . div . id . = . \"home\" . &gt; .         &lt; . div . style . = . { . {  . height: . \"2000px\" .  } . } . &gt; . I'm really tall! . &lt;/ . div . &gt; .         &lt; . Log . logImpression . section . = . \"welcome\" . &gt; . &lt; . div . id . = . \"welcome-section\" . &gt; . Welcome! . &lt;/ . div . &gt; .          . &lt;/ . Log . &gt; .     &lt;/ . div . &gt; . &lt;/ . Log . &gt; .  Obviously, we don’t want to log the impression until the user actually scrolls the welcome section into view. To create smarter impression logging, let’s use the    Intersection Observer API   .   . Obviously, we don’t want to log the impression until the user actually scrolls the welcome section into view. To create smarter impression logging, let’s use the  . Intersection Observer API . .  .  First, in Log’s    componentDidMount   , we check if the    logImpression    prop is true. If it is true, we call the    setupObserver    method to instantiate an Intersection Observer instance:   . First, in Log’s  . componentDidMount . , we check if the  . logImpression .  prop is true. If it is true, we call the  . setupObserver .  method to instantiate an Intersection Observer instance:  .     . componentDidMount . () { .         . if .  ( . this . . . props . . . logImpression . ) { .             . this . . . setupObserver . (); .        } .    } .     . setupObserver . () { .         . this . . . observer .  =  . new . IntersectionObserver . ( . this . . . observerCallback . , { .             . root: . null . , .             . rootMargin: . '0px' . , .             . threshold: . 0 . , .        }); .         . // add code to observe our wrapped child element with this.observer .    } .  When creating the instance, we set the    root    to    null   , meaning we are checking when our observed element intersects the viewport, i.e., enters the screen. We also set the    rootMargin    and    threshold    params to  0  to avoid any offsets.  . When creating the instance, we set the  . root .  to  . null . , meaning we are checking when our observed element intersects the viewport, i.e., enters the screen. We also set the  . rootMargin .  and  . threshold .  params to  0  to avoid any offsets. .  Next, in Log’s    render    method, we wrap the Log component’s  children  in a  div  element. We need the additional  div  because we need to use    React Ref    to find our observed child element on the    DOM   . The  div  provides the  ref :  . Next, in Log’s  . render .  method, we wrap the Log component’s  children  in a  div  element. We need the additional  div  because we need to use  . React Ref .  to find our observed child element on the  . DOM . . The  div  provides the  ref : .    . constructor . ( . props . ) { .         . super . ( . props . ); .         . this . . . logDOMElementRef .  =  . React . . . createRef . (); .    }    .    render . () { .         . const .  {  . children . ,  . logImpression . , ... . directProps .  } =  . this . . . props . ; .         . return .  ( .             . &lt; . LogContext.Consumer . &gt; .                 . { . ( . consumedProps . )  . =&gt; .  { .                     . this . . . combinedProps .  = { ... . consumedProps . , ... . directProps .  }; .                     . return .  ( .                         . &lt; . Context.Provider . value . = . {this . . . combinedProps . } . &gt; .                             . &lt; . div . style . = . { . {  . display: . 'contents' .  } . } . ref . = . {this . . . logDOMElementRef . } . &gt; .                                 . { . children . } .                             . &lt;/ . div . &gt; .                         . &lt;/ . Context.Provider . &gt; .                    ); .                } . } .             . &lt;/ . LogContext.Consumer . &gt; .        ); .    } .  We have to be careful here because we don’t want the additional  div  to impact UI layout. To prevent that, we set the  div ’s    CSS display property    to    contents   .   . We have to be careful here because we don’t want the additional  div  to impact UI layout. To prevent that, we set the  div ’s  . CSS display property .  to  . contents . .  .  Now that we have our  ref , we find the Log component’s first visible child element by finding the first child whose    offsetParent    is not    null   . This is important because an element’s    offsetParent    is    null    if its    display    property is    none   . Once we have found our visible child element, we tell our observer to observe it! Let’s update    setupObserver    accordingly:  . Now that we have our  ref , we find the Log component’s first visible child element by finding the first child whose  . offsetParent .  is not  . null . . This is important because an element’s  . offsetParent .  is  . null .  if its  . display .  property is  . none . . Once we have found our visible child element, we tell our observer to observe it! Let’s update  . setupObserver .  accordingly: .     . setupObserver . () { .         . this . . . observer .  =  . new . IntersectionObserver . ( . this . . . observerCallback . , { .             . root: . null . , .             . rootMargin: . '0px' . , .             . threshold: . 0 . , .        }); .         . const . wrappedDOMElements .  =  . this . . . logDOMElementRef . ?. . current . ?. . childNodes . ; .         . const . firstVisibleElement .  =  . find . ( . wrappedDOMElements . , ( . el . )  . =&gt; . el . . . offsetParent .  !==  . null . ); .         . if .  ( . firstVisibleElement . ) { .             . this . . . observer . . . observe . ( . firstVisibleElement . ); .        } .    } .  An observer’s first argument is a    callback   . Ours is conspicuously named    observerCallback   . When an observed child element comes into view,    observerCallback    is called:  . An observer’s first argument is a  . callback . . Ours is conspicuously named  . observerCallback . . When an observed child element comes into view,  . observerCallback .  is called: .     . constructor . ( . props . ) { .         . super . ( . props . ); .         . this . . . logDOMElementRef .  =  . React . . . createRef . (); .         . this . . . state .  = {  . isInViewport: . false .  }; .        this . . . hasImpressionAlreadyBeenLogged .  =  . false . ; .         . this . . . observerCallback .  =  . this . . . observerCallback . . . bind . ( . this . ); .    } .    observerCallback . ( . entries . ) { .         . const . entry .  =  . entries . [ . 0 . ]; .         . if .  ( . entry .  !==  . undefined .  &amp;&amp;  . this . . . state . . . isInViewport .  !==  . entry . . . isIntersecting . ) { .             . this . . . setState . (()  . =&gt; .  ({ .                 . isInViewport: . entry . . . isIntersecting . , .            })); .        } .    } .  In    observerCallback   , we check if our   entry  , i.e. observed child, is intersecting the viewport and update our   Log   component’s state    isInViewport    value accordingly. The state update triggers    componentDidUpdate   , which checks if the    isInViewport    value is now  true :  . In  . observerCallback . , we check if our  . entry . , i.e. observed child, is intersecting the viewport and update our  . Log .  component’s state  . isInViewport .  value accordingly. The state update triggers  . componentDidUpdate . , which checks if the  . isInViewport .  value is now  true : .     . componentDidUpdate . () { .         . if .  ( .             . this . . . props . . . logImpression .  &amp;&amp; .             . this . . . state . . . isInViewport .  &amp;&amp; .            ! . this . . . hasImpressionAlreadyBeenLogged .        ) { .             . sendLog . ( . this . . . combinedProps . ); .             . this . . . hasImpressionAlreadyBeenLogged .  =  . true . ; .        } .    } .  If    isInViewport    is true, and an impression has not previously been logged (   this.hasImpressionAlreadyBeenLogged  is still  false   ), we call    sendLog    with    this.combinedProps   .   . If  . isInViewport .  is true, and an impression has not previously been logged ( .  this.hasImpressionAlreadyBeenLogged  is still  false  . ), we call  . sendLog .  with  . this.combinedProps . .  .  Since we are using the Context API, our Log component has inherited the    page=\"home\"    data tag prop from its parent Log component. Thus,    this.combinedProps    already contains the page tag along with the directly-passed    section=\"welcome\"    tag. We didn’t have to copy anything, and an impression is sent when the component is scrolled into view with all the data we want!    . Since we are using the Context API, our Log component has inherited the  . page=\"home\" .  data tag prop from its parent Log component. Thus,  . this.combinedProps .  already contains the page tag along with the directly-passed  . section=\"welcome\" .  tag. We didn’t have to copy anything, and an impression is sent when the component is scrolled into view with all the data we want!   .  We’ve improved data sharing and impression logging. Let’s see if we can improve logging events like button clicks, typing in an input, and choosing a dropdown option.   . We’ve improved data sharing and impression logging. Let’s see if we can improve logging events like button clicks, typing in an input, and choosing a dropdown option.  .  Traditionally, event logging has been done by adding logging code into event handler methods alongside the substantive code. Remember our promo banner? Say it has a close button on it. Following the traditional way, we’ll have an    onClick    property on the button component, with a    handleClick    event handler as its value. In    handleClick   , we’ll have code that hides the banner and calls    sendLog   . We would want the click data to contain the context of where the button is on the page. So we would call:   . Traditionally, event logging has been done by adding logging code into event handler methods alongside the substantive code. Remember our promo banner? Say it has a close button on it. Following the traditional way, we’ll have an  . onClick .  property on the button component, with a  . handleClick .  event handler as its value. In  . handleClick . , we’ll have code that hides the banner and calls  . sendLog . . We would want the click data to contain the context of where the button is on the page. So we would call:  . sendLog . ({  . page: . 'home' . ,  . section: . 'welcome section' . ,  . component: . 'banner' . ,  . action: . 'click' . ,  . elementType: . 'button' . ,  . elementName: . 'promo-close-button' .  }); .  Examining the    sendLog    data, we may ask: what are the data tags that are actually unique to this close button? Just one:    elementName: 'promo-close-button'   . The data tags    page: 'home'   ,    section: 'welcome section'   , and    component: 'banner'    all provide context but aren’t used only for the button. Meanwhile,    action: 'click'    and    elementType: 'button'    are likely the same for all buttons everywhere in our application. So ideally we would just want to pass    elementName: 'promo-close-button'    directly into our close button:  . Examining the  . sendLog .  data, we may ask: what are the data tags that are actually unique to this close button? Just one:  . elementName: 'promo-close-button' . . The data tags  . page: 'home' . ,  . section: 'welcome section' . , and  . component: 'banner' .  all provide context but aren’t used only for the button. Meanwhile,  . action: 'click' .  and  . elementType: 'button' .  are likely the same for all buttons everywhere in our application. So ideally we would just want to pass  . elementName: 'promo-close-button' .  directly into our close button: . &lt; . Log . page . = . \"home\" . &gt; .     &lt; . div . id . = . \"home\" . &gt; .         Home!  . { . /* more home screen jsx */ . } .          . &lt; . Log . logImpression . section . = . \"welcome\" . &gt; .              . &lt; . div . id . = . \"welcome-section\" . &gt; .                 Welcome!  . { . /* more welcome section jsx */ . } .                  . &lt; . Log . logImpression . component . = . \"banner\" . elementName . = . \"promo-banner\" . &gt; .                      . &lt; . div . id . = . \"promo-banner\" . &gt; .                         Promo!  . { . /* more banner jsx */ . } .                          . &lt; . Button .                              . text . = . \"Close!\" .                              . onClick . = . {this . . . handleClick . } .                              . logEventProps . = . { . { .                                  . elementName: . 'promo-close-button' . , .                             } . } .                          . /&gt; .                      . &lt;/ . div . &gt; .                  . &lt;/ . Log . &gt; .              . &lt;/ . div . &gt; .          . &lt;/ . Log . &gt; .      . &lt;/ . div . &gt; . &lt;/ . Log . &gt; .   *The ugly nesting is just so you can see everything together. In a real implementation, each div would likely be its own component with a Log component wrapping it.   . *The ugly nesting is just so you can see everything together. In a real implementation, each div would likely be its own component with a Log component wrapping it. .  In the code, we pass    logEventProps    directly into the Button component, with   the one data tag we need,    elementName: 'promo-close-button'   . In order to make this work, we must prepare our Button component to use    logEventProps   . We wrap Button with a new component, LogEvent:  . In the code, we pass  . logEventProps .  directly into the Button component, with .  the one data tag we need,  . elementName: 'promo-close-button' . . In order to make this work, we must prepare our Button component to use  . logEventProps . . We wrap Button with a new component, LogEvent: . export . default . function . Button . ({  . logEventProps . ,  . onClick . ,  . text .  }) { .     . return .  ( .         . &lt; . LogEvent .             . logEventProps . = . { . logEventProps . } .             . actionProps . = . { . {  . onClick: .  {  . action: . 'click' .  } } . } .             . elementType . = . \"button\" .         . &gt; .             . &lt; . button . onClick . = . { . onClick . } . &gt; . { . text . } . &lt;/ . button . &gt; .         . &lt;/ . LogEvent . &gt; .    ); . } .  In our new LogEvent component, we specify that the Button component’s data tag    elementType    is always    'button'   . Further, on an    onClick    event, the Button’s    action    tag is always    'click'   . By placing these tags in the LogEvent component that wraps Button, we never have to copy them again when we log a click of a Button instance anywhere in our application!      . In our new LogEvent component, we specify that the Button component’s data tag  . elementType .  is always  . 'button' . . Further, on an  . onClick .  event, the Button’s  . action .  tag is always  . 'click' . . By placing these tags in the LogEvent component that wraps Button, we never have to copy them again when we log a click of a Button instance anywhere in our application!     .  Well how does a basic LogEvent component work?  . Well how does a basic LogEvent component work? . import React from 'react'; . import .  {  . forEach .  }  . from . 'lodash' . ; . import . Log . , {  . LogContext .  }  . from . './log' . ; . import .  {  . sendLog .  }  . from . './utils' . ; . function . getEventHandlers . ( . child . ,  . consumedProps . ,  . actionProps . ) { .     . // create modified event handlers that log . } . export . default . function . LogEvent . ( . props . ) { .     . const .  {  . elementType . ,  . actionProps . ,  . logEventProps . ,  . children .  } =  . props . ;\r  .     . if .  (! . logEventProps .  || ! . actionProps . ) { .         . return . children . ; .    } .     . return .  ( .         . &lt; . Log . elementType . = . { . elementType . } . { . ... . logEventProps . } . &gt; .             . &lt; . LogContext.Consumer . &gt; .                 . { . ( . consumedProps . )  . =&gt; .                     . React . . . Children . . . map . ( . children . , ( . child . )  . =&gt; .                         . React . . . cloneElement . ( .                             . child . , .                             . getEventHandlers . ( . child . ,  . consumedProps . ,  . actionProps . ) .                        ) .                    ) .                 . } .             . &lt;/ . LogContext.Consumer . &gt; .         . &lt;/ . Log . &gt; .    ); . } .  The first thing we do in the   LogEvent   component is check if    logEventProps    and    actionProps    are present. Since our logging functionality requires    logEventProps    and    actionProps    to work, if either is missing, we simply return the unmodified child component (e.g. button element).   . The first thing we do in the  . LogEvent .  component is check if  . logEventProps .  and  . actionProps .  are present. Since our logging functionality requires  . logEventProps .  and  . actionProps .  to work, if either is missing, we simply return the unmodified child component (e.g. button element).  .  If the required props exist, we wrap our entire   LogEvent   in a Log component. Why? We want our Log component to inherit data tag props from parent Log components via the Context API and combine them with the    elementType    and other    logEventProps    passed into LogEvent. This eliminates the need for data tag copying and provides all the contextual data tags for our event log. Also, if we want to log an impression of the child component (e.g., the button), we can do that by passing the   logImpression   flag in    logEventProps   .  . If the required props exist, we wrap our entire  . LogEvent .  in a Log component. Why? We want our Log component to inherit data tag props from parent Log components via the Context API and combine them with the  . elementType .  and other  . logEventProps .  passed into LogEvent. This eliminates the need for data tag copying and provides all the contextual data tags for our event log. Also, if we want to log an impression of the child component (e.g., the button), we can do that by passing the  . logImpression .  flag in  . logEventProps . . .  Once Log has created the data props object, we need to consume it with    LogContext.Consumer   . Now, we have all the data tags we need!    . Once Log has created the data props object, we need to consume it with  . LogContext.Consumer . . Now, we have all the data tags we need!   .  Next, we address the fact that the children prop is plural. In our example, we wrapped just one element, a button. But, we may wrap multiple components with LogEvent (e.g. radio button group). Thus, we use    React.Children.map    to iterate and return the wrapped child or children. We don’t want to simply return an unmodified wrapped child. What we want to do is inject our logging code into the child’s event handler! For that, we are going to use    React.cloneElement   .   . Next, we address the fact that the children prop is plural. In our example, we wrapped just one element, a button. But, we may wrap multiple components with LogEvent (e.g. radio button group). Thus, we use  . React.Children.map .  to iterate and return the wrapped child or children. We don’t want to simply return an unmodified wrapped child. What we want to do is inject our logging code into the child’s event handler! For that, we are going to use  . React.cloneElement . .  .  The    cloneElement    function returns a copy of an element. You can use it   to add or modify the copied element’s props. Perfect! Since we want to add logging code to the child’s event handler prop, we need to modify the handler and pass it to    cloneElement   . Our    getEventHandlers    function contains our modification code:  . The  . cloneElement .  function returns a copy of an element. You can use it .  to add or modify the copied element’s props. Perfect! Since we want to add logging code to the child’s event handler prop, we need to modify the handler and pass it to  . cloneElement . . Our  . getEventHandlers .  function contains our modification code: . function . getEventHandlers . ( . child . ,  . consumedProps . ,  . actionProps . ) { .     . const . eventHandlers .  = {}; .     . forEach . ( . actionProps . , ( . specificHandlerProps . ,  . eventHandlerName . )  . =&gt; .  { .         . eventHandlers . [ . eventHandlerName . ] = (... . eventHandlerArgs . )  . =&gt; .  { .             . child . . . props . [ . eventHandlerName . ]?. . apply . ( . child . ,  . eventHandlerArgs . ); .             . sendLog . ({ ... . consumedProps . , ... . specificHandlerProps .  }); .        }; .    }); .     . return . eventHandlers . ; . } .  In    getEventHandlers   , we initialize an empty    eventHandlers    object. It will store all the event handler functions we will modify. For our example, in LogEvent’s    actionProps   , we specified a single event handler prop function’s name,    onClick   . In some cases however, we may have multiple event handlers we may want to modify. Therefore, we loop through each of our    actionProps   .   . In  . getEventHandlers . , we initialize an empty  . eventHandlers .  object. It will store all the event handler functions we will modify. For our example, in LogEvent’s  . actionProps . , we specified a single event handler prop function’s name,  . onClick . . In some cases however, we may have multiple event handlers we may want to modify. Therefore, we loop through each of our  . actionProps . .  .  The key of each of the    actionProps    is the event handler name (e.g.    onClick   ). The value is an object containing additional data tags specific to that event handler. In our example, the value for    specificHandlerProps    is    { action: 'click' }   . In our loop, we use the    JS apply function    to ensure that the cloned child’s original event handler still gets called with all of its arguments. After all, we want our close button to still close the promo banner! Finally, we also add our    sendLog    call with the data tags from our consumed props and specific handler props.   . The key of each of the  . actionProps .  is the event handler name (e.g.  . onClick . ). The value is an object containing additional data tags specific to that event handler. In our example, the value for  . specificHandlerProps .  is  . { action: 'click' } . . In our loop, we use the  . JS apply function .  to ensure that the cloned child’s original event handler still gets called with all of its arguments. After all, we want our close button to still close the promo banner! Finally, we also add our  . sendLog .  call with the data tags from our consumed props and specific handler props.  .  Now that we’re done, when someone clicks our close button, it will close the promo banner and log all of the data we want. Eureka! Also, since we’ve wrapped our Button component, it is ready for easy-to-use logging anywhere in our codebase.   . Now that we’re done, when someone clicks our close button, it will close the promo banner and log all of the data we want. Eureka! Also, since we’ve wrapped our Button component, it is ready for easy-to-use logging anywhere in our codebase.  .  Thanks to   LogEvent   there is no more error-prone data copying for each logged event or mixing substantive code with logging code in event handlers. We can also easily use   LogEvent   to wrap other action components like selects and inputs by passing in different    actionProps   !  . Thanks to  . LogEvent .  there is no more error-prone data copying for each logged event or mixing substantive code with logging code in event handlers. We can also easily use  . LogEvent .  to wrap other action components like selects and inputs by passing in different  . actionProps . ! .  By making it easier to implement impressions and event logging, we saw a 66% decrease in log code length. Also, it takes devs a quarter of the time to implement logs.    . By making it easier to implement impressions and event logging, we saw a 66% decrease in log code length. Also, it takes devs a quarter of the time to implement logs.   .  In the next installment, we will examine how we abstracted the library for use by any team. We will also look at how we added two powerful features: keeping log history and building a live log viewer. Finally, we will discuss the library’s adoption and impact at Slack. Stay tuned!    . In the next installment, we will examine how we abstracted the library for use by any team. We will also look at how we added two powerful features: keeping log history and building a live log viewer. Finally, we will discuss the library’s adoption and impact at Slack. Stay tuned!   ", "date": "2020-11-06"}, {"website": "Slack", "title": "Migrating Slack Airflow to Python 3 Without Disruption", "author": ["Ashwin Shankar"], "link": "https://slack.engineering/migrating-slack-airflow-to-python-3-without-disruption/", "abstract": " Last year, we migrated Airflow from 1.8 to 1.10 at Slack (see  here ) and we did a “Big bang” upgrade because of the constraints we had. This year, due to Python 2 reaching end of life, we again had a major migration of Airflow from Python 2 to 3 and we wanted to put our learnings from 2019 into practice. This post describes how we moved our Apache Airflow infrastructure, and hundreds of DAGs, from Python 2 to Python 3 in a more reliable “Red-black” deployment using  Celery  queues, resulting in the migration being completely transparent to our users. .  Apache Airflow  is a tool for describing, executing, and monitoring workflows. At Slack, we use Airflow to schedule our data warehouse workflows, which includes product metrics, business metrics, and also other use-cases like search and offline indexing. We run hundreds of workflows and tens of thousands of airflow tasks every day. . Here are the list of things we did for the migration: . We did these steps in a dev environment first and then in prod. Let’s delve into each step in the following sections (except for the clean-up steps, which are self-explanatory). .  Pro tip:  The above order of migration is important; migrating scheduler or webserver to Python 3 before fixing DAG incompatibilities will result in errors because the scheduler in Python 3 will be unable to parse the DAG. . To set up a working python3 virtual environment, we did the following: .   .   . Now that we have the infrastructure in place, the next step is to make the DAGs Python 2/3 compatible. It’s important that it’s Python 2/3 compatible for two reasons; Firstly, the services still on Python 2, like the scheduler, should still be able to parse the DAGs. Secondly, when we migrate to Python 3 workers, the rollback is simpler since there will be no code change in the DAGs except for the queue configuration. Here are some details about fixing incompatibilities: .   . Once all the DAGs were moved to Python 3 workers, we switched over the Airflow scheduler, web server, and flower to Python 3 by shipping the Python 3 virtual environment built in step 1. Finally, we cleaned up all Python 2 related code and terminated the python2 workers. Now, we can call the migration done! . This Airflow migration proved to be reliable, completely transparent to users, and had zero SLA misses since we did a “red black” deployment. This was done by running both Python 2 and 3 infrastructure side-by-side and eventually moving all the traffic to Python 3. In conclusion, this migration went well, and we are excited that it enables us to use Python 3 features, not to mention the ability to upgrade to Airflow 2.0, in the future. . Big thanks to Prateek Kakirwar, Diana Pojar, Travis Cook, Joohee Yoo, Ajay Bhonsule for their contributions to the project and to Ross Harmes for reviewing this tech blog. .  We are always looking for engineers to join our Data Engineering team. If you’d like to help us, please check out    https://slack.com/careers   ", "date": "2020-11-19"}, {"website": "Slack", "title": "Scaling Datastores at Slack with Vitess", "author": ["Arka Ganguli", "Guido Iaquinti", "Maggie Zhou", "Rafael Chacón"], "link": "https://slack.engineering/scaling-datastores-at-slack-with-vitess/", "abstract": "  From the very beginning of Slack, MySQL was used as the storage engine for all our data. Slack operated MySQL servers in an active-active configuration. This is the story of how we changed our data storage architecture from the active-active clusters over to Vitess — a horizontal scaling system for MySQL  .   Vitess is the present and future of Datastores for Slack and continues to be a major success story for us. From the solid scalability fundamentals, developer ergonomics, and the thriving community, our bet on this technology has been instrumental for Slack’s continued growth   .    . From the very beginning of Slack, MySQL was used as the storage engine for all our data. Slack operated MySQL servers in an active-active configuration. This is the story of how we changed our data storage architecture from the active-active clusters over to Vitess — a horizontal scaling system for MySQL . .  . Vitess is the present and future of Datastores for Slack and continues to be a major success story for us. From the solid scalability fundamentals, developer ergonomics, and the thriving community, our bet on this technology has been instrumental for Slack’s continued growth .  Our migration to Vitess began in 2017 and Vitess now serves 99% of our overall query load. We expect to be fully migrated by the end of 2020. In this post, we will discuss the design considerations and technical challenges that went into choosing and adopting Vitess, as well as an overview of our current Vitess usage.  . Our migration to Vitess began in 2017 and Vitess now serves 99% of our overall query load. We expect to be fully migrated by the end of 2020. In this post, we will discuss the design considerations and technical challenges that went into choosing and adopting Vitess, as well as an overview of our current Vitess usage. .  Availability, performance, and scalability in our datastore layer is critical for Slack. As an example, every message sent in Slack is persisted   before   it’s sent across the real-time websocket stack and shown to other members of the channel. This means that storage access needs to be    very    fast and    very    reliable.          In addition to providing a critical foundation for message sending, over the last three years Vitess has given us the flexibility to ship new features with complex data storage needs, including    Slack Connect    and    international data residency   .          Today, we serve 2.3 million QPS at peak. 2M of those queries are reads and 300K are writes. Our median query latency is 2 ms, and our p99 query latency is 11 ms.          The beginning  . Availability, performance, and scalability in our datastore layer is critical for Slack. As an example, every message sent in Slack is persisted  . before .  it’s sent across the real-time websocket stack and shown to other members of the channel. This means that storage access needs to be  . very .  fast and  . very .  reliable. .    .    . In addition to providing a critical foundation for message sending, over the last three years Vitess has given us the flexibility to ship new features with complex data storage needs, including  . Slack Connect .  and  . international data residency . . .    .    . Today, we serve 2.3 million QPS at peak. 2M of those queries are reads and 300K are writes. Our median query latency is 2 ms, and our p99 query latency is 11 ms. .    .    .  Slack started as a simple LAMP stack: Linux, Apache, MySQL, and PHP. All our data was stored on three primary database clusters based on MySQL:  . Slack started as a simple LAMP stack: Linux, Apache, MySQL, and PHP. All our data was stored on three primary database clusters based on MySQL: .  These virtually contained all the customer data tied to using Slack, such as messages, channels, and DMs. The data was partitioned and scaled horizontally by workspace id (a workspace is the specific Slack domain you login into). All the data for a given workspace was stored on the same shard, so the application just needed to connect to that one database. . : The metadata cluster was used as a lookup table to map a workspace id to the underlying shard id. This means that to find the shard for a particular Slack domain to a workspace, we had to lookup the record in this metadata cluster first. .  This cluster stored all the other data not tied to a specific workspace, but that was still important Slack functionality. Some examples included the app directory. Any tables that did not have records associated with a workspace id would have gone into this cluster. .  The sharding was managed and controlled by our   monolith application, “webapp”. All data access was managed by webapp, which contained the logic to look up metadata for a given workspace, and then create a connection to the underlying database shard.  . The sharding was managed and controlled by our . monolith application, “webapp”. All data access was managed by webapp, which contained the logic to look up metadata for a given workspace, and then create a connection to the underlying database shard. .  From a dataset layout perspective, the company started out using a workspace-sharded model. Each database shard contained all of a workspace’s data, with each shard housing thousands of workspaces and all their data including messages and channels.   . From a dataset layout perspective, the company started out using a workspace-sharded model. Each database shard contained all of a workspace’s data, with each shard housing thousands of workspaces and all their data including messages and channels.  .  From an infrastructure point of view, all those clusters   were made up of one or more shards where each shard was provisioned with at least   two   MySQL instances located in different datacenters, replicating to each other using asynchronous replication. The image below shows an overview of the original database architecture.  . From an infrastructure point of view, all those clusters . were made up of one or more shards where each shard was provisioned with at least  .  MySQL instances located in different datacenters, replicating to each other using asynchronous replication. The image below shows an overview of the original database architecture. .   . Advantages .  There are many advantages to this active-active configuration, which allowed us to successfully scale the service. Some reasons why this worked well for us:  . There are many advantages to this active-active configuration, which allowed us to successfully scale the service. Some reasons why this worked well for us: . : During normal operations, the application will always prefer to query one of the two sides based on a simple hashing algorithm. When there are failures connecting to one of the hosts, the application could retry a request to the other host without any visible customer impact, since both nodes in a shard can take reads  .  writes. . : Designing new features with the model of having all the data for a given workspace stored on a single database host was intuitive, and easily extensible to new product functionality. . : An engineer at Slack could connect a customer report to a database host within minutes. This allowed us to debug problems quickly. . : As more teams signed up for Slack, we could simply provision more database shards for new teams and keep up with the growth. However, there was a fundamental limitation with the scaling model.  .   .                       View into how a single shard is configured with multi-primary replication   .                     View into how a single shard is configured with multi-primary replication . Disadvantages .  As the company grew, so did the number of product teams working on building new Slack features. We found that our development velocity was slowing down significantly in trying to fit new product features into this very specific sharding scheme. This led to some challenges:  . As the company grew, so did the number of product teams working on building new Slack features. We found that our development velocity was slowing down significantly in trying to fit new product features into this very specific sharding scheme. This led to some challenges: .  As we onboarded larger and larger individual customers, their designated shard reached the largest available hardware and we were regularly hitting the limits of what that single host could sustain. . : As we grew, we launched new products such as Enterprise Grid and Slack Connect, both of which challenge the paradigm that all data for a team will be on the same database shard. This architecture not only added complexity to developing these features, but also a performance penalty in some cases.  .  We found that we were hitting some major hotspots, while also massively underutilizing the majority of our database fleet. As we grew, we onboarded more and more enterprise customers with large teams, consisting of thousands of Slack users. An unfortunate outcome with this architecture was that we were unable to spread the load of these large customers across the fleet and we ended up with a few hot spots in our database tier. Because it was challenging to split shards and move teams, and difficult to predict Slack usage over time, we over provisioned most of the shards, leaving the long tail underutilized.  . : All core features, such as login, messaging, and joining channels, required the database shard that housed the team’s data to be available . .  . This meant that when a database shard experienced an outage, every single customer whose data was on that shard also experienced a full Slack outage. We wanted an architecture where we can both spread the load around to reduce the hot spots, and  . isolate different workloads so that a unavailable second tier feature couldn’t potentially impact critical features like message sending . This is a not standard MySQL configuration. It required us to write a significant amount of internal tooling to be able to operate this configuration at scale. In addition, given that in this setup we didn’t have replicas in our topology and the fact that the application routed directly to the database hosts, we couldn’t safely use replicas without reworking our routing logic.  .  What to do?           In the fall of 2016, we were dealing with hundreds of thousands of MySQL queries per second and thousands of sharded MySQL hosts in production. Our application performance teams were regularly running into scaling and performance problems and having to design workarounds for the limitations of the workspace sharded architecture.— we needed a new approach to scale and manage databases for the future.  . What to do?  .    .    . In the fall of 2016, we were dealing with hundreds of thousands of MySQL queries per second and thousands of sharded MySQL hosts in production. Our application performance teams were regularly running into scaling and performance problems and having to design workarounds for the limitations of the workspace sharded architecture.— we needed a new approach to scale and manage databases for the future. .  From the early stages of this project, there was a question looming in our heads:   should we evolve our approach in place or replace it?   We needed a solution that could provide a flexible sharding model to accommodate new product features and meet our scale and operational requirements.  . From the early stages of this project, there was a question looming in our heads:  . We needed a solution that could provide a flexible sharding model to accommodate new product features and meet our scale and operational requirements. .  For example, instead of putting all the messages from every channel and DM on a given workspace into the same shard, we wanted to shard the message data by the unique id of the channel. This would spread the load around much more evenly, as we would no longer be forced to serve all message data for our largest customer on the same database shard.  . For example, instead of putting all the messages from every channel and DM on a given workspace into the same shard, we wanted to shard the message data by the unique id of the channel. This would spread the load around much more evenly, as we would no longer be forced to serve all message data for our largest customer on the same database shard. .  We still had a strong desire to continue to use MySQL running on our own cloud servers. At the time there were thousands of distinct queries in the application, some of which used MySQL-specific constructs. And at the same time we had years of built up operational practices for deployment, data durability, backups, data warehouse ETL, compliance, and more, all of which were written for MySQL.  . We still had a strong desire to continue to use MySQL running on our own cloud servers. At the time there were thousands of distinct queries in the application, some of which used MySQL-specific constructs. And at the same time we had years of built up operational practices for deployment, data durability, backups, data warehouse ETL, compliance, and more, all of which were written for MySQL. .  This meant that moving away from the relational paradigm (and even from MySQL specifically) would have been a much more disruptive change, which meant we pretty much ruled out NoSQL datastores like DynamoDB or Cassandra, as well as NewSQL like Spanner or CockroachDB.      . This meant that moving away from the relational paradigm (and even from MySQL specifically) would have been a much more disruptive change, which meant we pretty much ruled out NoSQL datastores like DynamoDB or Cassandra, as well as NewSQL like Spanner or CockroachDB. .    .  In addition, historical context is always important to understand how decisions are made. Slack is generally conservative in terms of adopting new technologies, especially for mission-critical parts of our product stack. At the time, we wanted to continue to devote much of our engineering energy to shipping product features, and so the small datastores and infrastructure team valued simple solutions with few moving parts.  . In addition, historical context is always important to understand how decisions are made. Slack is generally conservative in terms of adopting new technologies, especially for mission-critical parts of our product stack. At the time, we wanted to continue to devote much of our engineering energy to shipping product features, and so the small datastores and infrastructure team valued simple solutions with few moving parts. .  A natural way forward could have been to build this new flexible sharding model within our application. Since our application was already involved with database shard routing, we could just bake in the new requirements such as sharding by channel id into that layer. This option was given consideration, and some prototypes were written to explore this idea more fully. It became clear that there was already quite a bit of coupling between the application logic and how the data was stored. It also became apparent that it was going to be time consuming to untangle that problem, while also building the new solution.   . A natural way forward could have been to build this new flexible sharding model within our application. Since our application was already involved with database shard routing, we could just bake in the new requirements such as sharding by channel id into that layer. This option was given consideration, and some prototypes were written to explore this idea more fully. It became clear that there was already quite a bit of coupling between the application logic and how the data was stored. It also became apparent that it was going to be time consuming to untangle that problem, while also building the new solution.  .  For example, something like fetching the count of messages in a channel was tightly coupled to assumptions about what team the channel was on, and many places in our codebase worked around assumptions for organizations with multiple workspaces by checking multiple shards explicitly.  . For example, something like fetching the count of messages in a channel was tightly coupled to assumptions about what team the channel was on, and many places in our codebase worked around assumptions for organizations with multiple workspaces by checking multiple shards explicitly. .     .   . On top of this, building sharding awareness into the application didn’t address any of our operational issues or allow us to use read replicas more effectively. Although it would solve the immediate scaling problems, this approach seemed positioned to run into the very same challenges in the long term.  For instance, if a single team’s shard got surprisingly hot on the write path, it was not going to be straightforward to horizontally scale it. . Why Vitess? .  Around this time we became aware of the Vitess project. It seemed like a promising technology since at its core, Vitess provides a database clustering system for horizontal scaling of MySQL.   . Around this time we became aware of the Vitess project. It seemed like a promising technology since at its core, Vitess provides a database clustering system for horizontal scaling of MySQL.  .  At a high level Vitess ticked all the boxes of our application and operational requirements.  . At a high level Vitess ticked all the boxes of our application and operational requirements. . : Vitess is built on top of MySQL, and as a result leverages all the years of reliability, developer understanding, and confidence that comes from using MySQL as the actual data storage and replication engine. . : Vitess combines many important MySQL features with the scalability of a NoSQL database. Its built-in sharding features lets you flexibly shard and grow your database without adding logic to your application. . : Vitess automatically handles functions like primary failovers and backups. It uses a lock server to track and administer servers, letting your application be blissfully ignorant of database topology. Vitess keeps track of all of the metadata about your cluster configuration so that the cluster view is always up-to-date and consistent for different clients. . : Vitess is built 100% in open source using golang with an extensive set of test coverage and a thriving and open developer community. We felt confident that we would be able to make changes as needed to meet Slack’s requirements (which we did!). .   .          Image from SquareCash Vitess blog post. Check out their cool work      too     !    .        Image from SquareCash Vitess blog post. Check out their cool work  . too . !  .  We decided to build a prototype demonstrating that we can migrate data from our traditional architecture to Vitess and that Vitess would deliver on its promise. Of course, adopting a new datastore at Slack scale is not an easy task. It required a significant amount of effort to set up all the new infrastructure in place.  . We decided to build a prototype demonstrating that we can migrate data from our traditional architecture to Vitess and that Vitess would deliver on its promise. Of course, adopting a new datastore at Slack scale is not an easy task. It required a significant amount of effort to set up all the new infrastructure in place. .  Our goal was to build a working end-to-end use case of Vitess in production for a small feature: integrating an RSS feed into a Slack channel. It required us to rework many of our operational processes for provisioning deployments, service discovery, backup/restore, topology management, credentials, and more. We also needed to develop new application integration points to route queries to Vitess, a generic backfill system for cloning the existing tables while performing double-writes from the application, and a parallel double-read diffing system so we were sure that the Vitess-powered tables had the same semantics as our legacy databases. However, it was worth it: the application performed correctly using the new system, it had much better performance characteristics, and operating and scaling the cluster was simpler. Equally importantly, Vitess delivered on the promise of resilience and reliability. This initial migration gave us the confidence we needed to continue our investment in the project.     . Our goal was to build a working end-to-end use case of Vitess in production for a small feature: integrating an RSS feed into a Slack channel. It required us to rework many of our operational processes for provisioning deployments, service discovery, backup/restore, topology management, credentials, and more. We also needed to develop new application integration points to route queries to Vitess, a generic backfill system for cloning the existing tables while performing double-writes from the application, and a parallel double-read diffing system so we were sure that the Vitess-powered tables had the same semantics as our legacy databases. However, it was worth it: the application performed correctly using the new system, it had much better performance characteristics, and operating and scaling the cluster was simpler. Equally importantly, Vitess delivered on the promise of resilience and reliability. This initial migration gave us the confidence we needed to continue our investment in the project.    .  At the same time, it is still important to call out that during this initial prototype and continuing for the years since, we have identified gaps in Vitess in ways that it would not work for some of Slack-specific needs out of the box. As the technology showed promise at solving the core challenges we were facing, we decided it was worth the engineering investment to add-in the missing functionality.   . At the same time, it is still important to call out that during this initial prototype and continuing for the years since, we have identified gaps in Vitess in ways that it would not work for some of Slack-specific needs out of the box. As the technology showed promise at solving the core challenges we were facing, we decided it was worth the engineering investment to add-in the missing functionality.  .  Some key contributions by Slack include:   . Some key contributions by Slack include:  . Refactoring the topology metadata service for scalability across isolation regions . . . Closing some of the gaps in full MySQL query compatibility. [ . a . ], [ . b . ], [ . c . ], [ . d . ], [ . e . ].   . New tools to enable migrations of data into Vitess. . New tools to load test and introspect Vitess. . More robust integrations with  . Prometheus . ,  . Orchestrator . , and Percona  . xtrabackup . . . And more!  .  Today, it is not an overstatement to say that some of the folks in the open source community are an extended part of our team, and since adopting Vitess, Slack has become and continues to be one of the biggest contributors to the open source    project   .   . Today, it is not an overstatement to say that some of the folks in the open source community are an extended part of our team, and since adopting Vitess, Slack has become and continues to be one of the biggest contributors to the open source  . project . .  .  Now, exactly three years into this migration, we are sitting at 99% of all Slack MySQL traffic having been migrated to Vitess.   We are on track to finish the remaining 1% in the next two months. We’ve wanted to share this story for a long time, but we waited until we had full confidence that this project was a success.     .  We are on track to finish the remaining 1% in the next two months. We’ve wanted to share this story for a long time, but we waited until we had full confidence that this project was a success. .  Here’s a graph showing the migration progression and a few milestones over the last few years:  . Here’s a graph showing the migration progression and a few milestones over the last few years: .   There are many other stories to tell in these 3 years of migrations. Going from 0% to 99% adoption also meant going from 0 QPS to the 2.3 M QPS we serve today. Choosing appropriate sharding keys, retrofitting our existing application to work well with Vitess, and changes to operate Vitess at scale were necessary and each step along the way we learned something new. We break down a specific migration of a table that comprises 20% of our overall query load in a case study in   Refactoring at Scale  , written with Maude Lemaire, a Staff Engineer at Slack. We also plan on writing about our change in migration strategy and technique to move whole shards instead of tables in a future blog post.  . There are many other stories to tell in these 3 years of migrations. Going from 0% to 99% adoption also meant going from 0 QPS to the 2.3 M QPS we serve today. Choosing appropriate sharding keys, retrofitting our existing application to work well with Vitess, and changes to operate Vitess at scale were necessary and each step along the way we learned something new. We break down a specific migration of a table that comprises 20% of our overall query load in a case study in  . , written with Maude Lemaire, a Staff Engineer at Slack. We also plan on writing about our change in migration strategy and technique to move whole shards instead of tables in a future blog post. . Has Vitess at Slack been a success? . Today, we run multiple Vitess clusters with dozens of keyspaces in different geographical regions around the world. Vitess is used by both our main webapp monolith as well as other services. Each keyspace is a logical collection of data that roughly scales by the same factor — number of users, teams, and channels. Say goodbye to only sharding by team, and to team hot-spots! This flexible sharding provided to us by Vitess has allowed us to scale and grow Slack. .  During March 2020, as our CEO Stewart Butterfield    tweeted   , we saw an unprecedented increased usage of Slack as the reality of the COVID-19 pandemic hit the U.S. and work/school shifted out of offices and became distributed. On the datastores side, in just one week we saw query rates increase by 50%. In response to this, we scaled up one of our busiest keyspaces horizontally using Vitess’s splitting workflows. Without resharding and moving to Vitess, we would’ve been unable to scale at all for our largest customers, leading to downtime.          As product teams at Slack started writing new services, they were able to use the same storage technology we use for the webapp. Choosing Vitess instead of building a new sharding layer inside our webapp monolith has allowed us to leverage the same technology for all new services at Slack. Vitess is also the storage layer for our International Data Residency product, for which we run Vitess clusters in six total regions. Using Vitess here was instrumental to being able to ship this feature in record time. It enabled our product engineering team to focus on the core business logic, while the actual region locality of the data was abstracted from their efforts. When we chose Vitess, we didn’t expect to be writing new services or shipping a multi-region product, but as a result of Vitess’s suitability and our investment in it over the last few years, we’ve been able to leverage the same storage technology for these new product areas.  . During March 2020, as our CEO Stewart Butterfield  . tweeted . , we saw an unprecedented increased usage of Slack as the reality of the COVID-19 pandemic hit the U.S. and work/school shifted out of offices and became distributed. On the datastores side, in just one week we saw query rates increase by 50%. In response to this, we scaled up one of our busiest keyspaces horizontally using Vitess’s splitting workflows. Without resharding and moving to Vitess, we would’ve been unable to scale at all for our largest customers, leading to downtime. .    .    . As product teams at Slack started writing new services, they were able to use the same storage technology we use for the webapp. Choosing Vitess instead of building a new sharding layer inside our webapp monolith has allowed us to leverage the same technology for all new services at Slack. Vitess is also the storage layer for our International Data Residency product, for which we run Vitess clusters in six total regions. Using Vitess here was instrumental to being able to ship this feature in record time. It enabled our product engineering team to focus on the core business logic, while the actual region locality of the data was abstracted from their efforts. When we chose Vitess, we didn’t expect to be writing new services or shipping a multi-region product, but as a result of Vitess’s suitability and our investment in it over the last few years, we’ve been able to leverage the same storage technology for these new product areas. .  Now that the migration is complete, we look forward to leveraging more capabilities of Vitess. We have been already investing in    VReplication   , a feature that allows you to hook into MySQL replication to materialize different views of your data.   . Now that the migration is complete, we look forward to leveraging more capabilities of Vitess. We have been already investing in  . VReplication . , a feature that allows you to hook into MySQL replication to materialize different views of your data.  .  The picture below shows a simplified version of what our Vitess deployment at Slack looks like.  . The picture below shows a simplified version of what our Vitess deployment at Slack looks like. .   . Conclusion .  This success still begs the question: Was this the right choice? In Spanish, there is a saying that states: “Como anillo al dedo”. It is often used when a solution fits with great exactitude. We think that even with the benefit of hindsight, Vitess was the right solution for us. This doesn’t mean that if Vitess didn’t exist, we would have not figured out how to scale our datastores. Rather, that with our requirements, we would have landed on a solution that would be very similar to Vitess  .   In a way, this story is not only about how Slack scaled its datastores.   It is also a story that tells the importance of collaboration in our industry  .   . This success still begs the question: Was this the right choice? In Spanish, there is a saying that states: “Como anillo al dedo”. It is often used when a solution fits with great exactitude. We think that even with the benefit of hindsight, Vitess was the right solution for us. This doesn’t mean that if Vitess didn’t exist, we would have not figured out how to scale our datastores. Rather, that with our requirements, we would have landed on a solution that would be very similar to Vitess . . .  In a way, this story is not only about how Slack scaled its datastores.  . .  .   We wanted to give a shout out to all the people that have contributed to this journey: Alexander Dalal, Ameet Kotian, Andrew Mason, Anju Bansal, Brian Ramos, Chris Sullivan, Daren Seagrave, Deepak Barge, Deepthi Sigireddi, Huiqing Zhou, Josh Varner, Leigh Johnson, Manuel Fontan, Manasi Limbachiya, Malcolm Akinje, Milena Talavera, Mike Demmer,  Morgan Jones, Neil Harkins, Paul O’Connor, Paul Tuckfield, Renan Rangel, Ricardo Lorenzo, Richard Bailey, Ryan Park, Sara Bee, Serry Park, Sugu Sougoumarane, V. Brennan and all the others who we probably forgot.    . We wanted to give a shout out to all the people that have contributed to this journey: Alexander Dalal, Ameet Kotian, Andrew Mason, Anju Bansal, Brian Ramos, Chris Sullivan, Daren Seagrave, Deepak Barge, Deepthi Sigireddi, Huiqing Zhou, Josh Varner, Leigh Johnson, Manuel Fontan, Manasi Limbachiya, Malcolm Akinje, Milena Talavera, Mike Demmer,  Morgan Jones, Neil Harkins, Paul O’Connor, Paul Tuckfield, Renan Rangel, Ricardo Lorenzo, Richard Bailey, Ryan Park, Sara Bee, Serry Park, Sugu Sougoumarane, V. Brennan and all the others who we probably forgot.  ", "date": "2020-12-01"}, {"website": "Slack", "title": "A Day in the Life of a Frontend Foundations Engineer at Slack", "author": ["Natalie Qabazard"], "link": "https://slack.engineering/a-day-in-the-life-of-a-frontend-foundations-engineer-at-slack/", "abstract": " First alarm rings. Snooze. . Second alarm rings. Snooze. . Final alarm rings and I know this is the last one, so I hop out of bed and immediately play some music. Music really has a way of waking me up, and I typically play Sofi Tukker or Rufus Du Sol radio on Spotify. I start getting ready to head to the gym for a morning workout. I’m not a morning person, but I’m a morning-workout person. I love working out before work because I arrive feeling very awake and ready to take on my work day! . I’m at the gym, jamming out to some workout playlists I’m very proud of. I start with some stretches, followed by strength training, and finish with cardio and more stretching. Afterward, I head to the shower and get ready for work. Between waiting for the shower and getting dressed, this all takes roughly half an hour. . Once I’m done, I’m ready for a 20-minute walk to the office. I like to listen to podcasts during this time. My favorites include On Purpose with Jay Shetty and Armchair Expert with Dax Shepherd. . I’m finally at the office. I’m typically not the earliest one at the office, so I’m greeted with many “good morning!”s from my amazing team. I then head to the kitchen and fill up my large canteen with water and grab breakfast. Back at my desk, breakfast in hand, I start going through my unread Slack messages. They’re typically a mixture of company-wide announcements, team-specific messages, and off-topic humor. A few of the messages are from Google Calendar, letting me know what I have on my calendar for the day: a team standup, a few 1:1s with my teammates, and an afternoon meeting to discuss our migration to TypeScript. . I’ve been preparing our frontend codebase for widespread Typescript adoption and one of the tasks I’m working on is a custom transform for webpack that generates PropType definitions for React components that are written in Typescript. What is a transform, you ask? It’s something that transforms code from one state to another, using an abstract syntax tree (AST). Why are we doing this? We want to bridge the gap between React components written in Typescript (which lack PropType definitions) and React PropTypes for existing components we have in our codebase. . We want consistent proptype validation as we migrate our Javascript to Typescript, so having this transform in our webpack config will bridge the gap between compile-time and run-time prop validation. As I’m sitting at my desk working on this, I’m looking at Typescript’s AST and the nodes that represent the structure of our Typescript code, including the node that represents the way we enforce a type definition for React component props. This is the node I’m interested in because I’ll be using it to generate a corresponding PropType definition. Anytime I inspect an AST, I’m fascinated. It’s not everyday that you find yourself looking through a tree that represents the human-readable code you write. . The node I’m taking a look at is an object with key-value pairs. What’s puzzling is the values are all numbers. Some match and some don’t. What could they represent? Hmmm… I’ll take a look at this after lunch. For now, I need to head to my team standup. . Team standup! Many folks are familiar with this type of meeting. It gives us a chance to share updates on our work, call out any blockers, and share what we’ll be working on next. After this brief meeting, we head to lunch. . Time to grab lunch! I usually have lunch prepped, but I love using this time to walk with my colleagues to nearby food trucks. It’s a great way to get some fresh air and walk around. Our office has a few cafes, so we typically head back to one of them after grabbing something to eat. Lunch is a great time for me to get to know my teammates better. On this particular day, we’re talking about what our high school experiences were like. It’s no surprise that I had some pretty nerdy interests and was involved in Model United Nations Club, Pi-memorization contests, and student government. . Alright, I’m back at my desk. I open up VSCode to start investigating what those numbers in my TypeScript AST could mean. I ask my teammate Andrew to pair with me on this, and he recommends I take a look at the Typescript npm package to see if those numbers exist inside the package. After a quick grep, I find a dictionary that contains mappings for the numbers I saw in the AST node! This is a win, because I’m no longer puzzled. I can clearly see they represent different “symbols”, which are what make up the syntax for the language. The keys in the dictionary represent both primitive and non-primitive types in Typescript. The values in the node refer to types. From this, we can build a new object that will later become our React PropType. . A meeting with my teammate, Mark. Mark is a principal engineer on our team and helps me understand how my work has an impact on Slack from a bigger-picture point-of-view. I really enjoy our 1:1s because it usually ends up being an unofficial mentorship session. We meet once every couple of weeks and share updates on what each of us have been working on and what’s on our minds. It’s a good time to ask for his feedback and advice on the manner in which I do my work and how to practice leadership. I hope I’m as helpful to him as he’s been to me during the time I’ve spent on our team. . I’m in a meeting with a diverse set of colleagues: two engineering managers, one backend engineer, and two frontend engineers (myself included). We’re here to talk about the recent migration our backend team went through to migrate from PHP to Hack. We’re curious to hear their take, as this migration is analogous to an effort currently underway to adopt TypeScript across our frontend codebase. The goal of this meeting is for us frontend engineers to gather learnings and gotchas from the PHP-to-Hack migration, and hopefully avoid some of the same pitfalls. As many engineers know, the only constant is change. In this case, we’re changing our frontend codebase to be safer to work with, self-documenting, and more easily tested by virtue of static types. We’re in the midst of supporting Typescript adoption across our monolithic codebase, and look forward to evangelizing its adoption in upcoming months. . I wrap things up at work and head home. This is the worst time to leave our building: I’ll be stuck on my commuter shuttle during peak rush-hour traffic in SoMa. It’ll take me 45 minutes to get home, even though I don’t live far from the office. Once I’m home, I unpack my gym bag and start prepping dinner. For all my interest in cooking and culinary pursuits, I rarely make anything interesting for myself for dinner. It’s usually eggs and toast, leftovers, or nothing at all (if I’ve just come home from our weekly work happy-hour on Thursdays). While I’m enjoying dinner, I’ll watch any of the Real Housewives shows on Bravo, or venture off into YouTube land and find videos from the team at Bon Appetit. During this time, I’ll also call and text members of my family, or wander aimlessly through Instagram. . Natalie is a senior frontend engineer at Slack. When Natalie joined Slack, she worked on features for new users and new team creators with the New User Experience team. More recently, she joined the Foundations team and works on frontend infrastructure, so that other frontend engineers at Slack are as productive as they can possibly be. She is currently introducing Typescript across Slack’s frontend codebase. . A conference speaker, Natalie most recently spoke at GraphQL Summit in San Francisco, and O’Reilly Fluent in San Jose. You can find links to her talks on Twitter @natqab. . Prior to Slack, Natalie worked as a full-stack engineer at Zillow Group in San Francisco, using React, Typescript, Node, and GraphQL to deploy web apps using AWS infrastructure (I know, lots of buzzwords). Before moving to San Francisco, she studied at the University of California, Davis where she obtained a Bachelor of Science degree in Computer Science. ", "date": "2020-05-14"}, {"website": "Slack", "title": "Prototyping at Slack", "author": ["Kyle Stetz"], "link": "https://slack.engineering/prototyping-at-slack/", "abstract": " Over the years at Slack — usually during periods of high growth — teams have defaulted to working in a waterfall where an idea is proposed and researched, then designed or spec’d, then built, in that order. It’s not the most efficient way to work and as a company grows — with more people involved at each step — it only gets slower. . There is an incredibly high cost to going backwards; if you realize you got it wrong during the build phase, going back to the drawing board can be seen as a departure or a failure to plan adequately. Hindsight is 20/20, as they say, so why wait until the end of the project to benefit from that clarity? Escaping this mode in order to work iteratively requires a high degree of trust and autonomy that takes time for teams to build, so it helps to talk explicitly about process. We’ve found success by focusing on  the prototyping process  as a way to break out of outdated, linear waterfall thinking. What we’ll share here is not a prescriptive methodology but rather a simple framework that can be adapted to a team’s needs. .  I’ll be borrowing some terms and ideas from the Design Thinking process championed by Tim Brown at IDEO and written up succinctly on    interaction-design.org   , so if this post resonates with you then the Design Thinking literature has a lot more to offer!  . Prototyping is a  process for learning and evaluating . If you are at a point in a project where you have an  assumption  you’d like to validate, a  specific direction  you’d like to get feedback on, or a  technical plan  you want to evaluate, prototyping can give you the information necessary to make a firm decision. . Prototyping can also be used to  evaluate broad opportunities  before investing the full resources of a team into them. Building a rough version of that feature or system you’ve been kicking around for a year gives you the chance to decide  not  to pursue it. This is a skill that enables product and engineering teams to assist with critical business decisions. . Most importantly, prototyping gives us the  guardrails to fail  safely and productively. Failure is an important part of the learning process and is just as valuable as success. . There are three steps in the prototyping process:  hypothesize ,  execute , and  evaluate . . Your hypothesis is the statement that the prototype should prove or disprove. It’s important to define what success looks like up front — especially when evaluating subjective design directions — but equally important is deciding what failure looks like. Remember that failure is not something to be avoided here, it’s simply one of the possible outcomes to plan for. A failure will almost always lead to a reformation of the problem and a new prototype; this is the relationship between the ideation and prototyping phases of the design process. . Let’s look at some example hypotheses we had at Slack and the success/failure criteria that might accompany them. .  💡 What-you-see-is-what-you-get messages can be displayed seamlessly alongside existing messages in the database.   ✅ Older messages continue to display correctly alongside newer WYSIWYG-formatted messages.  ✅ The search infrastructure can be modified to return results from both formats.  ❌ All old messages need to be migrated to a new format to support WYSIWYG.   Result : Success. This was one of the early prototypes that paved the way for the large WYSIWYG engineering project. .  💡 CSS-in-JS will make it easier and lower-risk to implement dark mode.   ✅ We put together a basic dark mode prototype using a CSS-in-JS solution.  ❌ The CSS-in-JS migration path causes us to fork our styles into two places that don’t stay coordinated without additional tooling.  ❌ CSS-in-JS doesn’t unlock new technical possibilities for us.   Result : Failure. The migration path using CSS-in-JS didn’t seem worth the investment at that point in time and we instead opted to use CSS variables, which you can read more about in  a previous blog post . .  💡 We could make Slack render in under 1 second by serving static HTML from the CDN rather than rendering a template from the server.   ✅ Our proof-of-concept is close to, or under, 1 second of load time.  ❌ We hit an insurmountable technical snag.  ❌ We can’t come convincingly close to 1 second.   Result : Success. This was the original prototype that led us to rearchitect the Slack client. You can read more about that project  in this blog post . . It’s important to write these down before you start. The goalposts are easy to move as the prototype comes together, especially when it’s exciting. . Prototyping is chance to validate an idea without all of the baggage that comes with production-quality engineering. If you’re planning to throw the code away at the end, then shortcuts like hardcoding data, skipping the tests, ignoring the linter, etc. are all fair game. . Working in a separate repo or isolating the prototype to its own page can help mitigate safety concerns and increase code readability if writing production code is necessary. Always take the time to plan your technical approach; safety is essential! . The takeaway is not the code or the output itself, but the  learning  that comes out of it. Take the time to evaluate and document the work and decide what you’ll do with this new information. What went well and what didn’t? Were there any surprises? Did you meet your success criteria? Were any of your assumptions incorrect? . Documenting the lessons learned is not just a useful way to recap the work — it can become an important historical artifact down the road when another team is considering similar ideas. The ability to say “we tried that; here’s what we found” gives others valuable signal to work with. . If your prototype failed, the next step might be to formulate a new hypothesis and try a different approach. If the prototype was successful and demonstrated the viability you were looking for, perhaps you capture what you learned in a detailed technical spec that serves as the basis for an engineering project. . Reading through this definition you might be struck by how formal and rigid it all sounds. Reality is always a little murkier — the stakeholders, goals, and personalities of each project are unique and can dictate aspects of our approach. It’s important to be aspirational in the definition of our process, though, so that we can evaluate how we’re doing over time and find ways to push for excellence. In that spirit, here’s are some anecdotes about how we prototyped our way through the  recent redesign of Slack’s desktop client . .  Suspending disbelief:  at the start of the project we had roughly the same number of designers and frontend engineers, so we paired up every day. We’d come up with the next iteration of a concept we wanted to try — for example, what if Slack only had a “start menu” from which all controls were accessible? — and spend the day building it, having decided on some criteria and scope. Our  development environments  allow us to build and share frontend code with others, so at the end of each day we’d link the team to a build and ask for feedback. For these prototypes we’d hack and slash our way through existing code, deleting and rewriting anything necessary to communicate the concept. It was messy and fun and it required that the team have a high degree of trust and an ability to suspend disbelief; evaluating prototypes is hard if you don’t communicate the success and failure criteria, and sometimes it requires that you ask the evaluator to use their imagination to fill in the gaps. .  Prototyping for an audience:  as our concepts got further along we reached a point where we needed to widen the audience. This required that we build several variations into the codebase with the ability to switch between them. The fidelity of these prototypes was higher — meant for a general internal audience with less of a tolerance for bugs and missing pieces — but we continued to ask for specific feedback based on the success and failure criteria we outlined for each variation. We spent time up front building in the guardrails to ensure that this code was clearly marked as prototype code and that it was safe to check into the codebase. .  Dialing it in:  even as we wrote the final production version of our designs, we took the time to iterate on smaller and smaller details. We’d finish a piece and ask, what if this was centered instead? What if it used a color from the user’s theme? What if we built this the other way? Keeping this iterative spirit alive helped us stay connected to the larger goals we were working towards. . Our use of prototyping is still ramping up at Slack. The challenge of communicating this process alone has kept us busy, but to that end we’ve added it to a set of company-wide product principles and are encouraging teams to find more ways to iterate. If this sounds like the style of work you’re looking for,  join us ! ", "date": "2020-05-19"}, {"website": "Slack", "title": "Scaling End-to-End User Interface Tests", "author": ["Kavita Chodavarapu", "Bryant Ung"], "link": "https://slack.engineering/scaling-end-to-end-user-interface-tests/", "abstract": " At Slack, Quality is a shared responsibility. The Quality Engineering team is focused on creating a culture of testing, increasing test coverage, and helping the company ship high-quality features faster. We encourage all our developers to write and own end-to-end (E2E) tests. In turn, Quality Engineering (QE) is responsible for the frameworks used and provides best practices for writing reusable, scalable, and maintainable tests. . In this post, we are going to walk you through our journey on how we came up with a reusable automation framework and some of the positive impact we have seen. . E2E UI automation at Slack started as a project built during HackDay, Slack’s internal hackathon. We wanted to implement and adopt a framework which supported JavaScript and used  cypress.io  for running tests. The framework built during the hackathon quickly gained adoption and several engineers contributed by adding new tests. Since there were no guardrails on how to add these tests, the framework ended up with a lot of duplicate code and flaky tests. This led to random test failures and longer triage shifts. . But where there is a problem, there is always an opportunity! The QE team decided to look for a better way to solve some of the pain points we were having within cypress. Our solution is a variation of the Page Object Model: We created a layer of abstraction between user interface and the actual test. We time-boxed the effort to one month and worked on using the proof of concept on a set of tests. . To make it easier to read, write, and maintain our end-to-end tests, we created a number of Slack-specific methods and bundled them up in a library. Thanks to this abstraction layer, we have a centralized place to define UI actions. Since Slack is largely an application as opposed to a traditional website, we thought about components rather than pages — allowing engineers to interact with, for instance, the channel sidebar. To make the code easier to write and read, our JavaScript objects are chainable. . For the UI Abstraction/Page Object Model approach, we broke things out in the components. . We came up with a few  best practices  to guide our work: . So, with those practices in mind, let’s take a look at how we organized our classes and components. . The above diagram represents a simplified version of our page object model (POM): . If we take a look at Slack client we can break things up into components as such: . Let’s take a look at  MessageInputComponent : . The  MessageInputComponent  extends  BaseComponent  where all the common methods and properties are defined for a component. From there, we pass in the selector for the  MessageInputComponent  and define what selectors live within the  MessageInputComponent . . We created a top-level object called client. If, within a test, we want to type  hello world , it would look like this: . When we compare the test results from before versus the tests that have been migrated to use UI Abstraction framework, we’re able to see a 60% reduction in flakiness. Both automation engineers and front end engineers found it easier to add and maintain more tests. . Our team is currently working on migrating all the E2E tests to use UI Abstraction. There are several efforts underway to reduce test flakiness further. . As you can see, we are working on solving hard problems. Interested in joining the team?  Check out our open roles ! ", "date": "2020-06-11"}, {"website": "Slack", "title": "The App Sandbox", "author": ["Charlie Hess"], "link": "https://slack.engineering/the-app-sandbox/", "abstract": " Batten down the hatches! The app sandbox is now enabled for all web content. This is a fancy way of saying we’ve dialed up the security of the app. It wasn’t unsafe before, but it’s double safe now. . What is the “app sandbox,” what is it protecting against, and why does it matter? This post attempts to answer those questions, and provides a technical guide for Electron developers that want to bring their app’s security model more in line with Chromium’s. It’s divided into two parts: . Let’s start with an analogy for sandboxing, brought to you by political science. . Imagine that you’re designing a government, and you install a leader (call it a monarch or president) at the top of your hierarchy. This president is granted special powers via election, and those powers make their job easier by removing obstacles. Phrased differently: they have shortcuts to alter the system. This is all well and good until a malicious leader begins acting unilaterally to accomplish his goals: creating new laws out of thin air, declaring war over trifles, etc. Worse still, his goals are detrimental to the system and out of alignment with prior leaders. . To handle this case you might add  safeguards  that limit the damage a bad actor can do. You might start with a clause that allows the leader to be removed in some exceptional cases. You could add your own version of the War Powers Act, forcing them to seek approval from an external committee before taking military action. If all of these were ineffective you might mark the leader  untrusted  and begin to ignore some of their requests. And that, in a nutshell, is a kind of sandbox. It’s a fundamental shift in assumptions that lets us handle malicious actors  within  the system. . It’s hard to do at a time like this, but let’s take this analogy back to our problem domain. Recall that Electron is a marriage of traditional web content (Chromium) with some blessed JavaScript that can alter the system (Node). When our app is functioning normally, giving a traditional webpage the ability to write files or create new windows enables the rich functionality that folks have come to expect of a desktop app. But if a bad actor has control of the webpage, via something like a cross-site scripting (XSS) vulnerability, its Node powers can be co-opted for evil: . And you might think, “I won’t pollute the global scope like that,” or even “None of my content uses Node integration.” But don’t underestimate the ingenuity of would-be attackers! Our bug bounty program is filled with clever trapdoors — here’s an example that relied on us overriding  window.open . . You might say, “If I were building a website, I’d simply prevent all XSS.” Alas, there are other routes of attack. For example, does your app display an image from the user? Cleverly constructed images can lead to RCE ¹ , sometimes even those sent to your backend for processing ² . Even the innocuous task of parsing JSON has been exploited before. ³  RCE, like love, is all around us. In an environment with so many potential avenues to defend, how do we keep our users safe? Kenton Varda, at CloudFlare, says it best: ⁴  . Well, here’s the thing, nothing is secure. Security is not an on or off thing. Everything has bugs. Virtual machines have bugs, kernels have bugs, hardware has bugs. We really need to be thinking about risk management, ways that we can account for the fact that there are going to be bugs and make sure that they have minimum impact. . Submarine designers don’t ignore the possibility of a leak somewhere on the vessel, they make  containment  essential to its operation. The decision Chromium arrived at–many years ago–was a similar kind of last-resort mitigation. ⁵  In the browser landscape, they were uniquely positioned, with a multi-process architecture that separates web content from a trusted orchestrator. In that sense their “containers” were already drawn up. By flipping the assumption of web content from trusted to compromised, and constraining what it could do, they limit the consequences of a worst-case scenario. Electron is a fledgling framework in comparison, but, being based on Chromium, it can leverage these security features. In Electron this capability is available as the  sandbox  option, under  webPreferences . ⁶  . There are, as always, tradeoffs to consider before enabling this feature. Critically, turning on the sandbox means your renderer processes  cannot use Node , or any external module that depends on Node’s core modules (e.g.,  fs ,  crypto ,  child_process , etc.). It also dramatically reduces the surface area of Electron that’s available in the renderer: just about all you can do is send messages to the main process (and as it turns out, that’s all you’ll need). With the new abstractions Electron provides around V8 primitives, you should be able to sandbox your web app without any loss in functionality. How we did that is the topic of the next section. . In making the move to sandbox, the first area we stumbled on was the organization of our code. Although we weren’t often referencing Node in our renderer bundles, we had a common folder of utilities that, theoretically, could be shared across main and renderer processes. Over the years, that folder had accumulated  all kinds  of methods: not just ones that were genuinely reusable. They were often grouped into overly large and ambiguous files: e.g., a  logging-helpers  file with both (reusable) string utilities and (non-reusable)  fs  utilities. Renderer code might import one of those files to get at a string utility and take a dependency on  fs  as a side effect. Untangling this web of dependencies was no small feat, but one tool that made it more manageable was  dependency-cruiser . ⁷  It let us visualize import chains to see how code was getting pulled in, and, once rearranged, prevented similar imports with validation rules: like a linter for code organization. . Here’s a toy example illustrating the kind of graphs that helped us. In it we see that a renderer-side component is referencing  fs  and  path , from a chain that starts with  common/logger : . In practice it was not uncommon to see import chains dozens of files long, with thousands of nodes rendered, but with options like  exclude ,  focus , and  doNotFollow  you can prune the tree into something legible. An unexpected benefit of this exercise was trimming the size of our JS bundles (particularly the preload, which runs on every page navigation) by relocating unexpected code or removing unused code. After this Marie Kondo maneuver, our folder structure became more representative of the webpack bundles we make at build time, with distinct main, renderer, and preload folders. We used dependency-cruiser rules to restrict “crossing the streams,” that is, having references between bundles. You can even block Node imports with a rule like: . For those making apps that embed web content, you already know the importance of the preload script for extension and customization. It’s your only chance to expose desktop functionality to the page, and to do it safely you’ll need to use  context isolation . As a web preference, it’s a cinch to turn on, but once you do you’ll be unable to expose objects from the preload to your guest page–kind of the whole point of using a preload script. So, what gives? Well, a preload script is a type of  content script , and, in Chromium, content scripts are as isolated from one another ⁸  as my Flatbush mate’s Twitter timeline is from your uncle’s Facebook feed in Minnesota. While undesirable for society, this is a good thing for JavaScript. It means that the script running in a Chrome extension you installed at the behest of a Redditor can’t, for example, redefine  JSON.parse  in the page you are viewing now. But it’s less convenient when it comes to the scripts running side-by-side in your Electron app: you sure wish they could have an honest dialogue. . To address this shortcoming, Electron added a new module called  contextBridge . ⁹  It lets you build a bridge between two isolated worlds–in our case, the preload script and the web app–in the form of a global object. Here’s a quick example to show how these pieces fit together. Say we want to use a Node module to display a notification when an event occurs in the guest page. Prior to the sandbox, we could have used the module  within  the preload script. Prior to context isolation, we could have exposed  myApi  to the page by assigning it on the global scope. But in this new world, our setup looks like: . Notice the following changes: . The takeaway is that everything your app could do prior to these security features, you can still do now. It might just take a few more hops. 🐇 . We hope this helps you make Electron apps that are every bit as secure as the browser. Sandboxing your app is not easy to do, particularly when carrying over legacy infrastructure: Slack is one of the first collaboration apps to do it. We’d like to thank the researchers who contributed bug reports nudging us in this direction: particularly  Oskars Vegeris  and  Matt Austin . If you work in security and this sounds interesting to you,  we’re hiring ! ", "date": "2020-06-24"}, {"website": "Slack", "title": "A Terrible, Horrible, No-Good, Very Bad Day at Slack", "author": ["Laura Nolan"], "link": "https://slack.engineering/a-terrible-horrible-no-good-very-bad-day-at-slack/", "abstract": "  This story describes the technical details of the problems that caused the Slack downtime on May 12th, 2020. To learn more about the process behind incident response for same outage, read Ryan Katkov’s post, “ All Hands on Deck ”.  . On May 12, 2020, Slack had our first significant outage in a long time. We  published a summary of the incident  shortly after, but this story is an interesting one, and we’d like to go into more detail on the technical issues around it. . The user-visible outage began at 4:45pm Pacific time, but the story really begins around 8:30am that morning. Our Database Reliability Engineering team was alerted for a significant load increase in part of our database infrastructure at the same time as our Traffic team received alerts that we were failing some API requests. The increased load on the database was due to a rollout of a configuration change, which triggered a longstanding performance bug. The change was quickly pinpointed and rolled back — it was a feature flag which performed a percentage-based rollout, so this was a fast process. We had some customer impact, but it lasted only for three minutes and most users were still able to send messages successfully throughout this brief morning incident. . One of the incident’s effects was a significant scale-up of our main webapp tier. Our CEO Stewart Butterfield has written about some of the  impact  of the lockdown and stay-at-home orders on Slack usage. As a result of the pandemic, we’ve been running significantly higher numbers of instances in the webapp tier than we were in the long-ago days of February 2020. We autoscale quickly when workers become saturated, as happened here — but workers were waiting much longer for some database requests to complete, leading to higher utilization. We increased our instance count by 75% during the incident, ending with the highest number of webapp hosts that we’ve ever run to date. . Everything seemed fine for the next eight hours — until we were alerted that we were serving more  HTTP 503  errors than normal. We spun up a new incident response channel and the on-call engineer for the webapp tier manually scaled up the webapp fleet as an initial mitigation. Unusually, this didn’t help at all. We very quickly noticed that a subset of the webapp fleet was under heavy load while the rest of the webapp instances were not. Multiple strands of investigations began, looking into both webapp performance and our loadbalancer tier. A few minutes later, we identified the problem. . We use a fleet of HAProxy instances behind a layer 4 load-balancer to distribute requests to the webapp tier. We use Consul for service discovery, and  consul-template  to render lists of healthy webapp backends that HAProxy should route requests to. . We don’t render our webapp host list directly into our HAProxy configuration file, however. The reason for this is that updating the host list via the configuration file requires reloading HAProxy. The process of reloading HAProxy involves creating a brand-new HAProxy process while keeping the old one around until it’s finished dealing with in-flight requests. Very frequent reloads could lead to too many running HAProxy processes and poor performance. This constraint is in tension with the goal of autoscaling the webapp tier, which is to get new instances into service as quickly as possible. Therefore, we use  HAProxy’s Runtime API  to manipulate the HAProxy server state without doing a reload each time a web tier backend comes into or goes out of service. It’s worth noting that HAProxy can  integrate  with Consul’s DNS interface, but this adds lag due to the DNS TTL, it limits the ability to use Consul tags, and managing very large DNS responses often seems to lead to hitting painful edge-cases and bugs. . We define HAProxy  server templates  in our HAProxy state that are effectively ‘slots’ which our webapp backends can occupy. When a new webapp instance is provisioned, or an old one becomes unhealthy, the Consul service catalog is updated. Consul-template renders a new version of the host list, and a separate program developed at Slack, haproxy-server-state-management, reads that host list and uses the HAProxy Runtime API to update the HAProxy state. . We run M parallel pools of HAProxy instances and webapp instances, each pool in its own AWS Availability Zone. HAProxy is configured with N ‘slots’ for webapp backends in each AZ, giving a total of N * M backends that can be routed to across all the AZs. A few months ago, this total was more than ample headroom — we’d never needed to run anything even approaching that number of instances of our webapp tier. However, after the morning’s database incident, we were running slightly more than N * M instances of the webapp. If you think of the HAProxy slots as a giant game of musical chairs, a few of these webapp instances were left without a seat. That wasn’t a problem — we had more than enough serving capacity. . However, over the course of the day, a problem developed. The program which synced the host list generated by consul template with the HAProxy server state had a bug. It always attempted to find a slot for new webapp instances before it freed slots taken up by old webapp instances that were no longer running. This program began to fail and exit early because it was unable to find any empty slots, meaning that the running HAProxy instances weren’t getting their state updated. As the day passed and the webapp autoscaling group scaled up and down, the list of backends in the HAProxy state became more and more stale. . By 4:45pm Pacific, most HAProxy instances were only able to send requests to the set of webapp backends that had been up since the morning, and this set of older webapp backends was now a minority of the fleet. We do regularly provision new HAProxy instances, so there would have been a few fresh HAProxy instances that had correct configuration, but most of them were more than eight hours old and therefore were stuck with full and stale backend state. The outage was eventually triggered at the end of the business day in the US because that’s when we begin to scale down the webapp tier as traffic drops. Autoscaling will preferentially terminate older instances, so this meant that there were no longer enough older webapp instances remaining in the HAProxy server state to serve demand. . Once we knew the cause of the failure, it was resolved quickly with a rolling restart of the HAProxy fleet. After the incident was mitigated, the first question we asked ourselves was why our monitoring didn’t catch this problem. We had alerting in place for this precise situation, but unfortunately, it wasn’t working as intended.The broken monitoring hadn’t been noticed partly because this system ‘just worked’ for a long time, and didn’t require any change. The wider HAProxy deployment that this is part of is also relatively static. With a low rate of change, fewer engineers were interacting with the monitoring and alerting infrastructure. . The reason that we haven’t been doing any significant work on this HAProxy stack is that we’re moving towards  Envoy Proxy  for all of our ingress load-balancing (we’ve recently moved our websockets traffic onto Envoy). While HAProxy has served us well and reliably for many years, it also has some operational sharp edges, of exactly the kind highlighted by this incident. The complex pipeline we use to manipulate HAProxy server state will be replaced by Envoy’s native integration with an xDS control plane for endpoint discovery. The most recent versions of HAProxy (since the 2.0 release) also solve many of these operational pain points. However, Envoy has been our proxy of choice for our internal service mesh project for some time, and this makes a move to Envoy for our ingress load-balancing attractive. Our initial testing of Envoy + xDS at scale is very exciting and this migration should improve both performance and availability going forward. Our new load-balancing and service discovery architecture is not susceptible to the problem that caused this outage. . We strive to keep Slack available and reliable, and in this case, we failed. We know that Slack is a critical tool for our users, and that is why we aim to learn as much as we can from every incident, whether customer visible or not. We apologize for the inconvenience this outage caused and will continue to use the lessons learned to drive improvements in both our systems and our processes. ", "date": "2020-06-29"}, {"website": "Slack", "title": "All Hands on Deck", "author": ["Ryan Katkov"], "link": "https://slack.engineering/all-hands-on-deck/", "abstract": "  This story speaks to the process behind incident response at Slack and uses the May 12th, 2020 outage as an example. For a deeper technical review of the same outage, read Laura Nolan’s post,  “A Terrible, Horrible, No-Good, Very Bad Day at Slack”   . Slack is a critical tool for millions of people, so it’s natural when Slack going down can feel as stressful as when our power goes out, when the internet stops working, or when our smartphone runs out of battery. What does Slack do when Slack goes down? . On those rare times we suffer a total service disruption, the response is, essentially, All Hands on Deck. This happened on the afternoon of May 12th, 2020 at 4:45pm Pacific, which started as a normal day like any other at Slack. This is the story of that day, who was involved in the effort to restore Slack service, and, chiefly, what that process looks like. We hope it gives insights into the machinery, both human and computational, that runs Slack for our millions of customers. . Slack normally deploys software and configuration changes on a cycle each day. That morning it was business as usual, and we deployed a set of pull requests to our production environment. But suddenly, something was amiss. We saw a surge in requests against our Vitess database tier that serves user data and some customers began to experience sporadic errors. In order to accommodate the increased, unexpected demand, our API service automatically scaled up 80%. Within 13 minutes, the problem was remediated by a rollback of the code, and we scaled back down to normal host counts some time later. . The day went on as usual for all of us at Slack, until 4:45PM. . During one of our normal deploy operations, engineers noticed a surge of errors in dashboards used to monitor deploys. Questions were raised. Messages were posted in channels. Soon, alerts started to go off. It was clear there was a serious problem. . Our incident response begins when someone invokes our custom built Incident Bot with an “/assemble” slash command, which in turn pages critical responders in Engineering and Customer Experience via PagerDuty. . Before we move on with our story, it’s worth doing a short overview of Slack’s Incident Response process. We’ve spent well over a year developing and tuning the process to ensure we can rapidly recover from incidents and strive for industry-leading reliability. . Slack has grown rapidly in the last six years, and at first, our incident process didn’t evolve to keep pace. Once upon a time, there was an on-call rotation staffed with operations engineers, coined AppOps, short for Application Operations. This was the rotation that would  always  get paged for Slack troubles and be responsible for fixing Slack. They tirelessly responded to every page, kept Slack up, and moved on with their day. . It became clear that as the company and Slack’s usage grew, this hardy, heroic rotation could not keep up with the volume of incidents, nor could they amass the knowledge required to maintain operational awareness of each of Slack’s growing services. They could not, as a limited set of humans, sustain the pressure and toil of such a high-stress, on-call rotation. . In 2018, Reliability Engineering came into the picture and established an incident response process that was modeled around  FEMA’s Incident Response guidelines . As a result, the AppOps group on-call was disbanded, each development team at Slack became responsible for their own services via team on-call rotations, and the Major Incident Command was staffed by engineers trained to facilitate incident response. Reliability Engineering is responsible for the program material, the training courses, and ensures that the process is smooth and has full participation throughout the company. Each Incident Commander, traditionally, is not hands-on-keyboard, but is responsible for facilitation, information gathering, resource gathering, response coordination and decision making. . The timeline of an incident is composed of four distinct stages,  Customer Impact ,  Operations Impact ,  Learning &amp; Prevention.  Within these stages, there are distinct operations that lead to the final resolution of an incident, which we’ll briefly summarize. .  Detect:  Detection of a regression or customer impact via, ideally, SLO failure, automated alerts driven by metrics, or less ideally, customer reports. .  Assemble:  The Subject Matter Expert (SME) who gets the initial  Detect  page will invoke the Assemble process and alert the Incident Commander on-call. .  Verify:  Incident Commander discusses with SME on initial scope of the impact as well as severity. Our first Conditions, Actions and Needs (CAN) reports can begin. .  Dispatch:  Incident Commander, based on the CAN report, will page additional SME’s as needed. .  Mitigate:  A diagnosis is presented, and a mitigation action is proposed &amp; executed. .  Repair:  Typically, mitigations are temporary — an analogy would be duct-taping a leak. The repair action is a long term fix if needed. .  Review:  The Incident Review process kicks off and we review the incident and extract lessons and action items from it. .  Action Items:  We complete action items based on priority within defined SLA’s to further enhance prevention of the incident recurring. . Nearly every well-run incident can fit into the model above, and this particular incident was no exception. . Even though the incident response process started 19 minutes prior to the full outage, we were not yet completely aware that we were headed into a full outage. At the time, responders from our edge and load balancing traffic team, and our web infrastructure service team were still gathering information, collecting signals from dashboards and attempting to drill down the root cause of the alerts. Our Customer Experience teams, which are always closely aligned with our Incident Response process, were reporting incoming tickets describing customer trouble in real-time. All signals pointed to problems in our main web service tier, serving Slack’s API. The load balancers were reporting timeouts on the health checks to each API instance and thus removing them from service. Our instrumentation showed that API response times were elevated, as a result of the web workers being heavily loaded. . One of the responders declares, “we’re about to go offline, we just lost all main_wwws in the webapp pool”. This is the moment where you feel the blood drain from your face. . What does Slack do when Slack goes down? . In such unfortunate situations where we aren’t able to rely on Slack, we have prescribed alternative methods of communication. Following incident runbooks, we quickly moved to the incident Zoom and elevated the response to a Sev-1, our highest severity level. Executives were paged, per the runbook. A company-wide email was sent out, with links to department specific runbooks for full site outages. Our real-time client instrumentation, which sends telemetry to an isolated endpoint, showed that success rate on the clients had dipped to abysmal levels. It was all hands on deck at this point. . Engineers from different teams joined on the Zoom call, scribes were recording the conversation on Zoom for later dissemination, opinions and solutions were offered. Graphs and signals were shared. It’s a flurry of action and adrenaline that the Incident Commander must guide and keep the right people focused so that the most appropriate action can be taken, balancing risk vs. restoring service. Multiple workstreams of investigation were established and assigned with the goal of bringing remediation in the most efficient manner. . The process continues until a resourceful human finds the right answer. . At this point, our investigation was pointing to issues in the configuration for our  HAProxy  load balancing tiers. Something didn’t add up. We use a combination of  consul-template  and a bespoke HAProxy management service to maintain the manifest of active servers for our web service backend. There were servers in the manifest that weren’t supposed to be there. Soon, it became clear we had stale HAProxy configuration files, as a result of linting errors preventing re-rendering of the configuration. . An engineer proposed that we stop HAProxy, delete its state file, restart consul-template to refresh the manifest and restart HAProxy. There was a consensus on this being the most correct course of action and two engineers set off to test the hypothesis. The hypothesis proved correct as a control portion of servers began to once again report healthy and receive traffic. The engineers then used automation to safely execute the prescribed set of commands across our availability zones. . We saw near-immediate relief from the actions. 48 long minutes after the start of the outage, our remediation efforts were successful and Slack’s service was restored. Our Customer Experience experts declared reports of happy customers. Our Incident Commander declared “Under Control” at this point which is a signal that remediation was successful and that we should begin work towards the final state, “All Clear”. . In order to bring the incident to a close and declare an All Clear, the Incident Commander must review the current signals, pose questions to engineers such as, “What are the risks of recurrence?” or “Do we have appropriate alerting in place?”. In this case, part of the all-clear work also involved restoring the state of the Slack desktop client which we discovered experienced a regression as a result of this outage, requiring a restart to restore service. Frontend Foundations engineers consulted with Customer Experience on the best course of action to do so, and CE set upon to notify their customers of the workaround. . When it was apparent that most customer tickets had reached resolution, our metrics indicated stability for some time, and the engineers agreed that recurrence would not occur, the Incident Commander called All Clear. . The All Clear isn’t the end and it isn’t job-well-done and back-pats for everyone. The cornerstone of every incident response process is the Feedback Loop: a system of failure, remediation, learning and repair that allows engineering teams to apply targeted pressure at problem areas to ensure that incidents don’t recur. The remediation has been completed here, thus begins the learning. . The incident review is a ceremony practiced in many engineering circles that gives engineers a psychologically safe space to discuss and learn from an incident. The Feedback Loop continues after the meeting where action items are created, assigned to teams, and given a priority so they are completed on schedule. . A successful incident response process generates a positive feedback loop, improving knowledge and understanding of your complex systems and how they fail, driving down technical debt and risk of change, and ultimately reducing the blast radius of incidents and increasing the reliability of your service. ", "date": "2020-06-29"}, {"website": "Slack", "title": "Blocking Slack Invite Spam With Machine Learning", "author": ["Aaron Maurer"], "link": "https://slack.engineering/blocking-slack-invite-spam-with-machine-learning/", "abstract": " A fact of life for building an internet service is that, sooner or later, bad actors are going to come along and try to abuse the system. Slack is no exception — spammers try to use our invite function as a way to send out spam emails. Having built up the infrastructure to easily deploy machine learning models in production, we could develop a model relatively quickly that could accurately predict which invites were spam and block them from ever being sent out. There’s nothing ground-breaking here; instead, this post aims to describe the sorts of problems that can be best solved with machine learning, along with some tips and tricks on how to do so. . When you create a team in Slack, the first thing you do is invite the people you’ll be working with: your co-workers, friends, classmates, or teammates. You enter their email addresses, and Slack sends out the invites on your behalf. These invites usually have a high acceptance rate. However, in a small minority of cases, we see teams where no invites are accepted, or those few users who join, quickly leave. These are typically teams where spammers are trying to use the reputable Slack email domain to reach their targets. The result might look like this: .   . Here, the team was named “Go to www.definitely-not-a-scam.top to win $500!” The goal was never to get someone to join the team, just to have that text sent out in an email. This sort of phishing is dangerous for our customers and detrimental to our brand, so it’s imperative that we prevent it. . Slack’s first solution to spam was to block or rate-limit invites from teams based on hand-tuned rules. Some of these included checking IP addresses against a deny list, checking for certain strings in the text of the invite, and checking for a set of regex matches. We had a Slack channel that would include notifications both when we blocked a team and when, after the fact, it looked like we should have blocked a team. Based on what they saw from this, engineers would update the rules to hopefully catch similar patterns in the future. This system mostly worked, but at a cost: . The situation above, where a series of rules were being updated based on a stream of historical data, is exactly the sort of thing machine learning can do a lot better and cheaper than humans. We call this sort of problem a “supervised” machine learning problem, where we have a label (an invite being spam) that we want a machine to predict. However, to teach a machine to do it, you need data. In particular, for each of our observations (invites) we need historical records of: . The hand-tuned model was already based on a great feature set, which was straightforward to recalculate for old invites, so that part was handled. The labels were less straightforward; we hadn’t systematically recorded the teams we thought were spammy. For something like spam, where a human will know it when they see it, crowdsourcing labels are always possible. However, this is hard to do at scale. Instead, as a more readily available proxy, we tried to predict whether an invite would be accepted or not. We already had these labels for every invite. Though invites are regularly ignored or rejected for normal teams, on a team level the rate can readily distinguish between spam and not spam; it’s extremely rare for normal teams to have a low acceptance rate. This was a low-risk approach; if we started blocking invites that weren’t spam but people weren’t going to accept anyway, there’s little harm. . There are two other data tricks that are worth mentioning: . We trained a sparse logistic regression on our data, a classic go-to for any ML engineer. It considers approximately 60 million features, or facts, about each invite, winnowing out many that aren’t useful for predicting invite acceptance using regularization, and then comes up with a score for the presence of each feature. The total score for an invite is just the sum of all the scores for all the features for that invite. Passing this score through the logit function gives us a predicted probability of the invite being accepted. While simpler than most other models, this offers a few advantages: .   . We found that using several large sets of features gave our model a lot of predictive power, including: . Often, the biggest hurdle going from a model that can predict something accurately on data “offline” to having an impact in the product is figuring out how to run and score that model “online”. This is particularly a hurdle for data scientists, who often have the tools for the former but not the latter. Thankfully, here at Slack we built an in-house service for serving model predictions, which abstracts most of the difficult part of that process away. To deploy a machine learning model, one only needs to implement a lightweight Python class with a generic prediction method. The service then takes the class and deploys it as a microservice through Kubernetes, which can be queried from the rest of our tech stack. For our spam model, this has allowed us to quickly take the model trained offline and start scoring it on our production traffic. For regularly scheduled updates, the service just checks a static folder in S3 for new versions of the model. . Overall, the machine-learned model outperformed the old hand-tuned filter. While neither let through much spam, the machine learning model was much better at preventing false positives. Only 3% of the invites it would have flagged ended up being accepted when allowed through, while around 70% of the invites flagged by the old model actually ended up being accepted, indicating that most of them were pretty normal rather than spammy. Most importantly though, automating the entire process freed up hours of human time each week. The channels where we coordinated updating the hand-tuned model went from often seeing hundreds of messages a month to being basically dormant. While we still log all blocked invites to a channel and double check it periodically, human interaction is rarely required. This is what you hope to see from machine learning; let machines do what they are best at, and free up humans for the work only they can do. . Interested in taking on interesting projects, making people’s work lives easier, or just getting on the machines’ good side before the technological singularity?  We’re hiring ! ", "date": "2020-08-10"}, {"website": "Slack", "title": "How to Have an Impactful Internship… Virtually", "author": ["Nikita Ashok", "Jake Polacek"], "link": "https://slack.engineering/how-to-have-an-impactful-internship-virtually/", "abstract": "  The start of any internship brings a wide range of emotions, from excitement to nervousness. After months of anticipating our first day at Slack, reality sunk in that this summer would be extremely different from any other. Due to the pandemic, our entire experience would be virtual.   . The start of any internship brings a wide range of emotions, from excitement to nervousness. After months of anticipating our first day at Slack, reality sunk in that this summer would be extremely different from any other. Due to the pandemic, our entire experience would be virtual.  .  As the two interns for the Product Security Foundations (PSF) team at Slack, interning virtually gave us an opportunity to exercise our communication, collaboration, and technical skills. From the first day, the support from everyone at the company was evident; they wanted us to know that we were meant to be at Slack.   . As the two interns for the Product Security Foundations (PSF) team at Slack, interning virtually gave us an opportunity to exercise our communication, collaboration, and technical skills. From the first day, the support from everyone at the company was evident; they wanted us to know that we were meant to be at Slack.  .  Normally our internships would have started with us moving to a new city, commuting to a new office, and walking into a new building for what might be the first time. This summer though, all we had to do was sit down in our own houses and click “Join Meeting.” The first week of our internships consisted of the usual — setting up our computers, learning the language we were going to use, learning about our project, and of course, lots of introductions. For us, however, it all happened in an unusual manner — through Slack and Zoom. Using Slack for the onboarding process was unique as it helped us learn new ways of using the platform that we’d be working on (as well as proper Slack etiquette). Despite packed calendars, it was easy enough to hop between meetings as needed without having to worry about how long it would take to get to the next meeting room because all we needed to do was click a different link.  . Normally our internships would have started with us moving to a new city, commuting to a new office, and walking into a new building for what might be the first time. This summer though, all we had to do was sit down in our own houses and click “Join Meeting.” The first week of our internships consisted of the usual — setting up our computers, learning the language we were going to use, learning about our project, and of course, lots of introductions. For us, however, it all happened in an unusual manner — through Slack and Zoom. Using Slack for the onboarding process was unique as it helped us learn new ways of using the platform that we’d be working on (as well as proper Slack etiquette). Despite packed calendars, it was easy enough to hop between meetings as needed without having to worry about how long it would take to get to the next meeting room because all we needed to do was click a different link. .  The onboarding sessions fully prepared us to start working at the end of our first week, but the highlight for us was the time to just interact with the other new hires. One would expect it to be hard to meet people for the first time while sitting on a Zoom call, but all the opportunities provided to us — and the times when we scheduled our own intern hangouts — made the whole onboarding process enjoyable. Throughout the first week, much of the uncertainty of what a virtual internship would be like was washed away as we acclimated to the routine of joining meetings and working independently.  . The onboarding sessions fully prepared us to start working at the end of our first week, but the highlight for us was the time to just interact with the other new hires. One would expect it to be hard to meet people for the first time while sitting on a Zoom call, but all the opportunities provided to us — and the times when we scheduled our own intern hangouts — made the whole onboarding process enjoyable. Throughout the first week, much of the uncertainty of what a virtual internship would be like was washed away as we acclimated to the routine of joining meetings and working independently. . Jake’s Onboarding Tip: .  Don’t be afraid to hop into, or put together, hangouts with the other new hires. . Nikita’s Onboarding Tip:  . Looking at the busy calendar the first week can be a bit overwhelming, but having my calendar linked with Slack for notifications before meetings start has been super helpful. .  The week before Nikita started, we hopped on a Zoom call with our mentors and Suzanna, the senior manager of the Product Security Foundations (PSF) team. From the get-go, they were super welcoming and excited to teach us about our project. On each of the first days, we had 1-on-1 meetings with Suzanna to go over the onboarding documents that she put together for us. These contained relevant information about Slack, PSF, our roles as interns, what to expect for the summer, and what was expected from us. Having access to this information made getting acquainted with the company and the team easy, and the documents have guided us throughout the summer. She also put together a welcome lunch for us to meet the entire team. After a round of introductions, we already started feeling like part of the team as we learned more about everyone, such as what they’re working on, which flavors of La Croix are superior (the obvious answer being Coconut), and their least favorite t-shirt brand. Even though we haven’t had the opportunity to meet anyone on the team face-to-face, we still developed professional and personal relationships through coffee chats, playing virtual games together, daily stand ups, 1-on-1s, and our not-so-secret Secret Intern Meetings.  . The week before Nikita started, we hopped on a Zoom call with our mentors and Suzanna, the senior manager of the Product Security Foundations (PSF) team. From the get-go, they were super welcoming and excited to teach us about our project. On each of the first days, we had 1-on-1 meetings with Suzanna to go over the onboarding documents that she put together for us. These contained relevant information about Slack, PSF, our roles as interns, what to expect for the summer, and what was expected from us. Having access to this information made getting acquainted with the company and the team easy, and the documents have guided us throughout the summer. She also put together a welcome lunch for us to meet the entire team. After a round of introductions, we already started feeling like part of the team as we learned more about everyone, such as what they’re working on, which flavors of La Croix are superior (the obvious answer being Coconut), and their least favorite t-shirt brand. Even though we haven’t had the opportunity to meet anyone on the team face-to-face, we still developed professional and personal relationships through coffee chats, playing virtual games together, daily stand ups, 1-on-1s, and our not-so-secret Secret Intern Meetings. . The Product Security team celebrating Nikita’s birthday!  . Our Meeting-the-Team Tip:  . Similar to onboarding, take some time to just get to know the other members of your team in a non-work related way. For example, one of the first things we did when we started working was setting up 1-1s with the rest of our team. Throughout the summer, we also had team social hours over Zoom with various online games.   . The Product Security team during our bi-weekly meeting  .  As part of the Product Security Foundations team at Slack, we were given the opportunity to dive deep into the technical aspects of security, by developing a new library. Meeting the goals of our team and delivering a solid technical product was crucial in order for us to have an impactful internship.  . As part of the Product Security Foundations team at Slack, we were given the opportunity to dive deep into the technical aspects of security, by developing a new library. Meeting the goals of our team and delivering a solid technical product was crucial in order for us to have an impactful internship. .  In our first meeting, Nikita and I were introduced to a problem that security teams aim to prevent: Cross-site scripting (XSS) attacks. These occur when an attacker injects and executes malicious code into our own HTML or CSS code in an attempt to retrieve sensitive information. If an unwitting user sends a link in a Slack message, and if the HTML is not already properly sanitized, an embedded script tag written by the hacker could execute within the channel during the unfurling process, allowing them to run an attacker-controlled script in other users’ browsers. Slack’s Product Security team already had a solution to this — an HTML sanitization service. We were tasked with creating a better solution to this problem, a standalone HTML sanitizer in Hack, that would allow us to remove our dependency on a third party library in favour of a library that would live directly on webapp, our monolith application repo containing both backend and frontend code for Slack.  . In our first meeting, Nikita and I were introduced to a problem that security teams aim to prevent: Cross-site scripting (XSS) attacks. These occur when an attacker injects and executes malicious code into our own HTML or CSS code in an attempt to retrieve sensitive information. If an unwitting user sends a link in a Slack message, and if the HTML is not already properly sanitized, an embedded script tag written by the hacker could execute within the channel during the unfurling process, allowing them to run an attacker-controlled script in other users’ browsers. Slack’s Product Security team already had a solution to this — an HTML sanitization service. We were tasked with creating a better solution to this problem, a standalone HTML sanitizer in Hack, that would allow us to remove our dependency on a third party library in favour of a library that would live directly on webapp, our monolith application repo containing both backend and frontend code for Slack. .  Thus the beginning of our summer was dedicated to learning how an HTML sanitizer works, while the later part was dedicated to developing one. The start of the investigation process for the project posed a new challenge: In order for us to be successful, Nikita had to first create a roadmap for the project and scope various design choices.  . Thus the beginning of our summer was dedicated to learning how an HTML sanitizer works, while the later part was dedicated to developing one. The start of the investigation process for the project posed a new challenge: In order for us to be successful, Nikita had to first create a roadmap for the project and scope various design choices. .  Learning how to ask for help is a key part of any internship experience. Even in a non-virtual setting, figuring out who and when to ask for help can be quite difficult at first. With our team’s guidance, we were quickly able to reach out to specific people for more targeted help. Reading documentation and perusing Github repositories of past projects with similar infrastructure were two helpful approaches. Not only did this give us knowledge about specific technology and tools we might need, but also gave us access to people in the company we can reach out to — all of whom were very willing to help us during the summer!   . Learning how to ask for help is a key part of any internship experience. Even in a non-virtual setting, figuring out who and when to ask for help can be quite difficult at first. With our team’s guidance, we were quickly able to reach out to specific people for more targeted help. Reading documentation and perusing Github repositories of past projects with similar infrastructure were two helpful approaches. Not only did this give us knowledge about specific technology and tools we might need, but also gave us access to people in the company we can reach out to — all of whom were very willing to help us during the summer!  .  Communication is an essential part of any software development process, but effective, clear, and concise communication is especially important when it comes to distributed development. When messaging back and forth over Slack, we had to learn how to be direct with what we were saying without skimping out on important details. Some tips for clear and effective communication include removing fluff, using the proper jargon, asking specific questions, and linking important information (such as other Slack posts, repos, or websites).  . Communication is an essential part of any software development process, but effective, clear, and concise communication is especially important when it comes to distributed development. When messaging back and forth over Slack, we had to learn how to be direct with what we were saying without skimping out on important details. Some tips for clear and effective communication include removing fluff, using the proper jargon, asking specific questions, and linking important information (such as other Slack posts, repos, or websites). .  Each of us being on opposite sides of the country might have hampered our progres. Thankfully — due to effective communication and proper planning — the three-hour time difference between the two of us (Jake is in Upstate New York while Nikita is in the Bay Area) did not impede our project pace. In the five overlapping hours when we were both working, we hopped on Slack channels and Zoom calls to discuss what work we needed to do, how to go about implementing classes or functions, and what was going wrong while debugging. When only one of us was working, we could reach out to each other before or after business hours using Slack.  . Each of us being on opposite sides of the country might have hampered our progres. Thankfully — due to effective communication and proper planning — the three-hour time difference between the two of us (Jake is in Upstate New York while Nikita is in the Bay Area) did not impede our project pace. In the five overlapping hours when we were both working, we hopped on Slack channels and Zoom calls to discuss what work we needed to do, how to go about implementing classes or functions, and what was going wrong while debugging. When only one of us was working, we could reach out to each other before or after business hours using Slack. .  Despite being virtual, we were still able to leave an impact on Slack since this project lives in and is used by its codebase. Due to the fact that we no longer need to make extra calls to external libraries, our library is able to handle a larger volume of requests. In fact, implementing it directly in webapp also means that this additional volume gain does not come with the need to scale or manage the sanitizer itself as a service external to the Slack codebase.   . Despite being virtual, we were still able to leave an impact on Slack since this project lives in and is used by its codebase. Due to the fact that we no longer need to make extra calls to external libraries, our library is able to handle a larger volume of requests. In fact, implementing it directly in webapp also means that this additional volume gain does not come with the need to scale or manage the sanitizer itself as a service external to the Slack codebase.  .  Another benefit of our work here is the insight we gained from the library. This gave us more insight into how the library is structured, how it works, and why it works. The knowledge gained and documented is important because it will allow future engineers to understand what needs to be done to support more unfurl types securely.  . Another benefit of our work here is the insight we gained from the library. This gave us more insight into how the library is structured, how it works, and why it works. The knowledge gained and documented is important because it will allow future engineers to understand what needs to be done to support more unfurl types securely. . Retrospective meeting with the team! .  We didn’t expect to intern as part of a fully distributed team, but we were always amazed at the level of support we received to ensure that we could carry out this project, all while still feeling integrated into the team. Even with having to work across different time zones, this virtual internship went smoothly.   . We didn’t expect to intern as part of a fully distributed team, but we were always amazed at the level of support we received to ensure that we could carry out this project, all while still feeling integrated into the team. Even with having to work across different time zones, this virtual internship went smoothly.  .  The best part about all the lessons learned from interning virtually is that they are directly transferable to in-person workplace settings as well. Being willing to jump into conversations with other co-workers, whether it be for an in-person coffee break or a video call, can help build lasting relationships. Taking time to figure out the best person to ask questions on certain topics will improve efficiency. Improving the effectiveness of your communication is not only important in a virtual world, but it’s also important in face-to-face communication.  . The best part about all the lessons learned from interning virtually is that they are directly transferable to in-person workplace settings as well. Being willing to jump into conversations with other co-workers, whether it be for an in-person coffee break or a video call, can help build lasting relationships. Taking time to figure out the best person to ask questions on certain topics will improve efficiency. Improving the effectiveness of your communication is not only important in a virtual world, but it’s also important in face-to-face communication. .  We obviously wish we could have been in the office to meet everyone in person, but in the end we’re just thankful that we were still able to work at Slack. With all that we were able to achieve, learn, and enjoy throughout this summer, this internship is going to be one that we’ll remember forever.  . We obviously wish we could have been in the office to meet everyone in person, but in the end we’re just thankful that we were still able to work at Slack. With all that we were able to achieve, learn, and enjoy throughout this summer, this internship is going to be one that we’ll remember forever. .      We wanted to give a special thanks to our manager, Suzanna Khatchatrian, and our mentors Oliver Grubin, Jack Wilson, and Chenkai Gao – as well as everyone else we interacted with – for supporting us during our internship and making summer at Slack great.  .    . We wanted to give a special thanks to our manager, Suzanna Khatchatrian, and our mentors Oliver Grubin, Jack Wilson, and Chenkai Gao – as well as everyone else we interacted with – for supporting us during our internship and making summer at Slack great. .  Ready to help Slack solve tough problems? We’re hiring for summer 2021 internships! Check out the roles and  apply today .  ", "date": "2020-08-18"}, {"website": "Slack", "title": "A Day in the Life of a Frontend Product Engineer at Slack", "author": ["Christine Lee"], "link": "https://slack.engineering/a-day-in-the-life-of-a-frontend-product-engineer-at-slack/", "abstract": " The sound of coffee grinding wakes me up 15 minutes before my alarm. I’m not a morning person, but I always stumble bleary-eyed into the kitchen for some toast and coffee with my partner, which is my favorite daily ritual. I sit down, listen to my morning news podcasts — The Daily and AM Quickie — and check my phone for any Slack messages I might have gotten from coworkers in other timezones. . I run out the door to catch the Transbay bus, one of the more pleasant ways to commute. I knit a few rows of a blanket I’m working on until I’m conveniently dropped off at the Salesforce Transit Center right across the street from Slack’s HQ. It’s Friday, so I head to the cafeteria to grab a second breakfast 😋 and catch up with some coworkers. . Arriving at my desk, I take a look at my personal todo list to see what’s on the docket for today. I like to review a pull request or two before I dive into my own thing. We “bucket” our pull requests by tagging the whole team rather than an individual, so sometimes I get a chance to review something I’m not actively working on. I like this system because it spreads knowledge across the whole team, and I always learn something. . I have a quick stand up with the people I’m working with on our current shared channels feature. We discuss the progress of the major work streams for the project, and have any technical discussions that are easier to have in person. We take a second to check in with each other: Are we on track to hit our target dates? If not, what’s a more realistic timeline? I really appreciate this time because it gives me an opportunity to voice any concerns candidly. . Mid-morning is usually when cross-team functional meetings are held. Sometimes it’s a workshop, where someone makes a proposal for improvements affecting other frontend engineers (FE for short), like a smarter model cache eviction strategy. Other times, there’s a larger meeting for FE-wide discussions or announcements. . Today, one of the teams in Denver gives us a sneak peak of a new feature they’re calling  Workflow Builder . It’s an easier way to build apps without having to write any code or set up a server, and I’m excited about the potential uses for it. I love that it’s making the automation of tasks more accessible. After the demo, the engineers start testing it out, piping messages into the frontend channel. Not only is it fun, but it provides valuable feedback for the team in Denver. . I make a bot that’ll post questions in our tech-questions-only channel anonymously, just in case people ever feel sensitive about asking a “dumb” question. Everyone sticks to the channel’s ground rules of answering directly rather than posting a link, keeping messages relevant, and being kind to inexperience. It feels like an intellectually safe space. . Lunchtime! I take a stroll through the food truck alley outside our office, hoping my favorite samosa chaat truck is there. Today I’ve got lunch with a bunch of frontend people from different teams. We go around the circle introducing ourselves and welcoming new hires. I’m tasked with coming up with a weird icebreaker question, like, “What fruit or vegetable would you most want to be?” On days where there’s no group lunch scheduled, I’ll eat lunch with whomever is around. Yesterday, for example, the backend folks and I had a riveting discussion about what type of entrepreneurial endeavor we’d start if we had a clone. . A retrospective meeting gives us a chance to reflect on our last project. Before the meeting, I share some thoughts I had about the project in our retro channel. This includes things that went well (the thorough review of the technical spec paved a clear path!) and things we could improve on (decisions midway through could have been documented better). I take a minute to read through the thoughts shared by others, adding a ➕ plus emoji reaction to any I agree with. During the meeting, we discuss messages posted in the channel since the last retro date. There’s a clear sense from everybody of wanting the team to work better together, so we plan on actionable things moving forward. After the meeting, my manager posts a summary in channel for anyone that couldn’t attend. . Getting back to my desk, I put on headphones and dig down to work on a tweak for the context bar, a feature our amazing intern Sarah Raines from UPenn and I built together last summer. The context bar is an area right above the message input that shows up when you’re in a shared channel or direct message. It lets you know what timezone they’re in, or if they have notifications switched off. It’s a great way to avoid disturbing someone after hours. I’ll post a clarifying question to the designers and product managers in our feature channel and, once we come to a conclusion, we codify it in a message denoted by a custom gavel emoji reaction. It’s a lightweight system that allows us to keep track of decisions, and we review them with our partners in marketing and customer experience every week to ensure we’re all on the same page. . A bunch of my teammates and I sample obscure potato chip flavors someone brought from their visit to the Toronto office, where our designer works. I try a ketchup-flavored chip. It’s not bad! Our product manager also passes around a bag of egg-flavored chips. I love eggs. I did not love these chips. . It’s Ask Me Anything time in our frontend channel! Every Friday an engineer volunteers to be asked anything for fifteen minutes. We’re a distributed team, so it’s a nice way to bond and find out interesting things about my fellow teammates across physical divides. . I’ve finished up my pull request, complete with linking documents, JIRA tickets, and instructions for manual testing. I find that the more time I take to fill out these descriptions, the faster my changes are approved. Future me also appreciates it! In five months, when I ask myself, “Why was this line added?”, a handy git blame will remind me. 🙌 . I take a look at some logs I added a couple of days ago for a puzzling bug. I notice a pattern in the user agents: they’re all on Windows, and on an older browser. Eureka! I’m finally able to reproduce the issue and prepare a fix. As soon as it’s deployed, I get back to the Customer Experience Agent — CE agents are truly the MVPs at Slack — and let them know the cause and the fix. It’s an awesome feeling to be able to help. . I pack up my stuff and head off to meet my partner at the BART station. Once we’re back in the East Bay, we drive over to the YMCA for a workout, and then a quick bite at our favorite local taco joint with the most delicious vegetarian tacos and tortillas I’ve ever eaten. I round off the day with a cozy evening watching a movie and working on my blanket as it warms my lap. .  This is the second in a series of posts describing the everyday life of Engineers in different parts of Slack’s ecosystem ( see part one here ). Over the coming months, we’ll hear from Engineers on our Product Engineering, Mobile, and Frontend teams. If you’re interested in exploring job openings at Slack, visit our  Careers page !  . Christine is a Senior Software Engineer, based out of the San Francisco office. She works as a frontend product engineer to make users’ experience in shared channels better. Most recently she has worked on improving awareness of other organizations in a shared channel, admin controls, and making shared channels available on the Grid product. In her time off, Christine knits wearables and creates functional ware at her local community pottery studio. ", "date": "2020-02-05"}, {"website": "Slack", "title": "How Big Technical Changes Happen at Slack", "author": ["Keith Adams", "Johnny Rodgers"], "link": "https://slack.engineering/how-big-technical-changes-happen-at-slack/", "abstract": " Most new things in technology turn out to be fads: patterns of talking and doing that come and go without leaving a permanent mark. Microkernels; EPIC architectures like IA-64; object request brokers; and 1990s’-style neural nets are gone, and will not return. Sorry for the deep throwbacks; only time proves which things are fads, so for uncontroversial examples we have to reach pretty far back. . While it is hard to imagine today — at their height — all of these defunct technologies were wildly popular, with charismatic, sincere, and smart advocates. They were supported by plausible first-principles arguments that showed why their chosen tech would inevitably triumph. The fads spawned movements, manifestos, conferences, and companies. To be clear, these fads are not to be confused with deliberate frauds, which are much more rare. The motivations behind these technologies were heartfelt. Things just turned out differently, despite all available appearances at the time. . On the other hand, a crucial few new techniques are revolutions: potent, enduring changes that confer long-term advantages to their adopters. Object-oriented programming, hardware virtualization, the world wide web, public cloud, CI/CD, and 2010s-style neural nets (reborn as deep learning) are now permanent parts of the world of computing that were once indistinguishable from fads. We are, already, surrounded by concrete technical successes that we did not know how to achieve before these things came along. . Like all technology companies,  Slack wants to make sure we catch revolutions at the right time, while limiting the energy we spend chasing fads . What strategy can we follow to ensure this? This post outlines our approach to this problem, which we continue to refine and apply through our practice at Slack. . We can’t rely on individual leaders’ intuitions to pick winners; both  precision and recall  will be too low. Instead we strive to actively  invest in exploring new things , knowing that most of these investments will return nothing. To bias our investment towards useful new things and away from fads,  we are ruthless in killing experiments early  that do not prove valuable. We hope to try a little bit of everything, accepting that this means dabbling with a lot of fads; and we hope to end up riding some waves all the way to shore because our experiences with them keep providing positive returns. . This is enough philosophy to build a minimal, descriptive model of our adoption of new technology. .   . This curve is a typical  sigmoid  describing technology adoption over time. The S shape comes from changes in the rate of adoption. At first, when only a few experimenters are playing around, we have no choice but to adopt it  slowly ; later on, as it becomes clear what the benefits are, more hands pitch in to capture those benefits, and we  quickly  adopt the new technology into production use cases during the steeply upward-sloping section in the middle. As the majority of fruitful use cases get eaten up, fewer cases are left, and the ones that remain are the tough ones, so adoption  slows down  again towards the end of the cycle. . Let’s demarcate these three phases: .   . We’re not the first to observe that technology adoption follows this sigmoid pattern. Everett Rogers proposed this model in his “ diffusion of innovation ” theory in 1962. Rogers wasn’t describing Erlang or MongoDB; he was a rural sociologist observing patterns in the adoption of farming techniques. It turns out computing practice is not so different from other fields of human activity. . To ground this abstract view in something more concrete, let’s consider some of the technologies that have traversed the phases of Exploration, Expansion, and Migration at Slack. . The  React library  has swept frontend development since its first stable release in 2015. Its utilization of the virtual DOM for rendering elements and its approach to unidirectional data flow made it a compelling technology for Slack’s desktop UI. .   . Server-side, we’ve been  migrating from PHP to Hack since 2016 . A key part of that migration has been a gradual introduction of  types  to our PHP code: .  Vitess  is a database clustering system for horizontal scaling of MySQL that we have  turned to as we evolve our data sharding strategy . . In contrast to these technologies that have graduated through the phases of adoption,  our cross-platform C++ client library  did not move beyond Phase 2, and was eventually discontinued. . In the end, the runway to a full migration never appeared.  We took what we learned from the LibSlack effort  and applied it to our mobile and desktop clients in various valuable ways. The code artifact did not achieve enduring adoption but the project informed how we build our clients and organize our engineering teams. .  Note that these phases are a descriptive model, not prescriptive.  We’re not forcing adoption to follow this sigmoid curve; it just naturally must, no matter how we wish things were. There is no way for early exploration to proceed as quickly as midlife adoption, and there is no way for the final push to get to full adoption to go as quickly as the middle phase went. The three phases are not consequences of any milestones, processes, tools, or people at Slack. They are part of the fabric of technical change, and they would be there whether we noticed them or not. . But now we’ve noticed them, and we can use them to make our efforts more successful. The tactics and strategy for each phase are different. . Phase 1 is frictionless to enter. When an engineer first starts messing around with a technology they’re excited about,  no permission-granting process or ceremony is needed . It probably happens dozens of times a day at Slack: someone reads about or invents something new, and commences fiddling around with it. Perhaps they have read a blog post about  Elixir , or  Cassandra , or  WebAssembly , or  TCR . They download some software, build it, poke around a little, work through some introductory material, and maybe take a stab at applying it to their day job. .  Most exploration efforts sputter out here . This is good! Giving up here is one of the important ways we resist spending too much energy on fads. However, some things do make it out into our real workflows and codebases. Sometimes, an engineer can just apply this solution in place, because it solves a problem local to their team’s work. Sometimes, though, the situation is even more exciting: this new widget is useful for an entire class of problems that other teams face. Our intrepid engineer now believes they know something consequential that the rest of us in Slack Engineering do not: that there is a better way for us to do things.  Once work starts to affect others’ work , you’ve entered Phase 2. . Let’s take a moment to pity the poor engineer entering Phase 2! For they are now trying to modify other engineers’ behavior. This is going to involve communication, persuasion, and — if it is going at all right — substantial technical work. For most projects, Phase 2 is the most difficult, time-consuming, and discouraging phase. It is the “ product-market fit ” phase of the technology cycle, and many of the projects that enter it will not successfully complete it. .  At Slack, client teams are free to choose not to depend on your system,  with few exceptions. This may surprise you if you have a lot of experience at an “infrastructure-driven” engineering company. At some companies, leaders pick winners and losers before the product-market fit negotiation at Phase 2 has reached its conclusion. The goal of having a winner selected before it has been widely deployed is to provide clarity (“What does the future hold? Which system should I build on?”) and to economize on the expensive period in Phase 2 where more than one way of doing things needs to be supported. . While those are reasonable goals, it is not how Slack chooses to approach the adoption of new systems. We prioritize fad-resilience over speed of adoption. And so, we (intentionally) place the burden of getting other teams to adopt new technology mostly on the change agent. While this can be frustrating for the advocate of a new system, we know of no better substitute. Clearing this hurdle forces selection of Stuff that Works. If the new thing really is as wonderful as we hope it is, it should help the teams that depend on it get things done; this success can move them to adopt it and advocate it. . Some of the work of Phase 2 is fundamentally more like product work than like what-you-might-think-is-engineering. You need to do  user research  to figure out what problems matter. You need to  communicate  the value of your solution relative to previous practices, in ways your users are prepared to hear. You need to  build  things that close the gap between current practice and the change you’re making, to grease the skids for your busy and distracted clients. . Successful execution in Phase 2 eventually leads to some  self-propelled adoption , where people you did not explicitly sell on the new tech are freely choosing to use it. The end of Phase 2 is close at hand when the new system is a de facto standard, the default practice for new projects. It is unusual to accidentally achieve this kind of adoption.  It’s really hard , and draws on skills that are not part of every engineer’s professional experience. . The self-propelled adoption phase eventually starts to taper off. We are left with a residue of holdouts: use cases that seem especially resistant to the new way of doing things. Some systems that have been quietly working in the background are especially unmotivated to change just because they are not being actively developed. In some cases we are discovering late in the game some ways in which the previous system really worked better. Finally, there are always a few stubborn users who are overly invested in their muscle memory of the old way. . While we’ve been talking about “the” technology adoption curve, there is actually a fork in the road at Phase 3.  Even very successful projects might not migrate every last use case  to the new way of doing things. For instance, at Slack we have very widely adopted gRPC as an internal API technology. It is solidly in late Phase 3. However, we are unlikely to build a new version of memcached that uses gRPC; memcached’s custom protocol works well, and is well-supported in the clients we care about. The existence of exceptions like this doesn’t make gRPC adoption a failure. . In other cases, the costs of having More Than One Way (cognitive burden on engineers; operational burden from running the Olde Systeme) are high enough that we will migrate everything to the new way. For such projects, we need a plan to tackle the hold-outs.  Different tactics are appropriate for different obstacles . The systems that just haven’t changed in a long time might need the change agent to adopt them and start moving them into the future. If the holdouts are functionally motivated, by real capabilities the new system lacks, you may need to enhance the new system, or wrap it in code that emulates the old system’s capabilities. . In the occasional case of emotional attachment to the old system, person-to-person outreach is usually a lot more effective than public, high-stakes debate. And please be gentle; your beautiful, new system will be the Old Way some day, too — if it is successful enough to live that long. . OK, that is a lot of description. What about prescriptions? What do we expect of one another as engineers and engineering leaders at Slack to smooth our progress? . When in doubt, remember: you’re accountable for your team’s technical success, and your team’s technical success is–in the long run–judged by the people using your stuff. . Want to help us catch the next revolution and eliminate some fads along the way?  We’re hiring . ", "date": "2020-02-13"}, {"website": "Slack", "title": "Deploys at Slack", "author": ["Michael Deng", "Jonathan Chang"], "link": "https://slack.engineering/deploys-at-slack/", "abstract": " Deploys require a careful balance of speed and reliability. At Slack, we value quick iteration, fast feedback loops, and responsiveness to customer feedback. We also have hundreds of engineers who are trying to be as productive as possible. Keeping to these values while growing as a company means continual refinement of our deployment system. We had to invest in greater visibility and reliability in order to accommodate the amount of work being done. This post will outline our process and a few of the major projects that got us to where we are. . Every pull request at Slack requires a code review and all tests to pass. Once those conditions are met, an engineer can merge their code into master. However, merged code is only deployed during North America business hours to make sure we are fully staffed for any unexpected problems. . Every day, we do about 12 scheduled deploys. During each deploy, an engineer is designated as the deploy commander in charge of rolling out the new build to production. This is a multistep process that ensures builds are rolled out slowly so that we can detect errors before they affect everyone. These builds can be rolled back if there is a spike in errors and easily hotfixed if we detect a problem after release. . Each release starts with a new release branch, a point in our Git history that allows us to tag the release and a place where we can cherry-pick in hotfixes for issues discovered during a rollout to production. . The next step is to deploy the build to the staging servers and run an automated smoke test. Staging is a production environment that does not accept public traffic. We perform additional manual testing in staging because it gives us a higher degree of confidence that the change will work correctly than if we only tested in our development environment. . The rollout to production starts with the dogfood tier, a set of hosts that serves some of our internal Slack workspaces. As we’re very active Slack users ourselves, dogfooding has helped catch many problems early. Once we are confident that core functionality is unchanged, the build is deployed to canary, which has about 2% of production traffic routed to it. . If the charts remain stable and if there are no outstanding alerts, we continue with a percentage-based rollout to production. We break up the rollout in 10, 25, 50, 75, and 100 percent increments in order to slowly expose production traffic to the new build while giving us time to investigate if there are any spikes or anomalies. . Modifying code always presents risk, but we manage this by having trained deploy commanders at the helm of every new release, watching the charts and coordinating communication with the engineers pushing code out. . In the event that something  does  go wrong, we aim to catch it as early as possible. We investigate the issue, identify the PR that is causing problems, revert it, cherry-pick in that revert, and make a new build. However — sometimes we don’t catch a problem before it reaches production. In this scenario, it’s critical to restore service, so we immediately roll back to a previous working build before starting our investigation. . The workflow described above may seem obvious in hindsight but our deploy system went through many iterations in order to get there. . When the company was much, much smaller, our entire application could run on 10 Amazon EC2 instances and a deploy just meant a quick rsync to all the servers. There used to be just one tier before production: staging. Builds would be made, verified on staging, and then go straight to all production servers. This system was simple to understand and allowed any engineer to deploy their own code at any time. . But as the number of customers grew, so did the amount of infrastructure it took to run our application. Soon, our push-based deploy model couldn’t cope with the number of servers we were adding. Each additional server increased deploy time. Even strategies like parallel rsyncs had their limits. . Eventually, we resolved this issue by switching to a fully parallel pull-based system. Instead of pushing the new build to our servers using a sync script, each server pulls the build concurrently when signaled by a Consul key change. This allows us to maintain a high velocity of deploys even as we continue to scale. . Another project that paved the way for tiered deploys was atomic deploys. . Before atomic deploys, each deploy would cause a burst of errors as the process of copying over new files onto the production servers wasn’t atomic. This resulted in small windows where call sites of new functions were available before the actual functions. When these call sites were invoked, they returned internal errors that manifested in failed API requests and broken web pages. . The team that rallied around this issue resolved this by using hot and cold directories. The hot directory would serve production traffic, while the cold directory was being prepared. During a deploy, the new code would be copied to the unused cold directory. Then, once the server was drained of active processes, we would switch directories instantaneously. . In 2018, we hit a point where deploying as fast as possible was hurting the stability of our product. We had a very capable deploy system that was being heavily invested in but the process around deployments had to evolve. Reliability took center stage as we became a bigger company that powered increasingly more mission-critical collaboration around the world. . The need for safer deploys led us to overhaul our deploy system, which resulted in the percentage-based deploy system described earlier. Under the hood, we continue to use fast deploys and atomic deploys, but we changed the way we carry out deploys. This system rolls out changes in tiers, which — coupled with much better monitoring and tooling — grants us the ability to catch and mitigate bugs before they have the chance to affect all users. . But we’re not done yet — we’re continually improving this system through better tooling and automation. Stay tuned for more posts along these lines in the future. . Deployments at Slack consist of a complex set of systems with contributions from many teams across the engineering org. The work discussed in this post summarizes work led by the following people and projects: .  Jonathan  is a Staff Software Engineer, based out of our San Francisco Office. While at Slack, he works on internal tools and is currently with the release engineering team. His focus is on front end development but is known to do a little bit everything. .  Michael  is a Software Engineer on the Platform team at Slack. He has been working at Slack for almost 3 years, and in that time, he has worked on a wide-ranging feature set, including user-facing products, API infrastructure, and growth experiments. ", "date": "2020-03-29"}, {"website": "Slack", "title": "A Day in the Life of a Backend Product Engineer at Slack", "author": ["Madeline Shortt"], "link": "https://slack.engineering/a-day-in-the-life-of-a-backend-product-engineer-at-slack/", "abstract": " I’m definitely not a morning person, so when my alarm goes off, I can’t help but stay in bed a little while longer. I have two cats, Stella and Orion, who are especially cuddly in the mornings, so it’s hard to leave them and get out of bed. My cats are well known by my teammates as well as they make guest appearances in meetings whenever I’m dialed in and working from home. Because some of my teammates are much earlier risers than I am (or are in different timezones) I’ll hop on Slack as I’m getting ready to catch up with them as I start planning out what I’ll tackle at work today. I also take this time to catch up on any big releases going out or any interesting developments over the evening that might inform my work — there’s always lots going on in the Engineering org at Slack! . I’m finally out the door! I told you I’m not a morning person. I live in Bernal Heights, so I catch a bus down the hill in the mornings to the 24th &amp; Mission BART station. I live near a fantastic shop, Black Jet Bakery, and if I’m running a little early, I’ll stop there for a pastry before hopping on the bus. It’s so peaceful in the mornings. . Made it into the office! My first stop is the barista station on the 5th floor. Ah! It’s such a wonderful way to start a workday. I get a latte in a big mug and settle into my desk, dig into Slack for real now and plan out my work for the day in my notebook — I really like writing to-dos down physically. I’ll typically review any outstanding pull requests from my teammates during this time or answer questions that have come up in my team channels — some shorter tasks that I can complete during this window of time. . Standup time! I’m on the Messaging team and we’re responsible for the core part of the product — everything from the user experience to the infrastructure about how messages are stored and formatted. We have team members in Vancouver and New York (and soon  Toronto ), as well as our headquarters in SF, so standup is a chance for us to all sync up on what we’ll be working on for the day and resolve any questions that arose the previous day. Our standups can be particularly silly, which I think is delightful. We often close our standups with a fun fact. Did you know that most of the things we call berries, like strawberries, are not actually botanical berries, but “accessory fruits”? Or what do you think the Japanese name for the seven sisters Pleiades constellation is? It’s Subaru! Look at the car logo the next time you’re driving around and you’ll see, although the logo only has six stars since the 7th star in the constellation isn’t visible to the naked eye. . This is the first time today I’ve gotten to tuck in and plug into coding. I was a Sublime lover when I first joined Slack, but I’m a full VS Code convert now. At Slack I mostly write in Hacklang (which was new to me prior to joining) and the tooling with VS Code is :chef’s kiss emoji:! That and the awesome support from our Internal Tools team make it so easy to get up and running in a new language. My newest favorite plugin I’ve discovered is the  Bracket Colorizer 2 . It’s a super simple concept — it just colorizes the brackets so each pair is the same color, but in a complex codebase it helps simplify what you’re looking at so, so much. . One of the projects I’m currently leading is a very exciting, cross-functional project to migrate the messages table to our Vitess cluster. Slack has been in the process of migrating some of our most important tables to Vitess to increase our ability to scale with our largest customers (more information about how we chose Vitess and it’s impact  here ). As a product engineer it’s so different to have the goal of a project be, “Users notice nothing”! Because of the sensitivity and importance of the data we’re migrating, and what a big undertaking this is, we have worked as a team to make very calculated decisions, building theories about any unexpected things we see and using our different domain knowledge to bridge the gaps and cover the entire message sending, persisting, and rendering stack. I am really enjoying getting to learn more about our infrastructure at Slack, and getting to go into the weeds while still working out of a core product team. It has been extremely rewarding to partner with our Infrastructure team and create a group that can bring both the Vitess expertise and the product knowledge to this migration. While we still have some of the final climbs to go, it has been gratifying to look back and see how far we’ve come since we set off on this project together. . Lunchtime! The Composition team is part of the larger Messaging engineering pillar — we have a great group of backend engineers and it’s wonderful to feel their solidarity and support. A rotating subset of us will often get lunch together, which is a nice way to connect outside of a code review and pair programming context. Any interesting topics that warrant source materials or further discussion get posted to #lunch-action-items. The last post was  Blair Braverman’s twitter thread  describing her sled dogs! . The Messaging backend team is actively hiring! Some afternoons I’ll serve on an interview panel, helping our recruiting team interview potential new teammates. I’m particularly proud of the continuous work the backend engineers across Slack put into trying to make our interview process  the most representational and least stressful it can be . Interviewing also reminds me how many people are excited about the product we get to work on every day. It’s sometimes easy to get focused on the minutia when you work so closely to something and to lose sight of your own appreciation for it — being asked questions about Slack by our candidates helps me zoom back out and remember the positive and widespread impact it has. . Back at my desk! And back to proselytizing VSCode! One of the biggest contributors to me switching over was the debugger, which my teammates at Slack  have hooked up to our development boxes  . It’s crucial to the way we add to — and debug — our codebase, and has made figuring out and fixing bugs in our system much easier. As the Messaging pillar, we own so much of what makes Slack Slack — messages, file uploads, custom emojis, emoji reactions, threads, and the list goes on! This is incredibly exciting because our product development in these areas affects the core of the application. But it can also be a double-edged sword — since this is the very heart of the application — it’s not terribly uncommon to come across pieces of code that were originally written 4 or 5 years ago. . My afternoons are my main heads-down, glasses-on, headphones-on time to code. The product teams at Slack work hard to balance our technical time spent between new feature development and maintaining technical health and quality. One of the features the Messaging team just released is WYSIWYG, or “what you see is what you get”. This feature allows you to format your message as you type it — if you  bold , it’ll be bold in the input before sending the message! Previously we stored messages as simple strings using markdown format. But for WYSIWYG we wanted to give the user much more control over the message formatting, including formatting that cannot be represented by markdown, such as int raw ord formatting. While this feature at face value seems like it could be only client work (and don’t get me wrong, it has involved a heroic amount of client work — shoutout to the incredible frontend, Android and iOS engineers I work with everyday!) — it involved a good amount of backend work as well, as we changed the format of how messages at Slack are stored. . Two other interesting pieces of this project that we considered as we were designing were maintaining backwards compatibility and thinking forward to simplifying the code in the future. When designing for a mature application which already has hundreds of billions of messages and millions of users, there are a number of inherent constraints. To continue to support those billions of older messages, we designed the WYSIWYG format to be backwards compatible for devices which don’t yet have our new code. Older applications must still be able to receive, read, and best-effort render the newly formatted messages, as well as still be able to send the old format of messages. On the other hand, we also don’t want to support two different message formats across all clients in perpetuity. For the efficiency and sanity of the engineers across all the codebases, we wanted to design and think through how to get back to only supporting one message format by building a translation layer on the backend, and a plan to migrate all older messages through it. I am so, so proud of this team and all the hard work that went into this feature! . Time to head home! There was probably a  Susie Cakes  appearance during the day that I’ve left out (my team has such a ridiculous sweet tooth and I am here for it!) and perhaps someone dressed in a T. Rex costume. Most likely a few informal in-person discussions about a product question, a PR comment, or a new Hacklang discovery. Something I reflect on while writing this is that: while I love problem solving and writing elegant and high quality code, it’s really the people I work with and the team collaboration that makes me excited to come back every day. That isn’t to say that there aren’t super exciting technical challenges that we’re solving, but to me it’s about how the team together has an impact. . And with that I head home to recharge, watch some Queer Eye and get ready to come back tomorrow for another day. . Madeline Shortt is currently a Senior Software Engineer at Slack on the Composition team within the Messaging engineering pillar. The Composition team works on the experience of composing a message, from formatting to uploading files to custom emoji. Before coming to Slack, she was at Ripple leading a team building payment APIs for financial institutions. She double majored in Physics and Computer Science at Mount Holyoke College in Massachusetts. She currently lives in Bernal Heights in San Francisco with her two cats, Stella and Orion and rides ponies when she finds the time. ", "date": "2020-04-06"}, {"website": "Slack", "title": "Hacklang at Slack: A Better PHP", "author": ["Scott Sandler"], "link": "https://slack.engineering/hacklang-at-slack-a-better-php/", "abstract": " Slack launched in 2014 with a PHP 5 backend. Along with  several   other   companies , we switched to  HHVM  in 2016 because it ran our PHP code faster. We stayed with HHVM because it offers an entirely new language:  Hack  (searchable as Hacklang). . Hack makes our developers faster by improving productivity through better tooling. Hack began as a superset of PHP, retaining its  best parts  like the edit-refresh workflow and request-oriented memory model that enable speedy development. In addition to a number of quality-of-life improvements, Hack adds a better type system and a static type checker, which help catch bugs and allow developers to code and refactor with more confidence. . In this post we’ll talk about how and why we migrated to Hack, the benefits it gave us, and things to consider for your own codebase. . PHP’s type system has come a long way  since PHP 5 , when it was not possible to annotate return types,  class properties , or scalar types. Its remaining holes, like the lack of  generics , may be resolved in the future. But its biggest flaw is that types are only checked at runtime. This is the most costly time to find out about type-related bugs, either by breaking a test suite or worse — a user report or production error log. . With Hack, type checking happens  statically  (without running the code) and   as you type   .  Change the signature of a function with hundreds of call sites, and you’ll see errors for the ones that need updating before even hitting save. . This is a   game changer     for productivity — the difference between finding a bug milliseconds after typing compared to waiting for a comprehensive test suite (or finding out after deploying) is hard to overstate. It’s akin to the productivity difference between developing websites in PHP vs. C. With Hack, you don’t bother trying to run the code until the type checker is passing, and by then it usually  just works . This allows Slack developers to build, and refactor, with confidence, focusing testing efforts on higher value areas like  logic bugs  which static typing can’t help prevent. . Static type checking is possible in PHP with  community   packages , and if you’re using PHP I’d strongly recommend using one of these. However, Hack’s type checker has the advantage of a much more full-featured type system to work with. Hack is built from the ground up to enable static type checking, with features PHP lacks like  generics ,  shapes ,  enums ,  hack arrays , and a well-typed  standard library  to enable rigorous static analysis. . We started with Hack in  partial mode , which treats all untyped values as the “any” type, usable for any purpose. TypeScript takes the  same approach . This enabled an incremental migration—adding types over time. As files became fully typed, we changed them to the default  strict  mode so that they stayed that way. . Surprisingly, gradually adding types to a weakly-typed codebase made me  more thoughtful about type safety  than I ever was working in strongly-typed languages like Java or Go. Instead of a requirement to get the compiler to run, types were a conscious decision to add value to the codebase. We had to justify spending time adding types by observing how they changed our working lives. Some parts of the codebase were easy to type, but others required refactoring to enable type safety. . Not only have we found and prevented bugs, but types serve as a form of in-line documentation that are  verifiable  (unlike comment blocks), helping everyone read and understand the code. They also serve as a  contract  between different parts of the codebase. This has been crucial to productivity in a large, shared codebase like Slack’s backend. . Hack’s type system has one feature in particular, Shapes, that caught on like wildfire at Slack, and I believe it’s the reason we never looked back once we introduced Hack to our codebase. . PHP’s  array  type, bewilderingly, can act as both a list (an ordered set of values) and a map (a set of key value pairs) at the same time. Most programming languages use separate types for these. In my experience, this is an endless source of bugs in PHP code, especially as functions like   array_merge   treat list-like and map-like arrays differently. . Hack improves upon this by separating these into  different types  and using  generics  to describe the types of their keys and values. A list-like array containing strings is a  vec&lt;string&gt; , and a map-like array with string keys and integer values is a  dict&lt;string, int&gt; . .  dict&lt;string, mixed&gt;  is a valid, but not particularly useful type annotation, which says the dict contains  string  keys and values of  any  type. . Enter  shapes . A  shape  is an array that contains known keys with specific types. Keys may be optional if preceded by  ? . These example shape definitions represent the arguments of an http POST request, which has many optional fields: . This function signature uses that shape to type the  $options : . A call site might look like this: . Not only does this help ensure the correct types are used for each field, it also helps prevent typos for the names of keys both in the call site and in the function body where the shape is accessed. This makes the shape much more impactful to developer productivity than a simple  array  type annotation. Before shapes, assembling a call to such a function would require reading its body or a large doc block (which may not be fully up to date) to understand the names, expected types, and “optional vs. required” status for each argument. . Shapes are used for a variety of use cases at Slack, including: . As more features are added to Slack, each request tends to have more work to do. To keep the user experience snappy, concurrency is a common solution — doing multiple things at the same time in a single request. . In many programming languages, adding concurrency means adding significant complexity with  mutexes , thread-safe data structures, or callbacks. These things slow developers down, making code more difficult to reason about and debug. .  Hack  is one of a  handful   of   languages  that implements the  Async/await  pattern for  multitasking without multithreading .   Async/await is a simple abstraction that allows functions to be  paused  while waiting for I/O, freeing up the runtime to schedule other tasks. By simply adding the  async  and  await  keywords and following a few  guidelines , code can be migrated to take advantage of concurrency without breaking the mental model of how the code works. . Here’s an example using the  concurrent  code block to fetch data from two sources at once. These fetches were previously done sequentially. Adding  await  and  concurrent  keeps the code easy to read while allowing the fetches to take place concurrently. . HHVM has come a long way since Slack began using it.  Breaking compatibility with PHP  was a controversial decision which required us to eliminate every last line of PHP code and dependencies from our codebase, but has enabled huge efficiency and soundness improvements to the language. Since the  HHVM 4.0  release that removed PHP support, the developers have  rapidly removed  “PHPisms” that inhibit type safety and/or performance, while adding  useful   new   features . Keeping up with these updates in a large codebase is nearly a full time job. . The largest downside to leaving the PHP community is the loss of an extensive ecosystem of open source packages on  Packagist . Luckily, Hack projects can still be  published  on Packagist, and there are several high quality ones: . As Hack frees itself from its PHP past, I’m excited to see it become a first-class language in its own right. While it’s no longer feasible to gradually migrate a PHP codebase to Hack, I expect to see more developers choose Hack for new projects as the language stabilizes, especially if they have familiarity with PHP and are looking for something better. . There’s a general trend in the industry towards adding static type checking to interpreted languages, with multiple  options  for  Python ,  JavaScript , and  Ruby . Combining the convenience of interpreted languages with static type checking is worth considering for code bases of all sizes. .  Scott Sandler is a Principal Engineer on the Core Infrastructure team at Slack. Slack is    hiring    backend engineers.  ", "date": "2020-04-14"}, {"website": "Slack", "title": "A Day in the Life of a Mobile Product Engineer at Slack", "author": ["Kevin Lai"], "link": "https://slack.engineering/a-day-in-the-life-of-a-mobile-product-engineer-at-slack/", "abstract": " My alarm is set for 6:45 but I often wake up before it goes off. I tend to wake up earlier during Vancouver’s long summer days, when the sun is up from 5:10am to 9:10pm and already peeking through my blinds, but it sometimes happens in the winter when it’s still dark out. Every morning starts with a hot refreshing shower before getting ready for work. I recently became a morning person and found that showering in the morning goes a long way in preparing me for a busy day ahead, especially during Vancouver’s colder seasons. In the shower, I’m often thinking about what I’m hoping to accomplish at work today. I sometimes even think of a solution for a problem that I’m working on! . I head downstairs to the kitchen to prepare a morning protein shake to kickstart my metabolism; eating breakfast is another recent change to my morning routine. After breakfast, I take some time for myself by listening to music, washing the dishes, tidying up around the house, taking out the garbage, reading some news articles on my phone, checking the stock market, and going over my calendar events. By getting some daily chores done in the morning, I can focus on work as soon as I get into the office. . By 8:15, I’m out the door and on my way to work. I used to drive to the nearest SkyTrain station and take public transit downtown, much like I’ve done for the 15+ years I’ve worked in Vancouver’s downtown core, but recently I’ve been driving to and from work every day. I managed to find some reasonably priced monthly parking close to office and Slack’s $100/month transportation subsidy helps offset the cost a bit. I live around 18 km (~12 miles) from work, which is considered relatively far for Vancouverites, but I like driving and don’t mind commuting through a bit of rush hour traffic. . By 9:00, I’ve navigated through traffic and parked a block away from Slack’s Yaletown office. On a good day, Google Maps on my Android Auto display tells me that I’ve arrived earlier than predicted. Before heading into office, I head over to the nearby  Small Victory Bakery , where Slack provides its employees with coffee and tea perks. We just order and show our badge and the rest is taken care of for us. I’m sure the baristas know several of us by name now, seeing as the short walk is a great way to take a coffee break in the afternoon too. Their croissants are also some of the best in town, so I’m careful not to indulge in them too often. In the morning, I simply get my extra-hot London Fog then head into the office. . Once settled in at my desk, I open up Slack to catch up on any announcements, direct messages (DMs), and unread starred channels. From within Slack, I then check if there are any GitHub pull requests that require my attention so that I prioritize them and unblock my fellow Android engineers. . A lot of pull requests are usually sent my way since my name is attached to a lot of files’ git history. While I’m currently working as a Product Engineer on the Messaging team, Mobile Engineering at Slack was not always structured this way. Back when the Android team was a dozen or so engineers, we were a centralized group that was assigned to various feature projects. Everyone worked on the full stack, adhering to agreed upon data structures, design patterns, test requirements, and general code styles. I work most effectively when I’m intimately familiar with all aspects of a codebase, so I used that opportunity to learn about and work on all layers of the code. . Fast-forward to today, where the company has grown five times in size and the Android team three times; mobile engineers are now embedded in various pillar teams to effectively manage project resources. My focus is now on product development — more specifically, how users compose, digest, and interact with messages — where I lead Android Messaging efforts. During my time at Slack, I’ve been the primary developer for the Advanced Message Input composer that allows photos and files to be uploaded inline, the modernized message rendering pipeline that’s written in Kotlin and leverages RxJava, the Rich Text infrastructure that handles all the intricacies of displaying and storing “what you see is what you get” (WYSIWYG) messages, and our PowerPack testing infrastructure that I gave a  talk  about. A lot of this work serves as a foundation for us to build even better features for our end users. . To quickly touch base with my team of engineers (frontend, backend, Android, iOS), a designer, a product manager, and an engineering manager, we have a short 15-minute morning stand-up. Since I work out of Slack’s Vancouver office, the team members here dial into Zoom to talk to our San Francisco counterparts. We use this short sync to get a sense of what everyone’s working on and if there’s anything to call out for the entire team. Whenever possible, we also lighten the mood a bit with some friendly candor; it really helps reinforce the mindset that we get things done and have fun while doing it. . If I haven’t gotten into my work yet, I’m definitely nestled into an Android Studio frame of mind now. Sometimes a high priority bug comes to my attention during stand-up, so I’ll open up the Jira ticket to see if there are any repro steps or logs from the internal report or Zendesk ticket. We never want to inconvenience our users with bugs so we prioritize the really bad ones that come in from our internal testing. At Slack, we have a mobile dogfood release that all employees use as their main client. This version is distributed daily via the Google Play Store, so we usually catch problems internally before any of our users see it. . Once I’m in the clear for bugs, I’m heads-down and working closely with my San Francisco-based counterparts on the latest Messaging feature. Whether it’s making it easier to send messages or surfacing the most pertinent information at the right time, there’s never a shortage or product improvements to be made. With Slack as our collaboration tool, working in different offices doesn’t hamper our productivity at all. We’re very strategic on how we separate the work — often UI and infrastructure as a starting point — so when we coordinate our efforts over Slack, we can pretty much move in lockstep without stepping on each other’s toes. The Android team goes through design reviews before making significant client changes, so with those and pre-established patterns in hand, we’ve pretty much nailed down the approach at this point. . At Slack, there’s a lot to do so we’re always on the lookout for passionate engineers. I’ve been phone screening a steady stream of candidates lately, where I get a sense of their experience, work style, team fit, and passion for redefining how we communicate in the workplace. The Android community is a pretty open and welcoming group, so it’s always nice to see that sentiment come across in an interview. We hire for a wide range of levels and always value passion and potential. We also love bringing in new people with fresh ideas on some of the technical challenges we’ve been working on. Equally so, we love sharing our knowledge and mentoring those who might be relatively new to mobile development so that we can all grow in our careers. . While I love to cook at home and pack leftovers for lunch, Slack provides catered lunch two times a week in the Vancouver office. It’s always a surprise to find out what’s on the menu for the week, as we have a wide variety of cuisine offerings from curry, tacos, sandwiches, sushi, and everything in between. If we’re left to our own devices, I usually have a sit-down lunch with the other engineers here. There’s always someone craving ramen, so there are a few places that we like to hit up as a group. Whatever the case, I use this time to socialize with other engineers and get a sense of some of the challenges they’re facing. . The mobile engineers are particularly close, as we encounter a lot of the same things on Android as we do on iOS. We have a good representation from all pillars in Vancouver, allowing me to get a better sense of what the Enterprise, Infrastructure, or Platform teams are tackling next. We tend to talk about cross-functional Android themes, like our latest Kotlin adoption percentage, the progress of the RxJava2 migration, the code modularization effort, or the reduction in Gradle build times that our Developer Experience team takes lead on. I enjoy being around individuals who care deeply about their work, as that work ethic and culture tends to resonate throughout the team. . While we try to keep meetings to a minimum at Slack, afternoons are generally prime time for them. I actually look forward to some of them, since the Android engineers schedule regular cross-function meetings to maintain the health of the codebase and team culture. . One of these meetings is our Design Workshop. When an upcoming feature or infrastructure change has implications for other pillars, we write up a Design Doc so stakeholders have an opportunity to bring up considerations that may have been overlooked. This collaborative effort is one of my favorite aspects of the Android team since it not only leads to better and more future-proof designs, but also shows that we’re conscientious of others who work in the same codebase. . Another meeting around this time is for our Android Principles group. Internally, the Slack Android team has a reputation for being high-functioning, so a subset of us meets every other week to find ways to preserve that culture as the team continues to grow. This can be anything from improving our onboarding process and identifying more mentoring opportunities in code reviews, to making a new member feel more welcome and fostering better team rapport. It’s important to us to maintain a healthy and productive work environment at Slack and I appreciate how the Android team is very proactive about it. . Development continues and so do project discussions. Whether it’s clarification from our product managers and designers, or double-checking that I have a complex RxJava chain  just right , the afternoon is a good mix of writing code in Android Studio and discussing requirements in Slack channels. The Vancouver office also benefits from seating all the mobile engineers together, so I sometimes turn my chair around and tap a colleague on their shoulder to discuss implementation details for web socket connection states, channel sync background jobs, or file upload retry logic. There are a lot of complex moving parts that go into a Slack client and getting them all to play nicely together is often rewarding in itself. Building the next thing to make our users even more productive? Even better. . I’ll occasionally check Slack to see if there are any DMs or public channels with discussions that I need to respond to, but for the most part I’m pretty focused on my tasks for the day. . It’s not uncommon for a fellow Android engineer to ask about a very specific implementation detail or request feedback on a proposed change, so to avoid a lengthy back-and-forth discussion over Slack, I often direct them to my Office Hours that I host three times a week. The purpose of these Office Hours, which various Staff-level Android engineers have, is to minimize disruption during the day and to provide a more personable face-to-face way to hash out ideas. They also double as a mentoring and knowledge-sharing opportunity, particularly for new members to the codebase (regardless of their level). Over time, I found they’re a really good way to connect with other Android engineers whom I don’t interact with as much, and are quite efficient at coming up with a solution too. I sometimes go over their code together — akin to pair programming — while other times I help brainstorm solutions before asking the individual to put together a Design Doc for the wider team. Any noteworthy decisions or discoveries are then shared back in channel, so that other team members may benefit from them in the future. . By 4:00, all meetings have wrapped up and Slack conversations have dwindled down, so this tends to be my most productive time of the day. While listening to music on my noise-cancelling headphones, I work on completing the Jira tasks that I set my sights on this morning. My goal for the day is never too ambitious, but to ensure that I feel productive and maintain a good project velocity, I put in some extra time if I feel behind. I like to put in some extra hours early on to avoid a frantic push at the end of the project when we’re trying to ship. Over the years, I found the front-loaded approach suits my work style and is one of the main reasons why work never really stresses me out anymore. It also helps when Slack encourages a “work hard and go home” culture, so the onus is on us to take ownership of our work and use our best judgement on when we need to put a little extra time in. . I pack up my MacBook Pro and head home. I always take my laptop with me so I have flexibility to work from home the next day if something comes up. I rarely ever leave before 5:30 since leaving later actually works in my favor in terms of traffic. What is normally a 45-minute commute drops down to 25 minutes when there’s no traffic, so squeezing in a bit more work tends to be the norm for me. Once home and feeling accomplished, I prep dinner and unwind for the rest of the night. . Kevin is a Staff Software Engineer, based out of our Vancouver office. As a native resident of Vancouver, he’s worked for various software companies and specialized in Android for the past 8+ years. In his time at Slack, he’s worked on a few teams, starting with the centralized Android team before a reorg brought him to the Product Engineering side of things. As a leader on the Android Messaging team, he is the primary engineer behind Android’s composer, message rendering pipeline, and Rich Text infrastructure. In his spare time, he enjoys cooking without a recipe, going to the movies, working out, being in good company over coffee/tea, and driving sporty cars. He recently joined the Android App Infrastructure team, where he’ll focus on enabling product engineers to ship features faster and more reliably. He enjoys making the lives of his fellow engineers more pleasant and providing continual value to Slack’s customers. ", "date": "2020-04-21"}, {"website": "Slack", "title": "Development Environments at Slack", "author": ["Michael Deng"], "link": "https://slack.engineering/development-environments-at-slack/", "abstract": "  In this article, development environments refer to sandboxes where you can test your code changes before deploying, and should not be confused with integrated development environments (IDEs) like Eclipse or Microsoft Visual Studio.  . Dev environments have always been a mystery to me. Despite learning about them on my first day at Slack, and using them almost every day for the last three years, I have never understood how they truly worked. . Half a year ago, I set a goal to understand dev environments inside and out. I interviewed some of the most senior engineers at Slack, studied countless pages of documentation, and sifted through years of Slack conversations. What I discovered was a fascinating journey of how our dev environments evolved over time. . Dev environments are copies of our application that we can modify freely. Their main value is allowing us to  test changes to our application without impacting real users or putting real data at risk . . They enable us to iterate rapidly because it’s quick and easy to test changes on them. They also make it possible for us to easily share our changes with others for review. . Altogether, dev environments drastically increase development speed and safety. . Slack’s dev environments are copies of our application that live on remote servers — Amazon EC2 instances to be exact. These instances are configured to run the Slack application and the many services it depends on. . Each dev environment hosts its own Slack subdomain, which we can navigate to on our browser to see the changes we make. . No changes in dev environments can impact real users because they use their own set of infrastructure (e.g. databases) isolated from production. . At Slack, we develop remotely, meaning our dev environments live on servers. Another option is developing locally on our personal computers. Local development is great for smaller projects because it’s fast and doesn’t require an internet connection. However, for larger projects, remote dev environments offer significant advantages. . First, we don’t have to set up the Slack application locally. Given that Slack has a very complex architecture that depends on many different services, not having to set things up locally is immensely valuable. . Second, if a change works in dev, it’ll most likely work in production, because our dev environments are configured to mirror production. Some level of drift may still happen with especially long-lived dev environments, but the likelihood and magnitude are much smaller than when developing locally with unique machines that often end up with inconsistent configurations. . Third, remote dev environments don’t rely on a personal computer, which may crash or lag. Cloud hardware is much more affordable, resilient, and scalable. Further, they allow us to easily develop on multiple machines and share our work with teammates for review. . As the internet becomes increasingly faster and more reliable, it makes more sense to develop remotely. . The best way to illustrate Slack’s dev environment workflow is with an example. . Let’s say for some reason we wanted to test a version of Slack’s homepage with all-caps, purple, Comic Sans text. . We first create a feature branch, and then attach it to a dev environment using a command line tool called  slack sync-dev . It reserves a random dev environment then syncs our local changes to it, so whatever local edits we save automatically transfer to the dev environment. .  At its core,  slack sync-dev  is simply a wrapper around two well-known utilities —  fswatch  (detects changes) and  rsync  (transfers changes).  . If we make any frontend changes, we have to build them locally using webpack, an open source tool  we adopted as our build system . The command  slack run buildy:watch  builds our frontend assets and serves them to our dev environment over localhost. . When we’re done, we can navigate to dev575’s subdomain, and voila! Behold our violet masterpiece. . Now, we can poke around on the page for bugs, fine-tune our change, and share it with others for review. . Remember that our frontend changes are built and served from our personal computer? If we want others to be able to see them after we close our computer, we have to generate a static build, which builds our frontend assets on our dev environment instead of locally. . When we first introduced webpack in 2017, we built frontend changes remotely in our dev environments. Whenever someone made a frontend change while synced, the dev environment automatically rebuilt their assets. . However, as our codebase grew, webpack became more resource-intensive. One build would consume the memory of the entire instance. At the time, multiple developers worked on the same instance, and they were constantly interrupted. So, we moved webpack onto our local machines. . With just one dev environment per instance today, and with more advanced instances, it would be entirely possible to move webpack back onto our dev environments, which would make the developer experience a little smoother. But the current system is fast and scalable, so we don’t feel the need to fix what ain’t broke. . Let’s talk command line tools for a sec. We’ve already covered some of them, like  slack sync-dev . We can’t live without them at Slack because they make developing so much faster and easier. . Early on, when we didn’t have  slack sync-dev , we had to manually copy our changes over to the dev environment, which was slow and error-prone.   Now, we boast over 60 command line tools that simplify many mundane tasks like this one. . Other examples include  slack bot-me , which creates a bot user on the current dev environment, and  slack tail-dev , which tails remote logs from our current dev environment. If you’d like to read more about our dev tools,  check out our blog post from 2016 . . Back in 2014, we only had one dev environment that everyone shared. If one person broke it, nobody else would be able to test their changes. That wasn’t a big issue then, but as Slack grew, we had to add more. By the end of 2019, we were maintaining 550 dev environments, enough for every Slack engineer to attach to a different one. . However, this increasing trend did not continue, and in fact completely reversed in 2020. Before we talk about why, let’s first take a look at another interesting metric that changed over time: the number of dev environments per EC2 instance. . This number fell over time because we wanted to isolate dev environments from each other. When multiple environments share the same instance, one developer running a memory-heavy process on one environment would slow down all of the others. . There is a tradeoff though — fewer dev environments per instance means we have to pay for more EC2 instances. Also, these instances were statically managed, so lots of engineering hours were required to provision new ones and deprovision corrupt ones. To make matters worse, long-lived instances got gunked up over time and would stop behaving reliably. . To resolve these issues, we created a new system to provision dev instances based on demand. This caused a drastic reversal in the increasing trend shown in the first chart. Instead of keeping hundreds of instances running concurrently, we provision new instances when needed. Once developers are finished testing, their instances are automatically deprovisioned. With this system, we’re able to use our dev environments much more efficiently. We’ll be diving into these scaling evolutions in an upcoming post, so stay tuned! . Well-designed dev environments are critical to the success of any technology company. Here at Slack, our dev environments, and our tooling infrastructure at large, are entering an exciting stage of scale and automation — join us to build out this future. .  A huge thank you to Ross Harmes, Felix Rieseberg, Tito Sandoval, Harrison Page, Melissa Khuat, Kefan Xie, Shannon Burns, Nolan Caudill, Matt Haughey, and many others for helping with research and editing.  .  Michael  is a Software Engineer on the Platform team at Slack. He has been at Slack for almost 3 years, and in that time, he has worked on a wide variety of projects, including user-facing features, API infrastructure, and growth experiments. ", "date": "2020-04-28"}, {"website": "Slack", "title": "Happiness is… a freshly organized codebase", "author": ["Erica Engle"], "link": "https://slack.engineering/happiness-is-a-freshly-organized-codebase/", "abstract": " Imagine joining a new team at a new company and you finally get access to a codebase. .   Slack iOS Xcode File Hierarchy 2017 . Where do you add a source file for your team’s project? Great question. Your new team has three source directories for you to add files to, and oh, you’ll have to figure out which one you want to use, which one your teammate uses, and where you’re going to find the files that need refactoring. . The Slack iOS team lived in these conditions for a few too many years. We got here as a result of some attempts to organize source files (several times), a lack of architecture pattern in the codebase, and a high growth of developers over a couple years. To put things into context, we have roughly 13,000 files (and counting), about 27 top level directories, a mix of files in Objective-C and Swift, and around 40 iOS developers that work in one monorepo. . These are all real photos of the state of our file hierarchy. New hires constantly expressed frustration jumping into the codebase — and rightly so, since some of us just got used to navigating these chaotic directories and didn’t remember the pain of starting from scratch. Not only did we have numerous source code directories (our prized iOS, SlackCocoaSDK, and Slack directories), but it took a lot of time to settle on a directory and then to decide how to add a file. On top of this, we decided we wanted to add new tools to our codebase and the current state of our Xcode project did not lend well to supporting them. . As a result, a group of iOS developers and I set out on a mission: . We took on these goals in two stages: move high level directories into a coherent order (things like main target directories, extension directories, frameworks etc) and then — the bigger task — source folder organization. . The top level directory moves were neither contentious nor hard. It probably took us only a few weeks with a few developers. In these first moves, we learned a few things to take into our following stage — tackling big moves during off peak work hours, consistently merging master (who wants merge conflicts?), and working with folks on timely reviews. Merge conflicts weren’t the only struggle we’d encounter during this process, but we could have invested in a better way to mitigate merge conflicts with xcodegen since most conflicts were in the project file. We also wanted to think about preserving our git history and maintaining a clear contract with how we see files displayed in git and finder. However, we opted for easy enablement of moving files to get folks involved and dragged and dropped files into their new homes. .   Slack iOS Xcode Hierarchy 2017 (left) and 2018 (right) . If you look at this image from September 2018 you can see we were able to organize our top level fairly successfully. A place for every directory and every directory in its top level place, right? . Time to tackle the source files in iOS, SlackCocoaSDK, and Slack to move them all into App/Source. Honestly, I dreaded this part. We needed consensus on a pattern, a clear way for developers on all teams to get involved, and both rules and tools to make sure moves were easy and clear to engineers if they had done something wrong. I did a lot of investigation into patterns for our hierarchy and there’s a surprising lack of articles on folder organization. I had thought this would be a well documented topic going into this journey, but as a less glamorous part of the job, I can understand why it isn’t covered. Of the articles I did find, Uber wrote this  article  about how they approached the move to their monorepo. This gave me some insight into how we could chunk up our codebase into modules (but on a smaller scale). . I ultimately presented three options to my greater team: Feature organization, Topic (architecture based organization), and Ontology (relationship or similar grouping based organization). The group converged on Feature for top level and then Topic or MVVM+C organization inside of those feature directories. . Here’s an example Folder in our new structure: . Moving the source files proved tedious and cumbersome. Merge conflicts were annoying, small things like searching for a file namespace to see if you captured all the feature files into a directory proved harder to remember than originally thought, and lots of files we were moving fell outside of the folder rules we set out initially. Thankfully, we had a few heroes step in and make some giant moves to remove iOS, SlackCocoaSDK, and Slack all into App/Source. . A snapshot of our folder hierarchy from January 2020. .   . Slack Xcode File Hierarchy 2020 . While we were moving these three large source directories, we initiated a Danger rule to ensure folks would stop adding files to these directories and start using our new pattern.  Danger  is a tool we integrated into our Continuous Integration system that performs post-commit automated checks and posts warnings and errors onto PRs. This is what one of ours looked like: . You might think “Wow, that looks great, you must be done!”. Well, not entirely. We’re still working on moving around what’s within the new source directory, App/Source. Here are some of the rules we started to implement in our “folder housekeeping”: . One rule that really needed tooling was co-locating tests — for this we opted for a Danger rule. Any new PR with new files added could not be added to App/Tests. . This looks something like – . The folder committee created a Slack channel where folks can ask questions if they aren’t sure where to add a file. Confusion around where to add a file happens more than you would think and even the smallest moves can have the strongest opinions attached to them. . We’ve made a lot of progress, gotten a lot of support from the greater iOS team, but still have more to do. This is not a one-person-job, you need to enlist folks who work around all parts of the codebase. You need the greater team not just for support in moving files, but for modifying rules and adding more tooling. Having the extra hands means that a lot of folks will learn the process and be able to show newer folks how adding a file in our codebase works. . The recipe for success and happiness is surprisingly short: If you are like us and have a monorepo, create a committee of folks dedicated to the process, and set hard and fast rules. Rules can be broken, but with any number of developers you may have discussions and will need a core group to create tooling to support rules for file organization to become more natural. A core committee can also spend time thinking and researching the best file structure that works for your team organization or working patterns. . It makes a world of a difference when developers understand where to add files and can do so in a way that promotes speedier development and genuinely makes it more pleasant to be inside the codebase. Tools like SwiftLint, Danger, or homegrown scripts are great for initiatives like these. However, a big caveat is that you’ll first have to get to a point where tools become useful, which usually requires non-trivial manual labor. In short: Use tools, get folks involved, and tackle it like any other project that matters for your company. It’s an undertaking, but making it easier for everyone to find or add a file, allowing developers to understand our architecture pattern, and enabling tooling to enforce and promote a cleaner codebase is definitely worthwhile. ", "date": "2020-05-07"}, {"website": "Slack", "title": "The Gradual Design System: How We Built Slack Kit", "author": ["Garrett Miller", "Zack Sultan"], "link": "https://slack.engineering/the-gradual-design-system-how-we-built-slack-kit/", "abstract": "  This post was co-written with Zack Sultan, Lead Product Designer at Slack  . In 2016, Slack was two years old and already used by millions of people. Our codebase had grown rapidly, and like many companies that focused on product/market fit, our code was built in a way that favored time-to-market over maintainability, consistency, or reusability. . We never encountered a single breaking point in our user interface, but rather a slowly cascading series of inconsistencies,  quirks , and discrepancies. Our internal product teams were growing to match the momentum of the business, and with that scale came drift in our component parts. . What does a button look like in Slack? How do you build it? What words do you put in it? It was up to individual teams to make these decisions. . A group of engineers, designers, and writers began to centralize these standards, documenting the bits and pieces, small and large, that make up Slack. . We named it  Slack Kit . . It’s an evolving internal resource containing everything from typography to accessibility patterns. It helps us keep the product as consistent and accessible as possible, while speeding up design and engineering efforts. And with the  release of our new client , it’s now used throughout Slack. . It’s a system that grew retrospectively, taking advantage of lessons learned from early feature implementations to inform the patterns we wanted to codify. This post is about how we built it. . In those early days, ownership of standards or best practices between design and engineering were often unclear or anecdotal. We didn’t even have a straight answer to a question as simple as, “What color should I use?” . The first cross-functional group began to clarify these standards by answering that question. Carving off little bits of our weeks, we pruned the set to just 16 colors, working together to ensure that the final set fulfilled the varied use cases throughout our products and — most importantly — met our criteria for  accessibility . As we couldn’t feature-flag this change, we replaced every color variable inside Slack in one massive merge, then documented and shared the new library internally. . This first exercise in standardization created a space for those building the product to communicate with each other across teams and disciplines. Designers began to understand the ways in which variables can codify standards. Engineers learned the reasons designers need components to look and behave a particular way. .  Starting from the ground-up forced us to focus on an area that solved a real problem for our team, rather than choosing something overly-broad.  . A crucial turning point in Slack Kit’s success arrived when Slack’s Desktop Engineering team made the decision to modernize its codebase with React. While recognizing that most product engineers and designers would have to continue to focus on feature development, a few of us began to support the effort full-time. . We began codifying other foundations like typography, motion, and spacing, but ran into problems early on. Many of our existing components were built on top of a legacy code stack based on assumptions in our product that were no longer relevant (e.g., a member dropdown built for teams of 1,000 people would break down when trying to load data for a 100,000 person team). . We had to rebuild each of these components without slowing down Slack’s overall development progress. It was a bit like taking a car engine apart piece by piece, and cleaning, repairing, and replacing each part while it accelerated down the highway. . While doing this work, we could also reevaluate each of these component parts. Knowing that dropdowns would need to support future magnitudes of scale, we built support for asynchronous data fetching. We codified how the dropdown menu should behave, and ensured that it was fully accessible. Our first multi-select took almost two months to build, but subsequent feature implementations took just hours. . The scarcity of dedicated resources meant that the design system would rely on other engineering teams to help build the library’s components. The Slack Kit team focused on process, establishing clear guidelines for additions to the library. All Slack Kit components would be “carrier-grade”: . While relying on contributors from other product teams within Slack to help build the library meant slower progress, it guaranteed that the system was serving the needs of its eventual consumers. We made ourselves readily available to Slack’s product development teams, connecting threads between parallel efforts. In a fast-growing organization, Slack Kit became a hub for discussion of the fundamentals of our interface. And with each new component, the value of the system grew as patterns were reinforced. . Requiring teams to proactively reach out to us in order to prepare for upcoming initiatives was not always an elegant or efficient process. At times, we’d build a component and then later refactor it as requirements became clearer. We remained humble and supportive, and discovered that  a small team can have a tremendous impact with a culture built around process, communication, and collaboration.  . It took a long time for Slack Kit to reach the point where it was useful for our various product teams. When we started the process and looked to other design systems for inspiration, they were intimidating in their completeness. But just one component, thoroughly documented, was immediately valuable, and created a virtuous cycle for the system. . Since then, we’ve built off that foundation. Our designers and frontend engineers have created over 40 components for Slack Kit, in use throughout the desktop client and our various web surface areas. We’re bringing this same process to Slack’s various mobile apps, codifying components and design principles for iOS and Android respectively. . Slack Kit helps us improve the overall customer experience, as we’re able to rapidly build new foundational features into Slack.  We released Dark Mode  for the Slack desktop client today, built with relatively low effort on top of the color system that Slack Kit established. In the coming weeks, we’ll share more about building Dark Mode and how Slack Kit helped make it possible. . A design system must adapt to the changing needs of the product. It must be opinionated, prescribing solutions that can scale for yet-unknown use cases. It has to document these solutions, and ensure that they’re useful for a variety of audiences. And most importantly, it must embrace the tension between infrastructure and invention, helping others to navigate that difficult middle. . If any of this sounds interesting to you, perhaps you’d like to  work with us . ", "date": "2019-09-12"}, {"website": "Slack", "title": "How Slack Built Shared Channels", "author": ["Yingyu Sun", "Mike Demmer"], "link": "https://slack.engineering/how-slack-built-shared-channels/", "abstract": "  Written with contributions from the Shared Channels Team.  . Slack was originally built to be the collaboration hub for the work  within  your company. As the network of companies using Slack for internal work grew, we saw the value of allowing different companies to collaborate  together  in one channel. . We’re now making  shared channels  available to all customers! A shared channel is one that connects two separate organizations. You no longer need to go back and forth between external emails and internal Slack channels, or provision an endless number of guests into your Slack workspace: A shared channel creates one productive space for people from both companies to send messages, share files, and work together in Slack. . The idea of shared channels challenged Slack’s fundamental assumption that the workspace is the atomic unit of partitioning customer data, however. For this post, we’ll discuss a few of the most interesting technical challenges from this change. We’ll talk about our initial architecture, the decisions that went into reframing how Slack worked, the implications of those decisions, and how we’re working on scaling for the future. . Before we dive into shared channels, let’s take a look at how Slack works at a high level. Slack implements a client-server architecture where clients (mobile, desktop, web and apps) talk to two backend systems: . For example, a new message posted in a channel is sent via an API call to the webapp servers, where it is sent to the message servers and is persisted in the database. The real-time message servers receive these new messages and send them to connected clients over web sockets, based on who is in the channel. . From the time Slack was launched in 2014, this core system architecture centered around the concept of a “workspace.” Logically, workspaces enveloped all other objects in the system, including users, channels, messages, files, emoji, apps and more. They also provided the administrative and security boundary to implement access controls, as well as policy and visibility preferences. . The backend systems used the boundaries of the workspace as a convenient way to scale the service, by spreading out load among sharded systems. Specifically, when a workspace was created, it was assigned to a specific database shard, messaging server shard, and search service shard. This design allowed Slack to scale horizontally and onboard more customers by adding more server capacity and putting new workspaces onto new servers. The design also meant that application code interacting with a workspace’s content needed to first determine which workspace the request was for, and then use that context to route to a particular database or other server shard.  In essence, the workspace was the unit of tenancy in the multi-tenant service, used for data partitioning and segmentation.  . This design was very simple and effective: To find data or send messages, our code only needed to look up the appropriate workspace’s shard and route the request there to verify that the requesting user had access to the given channel. Over the years, more and more application code and services were developed around the core assumption that data lived within a workspace, further solidifying this walled boundary. . Of course, introducing shared channels meant that we needed to rethink these assumptions, since shared channels enable users to access some channels, messages and files  across workspace boundaries . This required us to re-examine the basic access control, visibility, and data sharding capabilities on which Slack had been built, and we discovered a number of interesting design challenges and trade-offs. . Shared channels required us to look at how messages should flow across workspaces. We needed to decide how to distribute and store messages so that members of both workspaces could join the channel, send messages, and use Slack normally. . We considered a number of different approaches. The first was to continue down the path we had started by assuming that everything that users interact with on Slack belongs to the user’s workspace. This design would mean that both workspaces would have a copy of the shared channel in their respective database shard, and messages would be written to both the sending user’s shard and the other side’s shard. As we had done before, we wouldn’t need to query multiple databases or worry about routing requests to the other workspace’s shard. . Though simple, this option had a lot of downsides. Considering  more than a billion messages  are sent on Slack weekly, duplicating data for both workspaces would restrict our potential to scale shared channels. This wasn’t limited to just messages — it also included pins, reactions and other channel-specific information. We also wanted writing data to be as real-time and consistent as possible, but writing to multiple databases and multiple real-time message servers risked having inconsistent write times, leading to inconsistent channel histories. . Instead, we went with an approach that was more consistent and performant and had more potential to scale. We decided to have one copy of the shared channel data and instead route read and write requests to the single shard that hosts a given channel. We used a new database table called shared_channels as a bridge to connect workspaces in a shared channel. . Every channel in Slack (including shared and non-shared channels) has a single channels row that lives on the originating workspace’s shard. In our example, Ben’s workspace originated the shared channel, so the channel would be written to the channels table on its workspace shard. All content in the given channel (messages, reactions, pins, and so on) is also written to that workspace shard alongside the channels table. . For a shared channel, there are two additional shared_channels rows: one row for each workspace, pointing to the other as the target workspace. These rows include the channel ID, target workspace ID, originating workspace ID, and overrides for certain channel properties. We can look at either workspace’s database shard to know who it has shared channels with and determine which side initiated each channel. Additionally, each workspace can have different views of the channel. The originating workspace fetches channel details, like name, topic and privacy setting, from its row in the channels table as it does with any other channel. For the target workspace (in this example, Jerry’s workspace), these fields are overridden from the properties in its shared_channels row, which allows each workspace to set its own name, topic and purpose for the channel. . Breaking the pattern by not storing channel data on a workspace’s shard required a lot of work — the messaging, database and API layers of our application needed to be made aware of the fact that channel data would sometimes need to be fetched from other shards, and that channel properties might need to be overridden from the shared_channels rows. . Since we went ahead with having one copy of a shared channel, we had to reconsider our implementation of channel privacy. Before shared channels, we used ID prefixes to distinguish public versus private channels. We stored private channels in the groups table with a G-encoded ID and public channels in the channels table with a C-encoded ID. This ensured that we had no mix of data between public and private channels and that there was no confusion about visibility based on its encoding.  With shared channels, we wanted to allow the individual workspaces to decide the privacy of a shared channel on their own side.  Given our design, in which we would have a single channel with a single ID shared on either side, we needed to decouple channel privacy from the table on which the channel lived and the prefix with which the channel ID was encoded. . To accomplish this goal, we decided that all shared channels would be created as C-encoded channels and stored on the channels table. This presented a consistent channel ID to the message server and all clients. Since an encoded channel ID no longer indicated whether the channel was public or private, we added an explicit boolean is_private flag that made it quick and easy to check if any channel was public or private. . This flag was the first step in a large effort to consolidate logic around different channel types. Before shared channels, the application code largely followed the original database pattern, with distinct (but often duplicative) logic for channels, groups and direct messages. These parallel code paths added greater complexity to each feature or performance improvement that touched any channel-like object. Enabled by the decision to decouple privacy from the database and ID logic, we were able to consolidate the database structure and application logic around a single channel-object model. This decision has continued to pay dividends as the product becomes more complex, channels become more powerful, and our architecture continues to scale. . To take advantage of these improvements, however, clients, who could once assume that all C-encoded channels would be public, now needed to correctly process this new channel object model when determining channel privacy. To make these changes compatible with pre-existing clients and third-party app developers, we introduced the conversations.* API to replace the old channels.* and groups.* endpoints. The new API can take as input any channel-like objects and is able to appropriately switch between them. . Since shared channels allow users to interact with members of other workspaces, one of the most significant problems was determining when and how different users could interact. . Prior to shared channels, a call to the users.info API method was straightforward: Given a user ID, we queried if that user belonged on the same workspace as the caller. If so, the API returned their profile, and if not, it returned an error indicating that the user could not be accessed. With shared channels, user visibility needed to expand so that profiles could be visible across the workspace boundaries. Furthermore, when a user on one workspace wanted to view an external user, both the content of the resulting profile and whether the two users could interact would depend on whether or not the users were in a shared channel together. . This required a few changes in Flannel, our edge cache service that we use to allow clients to load minimal data on startup. (You can read more about Flannel in  this blog post .) At the time, Flannel did not support the concept of external users, since it assumed that users would interact only with members of the same workspace. This meant that when an external user was mentioned or sent a message in a shared channel, Flannel would be unable to find the user and fall back to our web API to fetch the user. Depending on the size of the channel, simultaneous client actions ran the risk of overwhelming the back end and bringing down Slack. . The Flannel team worked to extend its workspace model to support user objects and visibility rules for shared channels. When a new user loaded Slack, Flannel would load information about all of the shared channels that the user was a member of, including external user profiles. This meant that Flannel also needed to subscribe to any presence changes for external users as well as channel membership events, such as channel_member_join and channel_member_leave, in order to keep its model updated. . Our customers continue to reshape the way Slack is used in the workplace, and shared channels are no exception. It’s a race to scale shared channels to fit the needs of our largest customers, some of which have upward of 160,000 active users and more than 5,000 shared channels. . The scalability of shared channels depends on (among other things) Flannel, to avoid overload. In many cases also related to user visibility, however, clients can’t completely rely on Flannel to efficiently cache updates. Falling back to our web API unveils several O(n) operations in which our APIs iterate over a group of entities (users, workspaces, channels, etc.) to determine things like the number of users in a channel and the number of external workspaces a user can see based on their shared channel membership. Since shared channels continue to become the place where work happens across organizations, we’ve worked to make sure that these O(n) operations are either efficiently cached or eliminated altogether. . We’ve also been working to close the gaps in shared channels for our large enterprise customers, specifically around data governance. These include policies like data retention, in which messages and files are retained for only a selected period of time and then automatically deleted. Another challenge is to make our  Enterprise Key Management  feature, in which each workspace in the channel can bring their own encryption keys to secure their own data, work for shared channels. . We’ve gone through a few of the challenges we faced when building this strategically important feature in Slack. Shared channels will gradually redefine what Slack is and how people choose to use it. Over time, we will continue to build upon shared channels and explore ways to make them a place where work happens for cross-organization collaboration of all shapes and sizes. . Expanding beyond the single-workspace model was just the first step. We imagine that someday, our customers will want to share a channel with as many external workspaces as they need. The value of shared channels will grow as we collaborate with more and more workspaces to get work done. . While we’ve made significant changes to our existing architecture, there is still plenty of work to do to evolve our data model as we continue to build the network. If you found these technical challenges interesting, you can also  join our network of employees  at Slack! ", "date": "2019-09-17"}, {"website": "Slack", "title": "Building Dark Mode on Desktop", "author": ["Kyle Stetz"], "link": "https://slack.engineering/building-dark-mode-on-desktop/", "abstract": " If you’re a dark mode user you might have seen the news: two weeks ago we flipped the switch and  gave users dark mode  across the desktop app and the browser. We wanted to give you a peek under the hood and some background on the process of getting to this point. . As is usually the case with large codebases, finding an implementation that works is only half the battle; gracefully changing infrastructural code and educating engineers on how to use new tools accounts for much of what we do when working on new capabilities of the product. Working in a large engineering organization — especially within a rapidly growing company — means that every change needs to consider the momentum and roadmaps of many other teams. The overarching question for this project was: how can we build  sustainable and maintainable  support for themes? . Over the last 18 months, we did a complete  rewrite of the desktop client  in React. We took advantage of the momentum we had toward a design system and formalized  Slack Kit , our UI component library, and along with it some new standards for writing CSS. . We maintain a set of  LESS  variables representing our colors and we’ve always known that this would serve as a great entry point to a dark mode implementation — but before we could think about what to build we had to develop a more sophisticated color system that could handle multiple themes. . Our text color variable used to be called @sk_black and our background color @sk_white. Dark mode is essentially an inversion of the color system, so let’s go ahead and flip those: . Not great! Beyond being a confusing reversal, using the terms black and white bias us towards thinking about the traditional light mode experience when what we really want is an abstraction that lets us think about it in any mode. If we orient the variable names around the  content  rather than the  appearance,  we get something easier to parse: . Using this foreground/background concept we renamed our color system. We changed our link and menu highlight colors to variations on  @sk_highlight  since they all change slightly in dark mode, but in general we didn’t have to modify our existing rainbow of color values except where they didn’t meet accessible contrast requirements. . The grayscale values were the toughest to get right; after workshopping some options with designers and engineers we settled on four values describing the level of contrast from the background:  @sk_foreground_min ,  @sk_foreground_low ,  @sk_foreground_high , and  @sk_foreground_max . For handling edge cases we created “hard-coded” variables with an _always suffix to denote that the color is theme-agnostic:  @sk_black_always  and  @sk_white_always , along with several grays that use opaque hex values rather than transparency. . Once our stakeholders felt good about the nomenclature, we had to do the unglamorous work of updating the codebase. We wrote a find-and-replace script using Node that looked through each LESS file and replaced old variables with new ones based on a few inferences (whether it was a color or background-color, for instance). We expected the final CSS output in our builds to be exactly the same so we ran diffs between webpack’s generated CSS before and after, ensuring that we would not introduce regressions. .  CSS variables  are a natural fit for this sort of feature; alternatives such as adding component-level style overrides or loading an alternative stylesheet with different variable values risk changing selector specificity and add ongoing maintenance overhead. . We felt it was important to give users a choice between themes irrespective of their OS setting, ruling out the use of the prefers-color-scheme media query directly in CSS; thankfully support was added this year for  window.matchMedia(‘(prefers-color-scheme: dark)’).addListener , giving us a hook for OS theme changes at runtime. . Prototyping with these tools gave us a good picture of where we wanted to end up — next we had to audit our CSS to see how we could transition to them as seamlessly as possible. The main roadblock for us was our ubiquitous usage of the LESS function fade, which adds transparency to colors using rgba(). We would need to either deprecate the use of fade or override it with a custom LESS plugin. . We found a solution by defining our CSS variables as a set of comma-separated RGB values rather than fully-defined colors: 26, 29, 33 instead of #1a1d21. This is possible because the var() function in CSS works at the level of tokens, as described by the Editor’s Draft of the  CSS custom properties spec : .  The  var()  function can be used in place of any part of a value in any property on an element.  . This allowed us to override the fade function with one that would return  rgba(var(--sk_primary_background), 0.4)  for any call to  fade(@sk_primary_background, 40%) . There were only a few instances of other LESS color functions in the codebase, making those much easier to deprecate and lint against. . Writing a LESS plugin avoided the problem of closely auditing many thousands of call sites (and corresponding features) and made the existing way of writing styles compatible with the new theming system by default. We shipped all of this work before we started working on the mode toggling behavior so that we could thoroughly QA the client and give engineers a bit more time to get used to the new variable names. . Dark mode is an unusual feature for us because it expands beyond the scope of a single workspace; our typical methods for storing this type of data rely on user prefs in individual workspaces fetched via API calls on boot. Previously it would have been an issue to use something like localStorage because each workspace was located at a different subdomain, but our recent rewrite keeps everything under the app.slack.com subdomain. This allows us to write to a single localStorage key for all workspaces in a given browser environment and gives us the ability to do some neat tricks around the first paint of the application (which we’ll detail below). . A typical Redux store populated by the localStorage key on boot allows us to access the theme from within React components. In our root React component we pull in the theme and add a body class  .sk-client-theme--dark  if dark mode is enabled, adding a style that redefines all of our variables: .  Note: You might notice that all of our grayscale variables are identical; as mentioned above, our CSS variables only represent the RGB components of a color, whereas the final color also includes an alpha value. This allows us to   fade colors in LESS that are backed by CSS variables at runtime.  . Our Slack Kit design system backs all common UI elements, getting us about 90% of the way for free. We audited every feature to fix one-off styles that didn’t use the core color variables and we worked with every feature team to make sure they had the support they needed to make in-progress work compatible with dark mode. . This is where we saw our architectural choices shine: since our other single page apps are built using Slack Kit components in our Gantry application framework (which also puts them under the app.slack.com umbrella), importing the Redux store and adding about four lines of code in a parent component allows all ancillary apps to support dark mode. We were able to add theme support to Slack’s Posts app with a single PR in an afternoon. . The last piece of the puzzle was operating system integration. Giving users the option to turn this on or off (currently a Mac-only beta feature, with Windows support coming soon) meant introducing a second piece of localStorage-backed state, which we called  OS sync . This state determines whether or not we’ll pay attention to the result of the window.matchMedia event listener. We communicate both the chosen theme and the OS sync setting back to the desktop app, where it works with native APIs to display the window chrome and context menus in the correct theme. . Having both pieces of state in localStorage gave us a much-desired capability: painting the app in the correct theme before we have any data. Since the client’s markup is static it does not come with any user data baked in, and waiting for API calls to resolve could take a little while. In the meantime, we’re going to see a flash of white — a side-effect painfully antithetical to the intent of dark mode. To avoid this we wrote some code in the static html file following a simple trick: write styles to the &lt;head&gt; before it is done being parsed, therefore intercepting the page before the first paint. . Now by the time the page is looking to paint something it’s already written and parsed a style tag pointing us to the correct colors. Works like a charm! We don’t recommend writing inline JavaScript this way, but sometimes we have to do what is necessary. . Success meant not just shipping a product feature, but making the cultural and structural changes necessary to ensure that dark mode is considered in every bit of work moving forward. Every audience is an important part of this process: frontends and designers, certainly, but everyone from PMs to content writers to executives needs to understand the thinking involved and what support they’ll have moving forward. Throughout this process we gave internal presentations, wrote documentation, reached out to project leads, led extensive QA to audit every square inch of the app, and even built Storybook and Figma plugins that flip components into dark mode — all to ensure that the system we created is sustainable and easy to work with going forward. We feel good about what we’ve built and even better that our customers can finally enjoy the fruits of this labor. . So, what’s next for themes? We’ve left the door open for additional themes like High Contrast, or perhaps even custom themes, if we find the right use cases. And there are still many improvements to make — one issue you may notice is that changing your theme in one browser tab will not change it in the other tabs until a refresh, a problem we could overcome by polling localStorage periodically or when a tab gains focus. We’ll be here listening to customer feedback and continuing to refine our day-to-day tools to make the most out of this new capability. . If this sounds like the kind of project you’d like to work on, there will be many more like it in the months and years ahead and we could certainly  use your help ! .  Thanks to Trish Ang, Alfred Xing, Charles Anderson, Kirstyn Torio, and Cory Bujnowicz for their help with this article and for their outsized contributions to dark mode’s implementation.  ", "date": "2019-09-25"}, {"website": "Slack", "title": "Client Consistency at Slack: Beyond Libslack", "author": ["Tracy Stampfli"], "link": "https://slack.engineering/client-consistency-at-slack-beyond-libslack/", "abstract": " Two years ago, I wrote a  post  about Libslack, Slack’s shared C++ client library. That post described how Slack used the Libslack library in its mobile applications to encapsulate shared business logic, and to handle syncing and caching of data. . In the intervening time, we decided to move away from using a shared C++ library in our clients, but we haven’t discussed that decision publicly. We were spurred to write an update when Dropbox published  this post  about why they also decided to stop using a C++ library in their mobile apps. . In this post, we’ll discuss the reasons why we decided to discontinue the Libslack project, and what we are doing in place of it. While we are no longer building a shared library, Slack still needs to maintain consistency and reduce duplication of effort, while developing separate implementations of client infrastructure. We will talk about some of the techniques we used to achieve that. . Many of the drawbacks Dropbox experienced with their shared library rang true for Slack as well. As described in our previous post about Libslack, there were certainly benefits to sharing code between client applications — a shared library increases consistency of behavior and prevents duplication of similar business logic in every Slack client. However, there are also downsides to this approach. . When the Libslack project began, the initial plan was for it to be used by the desktop, iOS, Android, and Windows Phone clients. Due to conflicting caching strategies and difficulty integrating into the build system, the desktop client never adopted Libslack, and the WinPhone client was discontinued entirely. This brought the number of clients sharing Libslack down to only iOS and Android, which reduced the benefit. Many companies successfully using shared libraries architected their mobile clients with one in mind. Slack added Libslack when its mobile apps were already mature, so it was replacing existing functionality, and it had to fit into two different established architectures. . To further complicate matters, there was overhead to integrate the library into the build system for each client, to ensure debugging worked properly across language boundaries, and to sync up Libslack with releases for each of the clients. (While we were developing Libslack, our mobile clients shipped on different schedules; they now share release cycles and ship on the same dates). Coordinating quality engineering for the library was difficult, as was determining when and what to hotfix. Most mobile engineers at Slack were not familiar enough with C++ and the processes for building and debugging Libslack to help fix issues in the library. Also, as mentioned in Dropbox’s post, hiring engineers with C++ experience, particularly on mobile, is difficult, which would have made it difficult to grow and sustain the project. . In the end, Slack decided the overhead of developing the library outweighed the benefits, and we sunsetted the project, moving back to implementing Libslack’s functionality separately in each client application. . When the Libslack project started, iOS and Android engineers were organized into two large teams, each working on the entire app. By the time we stopped Libslack development, the mobile client teams had been split into pillars, where mobile engineers worked with frontend and backend engineers on specific feature areas. In addition to the feature teams, we added an infrastructure team for each mobile client, which was responsible for supporting data syncing and caching, among other things, and could take over the work done by Libslack. Most of the engineers from Libslack joined these infrastructure teams. . To handle the responsibility of syncing data for the application, both of the mobile infrastructure teams developed DataProvider frameworks, which ensure that all the data needed by the app is present and up-to-date. While they were developed separately on iOS and Android (written in Swift and Kotlin, respectively), these frameworks provide similar benefits and functionality. The DataProvider frameworks vend information about users, channels, messages, or other Slack objects to the rest of the app. When a query for data is made to the framework, it returns what is currently in the local cache, fetches updates if necessary, and notifies observers of changes as they come through. Objects from DataProviders are returned as immutable models. On iOS, where the app uses a CoreData cache, this means the rest of the app no longer needs to access mutable CoreData objects directly, which reduces the need to worry about concurrency issues and avoids the crashes due to accessing data on the wrong thread that are common with CoreData. . The DataProviders frameworks are additionally responsible for assembling model objects from multiple data sources, and for ensuring that they are consistent with the current user’s preferences. For instance, to supply user information to the app, we need basic properties like name and status, but also user presence and “do not disturb” settings, which have to be fetched from separate sources. The user’s display name is determined according to team and user preferences. To display a message, the client may need to resolve information about users or channels mentioned in that message, as well as all the custom emoji. This becomes even more complicated once we bring in messages from channels  shared between different organizations . The DataProviders framework assembles all this information by sending queries to the Slack API and to Slack’s  edge cache . . The DataProviders frameworks make certain that we update data efficiently, through mechanisms like data versioning, lazy loaders and session caching. The framework also vends APIs to allow the rest of the app to update the local cache when new information comes in, or if the user makes changes. . By creating these frameworks on both iOS and Android, we encapsulated the logic around accessing the cache and syncing data, and moved it out of the main application, just as we planned to with Libslack. This also gives us greater freedom in future to alter the way we store and sync data without having to make changes across the app. . We have started other initiatives to reduce duplication of effort across Slack clients. One of them is our Client Foundations Council, a weekly meeting of engineers from infrastructure and foundational teams in each Slack client. In this meeting we discuss upcoming features and changes to Slack that will affect all the clients. We share knowledge about how each client works, and we try to develop common solutions to issues the clients are facing, to ensure consistency. We bring in feature teams and engineers that are seeking input on new proposals. The meeting has also given rise to new feature ideas which have improved the behavior of all Slack clients. . Additionally, the clients have been able to keep in sync by developing shared conformance suites for some features, particularly those where the rules for implementation are complicated and there are many possible edge cases. This allows us to ensure identical behavior without needing to enforce the exact same implementation. We make these suites by creating a JSON list of test cases, with expected inputs and outputs. Then each client can create a test harness to run the suite, and validate that we are getting the expected outputs on every platform. We have created conformance suites for features such as handling highlight words and link detection in messages. Other companies have used conformance suites to help third party clients conform to expected behavior (e.g. the conformance suites that Twitter has  published  to define how to parse tweets). An example from our highlight word conformance suite: . Finally, one of the chief ways we ensure consistency between our clients is through Slack itself! We have feature and team channels where the engineers working on a project can discuss proposals and hash out implementation details, and other engineers can follow along if interested. There are escalation channels, where engineers can see what issues have arisen and ensure they are fixed everywhere. Engineers can ask questions in public channels to get more eyes on them. . While Slack decided against using a common C++ library to share code across its clients, we still are striving to increase consistency of behavior across clients and reduce duplication of engineering effort. As the size of teams using Slack grows exponentially, so does the number of messages, channels, and custom emoji, and we have to be smarter about what data we sync and how often we update it. The infrastructure teams work together to ensure that Slack clients are kept up to date with the data that is most important to the user. . Our client infrastructure teams have many more challenges to solve,  join us ! ", "date": "2019-10-18"}, {"website": "Slack", "title": "Gantry: Slack’s Fast-booting Frontend Framework", "author": ["Anuj Nair"], "link": "https://slack.engineering/gantry-slacks-fast-booting-frontend-framework/", "abstract": " At any given time, Slack has many product teams working on different features. This allows us to build in parallel and quickly release new features reliably. But it also means the architecture of our applications can differ. And it’s not always easy to share the knowledge gained and the tough lessons learned across teams. . An important lesson we’ve learned over the last five years of developing Slack is to align on both   what   we create and   how   we create it. Early in our  rewrite of the Slack desktop application , we came across an opportunity to do just that: We’d prototyped a new boot flow capable of launching our client much more quickly than before and realized that if we could create a framework from this proof-of-concept that met the booting requirements of all Slack product experiences, we could align teams to better streamline product development, improve productivity, and have all applications automatically adopt our best performance practices. . Internally, this fast-booting framework became known as Gantry, and five Slack products now use it to bootstrap themselves. In this post, we’ll explore the technical decisions made, and the benefits we’re seeing from using a unified boot framework. . A lot of early architectural decisions at Slack were made with smaller workspaces in mind than what we support today. This had a great deal of impact on the way our desktop client behaved: . Slack workspaces rapidly outgrew these early assumptions and our frontend architecture had to scale accordingly. When we began prototyping a new client, we quickly found that the new boot architecture was also a good fit for other teams. It saved them from recreating what we had already done, allowing them to concentrate on product development instead. . By starting from a working prototype and making it flexible, we were able to build a boot framework that was reusable, whilst still being targeted to the needs of all projects using it. This, in turn, enabled all teams to move quickly, and in the same direction. Improvements made by one team could be shared among all the other projects using the Gantry framework. . Gantry is tailored to address the shortcomings of our legacy framework. We identified areas for improvement in our applications and looked to solve those specifically. For that reason, Gantry aims to achieve the following five goals: . Rather than loading everything upfront, Gantry apps fetch data incrementally, and only when absolutely needed. . This has multiple benefits: by moving to an incremental data model, we’ve been able to cut down on the number of initial queries that need to resolve before we can get Slack on a user’s screen. Breaking this data into multiple requests means we can request smaller chunks of data in parallel; minimizing the amount we are downloading results in less data being parsed by the main thread. By spending less time blocking on these calls, we can get a first meaningful paint on screen sooner. . We intentionally render our application client-side and forego any server-side rendering. This has allowed us to fetch everything we need to boot Slack from an edge server, cutting down round-trip request times. . Network latency can have a big impact on boot performance. By caching our data at the edge via  Flannel (our application edge cache)  and our assets on a CDN, we’re able to minimize latency as much as possible. . This is especially impactful for our customers located outside of the U.S. as all assets and data required to boot Slack are available at an edge location closer to them. . Whilst profiling our boot process, we identified our initial API requests as a major bottleneck. To combat this, we dispatch them as soon as possible. By utilizing code splitting and dynamic imports in webpack, we are able to create a small boot payload capable of knowing how to start these API requests and download the rest of our application in parallel. . This means we no longer have to wait for large bundles of JavaScript to download before API requests are started. The majority of the application is downloaded alongside these requests, making them less likely to block the boot later on. . Loading less code upfront also allows us to reduce the amount of time we spend in the  webpack runtime and manifest , resolving modules. Loading less JavaScript upfront is the best way to minimize this runtime cost, which can quickly snowball as you add more modules and components to your application. . This results in us parallelizing the downloading of data with the execution of JavaScript, resulting in a faster boot time. .  Gantry takes advantage of the powerful Service Worker  API to cache assets and data on the user’s local machine. This has allowed us to eliminate network requests on subsequent boots, speeding up boot time further and even allowing customers to use Slack while offline. . Service Workers  cache JavaScript on the bytecode level , so we can even eliminate parse- and compile-time costs on subsequent visits to Slack too. . When starting new projects, we weren’t just seeing engineers create new boot architectures; we were seeing duplicative effort spent setting up the same dev tooling, profiling, metrics, and integration into our deployment pipeline. Differences in architecture become barriers to entry when engineers want to jump into another codebase. Gantry looks to remove this mental overhead by standardising development and deployment. By running a single command in a terminal, engineers can start a new Gantry application with developer tooling, profiling, and deployment built into it. This allows engineers to concentrate on what’s important to them (product development) rather than tooling, security, browser compatibility, and so on. . Engineers are sometimes hesitant to couple products together, worrying about flexibility. Given that Gantry was tailor-made for the places it is used today, we have yet to run into this roadblock. Instead, we’ve found there to be some surprising upsides. . Applications are all built using the same webpack configuration, which means common code can be shared. After the first load of one Gantry application, subsequent loads of  any  other application will be much faster thanks to the way we use Service Workers and the browser’s cache. We only have to download the delta of missing assets to get another application up and running, and this is usually only a fraction of what’s already been downloaded. . Another advantage to sharing a base framework between products is around engineering productivity. In a fast-growing company such as Slack, multiple engineers support different initiatives and new engineers join the company weekly. Engineers can develop familiarity with a new codebase more quickly when there’s a common architecture to follow. They share best practices, and anyone can jump into any part of the codebase and find their way around easily. This has truly enabled our teams to move quickly and do their best work. . Gantry’s successful rollout internally came down to us taking a concept that was already working and abstracting it for general consumption. The intersection of multiple product requirements meant we covered most use-cases that a Slack application might need, and this has kept Gantry flexible enough to support many other applications since its inception. . Designing a shared framework for quick boots has enabled us to develop more rapidly and share our lessons with one another. As more teams start new projects with Gantry, they find they already have a wealth of knowledge at their disposal. This benefit cannot be emphasized enough: New products and improvements are developed and launched at a faster rate, making our customers lives simpler, more pleasant, and more productive every single day. . Gantry has changed the way we build our product but it’s far from finished. If this type of work excites you,  come and join us ! There’s so much we still want to do, and we’d love to have you along for the ride. .  A massive thank you to Rowan Oulton for spending time editing this blog post. This would not have been possible without your dedication!  ", "date": "2019-10-23"}, {"website": "Slack", "title": "A Day in the Life of a Backend Foundation Engineer at Slack", "author": ["Maude Lemaire"], "link": "https://slack.engineering/a-day-in-the-life-of-a-backend-foundation-engineer-at-slack/", "abstract": " While it’s not always easy to get up before seven, particularly when the sun hasn’t risen yet, being able to get a head start on the day before most of the city begins their commute makes it worthwhile. I sneak out of bed and get ready for work. I’m typically out the door, down the four flights of stairs, and walking to the BART by 6:50. I’ll pop in my earbuds and boot up the latest podcast in my queue: oftentimes an episode of Planet Money, This American Life, or Radiolab. The station is usually pretty quiet by the time I make my way to the platform; there are just a handful of us waiting for the next train. . I walk into the office and (sometimes) turn on the lights. My team recently moved into a new space about two blocks from Slack’s San Francisco HQ. Our desks are on one of the upper floors of a towering skyscraper with beautiful views of the Ferry Building and the SF Bay sprawling behind it on one side and the city stretching out to Twin Peaks on the other. Some mornings, when the fog is dense, it creeps right up to the windows and all I can see is the office building across the street. Other mornings, you can spot just a thin blanket rolling off the hills in the distance. Either way, it’s a truly San Francisco experience. . I absolutely love getting in early. Not only is the commute much more pleasant, but I’m regularly the first one in the office. It’s perfect for quietly catching up on Slack, and figuring out my focus for the day over some cereal and fresh fruit (courtesy of our micro kitchen). If I have a few pull requests ready to merge and deploy, I’ll merge those changes and monitor them as they’re shepherded out to production by our engineers in New York. I normally leave the office around 5pm when some folks are still hard at work, so it’s not uncommon for me to have a few pull requests from the previous evening to review. I pop open GitHub Enterprise and dive into those. . One of the many responsibilities our team is tasked with is shepherding our upgrades to our language stack. At Slack, we’re currently the second only to Facebook as the largest users of Hacklang and HHVM. The language is undergoing rapid development and the team building it out is currently shipping minor version updates nearly every week. If we want to adopt the latest language features, performance enhancements, and security patches, we have to ensure that we’re keeping everything as up to date as possible with each new version. While many of the changes are easy to automate, some are a bit trickier and require us to handle them manually. Either way, there’s always a healthy backlog of language-related code to update or pull requests to review. . At around 8, I’ll head down to the 31st floor to grab an oat milk latte from our barista. Of the many perks, this is one of my absolute favorites! I’ll mingle for a few minutes with our coffee staff and fellow early-morning coffee-drinkers. Eavesdroppers hear us discuss the latest scaling problem (do we have enough capacity to handle adding another million daily active users?) or a recent incident (what additional monitoring do we need to add so we’re alerted earlier when our systems are degrading?). It’s not always business: we’re no strangers to throwing around book recommendations, Netflix suggestions, sharing stories about our roommates, partners, weekends, and upcoming trips. I could easily lounge by the coffee bar for over an hour chatting with these folks but that obviously wouldn’t be great for my productivity so I attempt to strategically spot a lull in the conversation and head back up to my desk. . I jump right back in. I purposefully line up my most difficult work for this part of the day: digging into a pesky bug, ironing out a highly technical design document, or starting on a prototype. . I’m currently the primary developer and point of contact for our backend performance regression monitoring tool, Slerf. (I recently spoke about this tool at Strange Loop in St. Louis, MO; you can see my talk  here !) Slerf attempts to identify potential performance regressions while developers are actively writing code, before it gets merged and deployed to production. For every commit pushed up to our GitHub enterprise instance, Slerf runs a set of benchmarks which primarily track our database and  memcached  usage. It then compares these results with the same benchmarks run against the latest version of master. If it spots a regression, it alerts the code author and provides them with a report as to where the regression might be happening. Every quarter, our team identifies a few high-leverage features we think might take Slerf up a notch: either by surfacing better information to developers or tracking a more diverse set of metrics. Sometimes these features can be tricky to implement, making them great candidates for early-morning prototyping. . At Slack, we’ve been hiring so many new folks to join our ranks that we’re onboarding new engineers nearly weekly! A few of my teammates and I regularly alternate who teach one of the sessions new folks attend in their first week. We give an overview of Slack’s architecture, provide brief insight each of its major components, and trace a message through our whole system from both the HTTPS and web socket perspectives. The course has changed quite a bit over the years, and in just the few months I’ve taught it, we’ve had to update our slides to reflect shifts in our stack. . My husband and I love to cook at home. Slack provides employees with $500 in personal development funds every year, which I use in part to purchase recipe books (I highly recommend Cookie and Kate’s  recipe book ) and subscribe to the New York Times cooking app (which has an absolutely incredible collection of delicious recipes). We almost always end up with leftovers of our miscellaneous kitchen adventures, so we pack them up into individual containers for lunch throughout the week. We save a couple bucks and don’t have to brave the SOMA lunch lines! . On days where I have few afternoon meetings, I’ll make the time to eat lunch with my team. Our fall intern rejoined the team full-time just a few weeks ago and it’s been fun keeping up to date with all of the shenanigans of life after graduation. He was able to snag a free couch from one of our teammates who just moved into a new place which got us talking about all of the great scores we’ve gotten from other coworkers through our internal channel, #sf-classifieds. . Afternoons are speckled with meetings (except for Mondays which I block off for additional heads-down time). My team syncs up once per week on Wednesdays; we’ll discuss our progress since we last met and what we hope to accomplish in the upcoming week. The meeting frequently devolves into topics that are top of mind: an incident one of us was involved with, our latest mishap with testing infrastructure, impressions of the most recent all-hands. . We recently kicked off an effort to rewrite a significant portion of our backend codebase to follow a more object-oriented approach and promote better abstractions across distinct components in our application. Although most of our files are isolated into directories grouped by functional component, the code contained within those files isn’t particularly well isolated. When fixing a bug, it’s not uncommon for us to have to fix it in multiple places simply because every bit of code is responsible for handling every edge case. By gradually migrating our code to adhere to our new model, we hope to turn spaghetti code into lasagna code. . Every so often I’ll grab coffee with other engineers across the organization; it’s important to build a strong rapport with a wide variety of folks across the company when you’re on a team tasked with keeping folks productive. Plus, the early afternoon is when my brain is the slowest, so it’s a great time to grab a coffee and pick someone else’s brain. While the company doesn’t feel quite so small anymore (engineering has grown nearly four times since I joined the company three years ago), it doesn’t feel quite so big when taking the time to meet with new folks and learn about their challenges as they ramp up on our codebase. . If I haven’t gotten around to answering direct messages from earlier in the day, I take the time to do that. Having been at Slack for a while, I’ve worked on a number of different projects that frequently show up under git blame. I’ll normally get a few pings from folks throughout the day who have a question or two about a variety of things: Slerf, Hacklang syntax, a data warehouse query, finding errors in Logstash, or a few lines of code I authored a while back. If someone pinged me with a question that I think other folks would benefit from knowing the answer to, I kindly ask them to post the question to the most relevant public channel with a promise that I’ll answer it there. This is great for two reasons: first, if folks ever have the same question, they can use Slack search to find my answer again; second, it gives other folks the opportunity to provide an answer too, and give context that can be richer or more recent. . Time to wrap things up. I might leave a few notes for myself to reference tomorrow morning (where to pick back up debugging something, which pull request reviews are still outstanding, which folks to ping with questions). Some days feel more productive than others, and while I’m trying to associate productivity with more than just shipping code, it’s not an easy mindset to develop. If today didn’t feel particularly productive, I spend a few minutes poring over the day’s calendar, the messages I sent, the commits I pushed, and the code I reviewed. I take note of it all in a document I call my “Log of Unproductivity”. Not only does my perspective on the day change drastically, I’m able to spot patterns in how and where I spend my time on the days that I feel the least productive. For example, I’ve recently noticed days I spend primarily doing code review don’t intrinsically feel productive to me, so I’m deliberately trying to find ways to make code review feel just as productive as writing code. . I pack up my things and head home. Once home, I feed my sourdough starter and change into some leggings for either a short run around my neighborhood (it’s very hilly, so my pace varies pretty wildly) or a vinyasa yoga class at my local studio. . This is the first in a series of posts describing the everyday life of Engineers in different parts of Slack’s ecosystem. Over the coming months, we’ll hear from Engineers on our Product Engineering, Mobile, and Frontend teams. If you’re interested in exploring job openings at Slack, visit our  Careers  page! . Maude is a Staff Software Engineer, based out of our San Francisco offices. In her time at Slack, she’s worked on a few different teams: first as a product engineer building the Grid product, then as a performance engineer focused on scaling out the backend to handle some of our largest Enterprise customers. She has most recently been working with the Backend Foundation team, focusing on making sure that other backend engineers at Slack are as productive as they can possibly be. The team has its fingers in many pies: shipping the latest updates to our language stack (HHVM/Hacklang), ensuring linters and unit testing framework are in tip-top shape, stewarding core libraries, and building out tools to monitor performance regressions. . An avid conference speaker, Maude’s recently spoken at Strange Loop in St. Louis, MO, the Lead Developer in Austin, TX, and O’Reilly Velocity in NYC. You can find links to all her talks at  maudethecodetoad.com . ", "date": "2019-11-14"}, {"website": "Slack", "title": "Introducing Nebula, the open source global overlay network from Slack", "author": ["Ryan Huber"], "link": "https://slack.engineering/introducing-nebula-the-open-source-global-overlay-network-from-slack/", "abstract": " “What is the easiest way to securely connect tens of thousands of computers, hosted at multiple cloud service providers in dozens of locations around the globe?” If you want our answer, it’s  Nebula , but I recommend that you read the rest of this short post before clicking that shiny link. . At Slack, we asked ourselves this very question a few years ago. We tried a number of approaches to this problem, but each came with trade-offs in performance, security, features, or ease of use. We will gladly share those experiences in future presentations and writing, but for now, just know that we did not set out to write software to solve this problem. Slack is in the business of connecting people, not computers. . Nebula is a scalable overlay networking tool with a focus on performance, simplicity and security. It lets you seamlessly connect computers anywhere in the world. Nebula is portable, and runs on Linux, OSX, and Windows. (Also: keep this quiet, but we have an early prototype running on iOS). . It is important to note that Nebula incorporates a number of existing concepts like encryption, security groups, certificates, and tunneling, and each of those individual pieces existed before Nebula in various forms. What makes Nebula different to existing offerings is that it brings all of these ideas together, resulting in a sum that is greater than its individual parts. . Today Nebula runs on every server at Slack, providing a global overlay network that helps us operate our service. While this is the first time most people have heard of Nebula, it has been in use at Slack for over two years! . A few years ago, Slack was using IPSec to provide encrypted connectivity between regions. This approach worked well in the beginning, but quickly became an operational burden to manage our growing network. It also came with a small but measurable performance impact, because every packet destined for another region had to be routed through an IPSec tunnel host, adding a hop in the network route. We searched for an IPSec replacement, and even tried a few possible solutions, but none of them met our needs. . More importantly, as our software stack and service grew in complexity, network segmentation became increasingly difficult. One of our core problems was related to segmentation when crossing various network boundaries. Most cloud providers offer some form of user-defined network host grouping, often called “security groups”, which allow you to filter network traffic based on group membership, as opposed to individually by IP address or range. Unfortunately, as of this writing, security groups are siloed to each individual region of a hosting provider. Additionally, there is no interoperable version of security groups between different hosting providers. This means that as you expand to multiple regions or providers, your only useful option becomes network segmentation by IP address or IP network range, which becomes complex to manage. . Given our requirements, and the lack of off-the-shelf options that could meet our encryption, segmentation, and operational requirements, we decided to create our own solution. . We did a  LOT  of experimentation when creating Nebula, and probably discarded more code than exists in the final product. This experimentation was valuable because it allowed us to challenge our assumptions and come to more informed conclusions. Not all projects have the luxury of time, but in this case it was an enormous advantage, because we had the chance to try so many things. . The very first thing we did was research modern best-of-breed encryption strategies. In our research, we learned about the  Noise Protocol Framework , created by Trevor Perrin, co-author of the Signal Protocol, which is the basis of  Signal Messenger . We decided early in the project that Noise would become our basis for key exchange and symmetric encryption. Most importantly,  we did not roll our own crypto . . As we looked at Software Defined Network (SDN) and mesh networking software, a project some of us have used personally,  Tinc , came up. Some of the strategies Tinc uses to establish tunnels between hard-to-reach nodes informed our design goals for Nebula. . Instead of trying to cover the diverse set of technical design decisions behind Nebula today, this post is purposefully high-level. We are ready to share Nebula publicly, so others can kick the tires and let us know what they think, and future posts will dig into the nuts and bolts. . We have shared Nebula with a small community of engineers prior to this release, and received positive feedback on the simplicity and power of the system. Some of them are using it to connect systems in their organization and have provided extremely useful feedback. Nebula is useful for connecting thousands of computers, but equally useful for connecting two or three. . Nebula has undergone a paid security vulnerability assessment, along with numerous internal security reviews. We are adding Nebula to our  official bug bounty program , where we welcome submissions related to security bugs found in our software. (Note: while we may look at suggestions related to best practices, unless they constitute a vulnerability, these will likely not qualify for a bounty payment). . At Slack, we appreciate that we could not have built our service without open source software, and we hope this small contribution to open source can help others by providing software they need so they can focus on building software they want. . We hope you enjoy trying Nebula, and If you’re interested in helping us solve engineering problems large and small, check out our  job listings . .  Nebula was created by Nate Brown and Ryan Huber, with contributions from Oliver Fross, Alan Lam, Wade Simmons, and Lining Wang.  ", "date": "2019-11-19"}, {"website": "Slack", "title": "Reliably Upgrading Apache Airflow at Slack’s Scale", "author": ["Ashwin Shankar"], "link": "https://slack.engineering/reliably-upgrading-apache-airflow-at-slacks-scale/", "abstract": "  Apache Airflow  is a tool for describing, executing, and monitoring workflows. At Slack, we use Airflow to orchestrate and manage our data warehouse workflows, which includes product and business metrics and also is used for different engineering use-cases (e.g. search and offline indexing). For two years we’ve been running Airflow 1.8, and it was time for us to catch up with the recent releases and upgrade to Airflow 1.10; in this post we’ll describe the problems we encountered and the solutions we put in place. . As of September 2019, Slack has over 12 million daily active users performing 5 billion actions on average every week. This results in over 700 billion records loaded daily into our S3 Data Warehouse. This data is then processed by hundreds of workflows running Apache Hive, Apache Spark, and Presto on Airflow that submit jobs to our clusters running on thousands of EC2 instances. . We considered a couple strategies for the Airflow upgrade: . Since the red-black upgrade was not feasible and involved more risk to the data quality, we went ahead with the big-bang upgrade. We added some of our critical DAGs from prod to dev for testing. . Following are the high-level steps for the upgrade. We did these steps in dev first and then in our prod environment: . To optimize the upgrade based on the initial requirements, we considered some approaches for Database Backup and Schema Upgrade, which we will delve into next. . We wanted to upgrade the database in a way where we can rollback quickly and also minimize overall downtime. We considered two approaches: .   . We decided to go with the second approach for DB migration because we can rollback quickly with less downtime (we reduce catch-up time since we don’t have to take snapshots). . Airflow 1.10’s metadata DB schema has undergone many changes since version 1.8. To get from 1.8 to 1.10, the general recommendation is to go to 1.9 first; otherwise we cannot directly use the out-of-the-box command included with Airflow 1.10 to upgrade the DB (airflow upgradedb). However, we didn’t want to spend the time to do two upgrades, and instead wanted to go directly to 1.10. We also observed that the airflow upgradedb script, which was written in python using the alembic package, was taking a long time to run, likely due to our production metadata DB having over ten million rows in some tables. To resolve all of this, we wrote our own MySQL schema upgrade script. We noticed that running the same queries from MySQL was much faster compared to the Python scripts included in Airflow. We have contributed this script back to the community (see  https://issues.apache.org/jira/browse/AIRFLOW-6094 ). . We found a few issues during testing and some after the upgrade. .  1. adhoc attribute removal   The adhoc attribute was removed from the task object in 1.10. This previously told Airflow not to run a task in a scheduled manner, but rather marked it as a task that will only be run manually. We had a few DAGs with adhoc tasks. The solution was to consolidate those tasks in a new DAG and to mark that DAG as schedule_interval=None. .  2. The Presto operator stopped working   This is due to a more recent “future” package (0.16.0) being incompatible with Presto hook in Airflow when it is run on Python 2.7. To resolve this, we either need to convert the protocol field in Presto hook to Unicode or move Airflow to Python 3. We went with the former approach since Python 3 migration is a big effort on its own and we don’t want to have too many moving parts in the version upgrade. .  3. UI issues that could create data quality issues   In 1.10.3, when we click “Mark Success” on a task, instead of just marking the task as “success”, it marks the task and all its downstream dependencies to “success”. This could result in dependent DAGs kicking off and producing incorrect data, as the user could mark by mistake tasks that they did not intend to. We disabled “Mark Success” and instead use the “Task Instance” admin page to do the same. .  1. HiveServer2Hook stopped allowing   DESCRIBE   queries   This issue has been reported upstream in  AIRFLOW-3633  . To fix this, we wrote our own SlackHiveServer2Hook which allows DESCRIBE. .  2. boto2 vs boto3   There was a change in behavior of the AWS S3 list_keys API in boto3, which caused some tasks to fail. This was because list_keys started to ignore S3 folder markers($folder$ files). We resolved this by handling this new behavior appropriately in our library code. .  3. Timezone issues   Airflow 1.10 became timezone aware. This caused the execution dates in the metadata DB to return results in the time zone that the MySQL server or the host was configured to (which was Pacific time). However, some of our dashboards broke because they were expecting data in UTC. To fix this, we updated the default timezone in Airflow metadata DB to UTC. . After the upgrade was complete, we did a post-mortem. Here are some of the lessons learnt from the upgrade. .  Positive takeaways  .  Things we can do better  . Despite a few hiccups, the upgrade was overall successful. The issues that we found post-upgrade were fixed forward and most of the critical DAGs caught up quickly. The next step is to move Airflow into Kubernetes so that we can scale tasks horizontally. . This upgrade wouldn’t have been possible without efforts from multiple teams at Slack. Special thanks to Deepak Barge, Ananth Packkildurai, Derek Smith, and Atl Arredondo. . We have a lot more challenging problems in scale, reliability, and security coming up in the Data Infrastructure team. If you’d like to help us, please check out  https://slack.com/careers . ", "date": "2020-01-15"}, {"website": "Slack", "title": "Rewriting the Slack Python SDK", "author": ["Rodney Urquhart"], "link": "https://slack.engineering/rewriting-the-slack-python-sdk/", "abstract": " Have you ever been given a relatively inactive project and asked to fix a bug? What about having to update code that’s used by thousands of projects without the guidance of the original author? . I stepped into a circumstance like that when I joined the Developer Relations Tools Team at Slack. At the start of 2019, my manager tasked me with taking over as the lead maintainer of the  Slack Python SDK : a set of open-source tools, documentation, and code samples that aid in the creation of Slack apps written in Python. . This blog post is a view into my journey throughout the last few months: what motivated me to make big changes, the process I took to implement them, and how getting feedback from the community changed my original plans. . The Slack Python SDK, originally created in 2016, provides a simple HTTP and WebSocket client to help communicate with Slack’s Platform APIs. The goal was to make it easy to get started building a Slack app in Python, as well as to ensure that no matter how complex your app becomes, it’s scalable and maintainable. . As our platform grew over the years, the Slack Python SDK had challenges keeping up. Adding a new feature felt akin to making a late-stage move in Jenga. . Pagination, a feature available on the platform since July 2018, was difficult to support due to the complexity around how requests were made and responses were parsed. There was also code that existed for deprecated features, such as workspace token apps. . Ultimately though, my attempts to resolve existing issues in GitHub became the biggest motivator to make major changes. A lot of the errors encountered were difficult to reproduce and the most common were all related to our WebSocket interface. Proxy and SSL issues became the bane of my existence. This was due to the fact that we’re at the mercy of our internal client library dependencies. This prompted me to dig deep into the structure of our code and the decisions that were previously made. . Before diving into specific challenges, I wanted to make sure I understood the structure of the project. I spent a week evaluating our current state, the existing issues and PR’s, and the new features we wanted to add. I questioned the purpose of each function, the state it needed, and in which object it lived. I also loosely mapped how data flows when interacting with our Web API versus interacting with our RTM API. . At a high level, all Slack apps can be broken down into a few primary functions: . The Slack Python SDK should enable these functions with the least amount of effort. So I walked through the implementation to check if this was true. . To query for information from and enact change in a Slack workspace you’d utilize the Web API. The Web API is a collection of HTTP RPC-style methods, all with URLs in the form of  https://slack.com/api/METHOD_FAMILY.method . . To use the Web API, you instantiate a SlackClient and execute a function called api_call while passing the name of the Web API method, along with any data the method required. . Here’s an example of sending a message to a channel: . State changes in Slack, such as a message being posted to a channel, are represented by what we call “events”. Events are JSON payloads containing details about the change that occurred. . There are two ways to receive events: . A quick note about the Events API: It allows you to subscribe only to the event types you need, and is governed by OAuth permission scopes. It requires a running WebServer. The Events API is not yet supported in our SDK, but it’s on our backlog and will be available soon. You can, however, use our Flask-based extension slackeventsapi to get access to the Events API. . The RTM API solves a major problem for our customers who do not allow for incoming HTTP requests from 3rd parties. It is a WebSocket-based API that allows you to stream events from Slack in real time from an outgoing connection that you initiate. . To use the RTM API, you instantiate a SlackClient and establish a connection with the rtm_connect() method. While connected you then read from the stream as events come in by calling rtm_read(). . Below is a typical example of what this looked like: . After assessing the code, it was clear that using the existing client to make HTTP requests didn’t provide as much value as it could to developers. Further, using the client to listen for state changes inside of Slack with the RTM API was more complex than it needed to be. To best leverage Python’s features for creating clean, efficient code, it was time to restructure our codebase. However, since I was new to the project, I didn’t want to impose this decision unilaterally. Instead I opened  issue #384 , “RFC: Python Slack Client v2.0”, to break down how I thought we could improve our client. . In the v1 implementation, SlackClient lacked a separation of concerns. It’s a single class that contains both an HTTP client to support the Web API and a WebSocket client to support the RTM API. These objects, however, do not have a mutually beneficial relationship. So the first thing I proposed was splitting them into two separate classes. This would allow us to encapsulate all relevant information for both clients behind well-defined interfaces which would improve the Slack Python SDK’s maintainability. . In the RFC, the proposed WebClient would be a thin HTTP client wrapper that focused solely on preparing requests and parsing responses from Slack’s Web API. At the time, this was built using the requests library. . I saw the opportunity to reduce the boilerplate code by implementing the Web API methods directly. This would also afford us the ability to provide more information about the API methods while they were being used, as well as handle common functionality such as opening a file from a specified path. Here’s an example of what this would look like if implemented as suggested: . Inspired by the open-source  Ruby Slack SDK  by  Daniel Doubrovkine , I wanted to abstract away the complex looping that was previously required to respond to events from the RTM API. I wanted to allow developers to focus more on their application and less on reading from the WebSocket stream. Regardless of the APIs used, structuring your app as an event-based system is the simplest and most efficient way to build apps and bots that respond to activities in Slack. . The proposed RTMClient would allow you to simply link your application’s callbacks to corresponding Slack events. Below is pseudo-code of what this would look like: . In v2 I also wanted to ensure that: . The RFC was created and feedback from our open-source community started to roll in. I also received feedback on this proposal internally at Slack from a council of Platform engineers and Python developers. Initial feedback was overwhelmingly positive. People were excited that we were making a major update to the project. . In fact, most of the feedback from our community was from developers who wanted to take advantage of some of Python 3’s new features. Async support ending up being the top request. There was also interest in seeing Type Hints and Return Types added. So I posed the question internally at Slack… . “Can we drop support for Python 2 in this new version?” . Well it turns out this question is still one that ends friendships. I’m kidding, but seriously, it was a deeply debated decision. . On the one hand, we previously supported this version and it’s still in-use. Deprecating it would be a major inconvenience to some of our developers. Let’s also not forget that not everyone has control of their environments, making it hard, or even impossible, to upgrade to Python 3. . On the other hand, Python 2 will reach its end of life at the end of 2019. The active Python 2.7 usage in our project is trending down, and has gone from about 50% to 35% over the last few months. Why hold back the majority of our community from being able to take advantage of new features in Python 3? . As the lead of this project, I believe it’s important that we strive to follow and encourage the best practices. The Python community has decided to move away from 2.7, therefore so will our project. I acknowledge that this may not work for everyone, and for this population we’ll continue to support bug fixes in our previous version until Dec 31st, 2019 (Python 2’s EOL). . Okay so now that we’re targeting Python 3, let’s go through some quick wins. .  PEP 3102  allows us to enforce the use of keyword arguments. This makes usage of API methods and the arguments you’re passing in much more explicit. I decided to take advantage of this because it gives developers a much clearer picture of what each argument represents. This increases a projects maintainability over time. If Slack’s API changes, their code remains flexible. .  PEP 0484  allows us to provide argument type hints. In the same spirit of optimizing the Slack Python SDK for developer experience, I’ve implemented type hints for every method. This allows us to be explicit about what our API expects when sending data. . The next thing added was async support. However before we jump into how we implemented async, let’s walk through some basics first. . From a high level, asynchronous programming is a way to write code that allows other tasks to execute while it’s still performing some slow operation. It eventually regains control and resumes execution. . Async is not a one size fits all and should only be used only when it adds value. An app running CPU intensive operations will not see much gained from asynchronous programming. . Given the heavy I/O nature of Slack apps, taking advantage of async programming can help drastically improve the performance, responsiveness, and efficiency of your app. This is because your app will not sit idly by while sending data out to or waiting for a response back from Slack. . In Python, asyncio is a library that provides a foundation to write concurrent code using the async/await syntax. To add async support, I had to find an HTTP client and a WebSocket client that supported asyncio out of the box. . After some research and experimentation I ultimately settled on AIOHTTP. AIOHTTP is an asynchronous client/server framework for asyncio and Python. AIOHTTP supports both HTTP and WebSocket protocols. It’s exactly what we need. . Building the WebClient class was pretty straightforward. Simply put, it’s a class that implements methods that mirror Slack’s Web API methods. When executed, they construct and submit an HTTP request using the internal HTTP client class provided by AIOHTTP. When a response is received, it’s parsed into a JSON-like object we call SlackResponse. . Here’s what usage looks like: . While this works for projects that embrace asynchronous programming, it wouldn’t fit very well in those projects that don’t. In an effort to make our SDK more flexible, I pulled some of the async logic into the WebClient class. By default, async is turned “off”, which means that all API requests will run until completion. . Here’s what making the same API requests looks like run synchronously: . Now to enable async again, you simply set the client variable run_async to True. This will cause all API requests to return a Future just like before. . Here’s what async usage looks like now: . Building the RTMClient was a bit more complex. We had to support the same idea that it was easy for everyone to build apps, whether they wanted to brave the new world of asyncio or not. I wanted our SDK to do the heavy lifting for you when you first get started, and to also get out of the way when you want to take control. . The first feature I implemented was the ability to link your application’s behaviors to events that occur in Slack. To do this, I created a private dictionary called _callbacks on the RTMClient class, as well as a class method called on() that’s responsible for updating the callbacks container. This was done at the class level to ensure that applications respond consistently to events regardless of the teams they’re connected to. I generally recommend that all logic that’s related to specific teams be handled in your application layer. Linking callbacks to events should be akin to mapping routes to controller actions in the MVC pattern. . I also created the run_on decorator to make it more convenient to link your callbacks to events. . It’s important to note that your callback must accept keyword-arguments or “**kwargs”. If it does not, an exception will be raised immediately. I made this decision to ensure we could always pass the following collection of objects: the corresponding RTMClient instance, a WebClient, and an object called data which will hold any related data to the occurring event. . To connect to Slack’s RTM server, users simply call the start() function: . This function is responsible for retrieving the WebSocket URL from Slack’s Web API and establishing a connection to the message server. . Almost everything that happens in Slack will result in an event being sent to all connected clients. Throughout a connection’s lifetime, the RTMClient is responsible for dispatching these events as they occur. This means we execute any related callbacks when the specified trigger occurs. . There are also 3 WebSocket-related events that occur when the state of WebSocket connection changes: open, close, and error. . In my current approach, I forked the execution of callbacks into two separate private functions. One was responsible for executing callbacks asynchronously and the other for executing callbacks synchronously. The full implementation can be found in the RTMClient method  _dispatch_event() . . The function _execute_callback_async runs the following steps: . The function _execute_callback runs the following steps: . This worked pretty well initially when testing, but I’ve started to see that this approach is flawed. Manually changing a callback into a coroutine is a misguided attempt to optimize something that really didn’t need to be optimized. In hindsight, I also see that there are a number of places where executing code with ThreadPoolExecutor is necessary. Creating this object over and over again is wasteful and unnecessary. I’m currently working on an update to resolve these issues. I’ll likely handle the execution of non-async callbacks by utilizing the asyncio eventloop function  #run_in_executor() . . If you’d like to share some advice, suggestions, or have any additional ideas for how this could be handled, please let me know on this GitHub  issue . I’d love your feedback! . The last piece to this update was ensuring that we automatically reconnected to RTM if the connection was dropped for any reason other than the user stopping it. The initial implementation focused on ensuring that the wait time increased exponentially if the exceptions we specified continued to occur. We also added random number of milliseconds to avoid coincidental synchronized client retries. . My initial aim with v2 of this SDK was to address a few goals: . While most of this impact is reflected internally in the project, there are a couple things we can highlight that improves the experience for our developers. . I’ve taken basic Web API requests such as this: . And have made them simpler, while providing more value in your editor: . For those using the RTM API, previously you were required to loop over the stream of messages: . This is now slightly easier to scale since you simply link your callbacks to events: . To those who’ve struggled with SSL and Proxy issues, I believe you’ll be very satisfied with AIOHTTP’s advanced configuration options. . In our current state with v2, I’d say that we’ve made some pretty good progress! We’ve redesigned the project to improve the developer experience, to make it more maintainable, and to take advantage of the new features in Python 3. However, we’re just getting started. . For the next release of the Slack Python SDK, I’ll be focusing on improving the performance of both the RTMClient as well as the WebClient. Along with improving the quality of the code by fixing any missed bugs and increasing our test coverage. . I’ll also be working to build a Web Server capable of supporting the Slack Events API. This will most likely take advantage of the HTTP Server that comes built into the AIOHTTP library. It seems wise to utilize the same dependency to provide everything our customers need to build a Slack app. . Finally, I’ll be working to add more code and documentation to help get our developers up and running quickly. Do you have an idea for a Slack app you’d like to see built? Add a comment to this Github  issue . If it’s useful for others, I’ll consider building and open-sourcing it. . Interested in trying the new SDK? Want to build a quick app yourself? Take a look at the getting started  tutorial  in Github. You should be able to create a running app in under 10 minutes. . P.S. Interested in working on open-source tools?  My team is hiring!  . P.P.S. You can dig through the Slack Python SDK source code in the  GitHub Repository . ", "date": "2019-05-24"}, {"website": "Slack", "title": "🏄‍♀️ Surf’s Up! Preparing for Huge Waves of Traffic via Load Testing", "author": ["Serry Park", "Arka Ganguli", "Joe Smith"], "link": "https://slack.engineering/%ef%b8%8f-surfs-up-preparing-for-huge-waves-of-traffic-via-load-testing/", "abstract": " Over 10 million users across the globe rely on Slack everyday to collaborate with their colleagues. As our user base has grown, so has our focus on enhancing the performance of our features and ensuring their ability to perform under load. This was especially true for Enterprise Key Management (EKM), which  launched earlier this year  for  Enterprise Grid  customers. . EKM allows our most security-conscious customers to use their own keys to encrypt and decrypt their data in Slack, including fundamental aspects of Slack such as sending and receiving messages. Our goal was to ensure that it would work seamlessly at scale when we launched. . We’d like to share our strategy for load testing EKM, how our tooling reinforces service ownership, and how this tool (and others like it) can act as force-multipliers for other engineers. . Designing load tests for this feature required a meticulous understanding of EKM and a systems-level view of how it fits within the greater Slack architecture. . Slack has a (sometimes surprisingly) complex architecture with many services that work together in order to respond to requests as fast as possible. At a high level, there is a backend server which we will refer to as webapp, which is responsible for handling web/API requests and federating database access. Webapp talks to a set of real-time stateful services, which handle the fanning out of events to connected clients. This allows us to send messages in real time and know the moment our colleagues begin typing in a given channel. . EKM is a new service within Slack’s architecture which webapp hooks into. As a result, our native clients are unaware of the EKM service and encryption and decryption is handled by webapp. We designed our tests with a focus on the server-side APIs, where we could easily measure EKM’s impact on real-time services. . Our tests were built to place sustained load on our web APIs and observe the impact on database health and the real time services. . The first step was to identify endpoints that serve message and file data, since those requests result in a downstream connection from webapp to EKM for encryption/decryption. To do this, we analyzed API usage patterns by our native clients (iOS, Android, desktop, web) and selected endpoints that were (1) most frequently called and (2) interacted with the EKM service. . It was crucial for us to gather baseline values to guide our analyses and better understand our test results. We created a target benchmark based on the average rate of requests for our largest enterprise customers during peak hours. We then compared the success rate and response timings of the load testing against the benchmark. . Our goal was to simulate double the load created by our largest customers during our tests, so that we could be confident that the service will perform for customers today and into the future as we continue to grow. . A critical component of load testing is creating environments that will mimic production users and workspaces. To make this process easier, we created a tool for quickly spinning up workspaces and users, then populating them with ample message and file data. Since we wanted to compare our experimental data against a baseline, we relied on this tool to create and populate two orgs, one control and one with the EKM feature enabled. This tool allowed us to create near-identical enterprises with identical data, thus reducing the noise in our analysis. . Before we took on this project, there were many load testing tools available, scattered across different repos. Our first plan of action was to consolidate the different tools and make them more accessible, which led to the birth of a new loadtest service. This new service allowed us to generate API calls and execute them all within a single place. . At a high level, this service generates calls to a given endpoint and hits them at the specified rate. This allows us to fine tune and simulate load on our specified APIs. This service was written entirely in golang, taking advantage of features like goroutines and channels for concurrency. We created a single CLI tool which encapsulates fine-grained control over rate and concurrency of API calls as well as safety valves. This improves the accessibility of these tests, allowing any engineer at Slack to leverage this in their workflow. . It is very easy for an engineer at Slack to spin up a development host within our infrastructure. We leveraged that provisioning service, along with some additional tooling to automate this process, to increase diagnostics and error handling. In order to minimize time spent manually debugging, the script performs basic remediation steps on behalf of the engineer to work around common errors. An engineer who wants to run load tests can run slack loadtest –bootstrap which executes the following steps: . We empower engineers to leverage this form of testing as we decrease the work required to get started. If there’s a problem the tool doesn’t solve, engineers can copy-paste the command output and get better assistance thanks to the clear output. In summary, with a single CLI command, we can provision a bootstrapped machine for you with all required dependencies in under 5 minutes. . With the introduction of a new service, there are always uncertainties about how it will perform and integrate with the rest of the stack. Even while running tests, there are numerous things that could go wrong when certain parts of the system are put under too much load. While running our tests, data was piped into dashboards which we monitored during test runs. The main metrics we monitored were API method error rates, EKM service latency, and database cluster health — CPU idle, replication lag and the number of threads connected provided high signal into problem areas. . One of the main benefits from our testing was uncovering bugs and areas for improvement within Slack’s codebase and infrastructure. We uncovered several codepaths that generated excessive load on other systems or performed unnecessary expensive operations. We were able to ship fixes for these types of regressions and validate our fixes in a controlled environment through our tests before our customers ever experienced them. . Load testing allowed us to: . We tested the performance of sending messages using an organization with EKM enabled. The API that we use for sending messages for both internal and external clients is  chat.postMessage . . Using the loadtest tool, we auto-generated a .txt file containing URLs pointing to our target endpoint. These URLs are generated programmatically with unique session tokens to simulate load from different users into randomly selected channels. . These calls were piped into the loadtest executor, which allowed us to specify the length of test run, concurrency, and rate of requests. . With a simple invocation like the one shown above, we could simulate sending messages at a rate of 100 requests per second over a period of 5 minutes. We repeated this process for the identified API endpoints and incrementally ramped up the rate to sustain load for longer periods of time. . One of the most exciting things about working at a growing company is being able to create tools that act as force multipliers of developer productivity. Using this standardized workflow, others can now take the tooling we created and apply the same process to their own features, enabling them to ship with the same confidence that we had. . However, even the best designed tools are useless if people don’t know they exist nor how to successfully leverage them into their work. It is equally as important to onboard engineers interested in using the tools as it is to build them. To that end, we revamped documentation, presented at department-wide meetings, and held office hours where engineers could drop in to discuss how they could incorporate the load testing service into their workflows. As a result, we are already seeing people take ownership in the new service as folks contribute back to this tooling to improve it. . We knew that running load tests would be valuable for the scope of EKM. What we didn’t know was the impact this tool would have on workflows across the engineering organization. . A great bonus from this project was enabling our colleagues to easily grasp how their new feature will work end to end within the greater Slack ecosystem under load. Since releasing this new tooling, more and more teams are adopting load tests as a part of their feature development process to better understand how their feature performs at scale. . As a whole, this framework enabled us to see the comprehensive impact of a feature on an entire system under load. We were able to mimic harsh conditions to see how features behave in the extremes, something that is critical as some bugs only surface at scale. Most importantly, we were able to confidently ship EKM and know that it will perform well from day one. . Slack has 10M+ daily active users that rely on our service to do the best work of their lives. Our job as engineers is to make sure that the work we produce can support this scale of our largest customers so that the experience continues to be delightful as we grow. Load testing has become increasingly important in our ability to deliver this with confidence. . We believe that running load tests shouldn’t be hard — the hardest part of your work should be designing and implementing the feature. This project was built by engineers with minimal load testing experience, but we believe this allowed us to create a product that will enable anyone to participate in load testing whenever they need. ", "date": "2019-06-20"}, {"website": "Slack", "title": "When a rewrite isn’t: rebuilding Slack on the desktop", "author": ["Mark Christian", "Johnny Rodgers"], "link": "https://slack.engineering/rebuilding-slack-on-the-desktop/", "abstract": "  Conventional wisdom  holds that you should never rewrite your code from scratch, and that’s good advice. Time spent rewriting something that already works is time that won’t be spent making our customers working lives simpler, more pleasant, and more productive. And running code knows things: hard-won knowledge gained through billions of hours of cumulative usage and tens of thousands of bug fixes. . Still, software codebases have life spans. The desktop version of Slack is our oldest client, growing out of a period of rapid development and experimentation during the earliest moments of our company. During this period, we were optimizing for product-market fit and in a constant sprint to keep up with our customers as their use — and expectations — of the product grew. . Today, after more than half a decade of hyper-growth, Slack is used by millions of people with larger companies working with more data than we ever could have imagined when we first started. Somewhat predictably, a few internal cracks were starting to show in the desktop client’s foundation. Additionally, the technology landscape had shifted away from the tools we chose in late 2012 (jQuery, Signals, and direct DOM manipulation) and toward a paradigm of composable interfaces and clean application abstractions. Despite  our best efforts to keep things snappy , it became clear that some fundamental changes would be required to evolve the desktop app and prepare it for the next wave of product development. . The architecture of the existing desktop app had a number of shortcomings: . The  first   two  problems were the sort of things that we could incrementally improve over time, but getting multiple workspaces to run within a single Electron process meant changing a fundamental assumption of the original design — that there is only ever a single workspace running at a time. Although we made some  incremental improvements  for folks with lots of idle workspaces, truly solving the multiple process problem meant rewriting Slack’s desktop client from scratch. . The  Ship of Theseus  is a thought experiment that considers whether an object that has had each of its pieces replaced one-by-one over time is still the same object when all is said and done. If every piece of wood in a ship has been replaced, is it the same ship? If every piece of JavaScript in an app has been replaced, is it the same app? We sure hoped so, because this seemed like the best course of action. . Our plan was to: . The final step — and the most important one for our purposes — was to create a modern-only version of Slack that would start out incomplete but gradually work its way toward feature completeness as modules and interfaces were modernized. . We’ve been using this modern-only version of the app internally for much of the last year, and it is now rolling out to customers. . The first order of business was to create the modern codebase. Although this was just a new subdirectory in our codebase, it had three important rules enforced by convention and tooling, each of which was intended to address one of our existing app’s shortcomings: . The first two rules, while time-consuming to fulfill, were relatively straightforward. However, moving to a multi-workspace architecture was quite the undertaking. We couldn’t expect every function call to pass along a workspace ID, and we couldn’t just set a global variable saying which workspace was currently visible since plenty of things continue to happen behind the scenes regardless of which workspace the user is currently looking at. . The key to our approach ended up being  Redux , which we were already using to manage our data model. With a bit of consideration and the help of the  redux-thunk  library, we were able to model virtually everything as actions or queries on a Redux store, allowing Redux to provide a convenient abstraction layer around the concept of individual workspaces. Each workspace would get its own Redux store with everything living within it — the workspace’s data, information about the client’s connectivity status, the WebSocket we use for real-time updates — you name it. This abstraction created a conceptual container around each workspace without having to house that container in its own Electron process, which was what the legacy client did. . With this realization, we had our new architecture in place: . At this point we had a plan and an architecture we thought would work, and we were ready to work our way through the existing codebase, modernizing everything until we were left with a brand new Slack. There was just one last problem to solve. . We couldn’t just start replacing old code with new code willy-nilly; without some type of structure to keep the old and new code separate, they’d end up getting hopelessly tangled together and we’d never have our modern codebase. To solve this problem, we introduced a few rules and functions in a concept called legacy-interop: . Exporting new code to the old code was simple. Our original codebase did not use JavaScript modules or imports. Instead, it kept everything on a top-level global variable called TS. The process of exporting new code just meant calling a helper function that made the new code available in a special TS.interop part of that global namespace. For example, TS.interop.i18n.t() would call into our modern, multi-workspace aware string localization function. Since the TS.interop namespace was only used from our legacy codebase, which only loaded a single workspace at a time, we could do a simple look-up to determine the workspace ID behind the scenes without requiring the legacy code to worry about it. . Adapting old code for new code was less trivial. Both the new code and the old code would be loaded when we were running the classic version of Slack, but the modern version would only include the new code. We needed to find a way to make it possible to conditionally tap into old code without causing errors in the new code, and we wanted the process to be as transparent to developers as possible. . Our solution was called adaptFunctionWithFallback, which took a function’s path on our legacy TS object to run, as well as a function to use instead if we were running in the modern-only codebase. This function defaulted to a no-op, which meant that if the underlying legacy code wasn’t present, modern code that tried to call it would have no effect — and produce no errors. . With both of these mechanisms in place, we were able to kick off our modernization effort in earnest. Legacy code could access new code as it got modernized, and new code could access old code  until  it got modernized. As you’d expect, over time there were fewer and fewer usages of old code adapted for use from the modern codebase, trending toward zero as we got ready for release. . This new version of Slack has been a long time coming, and it features the contributions of dozens of people who have been working through the last two years to roll it out seamlessly to customers. The key to its success is the incremental release strategy that we adopted early on in the project: as code was modernized and features were rebuilt, we released them to our customers. The first “modern” piece of the Slack app was our emoji picker, which we released more than two years ago — followed thereafter by the channel sidebar, message pane, and dozens of other features. . Had we waited until the entirety of Slack was rewritten before releasing it, our users would have had a worse day-to-day experience with emoji, messages, channel lists, search, and countless other features before we could release a “big bang” replacement. Releasing incrementally allowed us to deliver real value to our customers as soon as possible, helped us stay focused on continuous improvement, and de-risked the release of the new client by minimizing how much completely new code was being used by our customers for the first time. . Conventional wisdom states that rewrites are best avoided, but sometimes the benefits are too great to ignore. One of our primary metrics has been memory usage, and the new version of Slack delivers: . These results have validated all of the work that we’ve put into this new version of Slack, and we look forward to continuing to iterate and make it even better as time goes on. . When guided by strategic planning, tempered by judicious releases, and buoyed by talented contributors, incremental rewrites are a great way to right the wrongs of the past, build yourself a brand new ship, and make your users’ working lives simpler, more pleasant, and more productive. . We are eager to share more about what we’ve learned during this process. In the coming weeks we’ll be writing more at  https://slack.engineering  about: ", "date": "2019-07-22"}, {"website": "Slack", "title": "Disasterpiece Theater: Slack’s process for approachable Chaos Engineering", "author": ["Richard Crowley"], "link": "https://slack.engineering/disasterpiece-theater-slacks-process-for-approachable-chaos-engineering/", "abstract": " Slack is a large and complex piece of software that’s been added to and changed many times over the last five years. We added features, grew to 10,000,000 DAUs, and made major architectural changes. We made assumptions and tested them with processes that often resembled science. . Whenever we launch features or make changes, we test the fault tolerance of that new code. Unfortunately, we seldom get to repeat these tests as the environment continues to change around that no-longer-new code. As the sands shift, those initial test results lose value. We remain confident in the resilience and robustness of our most critical systems but that confidence is less well-founded as time progresses. And luck is not an availability strategy, so something must be done. . If we were starting from scratch, we’d probably be practicing Chaos Engineering. After all,  “the best way to test the failure path is never to shut the service down normally.”  But we’re not starting from scratch — we operate a large-scale, business-critical service. So what do we need, right now? We need to make Slack as reliable as possible. We need our development environment to be a more confidence-inspiring place to test for fault tolerance, and we believe that testing the fault tolerance of  all  our systems — not just new systems — will help us meet these needs. We don’t want to cause user-impacting incidents, so whatever we do needs to be safe as well. We also don’t need false confidence, so whatever we do needs to be in production. . In January of 2018, we started a rigorous process of identifying failures that are likely to happen and that we must be able to tolerate, and then purposely causing them to happen in production. This isn’t (yet) Chaos Engineering as practiced and evangelized by Netflix. It’s the first step; we call it Disasterpiece Theater. . The process each Disasterpiece Theater exercise follows is designed to maximize learning while minimizing risk of a production incident. Each exercise takes place at a well-publicized time and place with all of the relevant experts in the same room or on the same video conference — we’re not (yet) trying to test our monitoring during these exercises. Before the exercise, one or two hosts write a detailed plan and share it widely. The plan is critical to the safety of the exercise but the plan on its own doesn’t teach us much about our fault tolerance. . The hosts are responsible for doing a “tabletop” exercise in which they think through the entire operation. They document precisely how they’re going to incite the failure, right down to the commands they’re going to run and how they’re going to select which EC2 instances are involved (we’ve taken to calling them “tributes”). We ask the hosts to go on the record for how confident they are that fault tolerance in the dev environment predicts fault tolerance in the prod environment for this exercise. They also document all the logs, metrics, and alerts that should be monitored, as well as runbooks that may be necessary during this exercise. Most importantly, they make a specific hypothesis explaining how the failure will be experienced by upstream and downstream systems and by Slack clients. An example of this might be, “Termination of a MySQL master will result in 20 seconds of increased latency for requests that depend on that database but no increase in latency for other requests and less than 1,000 failed API requests, all of which are retried by clients.” . We start each exercise by reviewing the plan and projecting/sharing dashboards in Grafana and searches in Kibana. Now we’re ready to incite failure. . We announce the exercise in our  #ops  channel where more than 700 people hang out. We don’t stop deploys or any other normal activities during the exercise but we do make those folks aware of our plans. We broadcast a few coarse status updates in  #ops  throughout the exercise and keep our play-by-play in  #disasterpiece-theater . . Exercises always begin by inciting the failure in dev. Then we inspect logs and metrics to confirm the failure is visible in all the ways we expect it to be and not visible in others. It’s a common instinct to want to  go fix something  but we control ourselves and watch the system take care of itself. We look for load balancers and other traffic management to route around the failure or for capacity to be replaced. Occasionally we have to follow runbooks to restore service. . Once the failure has been dealt with in the development environment, we pause to make a go or no-go decision about proceeding to production. The exercise isn’t considered a failure or a waste of time if we don’t proceed to production; in fact, some of our most valuable lessons have come from the development environment. We seriously consider aborting if automated remediations didn’t work, didn’t work  perfectly , took too long or, most importantly, if the failure would result in more disruption than a short and minor increase in latency for customers. If we’re aborting, we announce the abort in  #ops . . Hopefully, though, we’re encouraged by the results in development and are ready to incite failure in the production environment. We project/share the production dashboards in Grafana and searches in Kibana. We announce in  #ops  that we’re moving on to production. . Finally, the moment of truth arrives. We incite failure in production. Just like we did in development, we inspect logs and metrics, looking to confirm our hypothesis. We give automated remediation time to do its work. Usually, this moment that’s theoretically terrifying is actually quite calm. When we’re finished, we announce the all-clear in  #ops . . Then, we debrief: What was the time to detect and time to resolve? Did any users notice? Did any humans have to intervene? What was terrifying? Was any of our documentation wrong? Were any dashboards in Grafana out of date? . We’ve run dozens of Disasterpiece Theater exercises at Slack. The majority of them have gone roughly according to plan, expanding our confidence in existing systems and proving the correct functioning of new ones. Some, however, have identified serious vulnerabilities to the availability or correctness of Slack and given us the opportunity to fix them before impacting customers. Here are summaries of three particularly successful exercises: . The first time Disasterpiece Theater turned its attention to memcached it was to demonstrate in production that automatic instance replacement worked properly. The exercise was simple, opting to disconnect a memcached instance from the network to observe a spare take its place. Next, we restored its network connectivity and terminated the replacement instance. . During our review of the plan we recognized a vulnerability in the instance replacement algorithm and soon confirmed its existence in the development environment. As it was originally implemented, if an instance loses its lease on a range of cache keys and then gets that same lease back, it does not flush its cache entries. However, in this case, another instance had served that range of cache keys in the interim, meaning the data in the original instance had become stale and possibly incorrect. . We addressed this in the exercise by manually flushing the cache at the appropriate moment and then, immediately after the exercise, changed the algorithm and tested it again. Without this result, we may have lived unknowingly with a small risk of cache corruption for quite a while. . In early 2019 we planned a series of ten exercises to demonstrate Slack’s tolerance of zonal failures and network partitions in AWS. One of these exercises concerned Channel Server, a system responsible for broadcasting newly sent messages and metadata to all connected Slack client WebSockets. The goal was simply to partition 25% of the Channel Servers from the network to observe that the failures were detected and the instances were replaced by spares. . The first attempt to create this network partition failed to fully account for the overlay network that provides transparent transit encryption. In effect, we isolated each Channel Server  far more  than anticipated, creating a situation closer to disconnecting them from the network than a network partition. We stopped early to regroup and get the network partition just right. . The second attempt showed promise but was also ended before reaching production. This exercise did offer a positive result, though: It showed Consul was quite adept at routing around network partitions. This inspired confidence but doomed this exercise as we ended up doing a lot of work to not even cause any Channel Servers to fail. . The third and final attempt finally brought along a complete arsenal of iptables(8) rules and succeeded in partitioning 25% of the Channel Servers from the network. Consul detected the failures quickly and replacements were thrown into action. Most importantly, the load this massive automated reconfiguration brought on the Slack API was well within that system’s capacity. At the end of a long road, it was positive results all around! . There have also been negative results. Incident response often involves making configuration changes using an internally developed system called Confabulator. During one particularly bad incident, Confabulator didn’t operate as expected and we had to make and deploy the configuration change manually. I thought this was worthy of further investigation. The maintainers and I planned an exercise to directly mimic the situation we encountered. Confabulator would be partitioned from the Slack service but otherwise left completely intact. Then we would try to make a no-op configuration change. . We reproduced the error without any trouble and started tracing through our code. It didn’t take long to find the problem. The system’s authors anticipated the situation in which Slack itself was down and thus was unable to validate the proposed configuration change; they offered an emergency mode that skipped that validation. However, both normal and emergency modes attempted to post a notice of the configuration change to a Slack channel. There was no timeout on this action but there was a timeout on the overall configuration API action. As a result, even in emergency mode, the request could never make it as far as making the requested configuration change if Slack itself was down. Since then, we’ve made many improvements to code and configuration deploy and have audited timeout and retry policies in these critical systems. . Disasterpiece Theater has made the regular, safe testing of the fault tolerance of Slack’s most critical systems approachable and non-terrifying. It helps us understand and improve Slack’s basic reliability, one of the most important factors in earning and keeping our customers’ trust, even as we expand and evolve the product. . Exercises like the three highlighted above helped us to improve Slack’s reliability and built (or corrected) our confidence in our systems’ fault tolerance. Our Resilience Engineering team continues to expand and evolve this process all the time and, of course, is planning to run many more Disasterpiece Theater exercises. If you find this interesting and want to be a part of the next exercise,  come join us ! ", "date": "2019-08-01"}, {"website": "Slack", "title": "PanModal: Better support for thumb accessibility on Slack mobile", "author": ["Stephen Sowole"], "link": "https://slack.engineering/panmodal-better-support-for-thumb-accessibility-on-slack-mobile/", "abstract": " When the App Store first arrived, designing an iOS app was a straightforward process; you only had to worry about how the app would look and operate on a single device. But as the iPhone evolved, variations in screen sizes were introduced and it became a challenge to maintain consistency across devices whilst making efficient use of screen space. . Typically, when we think of user experience, the first thing that comes to mind is, “How do people interact with my product?”, but today when people use their devices standing up on public transport, walking, or running, it’s equally as important to consider how well your product reacts in those situations. . At Slack, we’re no strangers to these kinds of challenges, and we have embedded this way of thinking into how we design and develop new features. In this article, we’ll explore how Slack maintains the fine balance between usability and user experience on larger screen mobile devices, and how the need for  PanModal  arose. As such, we hope PanModal will benefit other iOS developers facing similar challenges. . Designing for thumbs refers to the process of designing a user interface that’s most comfortable for the thumb’s natural grasp. For brevity, we’ll analyze the issue of thumb reachability for right handed users, but this can also be applied for the left or both thumbs. . As you can see in the image, from a right handed perspective, there are three zones of reachability. Those three zones are: . The ideal approach taken when designing for thumbs is to limit the primary or secondary actions to sit somewhere between the green and yellow zone. Reserving the red zone for much less frequent actions. . Most Slack iPhone sessions can be described as  a user reading a message and then initiating an action . A sizable portion of these are actions on the messages themselves, including marking unread and adding reactions (both initiate from a long press). So naturally, we began to explore the various ways we could give users more freedom in what they can do with messages. . As part of this exploration, we analyzed how our current UI stacked up to the thumb reachability matrix before adding more content to the native iOS Action Sheet. . The results were far from ideal. Some frequently used actions were already outside the natural bounds of human thumb extension. Adding more actions to the already long list would mean pushing the most used actions further up into the red zone. This forces users to adjust the position of their operating hand (relative to their phones) every time they need to act on a message. In our pursuit of a more pleasant user experience, we concluded that we had to create a custom solution to solve the foundational problem. . In order to improve the overall experience, we needed to bring message actions closer to user thumbs. We worked closely with our talented product designers to explore the different approaches we could take. We even considered truncating and prioritizing the original list, leaving behind only the most crucial actions, relegating the rest accessible only via a “More” button or some other form of menu; whilst this would fix our problem in the short term, there was no guarantee this wouldn’t become a problem in the future. . So we came up with an alternative solution to the problem: create an expandable list in the form of a draggable modal. This would allow us to place the most frequently used actions right into the green zone, and should none of those actions be what you need, a simple upward swipe gesture will allow you to have access to the secondary actions without having to leave the optimal thumb reachability zone. . But what would it take to build something like this for iOS? . UIViewControllerTransitioning is a UIKit API included in the iOS SDK, enabling customization during the transition process in which one UIViewController is presented on top of another. It consists of three parts: the delegate, the controller and the animator, each playing a different role in the presentation. . The transitioningDelegate is essentially the starting point for the presentation process. It vends UIViewControllerAnimatedTransitioning and UIPresentationController objects on request, both required for a transition to occur. . The UIViewControllerAnimatedTransitioning protocol allows you to define an animation object used to present or dismiss a UIViewController within a set amount of time. Animations can be customized to translate a view within the coordinate system of the presentation bounds. . The UIPresentationController maintains the view hierarchy between both view controllers, responding to changes in environment appropriately, including device rotations and split view activation. . As both the animation and controller objects are weakly referenced during the transition process, they are both quickly released after the initial presentation — unless a reference is maintained elsewhere. . In order to initiate a custom transition, an object conforming to UIViewControllerTransitioningDelegate must be set as the transitioningDelegate on the viewControllerToPresent and its modalPresentationStyle set to .custom. . Understanding that a custom presentation is simply the sum of its components working in sync, we began to experiment with reusable UI modal transitions that could be utilized across the Slack app. . Before we began building though, we compiled a list of crucial goals that we wanted to achieve for the new project. . Since this would mean having similar functionality to the  modalViewController  (deprecated in iOS 6), with the addition of UIPanGestureRecognizer management, it felt appropriate to give the library a name that would consolidate both of these classes: PanModal. . Taking a bottom-up approach to the problem and having a focus on reusability turned out to be really useful when we were building out each component, from the animator to the controller. . It allowed us to be really strict on the responsibilities of each object, outside of the default transitioning system. For example: To be able to plug the PanModalPresentationController into any custom transitioningDelegate meant that we had to limit direct access to its internals, instead opting to only use the PanModalPresentable for configuration, which is the only protocol visible outside the scope of the framework. . This, in turn, allowed us to completely encapsulate the management of the internal UIPanGestureRecognizer and abstract the entire system down to one call. . However, the goal of supporting any subclass of UIViewController was not as straight forward. Once the view controller had a child UIScrollView, whether it’s the first view in the hierarchy (as is the case with UITableViewController) or the last, it interfered with the internal UIPanGestureRecognizer. . Pan gestures had to be tracked by both the child UIScrollView and the PanModal system, only responding when necessary (e.g. the user should only be able to scroll on the content when the PanModal is in long form). Simply disabling and re-enabling UIPanGestureRecognizers at appropriate points was not an option here as there is no way to continue tracking touches within the same touch event without retouching the screen (not an ideal experience for the user). We spent a lot of time building a system that would suspend one recognizer while the other responded to pan gestures — without both responding to touch events at the same time. . Once the system worked as expected, we had to find a way for the presentedViewController to provide a reference to a chosen child UIScrollView while still maintaining abstraction from the PanModal framework. . But even after solving the initial problem of gesture switching, we ran into another issue. To moderate scrolling, we were overriding the UIScrollViewDelegate on panScrollable, which, while it worked, actually limited the consumer from being able to also override the delegate (most common in the case of UITableView), since delegate patterns in iOS have a 1:1 relationship. . So, in order to maintain flexibility, we chose to use KVO instead of delegates to track content offset changes. . We then had to think about each device we supported and how we might further reduce the boilerplate code needed to present a PanModal. Positions are relative to each screen, whether it’s an iPhone 5 or the iPhone X, so we changed the way we thought about customization of the two PanModal display states. Relative positions to the safe area insets should be abstracted away and easy to understand at a glance. . With care and consideration, we were able to accomplish all of our goals for the project. You can dig into the PanModal source code in the  GitHub Repository ! . At Slack, we care deeply about how our users interact with our product and we’re constantly thinking of new ways to improve those experiences. If you’re interested in working with us, we would love to have you  join  ! ", "date": "2019-08-22"}, {"website": "Slack", "title": "Service Workers at Slack: Our Quest for Faster Boot Times and Offline Support", "author": ["Jim Whimpey"], "link": "https://slack.engineering/service-workers-at-slack-our-quest-for-faster-boot-times-and-offline-support/", "abstract": " We recently rolled out  a new version of Slack on the desktop , and one of its headlining features is a faster boot time. In this post, we’ll take a look back at our quest to get Slack running quickly, so you can get to work. . The rewrite began as a prototype called “speedy boots” that aimed to–you guessed it–boot Slack as quickly as possible. Using a CDN-cached HTML file, a persisted Redux store, and a Service Worker, we were able to boot a stripped-down version of the client in less than a second (at the time, normal boot times for users with 1–2 workspaces was around 5 seconds). The Service Worker was at the center of this speed increase, and it unlocked the oft-requested ability to run Slack offline as well. This prototype showed us a glimpse of what a radically reimagined desktop client architecture could do. Based on this potential, we set about rebuilding a Slack client with these core expectations of boot performance and offline support baked in. Let’s dive into how it works. . A  Service Worker  is a powerful proxy for network requests that allows developers to take control of the way the browser responds to individual HTTP requests using a small amount of JavaScript. They come with a rich and flexible cache API designed to use Request objects as keys and Response objects as values. Like  Web Workers , they live and run in their own process outside of any individually running window. . Service Workers are a follow-up to the now-deprecated  Application Cache , a set of APIs previously used to enable offline-capable websites. AppCache worked by providing a static manifest of files you’d like to cache for offline use… and that was it. It was simple but inflexible and offered no control to developers. The W3C took that feedback to heart when they wrote the  Service Worker specification  that provides nuanced control of every network interaction your app or website makes. . When we first dove into this technology, Chrome was the only browser with released support, but we knew universal support was on its way. Now  support is ubiquitous across all major browsers . . When you first boot the new version of Slack we fetch a full set of assets (HTML, JavaScript, CSS, fonts, and sounds) and place them in the Service Worker’s cache. We also take a copy of your in-memory Redux store and push it to IndexedDB. When you next boot we detect the existence of these caches; if they’re present we’ll use them to boot the app. If you’re online we’ll fetch fresh data post-boot. If not, you’re still left with a usable client. . To distinguish between these two paths we’ve given them names: warm and cold boots. A cold boot is most typically a user’s first ever boot with no cached assets and no persisted data. A warm boot has all we need to boot Slack on a user’s local computer. Note that most binary assets (images, PDFs, videos, etc.) are handled by the browser’s cache (and controlled by normal cache headers). They don’t need explicit handling by the Service Worker to load offline. . There are three events a Service Worker can handle:  install ,  fetch , and  activate . We’ll dig into how we handle each but first we’ve got to download and register the Service Worker itself. The lifecycle depends on the way browsers handle updates to the Service Worker file. From  MDN’s API docs : . Installation is attempted when the downloaded file is found to be new — either different to an existing Service Worker (byte-wisecompared), or the first Service Worker encountered for this page/site. . Every time we update a relevant JavaScript, CSS, or HTML file it runs through a custom webpack plugin that produces a manifest of those files with unique hashes ( here’s a truncated example ). This gets embedded into the Service Worker, triggering an update on the next boot even though the implementation itself hasn’t changed. . Whenever the Service Worker is updated, we receive an install event. In response, we loop through the files in the embedded manifest, fetching each and putting them in a shared cache bucket. Files are stored using the new  Cache API , another part of the Service Worker spec. It stores Response objects keyed by Request objects: delightfully straight-forward and in perfect harmony with the way Service Worker events receive requests and return responses. . We key our cache buckets by deploy time. The timestamp is embedded in our HTML so it can be passed to every asset request as part of the filename. Caching the assets from each deploy separately is important to prevent mismatches. With this setup we can ensure our initially fetched HTML file will only ever fetch compatible assets whether they be from the cache or network. . Once registered, our Service Worker is setup to handle  every  network request from the same origin. You don’t have a choice about whether or not you want the Service Worker to handle the request but you do have total control over what to do with the request. . First we inspect the request. If it’s in the manifest and present in the cache we return the response we have cached. If not, we return a fetch call for the same request, passing the request through to the network as though the Service Worker was never involved at all. Here’s a simplified version of our fetch handler: . In the actual implementation there’s much more Slack-specific logic but at its core the fetch handler is that simple. . The activate event triggers after a new or updated Service Worker has been successfully installed. We use it to look back at our cached assets and invalidate any cache buckets more than 7 days old. This is generally good housekeeping but it also prevents clients booting with assets too far out of date. . You might have noticed that our implementation means anyone booting the Slack client after the very first time will be receiving assets that were fetched last time the Service Worker was registered, rather than the latest deployed assets at the time of boot. Our initial implementation attempted to update the Service Worker after every boot. However, a typical Slack customer may boot just once each morning and could find themselves perpetually a full day’s worth of releases behind (we release new code multiple times a day). . Unlike a typical website that you visit and quickly move on from, Slack remains open for many hours on a person’s computer as they go about their work. This gives our code a long shelf life, and requires some different approaches to keep it up to date. . We still want users on the latest possible version so they receive the most up-to-date bug fixes, performance improvements, and feature roll outs. Soon after we released the new client, we added registration on a jittered interval to bring the gap down. If there’s been a deploy since the last update, we’ll fetch fresh assets ready for the next boot. If not, the registration will do nothing. After making this change, the average age of assets that a client booted with was reduced by half. . Feature flags are conditions in our codebase that let us merge incomplete work before it’s ready for public release. It reduces risk by allowing features to be tested freely alongside the rest of our application, long before they’re finished. . A common workflow at Slack is to release new features alongside corresponding changes in our APIs. Before Service Workers were introduced, we had a guarantee the two would be in sync, but with our one-version-behind cache assets our client was now more likely to be out of sync with the backend. To combat this, we cache not only  assets  but  some API responses  too. . The power of Service Workers handling every network request made the solution simple. With each Service Worker update we also make API requests and cache the responses in the same bucket as the assets. This ties our features and experiments to the right assets — potentially out-of-date but sure to be in sync. . This is the tip of the iceberg for what’s possible with Service Workers. A problem that would have been impossible to solve with AppCache or required a complex full stack solution is simple and natural using Service Workers and the Cache API. . The Service Worker enables faster boots by storing Slack’s assets locally, ready to be used by the next boot. Our biggest source of latency and variability, the network, is taken out of the equation. And if you can take the network out of the equation, you’ve got yourself a head start on adding offline support too. Right now, our support for offline functionality is straightforward — you can boot, read messages from conversations you’ve previously read and set your unread marker to be synced once you’re back online — but the stage is set for us to build more advanced offline functionality in the future. . After many months of development, experimentation, and optimization, we’ve learned a lot about how Service Workers operate in practice. And the technology has proven itself at scale: less than a month into our public release, we’re successfully serving tens of millions requests per day through millions of installed Service Workers. This has translated into a ~50% reduction in boot time over the legacy client and a ~25% improvement in start time for warm over cold boots. . If you like optimizing the performance of a complex and highly-used web app,  come work with us ! We still have a lot of work left to do. .  Huge thanks to Rowan Oulton and Anuj Nair who helped implement our Service Worker and put together this blog post.  ", "date": "2019-08-29"}, {"website": "Slack", "title": "Interning on Slack’s Product Security Team", "author": ["Ryan Slama", "Matt Dzwonczyk"], "link": "https://slack.engineering/interning-on-slacks-product-security-team/", "abstract": " I’m  Matt  and I’m a senior Computer Science student at North Carolina State University in Raleigh, NC (Go Pack!). In my free time I enjoy traveling, hanging out with friends, discovering new music on Spotify, running, and hiking. I’m also a self-proclaimed sushi enthusiast. . Coming to Slack, I was excited to be working at a company whose product I used so much. From when I first walked into the building, everyone stood out to me as friendly, driven, and excited to be working here. The first week mostly consisted of on-boarding and meeting my team and other interns. I found on-boarding to be well-organized and informative. We had a dedicated Engineering on-boarding that showed us how Slack deploys code and an interactive session about Slack’s human-first, feedback-driven culture. . Besides the people, one of my favorite parts of working at Slack is that, internally, we’re Slack power users. Before coming here, I used Slack for communication, but it’s capable of much more: code review workflows, interaction with tickets, and notifications for visitors and deliveries are just some of the ways we’ve integrated other tools into Slack, as the centralized hub. . Another part that I loved was the creative freedom. When I arrived, my team gave me a problem, and I was able to architect and develop my own solution to reach the desired impact. I had the freedom to create my own solution, but the support of an experienced manager and mentors providing feedback along the way. . I’m  Ryan Slama  and I’m a senior at Cornell University. I enjoy small concerts, hot beverages, and home-brewing beer. I’m always up for a modern board game or interesting discussion. Returning to Slack was a bit of a shock because the company had grown by around 40% in the nine months I was gone. All but one person on my team had joined Slack since I had been there last. Initially, I was worried because the excellent people I worked with last year were a large factor in my decision to return, but those worries proved unfounded; I found myself surrounded by a supportive, kind, and knowledgeable team. As a junior engineer, finding strong mentorship is pivotal. I’ve found that I can reach out to almost any engineer and set up a time to chat. This alone has helped me grow by exposing me to new perspectives on engineering and life in general. . Additionally, using Slack at Slack has also been transformative. I can jump in to any team’s channel and see what they are working on, as well as future plans. Watching other engineers solve problems helps me learn faster and gain experience more quickly than I have at other companies. For example, reading the history of #pwl-slack, a channel inspired by the external Papers We Love movement, helped me learn about cutting-edge papers in computer science. Through this channel, I was exposed to ideas above and beyond what I’ve done in school or as an intern project. I had the opportunity to attend the small group discussion on the Zanzibar paper alongside top-notch engineers, which proved to be a great learning opportunity. Slack’s transparent culture has empowered me to learn faster than I could otherwise. . We joined the Product Security Foundations team, which decreases Slack’s risk surface by developing new libraries, tools, frameworks, and services. It establishes a comprehensive and innovative approach to security while enabling safer development of new features and services. . When we got to Slack, we were given a problem: Slack wanted to do a better job auditing our use of external dependencies for security vulnerabilities. This problem is one of the  OWASP top ten most pressing security risks , listed as “Using Components With Known Vulnerabilities.” With Javascript and npm in particular, requiring one package often means adding an entire tree of packages to your project. Our main repository alone contains over 3,000 dependencies in Javascript and Hack. Slack is no exception here; this is normal for modern frontend apps and we have many in our largest repo, which contains the code for our web client and our API. . We started our project by building towards an MVP: detecting vulnerabilities in Javascript/npm packages with daily scans. We wanted to detect any newly discovered vulnerabilities, track their status towards remediation, and notify owners. We chose a tech stack using modern technologies and best practices like React hooks and Typescript. When we reached MVP, we could collect real usage data and feedback to better understand how to provide value to our developers. . The first thing we learned was that while we were providing visibility, it was hard to act on the information discovered. We require packages in a number of files, so knowing where to start looking was difficult. As we added support for Hack and Go, the app displayed vulnerabilities from multiple package managers in the same list. We started to track files to help identify ownership in large codebases and make remediation possible. . Our next discovery was that the majority of vulnerabilities were not in our direct dependencies, but rather somewhere deep in the dependency tree. Fixing vulnerabilities required manually reading npm lock files and following long trails of package entries. To this end, we created a force graph that shows dependency relationships so that we can easily identify which packages we require directly. Here, the package with the vulnerability, is-url, is shown on the far right. We map dependencies to find which packages we actually require, then show those in bold. This empowers developers to fix findings with minimal effort and wasted time. . We presented our work to the Product Security organization every other week during playback meetings. Demoing regularly allowed us to get feedback and iterate faster. We later presented to the entire security team and hosted an open Lunch &amp; Learn to get buy in from the rest of engineering. With our dependency detection tool, we’re able to catch vulnerabilities throughout our code quickly, making the Slack product more secure. We’re also able to provide visibility of open source usage risks to the Product Security team and service owners. . Looking back on our project, we identified a few things that we wish we had done better. Our initial data model wasn’t scalable when we started adding new features like modeling package relationships. We could have avoided replicating work by modeling logical objects and not using the simplest sufficient model to reach MVP quickly. Additionally, notifications would make more sense if they could be tied to specific files rather than repositories; this would help in cases where different teams are responsible for different parts of a codebase. Finally, the signal to noise ratio is low, and many findings that we record as vulnerabilities are not actually dangerous to production code. For example, some packages are just used for testing before deployment. We only pass known inputs into this code and never run the code with customer data. This means those findings do not pose a real risk to Slack’s security, but migrating away from some of those packages may be a significant engineering effort. Filtering signal from noise and finding effective remediation strategies remain for future work. . We wanted to give a special thanks to our manager, Suzanna Khatchatrian, and our two mentors Fikrie Yunaz and Oliver Grubin for supporting us during our internship and making summer at Slack great. .  Can you help Slack solve tough problems and join our growing team?    Check out all our engineering jobs and apply today   .  ", "date": "2019-09-04"}, {"website": "Slack", "title": "Re-architecting Slack’s Workspace Preferences: How to Move to an EAV Model to Support Scalability", "author": ["Alisha Ukani"], "link": "https://slack.engineering/re-architecting-slacks-workspace-preferences-how-to-move-to-an-eav-model-to-support-scalability/", "abstract": " Scaling is hard. Design decisions that initially seemed reasonable break down with little warning, and suddenly even the simplest parts of your data model need to go through a complex re-architecture. . We’re tackling this problem at Slack. A lot of our early design decisions made sense for small workspaces, but can be inefficient for large  Enterprise Grid workspaces , where we can have thousands of users in one organization. For example, we have a MySQL table for workspaces, which has information like workspace names and URLs. When Slack was growing, it seemed perfectly reasonable to store workspace preferences (like default language and channels, and permissions on who can create or archive channels) as a JSON blob in this workspaces table. . But Slack has grown, and workspace preferences have grown with it. We now offer 165 preferences that you can use to customize your workspace. These are utilized by more than 150 Enterprise Grid organizations, and 70,000+ paid workspaces. And with all these new users and large workspaces, the workspace pref JSON blob can become larger than 55 kB . The biggest problem with having such a large JSON blob is that it’s stored in the workspaces table, which we access  a lot . Often times, our code fetches one preference at a time — for example, we’ll check if a workspace’s analytics page is only accessible to admins before loading the page for a non-admin user. But that means querying the entire workspaces table, which we’re already accessing for other critical information about workspaces. Sending too many queries to this table could overwhelm it, reducing reliability for all of those other places in our code where we need to use it. . Caching helps reduce queries to the database, but the sizes of our workspace pref blobs hurt the current caching scheme. Our code caches all of a workspace’s information in the workspaces table in one object. So accessing one workspace pref from the cache means getting tons of extra information, which is a large unnecessary load on our Memcached hosts. And changing exactly one workspace preference invalidates that entire cached object! . I’m a rising junior at Harvard studying Computer Science, and a software engineering intern on Slack’s Enterprise Architecture team this summer. My team is responsible for restructuring parts of our data model to make Slack more scalable. For my summer internship, I contributed to my team’s efforts by re-architecting workspace prefs. . So how did I re-architect workspace prefs? The model I created (which might help for your own re-architecture efforts), can be split into three buckets: . There are many possible data models for storing workspace prefs. We could store a workspace ID with a JSON blob of all of those workspace’s preferences (but as previously discussed, lumping all preferences together made us retrieve a lot of unnecessary information when accessing a single preference). Another strategy would be to create a database table with a column for each preference, so each workspace would have a single row in the table. But as we continue to add more customization for workspaces, this would mean adding new columns to the table — an expensive procedure. . To split up workspace preferences  and  give us flexibility to add new preferences in the future, I chose to use an Entity/Attribute/Value (EAV) table. The table has three columns: the workspace ID, the name of the preference, and a JSON blob of the value for that preference. Each pref is stored as a new row in the table, meaning we don’t get any extra information when fetching only one pref. It also lets us add new preferences without needing to change the structure of the table. . Once the table’s schema was chosen, it had to be created in our  Vitess  cluster. Vitess is a tool that helps reduces the load on MySQL databases by creating multiple databases and routing each request to a specific database, like a hash table. Slack uses Vitess to ensure we can provide reliable access to our database as we continue scaling (check out  a talk  about using Vitess at Slack from one of our senior staff engineers, Mike Demmer, who also helped me by creating the new table in Vitess!). . Once the new table was created in Vitess, it was time to fill it with data. This process was complicated because of the new format of the data; while Slack’s infrastructure team has great support for migrating data from one table to another, I couldn’t use these tools since I was splitting up one row in the workspaces table into many rows in the new workspace pref table. So, I had to create functions that could split up the preferences into a list of key/value pairs, and turn each of those pairs into a row that could be inserted into the new table. . With the ability to transform the data into something writable, it was time to start writing the data. I started with doing “double writes,” meaning any time a workspace updated their preferences and updated data in the existing workspaces table, that data was written to the new table. I also created a backfill script that went through every single workspace in our database and inserted their preferences into the new table. These two processes meant that in a matter of days, the new table contained a complete and up-to-date view of a workspace’s preferences. . After the table was filled with correct data, I started a gradual transition to read workspace prefs only from the new table. I started with “dark mode” reads, where any time we want to access workspace prefs, we fetch them from both the existing and the new table. The prefs are compared to catch any inconsistencies, and then the existing data is returned. This helps ensure the data is accurate before actually switching to the new table. I also added some logging to track how long it took to fetch prefs from the new table, which ensured that this change would not impose any performance penalties. . This dark mode helped uncover bugs that weren’t even caused by the re-architecture work. For example, when creating new workspaces, we called a function to set some initial preferences for that workspace. But this function took the workspace by value, and not by reference. So we would write these initial prefs to the workspaces table, but once that function returned, we didn’t update the workspace object in our code to have those new prefs. My new table was inconsistent because it actually persisted those initial prefs, whereas the existing code would overwrite the prefs. This bug also showed the importance of the re-architecture work; if you store workspace preferences separate from the rest of the workspace object, you don’t need to worry about having to pass a workspace by reference or updating the object after a function returns. . Once I fixed that bug, and created and ran a backfill script to make the new table consistent with the existing one, it was time for “light mode.” In light mode, we get the data from the new table and return it instead of ignoring it. I rolled this out slowly, using feature flags to turn on light mode for a few test workspaces, then to our internal Slack, and then finally for all of our users. . At this point, our code was reading from and writing to the new table, but the work wasn’t done. There was still the problem of fetching unnecessary information; to read a single pref, our code often calls a function that fetches an array of  all  of a workspace’s prefs, and then access the one that’s needed. To fix this problem, I wrote a script that generated getter functions for each individual pref. Not only would this allow developers to get only the information they need, it would also enforce data types for each of the preferences, since each of the functions would have a return type (something we’re trying to do to all of our code as part of our conversion to the Hack language). . To create these getter functions, the script iterates through a list of all the existing workspace prefs. This list specifies default values, data types, and permissions about who can change these prefs. The script grabbed the name of each pref, as well as the data type to create the return value. But some of the prefs didn’t have types specified! I specified the types of 12 prefs, and even found 14 prefs that were deprecated; they didn’t affect any behavior for the users, but were still floating around in the codebase and needed to be removed. This work helped create a cleaner codebase and will make it easier for other Slack engineers to understand what these prefs should store and how they should be accessed. We value craftsmanship at Slack, and believe that these small efforts can have a big impact in improving code quality and helping other engineers. . Finally, I needed to change our caching structure to reflect the EAV format. I switched from caching all of the prefs together to caching each one individually. Then, when prefs are updated, only the new prefs are invalidated in the cache, instead of the entire workspace object. . With the new table, we’ve substantially reduced workspace cache invalidation, and enabled us to continue adding prefs. The getter functions have also allowed us to better validate workspace prefs and ensure type-safety in our code. . As a company grows, re-architecture work becomes inevitable. At Slack, we’ve seen this as we’ve grown to support more than 8 million daily active users and 70,000 paid teams. Seemingly sensible design decisions break down at scale; when we offer 160+ workspace preferences, the JSON blobs will grow and impact other work we do with workspaces. Slack’s workspace pref re-architecture can provide a model for how to change data models without affecting how people use Slack. . At a dinner with other Bay Area interns this summer, someone said “internships are great but you won’t learn that much; it’s not like you get to come in and re-architect the codebase.” Luckily, Slack let me do exactly that. I learned about sharded database management, data migration, and code generation — all while making our codebase more scalable and easier to navigate for future engineers. . Thank you to my manager Eric Vierhaus and my mentor Johan Oskarsson for their constant support and guidance during my internship! .  \t\t\t\tWant to work on hard technical problems this summer? Check out internships at Slack!\t\t\t\t Apply now  \t\t\t ", "date": "2018-08-09"}, {"website": "Slack", "title": "Highlights from Slack’s August Mobile Meetup", "author": ["Derek Hollis"], "link": "https://slack.engineering/highlights-from-slacks-august-mobile-meetup/", "abstract": " Slack believes in making our users’ working lives simpler, more pleasant, and more productive. This is true whether they are at their desk or on the go, and as a result our mobile experience must be fast, easy to use, and accessible. . On August 7th, we hosted a Mobile Meetup at our new San Francisco headquarters to showcase the things our team has been doing as we work towards these goals. We presented these topics: . If you are interested in attending future events, please sign up  here .  We will not use this information for any other purposes than sending information on future Slack Engineering events.  ", "date": "2018-09-12"}, {"website": "Slack", "title": "How to Fail at Accessibility", "author": ["Trish Ang"], "link": "https://slack.engineering/how-to-fail-at-accessibility/", "abstract": " Hi everyone! . My name is Trish, a.k.a.   @feesh   on the internet, and my pronouns are she/her. Presently, I’m a Front End Engineer at Slack, where I work on our  internal design system and component library , called  Slack Kit . More specifically, I work with fellow engineers to build React components that comprise the foundational layer of our Slack client, and help to ensure this base layer of components are fully accessible. . I’ve been working in the industry as a front end engineer for a bit over a decade at this point, and if there is one thing I am familiar with, it’s   failure  . But, I’m here to tell you, that’s okay! Failure is normal: everyone fails sometimes, and then we learn, and then probably fail again in an equally spectacular way and learn again.  It’s a cycle we all go through, and it’s how we grow as humans.  . If you’re also a front end engineer, I’m sure you’re equally familiar with this feeling of failure. Browser rendering inconsistencies often feel like you’re in the wild west, web compilers behave like a black box of nightmares, and then there’s that dreaded finishing step of any web project:   accessibility  . . Allow me to share a personal story of failure. A little over a year ago I was preparing to launch my first big feature in Slack. I felt like I was on top of the world, this new tool I’d spent months on would roll out to all the folks who use Slack to help make their working lives simpler, more pleasant, and productive. And then, I got a message like this: . It was really hard for me to not feel like a failure at this point. But, I’m proud to say that  at Slack, we treat accessibility bugs as   broken functionality, and product blockers.  If your website is inaccessible, that means that real humans on the internet cannot use it. And for Slack, where we have over 10 million users daily, being unable to communicate with your teammates, interact with content, or being blocked from doing your work is simply unacceptable. . I’ll be honest,  I’m not an accessibility expert.  Luckily, this message showed up after an  internal launch , flagged by our incredible accessibility team, and was just letting me know that I had a lot more to learn and a little more work to do. Since this time, I’ve learned a ton about the complexities of screen readers, building for inclusivity, and creating accessible experiences. I’m not an accessibility expert,  but as it turns out,  you don’t need to be to do this work   .  . So, let’s flip the script, and dive into —  “How  not  to fail at accessibility”  . We’re going to cover three main areas: . What does accessibility on the web even mean? It is very broadly scoped, but thankfully also thoroughly documented. The W3C established what is known as the  Web Content Accessibility Guidelines , or WCAG. If you check out  this lovely doc , you’ll find everything you’ve ever wanted to know about web accessibility in a lengthy format. . These guidelines break down accessibility compliance to three grades: Levels A, AA, and AAA, with specific criteria for each on what is or isn’t WCAG compliant. Each level defines stricter adherence to supported accessible functionality, with Level A being the lowest level of compliance, and Level AAA being the most strict. At Slack, we aim to meet Level AA for all of our features, and for Level AAA compliance wherever possible. . The spectrum of human abilities and the technology to support them is infinitely broad, and there are complexities that even these guidelines don’t properly address. For now, we’ll get you started on four areas you can test now — the  Hello, World  of accessibility, if you will — and I’ll let you dig into more nuanced support later. These four areas are:  color contrast, user interface zoom support, keyboard interaction models, and screenreader support.  . When looking at contrast, we’re not evaluating  every last color  across your beautiful website: instead we’re interested in making sure the information and interactive elements on your website are  perceivable and understandable.  That means, specifically, the body copy, large text (meaning anything bold 14pt or 18pt non-bold and up), and UI elements on your website, such as iconography and form elements. . The best tool for the job here is the   Colour Contrast Analyzer  , which lets you use an eyedropper or hex code to check foreground and background colors, and will immediately tell you whether or not you meet the criteria at the bottom. It should come as no surprise that light gray text on a white background does not meet the contrast requirement. . UI Zoom refers to the browser’s built-in zoom functionality that is toggled using CMD or CTRL +/ -. At Slack, our designers provide mock-ups at the different zoom levels we support whenever relevant. The easiest way to go about testing this is to simply zoom in a few levels and spend a day or two looking for broken areas in your interface. . Also known as the  Keyboard Interaction Model . This refers to navigating your web content using only a keyboard, most commonly using the Tab and Shift + Tab keys to move forward and backward, and using  Arrow keys , Enter, and Esc to interact with UI elements. . Every year in May, our accessibility team hosts a week-long challenge for   Global Accessibility Awareness Day   where they encourage us to put our mice away in our desk drawers, and spend the week using Slack only by keyboard. I remember trying this last year and finding both how many ways I was able to be more efficient, and also ways where I struggled because I didn’t know what the different common patterns are. . Within the Slack client, we provide this handy palette of key commands to help folks who may have mobility limitations, and also to enable people to become power users of Slack. . In addition to keyboard-support, accessibility guidelines require that you indicate what has current focus, visualized here by a fuzzy blue ring. . An important thing to look out for while you’re testing out your interface is if you notice that you’re able to tab into a section, but then are unable to tab away from it. These are known as keyboard traps and should be avoided. . If you’ve fixed up your keyboard interaction model, you’re already halfway there! Screenreaders are a type of  assistive technology  that audibly dictate what a user is currently interfacing with, particularly useful for folks with low or no vision. . If you have a Mac, screenreader software is built in with   VoiceOver  . To turn this on, open up Systems Preferences, navigate to the Accessibility tab, and enable VoiceOver. There are equivalent free software options for PCs as well, such as   NVDA  . . Here’s a quick preview for what it’s like using a screenreader: you’ll notice as the keyboard focuses on an element, the screenreader will first announce what type of element it is, how it’s defined, and then will update as you’re interacting with it. Can you guess what interface this is just by listening? . Additionally, screen readers use what’s known as a  rotor  for quickly navigating around sections on a webpage. This is one of the reasons to make sure you’re using well-defined and semantic HTML markup to begin with, as that’s how the rotor indexes your content. You can pull the rotor up using Control + Option + U, and move left or right to access the different menus available. . Now that you’ve identified a few areas you want to clean up, how do you improve your code to provide accessible experiences? Once you start  building , there are three broad-level things to consider. For screenreaders, you’ll need to provide proper HTML and ARIA markup. You’ll get very, very comfortable with programmatic focus management. And you can leverage modern coding practices like variable usage and managing state to help as well. . Let’s step back to that earlier call out from George. The feature I worked on involved a fairly common interface: an input box with an associated list, that would be filtered with whatever is typed into the box. This is known as a  combobox . . Taking a look at the before and after results, can you determine what’s different here? You still have a box, and a list below it. The issue here was that I approached it exactly as it sounds: I used a Text Input, and then below it, I used a List Menu component. To me, it was clear that even though there were two components involved, they were related: they both had a similar style and typography, they were united closely in composition, and were contained by a box with a white background. Simple, right? . The problem here was in that initial assumption. Even though to me they were visually distinct as related elements, it turns out that screenreaders do not have the benefit of those visual clues. Instead, to a screenreader, they were two distinctly different components. . In order to fix this, I needed to switch to an entirely different underlying architecture: instead of two components, we now have one component, a Select, which internally has measures to connect the input and it’s associated list of options. Much like your keyboard, screenreaders can only maintain focus on one component at a time. This way, the screenreader remains focused on the container Select, and both internal components update their parent while someone navigates the list or types in the box. . But, how does that work, you might ask? . Back to that handy W3C document. They’ve defined an  API for indicating to assistive technology what is happening on screen , known as ARIA, or  Accessible Rich Internet Applications . What’s going on behind the scenes is that this accessibility API parses the DOM on your website, communicates that to the screenreader or whichever assistive technology, which then outputs the information to a user. . Just in the same way that browsers are inconsistent with how they may render CSS, it’s the same with how each browser interprets ARIA labels. This is no problem, just continue thoroughly testing your web content across multiple browsers and platforms as I’m sure you all already do. . We’ve learned to build out an ARIA pattern with all of our tech specs, so that we know before we start coding anything that our component should be accessible from the beginning. This is not always easy though, since many of our components need to support many different use cases, sometimes building that out looks a little more like this. Just be sure to workshop it with your peers to make sure you’re covering all your bases. . Once you finally get to the code level, things start looking a little more like this, where you may have a feature spread out across a few components, and can use Javascript utilities to build and connect the IDs that are informing your ARIA attributes. For a more concrete example, here’s the finished combobox where it reads the proper content. . Building an  internal design system at Slack  has helped us formalize additional best practices around a  consistent code base . For example, last year we undertook the massive task of auditing all of our existing colors throughout the code base, simplifying, ensuring they were all compatible, and then setting up linters to prevent any additional one-off colors. This is just one of the steps we’ve done to help prepare for dark mode, and to ensure that it’s still fully accessible. . How do you improve your process to prevent bugs from creeping in in the first place? Let’s step back to what things looked like over a year ago at Slack. . A bit after I joined, we had around ~75 front end engineers. But amongst them, we only had one accessibility expert. As I mentioned before, I am not an accessibility expert: we have the famed Todd Kloots, who is incredibly talented and amazing to work with. However, as skilled as he is, it became very clear that with all the accessible features we wanted to build out and support, one front end alone was not enough to do it. . So over the course of the past year, we’ve rolled out training similar to this to ensure that all front end engineers at Slack understand what accessibility criteria are, how to test for it, and how to build these best practices into their feature development. More importantly, this means that we’re equipped to test each others’ features through code reviews. . We’ve even gone a step further and now ensure that even at the designer level, the entire team knows what accessibility is and how to design for it. Our designers now provide articulated keyboard interaction models when necessary. We even enlist our product writing and internationalization teams where possible, to ensure the content being output to screen readers is clear and comprehendible to users no matter where they are. . Through the development phase, having our front end team utilize a centralized design system means the components they use are accessible by default. At this point, we have Slack Kit components used in 75% of applicable areas of our codebase. And where folks are developing those components from scratch, we require them to write out a tech spec including accessibility considerations, and we check in regularly with our accessibility expert, Todd, throughout the build process. The single most useful thing I’ve learned about accessibility is to consider it as early on in your process as possible: as someone once said, it’s easier to add the blueberries in before you’ve baked the muffin. . Finally, as we’re preparing for launch, we’ve got a dedicated QA team to thoroughly test for accessibility, in addition to a third party service, Ultra, that conducts an audit as well. . Even if your company doesn’t have the benefit of a component library, design system, or QA team, hopefully this  Starter Kit  enough to get you started to become your own resident expert, and to keep learning more about what is possible. . Awesome, nice job everyone! You’ve learned a ton and are ready to apply all these best practices to your own web content. . I want you to stop for a minute and think about everything you did while browsing around the internet today. Let’s say you woke up and checked a weather website to decide how you should dress for the day. Then maybe you checked out Twitter to catch up on what’s happening. You might have seen an interesting new Javascript meetup happening soon, so you clicked on over to their website, read the description, and then bought a ticket. Then perhaps you needed to look up a great restaurant nearby to celebrate a friend’s birthday after work. . A recent study in the UK found that around 70% of all websites did not meet accessibility standards. Can you imagine if, as you were perusing the internet, looking up information and filling out forms, 70% of the time you hit a Javascript error and were unable to continue? I don’t know about y’all but I would feel incredibly frustrated and would probably hate using the internet. . I’ve always found it interesting that other engineering professions undergo strict certifications and testing, for example civil engineers need to be certified to ensure the bridges and buildings they help build are safe for humans to use. While I don’t want that type of gatekeeping behavior in our industry, I  do  want us to hold ourselves and each other more accountable to being responsible with our code:  we should be making the internet safe for all humans to use, too.  . I hope that this is something you all remember as you’re doing your day-to-day work, and especially the next time your manager or PM is breathing down your throat about a deadline. Your peers are trusting your expertise in all-things front end, and that includes knowing that you  need  to make your products accessible, because even though it takes a little more time up front, it will ultimately save you infinitely in the long run. . It’s become more and more apparent that there are  no average users , this is a myth. Instead, we should design for the extremes of human ability. Much like everything, prioritizing inclusivity and designing for the most marginalized groups actually makes a better experience for everyone. . There are countless examples of how technologies or products developed originally for folks with disabilities are helpful for everyone: if you think about the last time you used closed captions while watching a video in a noisy cafe, or even if you enjoy text messages or using Siri, all of these have their roots in assistive technology. .  So I hope you’ll all take this as a sign that you  can  build for accessibility , and if you get stuck along the way, want to chat about your favorite accessibility bug, or are interested in coming to work on our internal component library at Slack, check out our  available positions  or hit me up @feesh on the internet! Good luck everybody! ", "date": "2019-02-22"}, {"website": "Slack", "title": "Defining a career path for Developer Relations", "author": ["Bear Douglas"], "link": "https://slack.engineering/defining-a-career-path-for-developer-relations/", "abstract": " Developer Relations (DevRel) is an interdisciplinary role that sits in a border space between product, engineering, and marketing. The daily work looks  very different from company to company , and because DevRel is often rolled into other organizations like product or marketing, it may not have its own separate career path – a document that describes expectations for your work at varying levels of seniority. . In recent months, more and more DevRel managers and ICs (individual contributors, meaning you don’t manage people) have been talking about how to write job descriptions and define career paths for Developer Relations. Today I want to help move the conversation forward in a more transparent way by sharing the career path I use for Developer Relations at Slack. . Developer Relations at Slack is currently a 12-person-strong team, and at this size, we’re generalists by preference and by necessity. Everyone writes docs, everyone builds tools and sample code, everyone gathers and shares developer feedback, and everyone works on events. We also come from different backgrounds: people on our team have joined us from core engineering, from developer support, from non-engineering operations roles, and some have been doing DevRel their entire careers. For our whole team, we have a single path. . Even for us, this path is imprecise, designed to be a set of guidelines rather than a series of job descriptions. Not everyone’s role will be neatly outlined by the content of the path, and, in my opinion, that’s not the point. . A good career path is general enough that it doesn’t box you in to a set of predefined activities. It’s also specific enough that you’re being held to a fair and clear standard of performance, while being shown a way forward to growth. . In the past, I’ve found paths that described day-to-day work gave people an overly-prescriptive map of what to do to get promoted rather than a holistic understanding of how the organization defines seniority. It’s the latter that helps people create their own route to growth and promotion, which ultimately serves them and the company better. . In one of the first career paths I wrote, I described activities for a mid-level DevRel that included “running community events, like meetups.” As the team grew, the person who had been running our meetups asked me whether they would “lose the checkbox” toward promotion if they handed that responsibility over to someone else. To me, it was obvious that I wouldn’t care if this person  gave away their Legos – that was the healthy and correct thing to do in a growing team. To them, it was not so clear, because I had wrongly framed the conversation around  activities  rather than  behaviors . . To me, it was obvious that I wouldn’t care if this person gave away their Legos– that was the healthy and correct thing to do in a growing team. To them, it was not so clear, because I had wrongly framed the conversation around  activities  rather than  behaviors . . By contrast, I discussed Slack’s current DevRel path with one of our summer interns who wanted to understand what a full-time role and long-term career on our team might look like. After reading our path, our intern was able to accurately infer the levels of the people on our team he worked with most closely. . I realize that might make some people uncomfortable– shouldn’t that be private information, or something only discussed between you and your manager? But this information isn’t exactly private. Levels are encoded in job titles, and expressed in daily behaviors. If someone you work with casually can figure out what level you’re at, it shouldn’t feel risky, like you’ve been exposed– it’s a sign that your team is on the same page about what levels  are . . OK, OK. You can jump to  reading the full path here , or I’ve excerpted it below for ease of reading. . Here’s what our level description looks like for a new grad: . And here’s what it looks like for someone with a few years’ experience: . We define senior like this: . And staff like this: . You’ll notice that the levels have three components: technical quality, collaboration, and execution (h/t to Ross Harmes, Matt Schweitz, and Lauren Ficklin, who wrote the Slack Engineering path that I adapted the DevRel path from). It’s meant to encompass growth in skills, in scope of work, and systems thinking– contextualizing our daily work in the broader landscape of businesses and technology. . Growing in your DevRel career means honing technical skill; it means being a better internal advocate for your developer customers; it means working to level your team up with you; and it means that you are a trusted communicator on behalf of the company. If you’re a great external communicator but you can’t get anything shipped for developers because you have no credibility with the product team, you aren’t becoming more senior in the organization. The same is true if you’re technically brilliant, but you never mentor anyone around you. . The document describes a path that starts and grows purely in DevRel. But the path in (and out) of DevRel is highly individual, because the role is so interdisciplinary in nature. . You may stay in DevRel for years, or spend some time here as a transition between engineering, marketing, product, sales engineering, business operations, user research, or data science (that’s a run-on on purpose– great DevRellians can come from anywhere). . Perhaps more than in other disciplines with traditional, linear career progressions, it’s important to distinguish between growth within the role and the organization, and growth in the broader context of a person’s career. Many people come to DevRel because they want to hone their technical presentation skills, or they want more exposure to customer developers. People who spend time in DevRel to better understand developer customers as part of a path to product or developer product marketing may care less about growing technically. For others, a role in DevRel may be the first time they’re responsible for production code. . Because people may come in with skill sets at different levels, and because it’s relatively rarer to spend your entire career in DevRel, it’s even more important to treat career paths as the jumping off point for conversations with your manager. Expectations for the role and a plan for growth should be individualized and made concrete in 1:1s. . Ultimately, the point of a career path is to help make sure that both you and your management chain see a path forward for you, as an individual contributor or a manager. It’s a level set, not a checklist, and with any luck, us sharing this detail can help you kick off that transparent conversation. . Does this sound like how you’re interested in growing your career?  We’re hiring . Watch this space, and in the meantime, you can find me on Twitter at @beardigsit. ", "date": "2019-03-14"}, {"website": "Slack", "title": "Engineering dive into Slack Enterprise Key Management", "author": ["Audrei Drummond"], "link": "https://slack.engineering/engineering-dive-into-slack-enterprise-key-management/", "abstract": " Over the years, customers have asked us for stricter control over and visibility into their data in Slack, all while maintaining the product’s most essential features, such as link unfurling, mentions, notifications, and search. We talked with many of these customers to get a better understanding of their threat models, how they’d like to assert that control, and what kind of visibility they were lacking. . That brings us to this most auspicious day:  The release of Slack Enterprise Key Managemen t, or Slack EKM for short. Slack EKM is a tailor-made solution for security-conscious customers. It allows organizations to have greater visibility into their data in Slack and more control over the keys used to encrypt and decrypt their data. . I’d like to talk a bit about how we designed and made Slack EKM an engineering reality. You can read more about EKM at  https://slack.com/ekm . . Our initial research focused on a couple of potential solutions: . These two solutions were on the extreme ends of possibility. End-to-end encryption would have some performance impact and make certain features unavailable; policy controls would allow the full use of Slack but not give customers full control over the encryption keys. . These options didn’t meet our requirements: to maintain usage of Slack’s features and preserve performance and support the customers’ need for control over their encryption keys and visibility into their data. . Instead, we designed a solution for customers to bring their own encryption keys into the Slack service — and that’s what we now know as Slack EKM. . On our initial release, we support Amazon Web Services Key Management Service  (AWS KMS)  for our first third-party integration to store keys. In the future, we may decide to expand Slack EKM to integrate with other key management services or hardware security modules (HSMs) based on customer demand. . Now that we have some background, it’s time to get into the details. What follows is a high-level diagram of Slack’s systems with EKM: . Phew, there’s a lot there! Let’s take it piece by piece. . Before we had Slack EKM, this was the state of the world: . At a high level, when the Webapp received a new message, we would immediately store the message in our databases, end of story. This is pretty straightforward, but even though our databases are encrypted at rest, customers do not have control over those keys. . When it came time to design how EKM would work in Slack, we decided very early on that any EKM-related encryption and decryption would run as a separate service. The following diagram depicts the interaction between our Webapp and our new EKM service: . There were a number of reasons to do this. One big reason was that we could add other security measures to this service, such as  no one  being allowed to log into the boxes running the service. This isolation helps ensure that the encryption keys sourced from AWS KMS are never leaked. . Also, building a new service meant we had the chance to pick a language. At Slack, we use a number of languages, including Hack, Java, Elixir and Go. We decided to use Go because it is already a part of our ecosystem, is well suited for doing CPU-intensive cryptographic operations, and has a great AWS software development kit. (We also had Go experts working on the team.) . Even though our focus was on building Slack EKM to integrate with AWS KMS, we also wanted to ensure that this foundation would scale as we continue to evolve EKM in the future. After introducing an abstraction layer, our interface was not only cleaner but more future-proof too. . I previously mentioned that Slack EKM gives customers control of and visibility into their sensitive data, but I haven’t described  how . To recap, here’s our revised diagram of Slack’s architecture with EKM: . The customer’s key that is stored in AWS KMS is referred to as the Customer Master Key, or CMK. Other key management services may refer to this key as the Master Key or Key Encryption Key. Instead of encrypting every piece of data with that single key, the EKM service requests a  data key to be generated  based on a supplied  scope , which KMS calls the encryption context. That data key is then used to encrypt a file or a small slice of messages. That’s a mouthful. What does it actually mean? . We have five scopes that we can use when encrypting message or file data: Organization, Workspace, Team, Hour and File. If we were encrypting a message, we would use the Organization, Workspace, Channel and Hour scopes, which reflects all the context of a message. The Hour scope is a bit special — it changes every time the hour changes. This means that we have built-in key rotation for our messages, since a new data key will be used every hour. . Another great property of our granular scopes is that they enable customers to  create policies in AWS  to control or limit access to their keys, and thus their data, in a more precise and targeted way. For example, customers can create sophisticated policies that revoke access for a channel for a particular period of time. . Files use just the Organization and File scopes. This means that an admin can set a policy in AWS to revoke access to a particular file. . Once we built support for granular scopes, we needed to make sure all Slack engineers used the right scopes when building new features or storing data. As part of our internal implementation, we created EKM encrypt and decrypt objects that help guide Slack engineers through the encryption and decryption process and creating the right scope. There’s a hefty amount of documentation, too. . After creating the objects and documentation, which helped make building EKM a lot easier, we found some fun problems, such as  What should the scope be for a message if a customer moved channels between workspaces?  . The answer to that one is simple. All we had to do was: . Yup. Easy peasy… . So far, I’ve talked about encrypting new data and rotating keys, but I’d be remiss if I didn’t mention that Slack EKM also supports the rekeying. This means that when a customer decides to rotate their Master Key, Slack will re-encrypt all of the existing messages and files with data keys encrypted by the new Master Key. . You may ask yourself,  Does Slack’s EKM service make a request to AWS KMS for every message and file?  Don’t worry; we don’t . We’ve done extensive performance and load testing on the EKM service and Slack (be on the lookout for a future blog post on that). During our testing, it became very clear that we would need to cache the scoped data keys, so we implemented an in-memory cache with a TTL (time-to-live) of five minutes. . In order to address our customers’ threat model, we keep the data key caches in memory so they will not be stored persistently. Based on our testing, a five-minute cache has enough of a hit rate to minimize any performance impact during normal use, such as sending messages. Caching also helps us make sure we don’t hit the rate limits for AWS KMS. . In addition to empowering customers to control their data, Slack EKM also gives them visibility into when their KMS keys are requested. When customers set up EKM, they have the ability to set up  AWS CloudTrail . CloudTrail provides customers with logs about key requests that are created directly from AWS KMS, thanks to the seamless integration between CloudTrail and KMS. Each log contains detailed information, including the action that was taken, the key scope and the timestamp. Customers can take these logs and ingest them into their security information and event management (SIEM) providers, where they can analyze the logs and use that information to gain insights. . Since CloudTrail logs only actions taken through AWS KMS, we provide additional logs through  AWS CloudWatch  that will log for any activity, including the in-memory data key cache hits. The CloudWatch logs also provide additional information, such as the reason we made the key requests. . Searching over encrypted data is a hard problem, fraught with trade-offs around security and performance. We decided to extend the same model for search as we do for messages: Customers control the key used to encrypt their data. . Without getting into all the details of how  search at Slack  works, an EKM customer’s search index lives on an encrypted filesystem, where the filesystem is encrypted with the customer’s Organization scope. This means each EKM customer’s search index is also built separately from everyone else’s. . When a search occurs, we check to see if we have access to the key needed to decrypt the index. If we have access to the key, we mount the index so users can perform searches. We keep the index mounted for five minutes (the same period we cache the data keys) and verify access with KMS at the end of that time period. . What about the organizations and workspaces not using Slack EKM? While we are excited for all the customers who have Slack EKM, we needed a way to differentiate them from all of our non-EKM workspaces so we could tell if we needed to perform an EKM operation. . As you can imagine, with EKM encrypting all messages and files, where we store the EKM status would be a very popular database table. Enter the EKM status cache: Not just a cache that keeps track of who has EKM enabled, it also negatively caches the workspaces that don’t have EKM. This way, we can quickly tell if we need to perform an EKM operation without stressing a database table that doesn’t change often. . Even with all the changes made to Slack to support EKM, end users won’t experience any disruptions or notice anything different. And administrators have peace of mind knowing that with Slack EKM, they have full oversight over their data and can mitigate potential risks at the most granular level. . And we’re only getting started! Now that we have a framework for EKM within Slack, we have opportunities in the future to encrypt other pieces of data. . With that, you now have a solid introduction to Slack with EKM. We’ve covered a lot of information here, and this is just the tip of the EKM and Slack iceberg. If you’re curious about other problems we’ll solve with EKM, come join us and see what else we’re up to !  https://slack.com/careers  ", "date": "2019-03-18"}, {"website": "Slack", "title": "Highlights from Slack’s March Growth Meetup", "author": ["Shivani Sharma"], "link": "https://slack.engineering/highlights-from-slacks-march-growth-meetup/", "abstract": " The mission of the Slack Growth team is to ensure that every company can successfully adopt and scale Slack. We act on this mission in the following ways: . On March 13th, we hosted our first Growth Meetup at Slack’s San Francisco headquarters to showcase how we think about growth, how we work cross-functionally, and how we maintain our customer-centric focus. We presented these topics: .  1.    8 Lessons for Growth Teams    by    Fareed Mosavat   , Director of Product   Fareed offered cautionary tales on how not to grow your business. How you shouldn’t become reliant on the performance enhancing drugs of Growth, such as excessive push notifications, emails, or promotions. Because once you start taking them, you can never quit. .  2. Experimentation in Monetization by    Kat Vellos   , Senior Product Designer   Kat walked through three experiments we ran at Slack (a full redesign of the plans page, adding plans to the team menu, and redesign of the invite modal). We were surprised, and so was our audience; Kat described how running experiments is an exercise of humility, and how we’re always open to being surprised by our users. .  3.    Streamlining Mobile Sign-in    by    Chase Rutherford-Jenkins   , Staff Backend Engineer &amp;    Sam Wolfand   , Android Engineer   It’s possible to focus on Growth without building quick and hacky solutions. Chase and Sam talked about how we own our core systems end-to-end, how we’ve improved performance by orders of magnitude, and how we’ve improved security for a better login experience for our users on mobile. .  4.    Balancing Big Bets with Experimentation    by    Carla Gonzales   , Staff Product Designer   Carla talked about how we create value for our customers and build a product worth paying for. She discussed how we approach acting on our big bets optimally so that we’re able to measure, learn, and iterate. .  If you are interested in attending future Slack Growth events, you can give us your email address     here     and we’ll let you know about the next one. We will not use this information for any purposes other than sending information on future Growth events.  ", "date": "2019-03-26"}, {"website": "Slack", "title": "Refactoring Backend Engineering Hiring at Slack", "author": ["Chase Rutherford-Jenkins", "Duretti Hirpa", "William Kimeria"], "link": "https://slack.engineering/refactoring-backend-engineering-hiring-at-slack/", "abstract": " For anyone who’s ever been involved in the hiring process, it’s no easy feat — particularly in a growing company. To get hiring practices right, it takes iteration based on feedback — both on the internal processes within your company as well as on the external process a candidate experiences. Continuously improving hiring is important for a host of reasons, and chief among them is the high cost of hiring the wrong person, or missing out on the right one. . At Slack, we put a lot of care in hiring, and Engineering hiring is certainly no exception. When it comes to hiring backend engineers, we’ve always given a take-home exercise, preferring that candidates complete their programming practicum in an environment cozier to them: on their own machine, and in their own time, within reason. Once candidates have passed our phone screen and this take-home exercise, we invite them onsite for two technical interviews and two non-technical interviews. . The take-home exercise and the technical design onsite interview look for a number of meaningful attributes in candidates. We look for candidates who display a high degree of craftsmanship, who are security-minded, and who are concerned with system performance and reliability. For a long time, both assessments were giving us a good signal on these attributes and others. . The take-home exercise had a number of factors in its favor: . From a candidate’s point of view, there were no surprises. There were no “gotcha” questions, and expectations were clear. In some cases, we would ask candidates questions about their solutions when they came onsite — particularly if they took a novel approach in their solution. Investigating the thought process behind these solutions was often illuminating. . With coding well-covered in the take-home exercise, the onsite technical design interview was modeled on how engineers approached and analyzed problems in their daily work. There was no whiteboard coding, though candidates were free to use a whiteboard to sketch out their ideas. Candidates were given a technical problem and asked to design a solution, and both the candidate and interviewer spent time digging into the various aspects of the problem and the solution together. . Slack was growing, and growing quickly. We needed to hire engineers, but we soon realized that our growth was outpacing the rate at which we could fill open roles. Our take-home exercise, while loved by many, was also time-consuming. Its open-ended qualities meant that candidates, wanting to show off their best work, could end up spending many more hours of their own time to complete it than we had expected. This was often a barrier for candidates who couldn’t afford to invest the time needed to complete the exercise to our desired level of quality. . The end result was that, by our estimates, it would have taken a year to fill our existing open headcount, future growth aside. This timeframe clearly would not allow us to grow at the speed we needed. However, we were also unwilling to sacrifice quality. We needed an approach that would give us good signal and help us hire great engineers, but at a reduced time cost to the candidate and to us. . To satisfy these needs, we decided to create two new take-home exercises: an API design exercise and a code review exercise. In creating these exercises, we sought to create a problem that was not an onerous time investment on the part of the candidate. We wanted something that would give us good signal on the attributes we cared about while taking at most two hours to complete. . We started by defining those attributes. Craftsmanship involves code correctness and code style, an attention to detail and design, and an understanding of the importance of testing. We also wanted candidates who could keep an eye out for security concerns and performance issues. On top of this, we also wanted to know that these folks were good teammates — that they were concerned with maintainability and documentation, and that they could express themselves with empathy and a mindset towards collective learning. . Both of the new exercises also preserved what we liked about the existing take-home, such as anonymized grading and clear requirements, while improving on its major weakness — the amount of time required in order to complete the exercise. After running both exercises side-by-side for some time, we measured how well the exercises performed at achieving our goals of substantial signal and reduced time. In the end, we chose to move entirely to the code review exercise as it better fulfilled both of these goals. . While the content of the take-home exercise is incredibly important, we also had to think about the way in which it would be administered and graded. Though we as engineers were heavily involved in the pilot process for the new code review exercise, in order to scale it to all of engineering recruiting, we also needed to ensure that the exercise could be sent to new candidates and completed with minimal intervention needed by an engineer. . We started by evaluating tools that could be used to take the exercise. Our first internal pilot used docs containing markdown code blocks that were shared with candidates, who were asked to leave comments for any code issues they found. This approach had a number of issues: . With that in mind, we decided to move the exercise to GitHub. We created a new organization to hold repositories for each candidate exercise along with a repository to hold the exercise template itself. For each new exercise, we would copy the contents of the template repo and create two commits on the candidate’s repo — one initial commit against the  main  branch, and one in a separate  dev  branch with the content they were meant to review. Then, we would create a Pull Request (PR) from  dev  to  main  so that we could use the code review tools built into GitHub. We’d invite the candidate to the repository and ask them to review the PR. Problem solved! . Well, not quite. The approach was sound, but not scalable. In order to streamline exercise creation and assignment, we started by writing a Python script that would invoke git on the command line in order to perform the desired git operations, and then hit the GitHub API to open the PR and assign access. We also wrote a script that would download the PR content and convert it into a markdown file for grading — this way, graders wouldn’t see the identity of the candidate in GitHub. . The scripts, however, had their own pitfalls. Python and the script dependencies needed to be installed, git needed to be installed and authenticated, and any error in the flow resulted in cryptic errors on the command line. This may have been fine for engineers, but not for recruiters. They needed a robust and easy-to-use way to administer the exercise, and the CLI script approach definitely was not that. . With that in mind, we built a new electron-based app for the recruiters to use:  Slack Engineering Recruiting Tools . We wrote new logic in TypeScript that no longer relied on command-line tools for git operations. Instead, it used the  Git Database API  to manipulate git objects directly in GitHub — creating files, refs, trees, and commits as needed in order to construct the content of the exercise. It continued to use the GitHub API to create the repositories and pull requests and to assign access as needed. We wrote a frontend for this new logic in React, allowing recruiters to easily create exercises, assign access, and download completed exercise content for grading. . With this new app, we finally had a well-performing exercise, the right platform for taking the exercise, and the right tools for administering it efficiently at scale. . Having refactored the take-home exercise into a code review exercise, we needed a concrete way to measure candidate programming ability. In parallel to the work on the take-home exercise, we also reworked the onsite technical assessment into a coding session. . There were a number of factors that were important to us in an onsite coding session. First, we wanted to retain the “no whiteboard coding” precedent, as we did not and still do not believe that whiteboard coding is beneficial in assessing a candidate’s practical technical skillset. . Second, we strived for the problem itself to be realistic. We wanted candidates to implement a basic version of a real feature — for the implemented program to be loosely related to the kinds of work you could expect to do if you were hired. . Lastly, we wanted the experience to be as realistic and close to everyday work as possible. We wanted the candidate to have the comfort of working on their own machine along with access to reference materials such as Google, Stack Overflow, and any other sources a programmer might use in the course of their normal day-to-day work. Interviewers and candidates would work together through the problem — no gotchas, no purely algorithmic assessments. As technologists, we’re deeply pragmatic. While we know that programming is often a solo activity, building software is a team sport. We believe our technical assessment should reflect that. . Finally, with the exercises themselves reworked, we had to train our engineers on the new exercises. We developed grading rubrics for both, and we set up training sessions where we walked through examples. New interviewers and graders shadowed our initial team in order to learn the new content. Along the way, they also provided invaluable feedback that allowed us to continue to improve the exercises. . In the end, we saw tangible improvements against our goals. We saw a decrease in our time-to-hire — the time from when a recruiter first reaches out, to the candidate’s first day in the office. The time-to-hire metric decreased from an average of 200 days to below 83 days — and it continues to drop. We’ve seen positive feedback from candidates and employees in all parts of the process: . I really liked the take home code review exercise. It covers a practical part of the job that normal interview processes don’t usually focus on, and for an interviewee it has clearer expectations (especially around time) than unbounded take-home exercises usually do.  — Candidate . The interview process was very thoughtful. [It] tested my knowledge and skills I had gained during my years of work experience vs. testing textbook knowledge or concepts.  — Candidate . Our original take-home was too burdensome of a time-commitment. Our revamp required less time from our candidates as well as from our internal graders. Moreover, the streamlining of our process helped us to be more competitive with other technology firms. It enables us to better attract more kinds of candidates: both active candidates that are searching for jobs, as well as passive candidates that weren’t previously looking to move to a new company. . Throughout this refactoring process, we tried to consistently balance trade-offs by viewing the interview process holistically. We shortened the amount of time needed for the take-home exercise by reworking a coding exercise into a code review exercise. As a result, we needed to lengthen the onsite technical sessions to accommodate a programming evaluation. We also had to let go of some of the open-ended creative aspects of the previous take-home exercise. In return, the code review exercise gave us a clearer sense of what it would be like to work with candidates on a daily basis. . After all, much of software development is working with each other — sharing our experiences in order to help each other improve, as well as listening and learning in order to improve ourselves. Much like the process of self-improvement we apply to ourselves, we continue to listen, to learn, and to improve the processes we use at Slack. .  Many thanks to the rest of the folks who also worked on this refactoring: Andy King, Bianca Saldana, Jina Yoo,    Maude Lemaire   ,    Ryan Greenberg   ,    Ryan Morris   ,    Saurabh Sahni   ,    Scott Sandler   , Stacy Kerkela, and    Steven Chen   .  .  Interested in joining our team? Check out our  engineering jobs  and apply today!  ", "date": "2019-04-03"}, {"website": "Slack", "title": "How Slack Hires a Red Team (and you can too!)", "author": ["John Sonnenschein"], "link": "https://slack.engineering/how-slack-hires-a-red-team-and-you-can-too/", "abstract": " What is a pentest or a red team? Before we go any further, we should define our terms, though you may find they’re often used interchangeably: . Pentesting or red-teaming serves a number of valuable purposes. It can help establish confidence in your security practices, or highlight areas that need to be shored up. It can provide evidence needed for certifications or customer security questionnaires. It can help build a case for investing in tools and approaches that keep your users’ data safe. . Much of the security industry, for better or worse, focuses on selling services related to adversary simulation, and so much of the marketing and sales around security services does as well. Knowing what you’re hoping to get out of a pentest can help you navigate the marketing hype and find a firm that pushes your application’s security past its limit. . Well before contacting a vendor or responding positively to a sales inquiry, an organization should ask itself: is it even ready for a pentest? There are a number of presentations, blog posts, tweets, and podcast comments about readiness for bug bounties; pentests, like bug bounties, largely serve to illustrate gaps, and to generate  new  work without doing anything to clear the backlog of  existing  bugs. It’s important to realistically assess your security program’s ability to triage and schedule fixes for new bugs that come in, especially since some may be quite serious and require major architectural changes. It is also important to realistically assess the maturity of your organization’s infrastructure security. Are you comfortable with your organization’s inventory management? Your story about vulnerability management? Access controls? Think about the things that keep you up at night regarding your security posture, as those will be the first things that pentesters will find. . Unfavourable answers to the questions of security readiness may in fact be the purpose of seeking a pentest. It’s important to determine what value you intend to derive from a pentest or red team engagement. You, as the red team coordinator, may wish to think about who should be the primary consumer of the delivered report as you decide the scope, target, and other details. There are several reasons to engage offensive services and different forms of value you can derive from each. . You may be seeking approval for security headcount or products from executives or decision makers, but are having a difficult time communicating the scope of the problem or the return on investment. In these cases a pentest report can be a fantastic tool, with graphs full of “CRITICAL vulnerability” in bright red ink and a narrative structure in bullet points showing a simulated attacker starting from the internet and getting access to whatever crown jewels your organization is most concerned with. Here at Slack, our utmost concern is your customer data, but your organization may find quarterly financial reports to be the most important bits of digital information, or journalistic sources, or customer metadata. It depends entirely on the needs of your group. . You may be looking for a pentest report to respond to a customer or regulatory request, since many companies will ask for, if not demand, your most recent pentest as part of their procurement process. A pentest report from a trusted and competent vendor may be the difference between closing a deal and not. Additionally, in a regulated space, you may be required to periodically engage pentesters to maintain certifications. Slack has many certifications, including HIPAA, SOC2 and ISO27001, but it’s important to note that regulatory requirements are a  minimum  rather than a goal, and your pentest alignments should still reflect a desire to do security right (which, if done well, should make audits easy to pass). . You may be confident in your security policies, threat modelling, and mitigations and practices. You may feel like you’re able to fend off all but the most sophisticated attacker. And you might want to test that assumption. That’s great! Pentests, and even more so red team engagements, can provide a level of confidence that your organization proves a difficult target to determined, skilled, professional attackers. Assuming, of course, you’ve selected your vendor’s skill strengths for your environment. A vendor that excels at penetrating Windows environments may be able to stumble through your Mac office with a reasonable degree of success, but isn’t given the opportunity to show their strengths. . So how do you find a vendor, anyway? It is important to point out that organizations don’t innovate. People innovate. Organizations can, at best, support the people that comprise them and seek out people who have the skills and interests to further their goals. And red teams are made of people with motivations and feelings and personal lives outside their job. Some of the best hackers in the world may work for boutique, expensive firms tailored to your environment, with a small staff and a selective customer base, because it regularly engages them with exciting work But some of the best may also enjoy the stability (both in workload and pay) and the flexibility that a bigger company affords them. . The other thing to consider when choosing a vendor is your own environment. Are you primarily a top-down Windows-on-AD based shop? Or do you issue MacBooks to everyone? Is your production infrastructure a cheffed AWS machine, or have you gone all-in on Kubernetes? You’ll want to ensure that your simulated intruders are skilled with your particular infrastructure. The less time they spend ramping up, the more value you’ll get out of their time. . Another great strategy is to find individuals doing talks and writing blog posts about your particular infrastructure needs and reach out to their employers (or the people themselves). Often people working at an organization will bring other people with similar expertise with them, so it’s a good marker that the consulting firm will work for your environment. A boutique firm can work against your goals, if your goals for a pentest are customer- or compliance-facing. Your customers or compliance auditors may not be familiar with or trust the specialized firm you find. Luckily many of the large, well-known vendors employ great people with a wide range of skills. It is important in this case to make your needs known early on in the contract negotiations, so the best people for your environment at the contracting firm can be scheduled. . The initial scoping call usually includes both technical and financial components, usually with the latter following agreement on the former (you figure out what work needs to be done and  then  figure out what it’ll cost). To get the best engagement possible you’ll want to prepare. . The big questions are going to be about scope and target: What systems are off-limits (perhaps because they contain sensitive data, or are so mission-critical that any disruption could impact the business) and what are the “crown jewels” that the team should aim to access as a metric for success? You should also think about how involved you’d like your own teams to be. Are you testing the defenders and incident response teams’ capabilities? Is this a “purple team” exercise, with the defenders and attackers working closely together to write new detection rules? . Here at Slack we like to give our red team engagements a wide-open scope, but keep a referee who’s invested in the success of the red team that won’t leak operational information to the defending blue team. One approach we’ve used to get the most out of the limited time we have with the red team is to give them documentation describing our architecture and all the systems in it. This way our pentest team has access to information that a real attacker with months of recon time might get. . Once the engagement begins, it can proceed in a few ways. You may choose to keep it as a pure black box adversary simulation, giving neither the defending party nor the attackers any information on how the engagement proceeds. On the other hand, it may make your internal teams more comfortable if someone knowledgeable receives periodic updates. It’s common to give daily updates of where the pentesters have been and what their plans are. At Slack we take it a step further and integrate our referee in to the red team directly by inviting them to shared private channels that have a constant stream of updates from the pentesters, along with suggestions from our referee on where to proceed. This has the added bonus of better simulating a long-term attacker that quietly collects information about an organization. . After the engagement is complete, the pentest team will deliver a final report that includes a high-level overview, explanation of their procedures and tools, along with a list of vulnerabilities found along the way. You may wish to discuss the particular findings for your environment, as the rubric pentesters might use is quite general. It’s possible that you’ll want to reclassify some vulnerabilities based on your mitigations and architecture, or based on the privileged position you may have given the attackers. Once the report is delivered, someone in your organization will need to be responsible for the project management tasks of filing bugs and following up with affected teams to remediate. . Pentests can show you the weaknesses of your systems, and give you and your customers peace of mind that you’re ready to weather attacks. But each test only assesses a particular moment in time — as your systems change and mature, it’s likely that the surface area for attack will also change, so regular pentests should be scheduled to exercise not only the protections you have in place, but also test against new intrusion approaches. . Hiring pentesters or red teams can seem like a daunting task, especially in the world of high-pressure sales, but it doesn’t have to be difficult. It can be as simple as: . If you’re a security engineer and you’re interested in helping us keep Slack data safe, take a look at our  Careers page  and apply today! ", "date": "2019-05-07"}, {"website": "Slack", "title": "Android UI Automation: Part 2, Making It Easy", "author": ["Kevin Lai", "Valera Zakharov"], "link": "https://slack.engineering/android-ui-automation-part-2-making-it-easy/", "abstract": " In  Part 1  we introduced the targeted and hermetic UI test approach that helped Slack Android engineers write hundreds of stable UI tests. However, as our UI test suite expanded, we detected a problem. When asked how easy it was to write tests on a scale from 1 (terrible) to 4 (excellent), developers ranked it a 2.5 (a resounding “ meh ”). In fact, the only category that ranked worse was related to build times. . As one developer put it —  “Writing quality UI tests is still a difficult thing to do. It would be great to have some time put into building up a base to work from.”  What a great suggestion! In the following Test Week (a quarterly hackathon focused on improving test infrastructure), we set out to address this. . As a starting point, we looked at existing solutions that could help us better structure and organize our test code. The  robot pattern  — a layer of abstraction encapsulating user-facing functionality on the screen — was a good place to start. . By separating out  what  the test does with the UI from  how  it interacts with the UI, robots reduce code duplication, make tests more readable, and take some friction out of authoring tests. . However, in the case of targeted and hermetic UI tests, robots only solve part of the problem. What about the other side of the View? . We found that the other side of the view — dealing with setting up hermetic state — is where developers struggled the most. . We previously showed a couple lines of code demonstrating how we feed data to the test via a mock data provider. In practice, it’s rarely just a couple lines, but more like this (except a lot longer in the case of more complex screens): . We have many more components that need to know about a user and a channel within our app, but this should help illustrate the complexity of our test setup. These components need some minimum state before a View will display properly and give us access to part of the app under test. Before a developer can add a test for their feature, they must swap in test doubles for  everything else  that’s powering that View — a time-consuming endeavor that often requires sifting through code that was written by others. What’s more, each developer would find themselves in a similar situation, causing potentially every member of the team to go through the same cumbersome test-writing process over and over again. . Our attempts to tuck repetitive setup away into a Base class only got us so far. Tests inheriting from BaseActivityTest needed to set up additional test data, leading to the proliferation of repetitive mocking throughout our codebase. And it turned out this was the most time-consuming part of creating a UI test, prompting us to look for a better way. . If we consider the data as part of our test — which we should if the test is going to be targeted and hermetic — the UI Test Architecture starts to look a lot more like Diagram 3. Before interacting with Robots, the test has to do some setup on the other end of the spectrum so that our View is “powered” accordingly. Naturally, this starts to look like a test sandwich of sorts, albeit a bit of an imbalanced one since we have both the Models and Tests providing “What” data we’re testing. However, when we take a step back and consider what a Test is actually doing in the setup phase, it’s essentially creating some test models and returning them from the Presenters/Components. . We saw a great opportunity to improve the developer experience. If we look at the UI Test Architecture from a slightly different perspective, with the Test interacting with the Robots on one side of the View and providing the Models on the other side of the View, it made sense to swap in something for the “How” behind the View — similar to Robots in front. In Diagram 4., we introduce the  PowerPack pattern  to encapsulate the complexity of  powering  the View. . Part of the complexity comes from figuring out what needs to know about the test data that’s provided, so that the test author can simply think of models and not components. In the case of a User, we introduced a UsersPack that leverages AutoValue to provide a builder interface. By default, the UsersPack is provided with a set of Components that need to know about Users as follows: . And it exposes a pack() method that encapsulates all of the repetitive mocking for all of the Components. . In comparison, the ChannelsPack has a lot more Components than we originally illustrated (shown below). As such, we’ll leave it up to your imagination on how much more involved its pack() method is. . A side benefit of these model-specific Packs is that they can be used by unit tests directly to mock a subset of Components, but their main benefit in UI tests comes from grouping them into an app-wide Pack, which is injected with mocked dependencies and provides them automatically in the model-specific builders. For us, this is a  SlackPowerPack , which reduces the complexity in a test setup as follows: . The end result is a lot more clear and concise. The test author approaches a test by thinking of the test data and not the Presenters/Components that need to be mocked out. . It’s worth noting that the complexity in setting up the Presenters/Components with the test data hasn’t been removed — it’s just been abstracted away as seen in the UsersPack.pack() example above. Still, the benefit is that a team of developers only has to take this cumbersome hit once and any changes to Presenter/Component APIs benefit from updates in a single Pack class. . Using the Robot and PowerPack patterns in conjunction, our UI Test Architecture ends up looking like a View sandwich in Diagram 5. The Test manipulates the layer immediately in front of and behind the View, giving us a very targeted and hermetic test where all the complexity in interacting with the View and setting up state is encapsulated and abstracted away. For the test author, just as the Robot pattern provides a fluent API for entering text, tapping buttons, etc., the PowerPack pattern does the same for initializing users, channels, messages, or whatever other test data the app is dependent upon. . With the PowerPack, we saw a significant improvement in the Ease of Testing category of the Developer Experience survey (a jump from 2.5 to 2.92). A friendly Slack developer wrote this —  “Ease of test writing: this has improved a lot. It would be nice to take the time to update some of our old tests to use PowerPack/our latest patterns.”  And if they stopped there, we would have packed up and gone home. But they also went on to say this —  “I would also love an easy way to set up a workspace.”  . Which got us thinking… . Many of the Slack UI tests need a test workspace with channels, users, messages, and other data. We could use the SlackPowerPack builder to set all that up, but what if the setup could be reduced to just one line? . To accomplish this, we extract the data used to initialize the SlackPowerPack into a json config: . From there, we can inflate the JSON into model objects and feed them to our PowerPack: . Having reduced test setup from a twisted mess of mockito invocations to one line, we got thinking about the next chapter in our testing story. . The JSON config … . … is strangely reminiscent of the data we retrieve from the  Slack API . What if, we could charge the PowerPack automatically? . We maintain a small set of end-to-end tests that communicate with a real backend (stay tuned for a blog post about this topic). Using a library like  OkReplay , we could capture network traffic from these tests. This API data can then be replayed directly in tests (a technique we use for performance testing, in order to preserve the fidelity of the application under test) or it could be used to auto-charge the SlackPowerPack. Sounds promising — we intend to explore this idea in the near future. . With our developer-driven test approach, we’ve been able to build up a robust test suite that actually resembles a balanced pyramid. . Our unit and UI tests gate every merge to master, ensuring that a developer can merge code with confidence. Equally important, the process of testing exposes gnarly areas of our app and naturally guides us towards writing better, more modular code. . There is still a lot of work to do: building PowerPack++, covering legacy parts of the codebase, continuing to fight flakiness, and taking infrastructure that executes and manages tests to the next level. We look forward to sharing our progress on these initiatives. Thanks for reading! .  If this is the kind of work that appeals to you, join our    Mobile Developer Experience team   . And if, instead, you’d like the idea of authoring UI tests themselves, we are looking for test-friendly    Android engineers   .  ", "date": "2018-01-11"}, {"website": "Slack", "title": "Keep webpack Fast: A Field Guide for Better Build Performance", "author": ["Rowan Oulton"], "link": "https://slack.engineering/keep-webpack-fast-a-field-guide-for-better-build-performance/", "abstract": "  webpack  is a brilliant tool for bundling frontend assets. When things start to slow down, though, its batteries-included nature and the ocean of third-party tooling can make it difficult to optimize. Poor performance is the norm and not the exception. But it doesn’t have to be that way, and so — after many hours of research, trial, and error — what follows is a field guide offering up what we learned on our path towards a faster build. . 2017 was an ambitious year for the frontend team at Slack. After a few years of rapid development, we had a lot of technical debt and plans to modernize on a grand scale. Top of mind: rewriting our UI components in React and making wide use of modern JavaScript syntax. Before we could hope to achieve any of that, though, we needed a build system capable of supporting a nebula of new tooling. . Up to this point, we’d survived with little more than file concatenation, and while it had gotten us this far it was clear it would get us no further. A real build system was needed. And so, as a powerful starting point and for its community, familiarity, and feature set, we chose webpack. . For the most part our transition to webpack was smooth. Smooth, that is, until it came to build performance. Our build took minutes, not seconds: a far cry from the sub-second concatenation we were used to. Slack’s web teams deploy up to 100 times on any given work day, so we felt this increase acutely. . Build performance has long been a concern among webpack’s user base and, while the core team has worked furiously over the past few months to improve it, there are many steps you can take to improve your own build. The techniques below helped us reduce our build time by a factor of 10, and we want to share them in case they help others. . It’s crucial to understand where time is being spent before you attempt to optimize. webpack isn’t forthcoming with this information but there are other ways to get what you need. . Node ships with an  inspector  that can be used to profile builds. Those unfamiliar with performance profiling need not be discouraged: Google has worked hard to explain how to do so in  great detail . A rough understanding of the phases of a webpack build will be of great benefit here and while  their documentation  covers this in brief you may find it just as effective to read through some of the  core   code . . Note that if your build is sufficiently large (think hundreds of modules or longer than a minute), you may need to break your profiling into sections to prevent your developer tools from crashing. . Profiling helped us identify the slow parts of our build up front, but it wasn’t well suited to the observation of trends over time. We wanted each build to report granular timing data so that we could see how much time was spent in each of our expensive steps (transpilation, minification, and localization) and to determine whether our optimizations were working. . For us, the bulk of the work was done not by webpack itself but by the scores of loaders and plugins we relied on. By and large, these dependencies didn’t provide granular timing data, and while we would love to see webpack adopt a standardized way for third-parties to report this kind of information, we found we had to hand-roll some extra logging in the meantime. . For loaders, this meant forking our dependencies. Although this is not a great strategy long-term, it was incredibly useful for us to decipher slowness while we worked on optimization. Plugins, on the other hand, were much easier to profile. . Plugins attach themselves to  events  which correlate to the different phases of the build. By measuring the duration of these phases, we could roughly measure the execution time of our plugins. .  UglifyJSPlugin  is an example of a plugin where this technique can be effective, as the bulk of its work is done during the  optimize-chunk-assets  phase. Here’s a crude example of a plugin that measures this: . Add it to your list of plugins, ahead of UglifyJS, and you’re good to go: . The value of this information vastly outweighs the nuisance of getting it, and once you understand where the time is spent you can work to reduce it effectively. . A lot of the work webpack does lends itself naturally to parallelism. Dramatic gains can be had by fanning out the work to as many processors as possible, and if you have CPU cores to burn, now’s the time to burn them. . Fortunately, there are a slew of packages built for this purpose: . Be warned that there is a non-trivial cost to spinning up new threads. Apply them judiciously and only for operations that are costly enough to warrant it based on your profiling. . As our implementation of webpack matured, we realized it was doing more work than necessary in several places. Chipping away at these areas saved us a surprising amount of time: . Minification is a huge time sink — it was between half and a third of our build time. We evaluated different tooling, from  Butternut  to  babel-minify , but found that UglifyJS in a parallel configuration was the quickest. . What really sealed the deal for us, though, was a note on performance  buried beneath a long readme  from the author: . It’s not well known, but whitespace removal and symbol mangling accounts for 95% of the size reduction in minified code for most JavaScript — not elaborate code transforms. One can simply disable compress to speed up Uglify builds by 3 to 4 times. . We tried it and the results were staggering. As promised, minification was 3 times as fast and our bundle sizes had hardly grown at all. React users wishing to disable compression in this way should be wary of one caveat: the  detection methods  used by  react-devtools  can report that you’re shipping a development version of React. After some trial and error, we found the following configuration fixed the problem: . Detection varies by version and React 16 users may get away with  compress: false  alone. . Fewer bytes for the end-user is often the priority so take care to strike the right balance between the needs of your engineering team and those of the people downloading your application. . It’s typical for the same code to find its way into more than one bundle. When this happens the minifier’s work will be multiplied unnecessarily. We put our bundles under the microscope with both the  webpack Bundle Analyzer  and  Bundle Buddy  to find duplicates and split them out into shared chunks with webpack’s  CommonsChunkPlugin . . webpack will parse every JavaScript file it sees into a  syntax tree  while it hunts for dependencies. This process is expensive so if you are certain that a file (or set of files) will never use import, require, or define statements, you can tell webpack to exclude them from this process. Skipping large libraries in this way can really boost performance. See the  noParse  option for more detail. . In a similar vein, you can  exclude  files from loaders, and many plugins offer  similar options  too. This can really improve performance for tools like transpilers and minifiers that also rely on syntax trees to do their surgical work. At Slack we only transpile code we know will use ES6 features and skip minification for non-customer facing code altogether. .  DllPlugin  will let you carve off prebuilt bundles for consumption by webpack at a later stage and is well suited to large, slow-moving dependencies like vendor libraries. While it has traditionally been a plugin that required an enormous amount of configuration,  autodll-webpack-plugin  is paving the way to a simpler implementation and is well worth a look. . webpack assigns an ID to every module in your dependency tree. As new modules are added and others removed, the tree changes and so too do the IDs of each module within it. These IDs are baked into every file that webpack emits and a high level of module churn can result in unnecessary rebuilds. Prevent this by using  records  to stabilize your module IDs between builds. . At Slack we use hashed filenames to cache-bust every time a new version is shipped. Open the Network tab of your browser’s developer tools and you’ll see requests for files like “ application.d4920286de51402132dc.min.js ”. This technique is fantastic for cache control, but means webpack can no longer map a module to its respective filename without the help of a digest. . The digest is a simple map of module IDs to hashes that webpack will use to resolve a filename when  importing modules asynchronously : . By default, webpack will include this digest in the boilerplate code it adds to the top of every bundle. This was problematic as the digest had to be updated every time a module was added or removed — a daily occurrence for us. Whenever the digest changed, not only did we have to wait for all of our bundles to be rebuilt but they were cache-busted too, forcing our customers to re-download them. . Keeping module IDs stable wasn’t enough. We needed to extract the module digest into a separate file entirely; one that could change regularly without us or our customers paying the cost of rebuilding and re-downloading everything. So we created a  manifest file  with the CommonsChunk plugin. This greatly reduced the frequency of rebuilds and had the added bonus of letting us ship only a single copy of webpack’s boilerplate code too. . Source maps are a crucial tool for debugging, but generating them can be incredibly time-consuming. Consult webpack’s  menu of devtool options  and see if a cheaper style will provide the debuggability you need. We found  cheap-source-map  struck a good balance between build performance and debuggability. . Our deployment cadence is rapid, and this means there are usually only small differences between the current build and its ancestors. With caching in the right place we could shortcut most of the work webpack would have done otherwise. . We use  cache-loader  to cache loader results (users of babel-loader can choose to use it’s  built-in caching  if they prefer), UglifyJSPlugin’s  built-in caching , and last but not least the  HardSourceWebpackPlugin . . A lot of the work that webpack does is outside of loader/plugin execution and much of that work has traditionally evaded caching altogether. To solve this problem, we brought in  HardSourceWebpackPlugin , a plugin designed to cache the intermediate results of webpack’s internal module processing. . For it to work we had to carefully enumerate all the external factors that might require the cache to be broken and test it thoroughly. In our case: translations, CDN asset paths, and dependency versions. This isn’t for the faint-hearted but the results were well worth the effort — after priming the cache our warm builds were a full 20 seconds faster. . As a final note, remember to clear your cache whenever package dependencies change — something you can automate with an  npm postinstall script . A stale, incompatible cache can wreak havoc on your build and break it in new and interesting ways. . In the webpack ecosystem it pays to stay up to date. Steady work has been done by the core team to improve build speed in recent times and if you aren’t using the latest release of your dependencies you may be leaving performance gains on the table. In our upgrade from webpack 3.0 to 3.4, we saw tens of seconds eliminated without any change to our configuration at all, and the improvements keep coming. . Upgrade regularly and keep abreast of new functionality like the parallelism mentioned earlier. At Slack we keep an eye out for releases on Github, try to contribute where we can, and follow the inimitable efforts of  webpack ,  babel , and others who blog about their work. . Don’t forget to keep your version of Node up to date too — packages aren’t the only avenue for improvement here. . At the end of the day your build has to run somewhere, and on something. That something can have a great deal of impact on your overall build performance and even the most heroic effort to optimize will be met with failure if, ultimately, the build runs on prehistoric metal. . When we began our quest, our build server was a member of the C3 Amazon EC2 family. By switching to an instance type in the more recent C4 offering, where processors are faster and more plentiful, we saw a significant improvement in both build time and in the options available to us for scaling parallelism as our codebase grew. Users worried about the transition from an instance-backed machine to EBS need not despair: webpack caches file operations aggressively and we saw no measurable degradation in performance on moving to EBS. . If it is within your power (and budget) to do so, evaluate better hardware and benchmark to find the sweet spot for your configuration. . Infrastructure-level projects like webpack can be surprisingly under-funded; whether it’s with time or with money, contributing to the tools you use will do much to improve the ecosystem for you and everyone else in the community. Slack recently donated to the webpack project to make sure the team is able to continue their work, and we encourage others to do the same. . Contribution can come in the form of feedback, too. Authors are often keen to hear more from the users of their software, to understand where their efforts are best spent, and webpack has gone so far as to encourage users to  vote on the core team’s priorities . If build performance is a concern to you, or you have ideas for how to improve it, let your voice be heard. . webpack is a fantastic, versatile, tool that does not need to cost the earth. These techniques have helped us reduce our median build time from 170 to 17 seconds and, while they have done much to improve the deployment experience for our engineers, they are by no means a complete work. If you have any thoughts on how to improve build performance further, we’d love to hear from you. And, of course, if you delight in solving these sorts of problems,  come and work with us ! . A huge thank you to Mark Christian, Mioi Hanaoka, Anuj Nair, Michael “Z” Goddard, Sean Larkin and, of course, Tobias Koppers for their contributions to this post and to the webpack project. ", "date": "2018-01-17"}, {"website": "Slack", "title": "Evolving the Slack API", "author": ["Brenda Jin"], "link": "https://slack.engineering/evolving-the-slack-api/", "abstract": " You know how to design a good API, but how do you evolve that API when it’s time to make changes? We’ve faced this with each major feature release here at Slack over the past few years, most recently while working on the highly anticipated  Shared Channels  feature. Using shared channels, two organizations connect directly in Slack and work together from the comfort of their own workspaces. . The idea seems simple, but building it wasn’t easy. From a user’s perspective, a shared channel looks and feels just like any other channel. Under the hood, shared channels fundamentally changed the relationship between workspaces, channels, and the web API. . Let’s take a look at how the backend design for channels has evolved over time, and how the release of shared channels turned into an opportunity to build the  Conversations API , a unified interface for all types of channels. . In the beginning when channels had a 1:1 relationship with a workspace, our backend database tables had a close relationship to the web API. Each table had corresponding API endpoints, and each API endpoint had a single corresponding  OAuth scope : . But Shared Channels were different. We merged them into the public channels table, because they needed a shared channel ID across workspaces. This was for two reasons: . Here’s where the problem arose for the web API: Using OAuth, customers had explicitly granted permission to public channels (channels:read), private channels (groups:read), or DMs (im:read). But with the merged data in the channels table, we were not able to control data access the same way. Any app installed with channels:read scope was about to be granted access to  private  shared channels as well. . When we considered what was best for both the Shared Channels feature and our developer ecosystem, we found that there was significant upside in creating a brand new unified interface for all types of channels: the  Conversations API . . At the top of our minds was how we were going to maintain backwards compatibility for all the developers who had already built apps using our web API. A new suite of endpoints would allow developers to opt-in when they were ready to update their code for shared channels. A new API would also allow us to address major developer pain points that had emerged over the years — things we really wanted to fix, but couldn’t — because of backwards compatibility. . For the rest of this article, we’ll be looking at the major changes in the new API: . Previously, developers had to know whether a channel was public or private before calling channels.info or groups.info. With the new API, developers no longer have to know the type of channel before calling conversations.info for any channel type. At the beginning of a request to any Conversations API endpoint, we now compare the type of channel requested with the OAuth scope of the requester’s token. If the requested channel type is not authorized, the developer will get a missing_scope error.  Developers can now remove app logic  that had been built to map a fragmented model of channels to OAuth scopes and API endpoints. . Our original channel API endpoints had been updated over time as our product evolved, and as a result, dramatic inconsistencies had developed. One endpoint might have a channel argument that required an ID, whereas another endpoint with the same argument might require a channel name instead. We fixed these in the new API so that argument and types are consistent across the whole suite. . To make the Conversations API as easy to use as possible, we also reduced the number of arguments whenever we saw a strong trend in historical requests. For example, all requests to channels.rename used the non-default argument validate=false. As a result, we made validate=false the new default behavior for conversations.rename. . In some cases, we changed the arguments to improve performance and scalability for apps. For example, channels.info used to include a nested list of members, but as larger and larger companies started to use Slack, response payloads grew, in some cases to as much as 290MB for a single channel. In addition, 72% of requests to channels.info specified no_members=true. To help apps scale to large teams, we now exclude members by default for conversations.info. To provide the channel membership data in a more optimal format to developers, we added a new conversations.members endpoint with pagination. . In the early days, we didn’t anticipate that workspaces would have over 10,000 channels, or that channels would have over 150,000 members. Endpoints that were never intended to be paginated grew to unreasonable proportions! Performance bottlenecks and timeouts hindered developers’ ability to scale their apps, and timeouts for large enterprises became frequent. . Even before Slack released the Conversations API, the Platform team had already started to upgrade older endpoints to a  new cursor-based pagination paradigm . However, some older endpoints were using a legacy form of pagination that required suboptimal database queries. We could never fully transition any endpoint that had used the legacy pagination, because we did not want to break any apps that were already using the older request format. For those endpoints, the old and new pagination had to exist side-by-side, which meant that we still experienced undue load on our databases and delivered suboptimal performance to a portion of app users. . In the Conversations API, we committed fully to cursor-based pagination so that apps could scale alongside workspaces — whether or not they used shared channels. . Previously, Slack backend engineers could update core libraries and accidentally change our downstream API output. App developers would discover bugs before we did when their apps broke from API inconsistencies. Additionally, we had response fields that were assumed false or null when unset, which worked for JavaScript developers but was painful for almost everybody else. . Before Shared Channels, we had already started adding  JSON schema  validation using the  json-schema-rspec  gem in our automated  RSpec  tests to increase reliability in our web API. Since we wanted to make the Conversations API reliable, we designed the endpoints up front and wrote schema validation tests in parallel to feature development. These tests add reliability to every deploy. As a bonus, we used the schema to add  Open API spec s for these endpoints. . While developing Shared Channels, the Slack engineering team took a step back to think through the best experience for end users and developers. Taking that step back to re-think the design gave us the opportunity to build something we’re proud of. . Evolving APIs is difficult, but we hope you like the changes we’ve made. .   Want to use the Conversations API?    Read the official docs    here    and check out the    Platform Blog    to learn more about building Slack apps. If you liked the teamwork described in this article,     come work with us    . Finally, stay tuned to the    Slack engineering blog    for updates on a forthcoming book    Designing APIs that Developers Love    by Saurabh Sahni, Amir Shevat, and yours truly.  ", "date": "2018-01-26"}, {"website": "Slack", "title": "Interop’s Labyrinth: Sharing Code Between Web & Electron Apps", "author": ["Machisté N. Quintana"], "link": "https://slack.engineering/interops-labyrinth-sharing-code-between-web-electron-apps/", "abstract": " While it’s no secret that the cross-platform Slack Desktop app is built on Electron, it might be slightly less well known that it’s a hybrid app built around our web app (slack.com). This is one of Electron’s most compelling draws — not only can you build a cross-platform desktop app from scratch with the web technologies you know and love, but you can also extend a pre-existing web app with new and powerful capabilities. For this to work, there would have to be some way to get these two pieces, Electron and web, to glue together — to  interoperate . However, here there be dragons — performance hiccups, security vulnerabilities, and maintainability woes lurk in the shadows, waiting for you to unwittingly take a wrong turn. This is Interop’s Labyrinth. In maintaining one of the first Electron apps, we’ve navigated this labyrinth in the quest for a more performant, maintainable, and secure hybrid desktop app, and we’d like to help you avoid some of the pitfalls we’ve found along the way. . Before you start running this maze, ask yourself — why are you building a desktop app when you already have a perfectly functional web app? Given the rise of progressive web apps (PWAs) and browsers-as-OSes, it’s worth taking some time to think through what capabilities your app can’t possibly have as a standard web app. For Slack, this includes fine-grained control over native notifications, screensharing and video calls, spellchecking and language detection, native app and context menus, and access to exciting new APIs like the Touch Bar and My People. These are all capabilities we can’t currently get through the web platform as it stands today. . If, for example, you’re interested in building a desktop app just to display a website as-is in a dedicated window,  a PWA desktop wrapper  will be a much more secure alternative. You can already develop  PWAs as desktop apps today on Windows , and  Google has plans to launch this (with cross-platform support) sometime in 2018 . If you’re looking to build some kind of highly customized browser-like app that can visit any arbitrary website, then avert your gaze — Electron isn’t a platform for building browsers, and you’ll avoid a whole lot of pain if you look elsewhere for a solution. . Once you’ve taken some time to consider what capabilities your app couldn’t have as a standard web app, let’s step into this labyrinth. . You scour the perimeter of the labyrinth, searching for the nearest entrance. As your eyes travel along the tiled, immaculate path, you notice that it continues, like a hallway, straight ahead without twist or turn. You spy an ornate, dazzling goblet, arrayed in jewels and gold, sitting atop a pedestal glinting off in the distance at the very end of the corridor.  • Proceed  • Turn Back . It’s important to remember what Electron is, aside from being so hot right now — Electron is Chromium and Node combined into one runtime, which you control by interacting primarily with two processes: the main / browser process, and the renderer process. The main process bootstraps the app and coordinates other processes in the background, while the renderer process is responsible for what the user sees and interacts with, much like a browser tab. The key difference here is that both processes have full access to Node built-ins like require and fs. . Given this, by far the most straightforward approach would be to treat Electron like a browser, and just spawn a renderer process pointing at your remote web app. . What might this look like in code? . Let’s see where this path takes us: . You sprint at a brisk pace down the corridor, the goblet’s glittering growing more alluring with every step you take. You are now upon the threshold of the pedestal holding the goblet. You reach out your hand to claim your prize. Suddenly, the ground beneath you starts to shake, giving way to reveal that the floor is, in fact, lava. You start to reconsider your life choices. . Oh. Oh no. Where did we go so wrong? . There do seem to be some benefits to this approach: with only a few steps and lines of code you can have a seemingly functional desktop app, spring-boarded off the efforts of your web app. You might feel even further validated in following this path because many introductory Electron tutorials will have you try this out at some point — it is quite illustrative of Electron’s power and flexibility after all, and doesn’t require deep API knowledge. . Unfortunately, this path is more potholes than road: Electron’s superpowers raise quite a lot of security concerns when treated like a regular ol’ browser. As with any potential security concern, it can help to ask yourself: what are you trusting, and what kind of risks might that trust expose you to? . In this case, we’re trusting our remote web app with unfettered access to Node, which means it can do absolutely  anything:  require / import arbitrary JavaScript and native modules, surreptitiously access your user’s files, mine Bitcoin, download ASCII art of a dancing hot dog, you name it. These are probably things you don’t want your app doing, but they can happen in some of the worst case scenarios, like your web app getting man-in-the-middled. Modern web apps tend to pull in a lot of third-party code (analytics frameworks, social media embeds, ads, etc) and user-generated content, none of which should be loaded with Node-level privileges. Ultimately, this approach is wildly insecure and puts your users at risk — it should never be used for a production app. . Don’t trust remote content, even if it’s from a trusted source. .  Should you do it?  🙅 Nope, it’s a trap . The Shortcut is the simplest and most straightforward path, but it’s so insecure out of the box that it’s ultimately a nonstarter. If you’ve shipped a production Electron app currently built around this approach (and there are many 😬), you’ve shipped a recipe for disaster. . You spy another entrance close by, and peer around the hedge to see what appears to be a hallway lined with glass on all sides. You notice the branches of either side of the hedge swaying in the breeze, grazing the sides of the long glass passageway. Given where the last path led, you tread carefully and slowly along the translucent walkway towards the telltale glint of treasure off in the distance.  • Proceed  • Turn Back . So what was the problem with the first approach? We gave our remote web app full access to Node, and Node is extremely powerful. Normally it only runs in Backend Server Land, which is why it has unrestricted access to whatever the user that spawned it has access to, all the way down to the file system and network stack. We don’t really want to extend this same level of trust to a remote web app. Instead, we can gate our remote web app’s access to Node by loading it inside an isolated context, so that it’s no longer directly accessible from within the renderer process. . You might be thinking, isn’t half the point of Electron precisely that you can have access to Node from within a web page? We wouldn’t have much of a desktop app if all of a sudden we had no access to desktop superpowers like file system access! There is a way out, however — we can instead define an interop API via a preload script that gets run before Electron loads our guest page into the renderer. Within this preload script we still have access to Node, so we can use that to expose only the bare minimum functionality our web app needs, and nothing else. So what does this path look like? . We’ll focus on just a handful of config changes here, but it’s worth noting that to achieve full remote isolation you have to fiddle with a lot of knobs and levers — many more than I can detail in this post. In addition to disabling Node integration and enabling context isolation, you’ll also need to investigate Chromium  process sandboxing  and potentially OS-level app sandboxing (for  Windows ,  Mac , and  Linux ) as well, to name a few considerations. For more details on security best practices, including a more thorough remote isolation pattern, please read through the entire  Electron security guide . . To achieve full remote isolation you have to fiddle with a lot of knobs and levers . So, what might a very minimal, somewhat non-comprehensive implementation look like in code? . Let’s see where this path takes us: . You reach the source of the reflection in the distance only to discover that the glint is fire. The walls have collapsed, fires blaze where there ought to be none, and countless would-be adventurers lay unconscious throughout the crumbling facade. You begin to question your life choices. . Uh oh. This seemed so promising too! . This path definitely brings some benefits to maintainability. Much like The Shortcut, this approach shares the same client as your web app — this means that you get to take advantage of your remote web app’s continuous deployment pipeline. As a result, your Electron app “should” automatically have feature parity with your web app (barring any bugs, of course). Instead of having to build a separate native client and go out of your way to ensure it meets all the same specs as your web app, you just get all that, essentially, for free. . The Remote Isolation approach also brings some significant improvements to security — ask yourself again, what are you trusting, and what kinds of risks might that trust expose you to? Unlike The Shortcut path, we’re no longer trusting remote content; we are, however, still trusting local content (the Electron shell JavaScript we’re shipping in our app bundle). Since we’re now restricting access to Node built-ins like require and third-party dependencies, if your remote web app were compromised, the worst it could do is abuse your interop API (in theory — we’ll come back to this later). . Despite these benefits, there are some pitfalls: this approach does add some CPU and memory overhead, especially if you’re using an Electron container. Every container / context is its own separate process, with its own execution context and process block structures, and this can add up if you happen to be spawning multiple containers. You’ll also need to communicate between these processes and the main and root renderer processes. Electron provides a remote procedure call (RPC) mechanism for this, but since these remote method calls boil down to synchronous IPC message passing, relying on these remote objects for communicating between the guest page and parent renderer process will likely make your app really slow. These calls can block threads and your users will experience this as a hang —  probably  not what you want. You could also use IPC directly, but messages must be serializable, and any state changes would have to be synchronized between all processes. . Moreover, depending on which remote isolation approach you choose, you may be introducing a wellspring of  endless bugs  for yourself and your users. If you’re curious about some of the webview bugs you might run into, take a gander at our  earlier post about re-architecting the Slack Desktop app for 3.0 . . There are also some maintainability pitfalls: when you load your entire remote web app into a container, it’s really difficult to reuse  parts  of your remote web app — it’s all or nothing .  For example, let’s say your web app had a delightful modal dialog UI component, and you wanted to reuse that for a code path exclusive to desktop apps (e.g. handling connections to corporate proxies). How would you accomplish that with the Remote Isolation approach? Well… you’re probably going to have to copy and paste it into your desktop app repo. Since this path treats Electron like a wrapper around your remote web app, unless your web app exposes something on the level of a single UI component independent from any web-app-specific state, you won’t be able to access it for reuse. . This pattern also encourages implicit dependencies on the global scope. Since we’re injecting our interop API as a global on the window object, it can be mutated by any module in our remote web app’s dependency tree at any point, with none of the guarantees of locks or safe concurrent access. This can make debugging a Sisyphean nightmare. Your app’s globals could also conflict with third-party dependencies (like some jQuery plugins), leading to unpredictable and untestable outcomes. . You must also be extremely mindful when designing your interop API. While in theory the worst that could happen if your remote web app were compromised is that your interop API could be abused, depending on the capabilities of that API and its design, that could be more than enough to wreak total havoc on your users’ machines. For example, don’t expose references to Node modules, because that is tantamount to requiring / importing them directly from your remote web app; don’t expose the require function itself; don’t expose generic IPC interfaces (because this would let your guest page send arbitrary messages to other processes). . It’s worth noting that these mitigations require serving your remote web app exclusively over HTTPS; as HTTP requests and responses are sent as plaintext, they are easily MITMed. It’s also really helpful to define a robust content security policy to prevent potentially untrusted code from violating Cross-Origin Resource Sharing (CORS). .  Should you do it?  Maybe — only if you’re currently following The Shortcut (loading your remote web app directly into Electron) . Remote Isolation is definitely an improvement over The Shortcut path for security, but at the cost of performance and maintainability, and its implementation requires a lot of nuance and careful design. This is how Slack Desktop is architected today, but we’re actually in the process of moving to a completely different approach (we’ll come back to this later). If your goal is a hybrid app experience indistinguishable from a fast, polished native app, then I highly recommend avoiding this path. . You stand at the entrance to a daunting sight — a gauntlet of moving platforms, sliding pedestals, and treacherous climbs. However, you see a few other adventurers blazing through this series of challenges with aplomb. You are filled with determination.  • Proceed  • Turn Back . Both The Shortcut and Remote Isolation approaches were predicated on sharing the same client as our remote web app (by loading it directly into a renderer or isolated context, respectively), but it seems like this may be at the root of many of their performance and maintainability tradeoffs. What if we avoid loading remote content and sharing the same client altogether? With this new path, we’ll build our desktop app as a completely separate client composed of resources extracted from our remote web app and shipped locally within our app bundle at build time. So, what would this look like? . For this approach, we’ve broken up our web app into discrete modules which are then published to some kind of publicly accessible rectangle. Next, we somehow import these into our Electron app. Finally, rather than pointing our BrowserWindow at a remote URL, we’re pointing it at a local file path that is responsible for booting the frontend of our desktop app. Unlike The Shortcut and Remote Isolation, the path of Local Resources brings along with it a lot of prerequisites and considerations before it can be implemented. Let’s break these down into rough steps: . Diving into the why of each step is a bit out of scope for this post, but here’s what a super minimal example might look like: . Let’s see where this path takes us: . You leap masterfully from platform to platform, avoiding traps and solving tricky puzzles along the way. You hear a voice emanate from the walls themselves: . “Your resourcefulness in overcoming this trial speaks to the promise of an Electron app developer… In the name of Slackbot I bestow upon you this reward.” . A treasure chest materializes before your eyes and begins to emit an orange glow. . Well that seems promising! Let’s see what benefits and tradeoffs come with this path. . There are some maintainability benefits to the Local Resources approach: it facilitates a de-duplication of efforts between our Electron app and our remote web app. Desktop-only code paths that would have had to duplicate components or logic (like the modal dialog UI component from earlier) can now reuse the same components and code paths from your web app. This allows you to be much more granular about what you want to use and how you want to use it in a desktop-specific context, without otherwise introducing subtle bugs and inconsistencies that may frustrate your users. . This approach also encourages building loosely-coupled, encapsulated modules as the building blocks for your app. This is fantastic in part because it makes them substantially easier to test, more predictable, and ultimately more maintainable as a result. No longer should you have to worry about one module’s instantiation interfering with another module’s operation (unless that was your intent). . There are some performance boons as well. You should see a much faster cold boot time compared to both the Shortcut and Remote Isolation paths — both approaches rely on a blocking remote network request to be able to boot successfully, so depending on your users’ network environments, that could either be really painful or preclude booting altogether. As a nice bonus, since this approach no longer requires a network connection to boot, this gets you part of the way towards having an offline mode for your desktop app, if that’s something you’re interested in. . Finally, there should be less resource overhead — fewer processes are spawned, especially compared to the Electron container approach, and you shouldn’t have to rely quite as much on IPC message passing for event handling. Electron apps don’t have to be resource-devouring monsters — the path you choose can have a huge impact on performance. . Let’s run this approach by our security gut check — what are we trusting, and what risks might that trust expose us to? In this case, we’re avoiding the question of trusting remote content altogether by exclusively loading local assets at runtime. We do still trust our local resources, however — any scripts located in our app bundle have privileged access to Node and Electron APIs, so we’re trusting that the contents of our app bundle haven’t been tampered with locally. These local resources now include modules extracted from our web app, so we need to be confident that those modules and any of their third-party dependencies aren’t doing sketchy stuff behind the scenes on users’ machines. . There are still some pitfalls to this approach, even though it mitigates many of the tradeoffs involved in the path of Remote Isolation. This path would require engineers working on the web app to adopt some kind of package publishing and versioning process — this could take a not insignificant amount of time and training to implement. . Relatedly, this path will force you to make some kind of call between backing your published packages with many repos or a monorepo. Say you’re going to publish shared code to a package registry — where will the code that gets published to that registry live? Will multiple packages live colocated in your web app repo, only being published as distinct units at publish time? Or will you split each module out into its own repo? And if you do that, how will you pull those modules back into your web app repo? How will you ensure each module repo meets the same continuous integration (CI) standards as your web app repo? These are questions you will end up having to address. . One major pitfall of this approach is that you no longer get continuous deployment for free. With the path of Remote Isolation, we were able to leverage our remote web app’s continuous deployment to maintain feature parity with the browser-based web app without extra intervention. If a bug fix were pushed to the web app, for example, your Electron app users would automatically get it too (on reload). Unfortunately, with Local Resources, that’s no longer the case. As opposed to the rapid iteration of web app development, traditional desktop app development consists in shipping discrete releases, with much longer turnaround times for QA and App Store review cycles. Ultimately, this means that your Electron app will always lag behind your web app, to some degree. . How granular do you want to get in your component or dependency trees? For example, you could extract every single UI component to separate packages for reuse, and assemble your Electron app up from the smallest, most atomic component, bit by bit, until you have built a comparable client from the same pieces as your remote web app. Conversely, you could just extract your root UI component (ie. the entire web app) and inject your desktop interop API at the top-most level, and trust that the pieces are composed in the right way on the web app side to work with Electron. Either approach is totally valid. . How would you maintain the interfaces provided by these shared modules over time? Adhering to  Semantic Versioning  to communicate breaking and non-breaking changes helps, but it depends on engineers actually bumping the major versions of these modules when there are indeed breaking changes. Adding some automation and tooling, like  static type checking  that warns if an API contract breaks without bumping the major version number of a package can help, but this is really a human challenge. . Does your app pull in remote data, and not just remote static assets? Is any of that data user generated? Do you need to handle use cases that entail loading third-party websites, like single sign-on (SSO)? If any of these things are true, you might still need to run in an isolated context to prevent untrusted content from having privileged access to Node. Indeed, you might also need to verify the local contents of your app bundle to ensure they haven’t been tampered with, for maximum paranoia. So, while Local Resources mitigates security concerns, it doesn’t quite eradicate them — always ask yourself what you’re trusting, and what risks that trust might expose you and your users to. .  Should you do it?  Sure, give it a shot! . There are many prerequisites and considerations to take into account, but the boons to performance and maintainability are worth it — so long as you’re comfortable with the tradeoffs made to continuous deployment and the level of trust extended to local modules shared with your web app. . You stand at the opening of a yawning abyss. A plexiglass bridge extends out over it. As you cross, you notice something resplendent in the distance, its shine filling you with more determination with each step you take. At the end of the bridge lies a grand door — it was its doorknob that you spotted from a distance.  • Proceed  • Turn Back . With both the paths of Local Resources and Remote Isolation, we had to make some kind of sacrifice in maintainability, performance, or security. What if there were some kind of hybrid approach we could use, combining the benefits of each while avoiding most of their pitfalls? With this path, we’re aiming to strike a balance between the trust and continuous deployment benefits of Remote Isolation and the maintainability and performance benefits of Local Resources. To accomplish this, we can ship a local snapshot of resources that we’ve extracted from our web app, and then update them remotely in the background, but all within an isolated context. Phew, that’s quite a mouthful — I think it helps to think of this as combining Remote Isolation and Local Resources, and adding background updates. So, what would this look like? . Let’s see how this path goes: . Upon opening the door, you stand in a glittering chrome room, made of dazzling forms that seem to fold in on themselves in impossible shapes — a room made of labyrinths. You begin to wonder if perhaps the real treasure of the labyrinth is the journey itself. . Hmmm, this seems like a Deep Future approach, but rather promising! . Unlike the path of Local Resources, with the Hybrid path we get to take advantage of our web app’s continuous deployment pipeline again. Moreover, we get the same performance benefits from booting from the bundled snapshot of shared modules, but with the rapid iteration of web app development. Whenever a shared module is published, our desktop app should be able to pull in these updates without having to rebuild or re-release the Electron and native bits along with them — powerful stuff. Unlike with Remote Isolation, our desktop app is no longer doomed to an all-or-nothing approach to reusing parts of the web app, preserving the maintainability benefits of Local Resources. . Let’s see how our security gut check fares — what are we trusting, and what risks does that trust expose us to? Where before we were trying to avoid the question of trusting remote content, with the Hybrid path we instead explicitly deny it. In addition to isolating our remotely fetched shared modules, we’re also isolating the snapshot of shared modules we bundle at build time. This means that all shared code should be running under isolation, whether local or remote. We are still trusting that our local Electron shell code is legitimate, but we’re no longer trusting anything that loads inside of a renderer process. . Despite the benefits, this path can introduce a bit of a wrinkle into your update process. Instead of one updater updating both the Electron shell and shared web app modules together in one fell swoop, you now have two distinct update paths: one that updates the Electron shell and any native Node modules, and one that updates shared web app modules. You’ll need to be mindful of how they could interact to avoid introducing incompatibilities. . Questions of granularity also rear their heads again — do you want to update each shared module on a per-package basis (ie. downloading each updated package tarball in a separate request)? Or do you want to instead download a collated bundle of the updated modules in one request? Although it might be tempting to ship your app with a full-blown package manager like npm to handle updates for you, this would likely be playing with fire. In addition to the security risks of running arbitrary package lifecycle scripts on users’ machines, these are developer tools designed to take full advantage of your machine’s hardware, and could end up taxing users’ machines for an operation that should be relatively inexpensive and seamless. . If you do decide to go the bundle route, should this just be a simple archive of package registry tarballs? Or should they be a precompiled asset ready to load straight into the browser, generated with a tool like webpack or Rollup.js? If the latter, how might you integrate this with code splitting, or hot loading? . When modules are updated, how quickly do you want your users to get the latest code? You might want to consider different caching strategies for shared module updates compared to Electron shell updates. For example, maybe you want to boot with the latest cached assets, and then fetch updates in the background to be used on next boot. Or maybe, to bring in granularity again, some module updates are higher priority than others, and should be able to immediately invalidate any cached versions on fetch. If that’s the case, what should “immediate” look like? Should your app prompt the user to restart the app or reload the window to get the latest assets? Or should your app hot load the latest assets and optionally communicate the update results to your user? . These are questions that’ll likely arise as you design your update flow, and we’re still working out how all the pieces will fit together. .  Should you do it?  Probably! . We’re currently far from realizing the full potential of Electron as a cross-platform app development platform — this Hybrid approach seems like a promising way to bring us closer to that dream without sacrificing maintainability, performance, or security. Indeed, this is an approach we’re really excited about at Slack — but since we’re still actively investigating we can’t definitively recommend it, since we don’t entirely know what all the tradeoffs might be. Nonetheless, it’s an exciting work-in-progress. . To recap, The Shortcut seemed the most straightforward way to follow our desktop dreams, but turned out to be a rather nightmarish trap; Remote Isolation was a much safer approach to loading a remote web app in an Electron shell, but came with a host of maintainability and performance woes; Local Resources raised a ton of prerequisites and considerations, but brought some fantastic performance and maintainability improvements at the cost of continuous deployment and isolation; and the Hybrid path weaved Local Resources together with Remote Isolation to mitigate the tradeoffs of each individually. . With the incredible power of Electron comes the responsibility, our responsibility, to navigate Interop’s Labyrinth so our users won’t find themselves trapped in the pitfalls of the paths we’ve chosen. In Interop’s Labyrinth, we must all face the choice between what is right, and what is easy. . If any of this sounds interesting and/or mildly horrifying to you,  come work with us ! ", "date": "2018-02-01"}, {"website": "Slack", "title": "Moving Fast and Securing Things", "author": ["Max Feldman"], "link": "https://slack.engineering/moving-fast-and-securing-things/", "abstract": " For development teams, process can often be antithetical to speed. Ease of deployment and security tend to have an inverse relationship, with some resentment for the security team occasionally mixed in. You may have seen the following tweet: . We believe things don’t have to be like that. In this post, we will discuss how we’ve implemented our Security Development Lifecycle (SDL) at Slack, our lessons learned, and the  tool  we’ve built — and are open sourcing — that makes it all possible! . Slack is a rapidly growing company. We have over six million daily active users and over nine million weekly active users. We’re still relatively young, so this reflects a rapid acceleration of people using Slack. To deliver features to a growing number of users, we’ve also grown as an organization. At the start of 2015, Slack had 100 employees. Today, we’re over 800 people! These teams are spread over multiple offices and multiple countries, and our Engineering organization has been growing at a faster rate than our Product Security team. . We have a culture of continuous integration and continuous deployment. The process of deploying code to production is very simple, and takes about ten minutes total. This results in a life cycle in which we deploy code to production approximately  100 times per day . We’ve built our SDL process to provide coverage without being blocking, and to emphasize self-sufficiency rather than having our team manually review every pull request. . Ideally our process isn’t this: . But rather, this: . “SDL” stands for “Security Development Lifecycle”. You may have seen the following image: . Microsoft uses this process to highlight security topics throughout numerous development stages and phases. But when talking about secure development, mentioning “life cycle” or “process” can cause people’s eyes to glaze over. The perception of “ Adding Friction ” doesn’t work well with the “ Always Shipping ” mantra. . To deploy the SDL at Slack, we had to account for a variety of constraints. We have a lot of work to do, and not a lot of time to do it (this isn’t a unique problem, of course). While we have excellent developers, inspiring secure design is sometimes difficult, and we wanted to avoid impeding a team’s output. A traditional waterfall approach to the SDL wouldn’t work in our process. . We wanted to maintain our culture of developer trust.  Empathy is an important value at Slack , and we apply that in our security procedures. An adversarial relationship between the security and development teams is a dangerous possibility if the security team is not thoughtful. Our goal was to empower the development team to be self-sufficient; security is everyone’s responsibility. . As part of our culture of trust, we place a huge emphasis on transparency and availability. We keep our discussions in an open forum, we host weekly office hours for anyone who wants to stop by and chat about security, and we offer security training during employee onboarding, to both educate and present the faces of the security team. All of these serve to foster mutual trust and collaboration. We wanted a process that was understandable, and had a visible scope from the outside. We wanted something that moved power back to our developers, who are awesome and care a great deal about their work. Developers understand best what they’re building, and they know where the risk is. We use Slack to keep all our discussions in the open and give important stakeholders a voice. . With all of this in mind, we created goSDL, a tool that brings all these concepts together, enabling our developers to produce secure features at high output with low friction. The tool (which you can find here:  https://github.com/slackhq/goSDL ) is a web application that guides anyone involved with a new feature, like developers or PMs, through questions and checklists to improve the security posture of whatever they’re working on. The name is derived from the process of initiating a feature review — a developer uses a  slash command  in Slack, ‘/go sdl’, to begin the SDL process (the app is written in PHP, not Go  ). . The process starts out with some simple questions. Based upon the user’s responses, an initial risk ranking can be easily determined. . The Initial Risk Assessment allows a team to estimate their feature or product’s risk before needing involvement from the security team. Developers have insight into their own codebase; they know if the code or feature might impact a sensitive part of the code, such as authentication or authorization functionality. The risk ranking helps inform the level of involvement needed from product security. . After the initial risk level is determined there are further questions specific to the components of the feature being developed. The Component Survey allows the developers creating the software to expertly scope the survey to their needs. By default, it gives an array of “opt-in” content, allowing the questions to be concise and specifically tailored to the team’s work. Components use a plugin-based architecture not specific to Slack’s infrastructure. This approach allows generalization of questions and content, and easy extensibility. If support for a new component, like a new language, is needed, it’s as simple as adding another JSON plugin. This encourages thinking about security at a higher level than just the code itself. . Each component consists of a primary module, and potential submodules. For example, a feature may include a “WebApp” component. This would be stored as a module, and issues from the  OWASP Top 10  could be included as submodules. We’ve included example modules and submodules  here . . After the Component Survey is completed, the tool generates checklists for the person working through the SDL. Our approach to checklists is partially inspired by their use in preventing aviation accidents. The vast majority of air accidents and crashes are a result of human error. Many accidents could be avoided by properly adhering to checklists. Aviation checklists aim to enforce safety in the skies, and we employ them in a similar manner to enforce security in our code. . Crash investigations attempt to identify causes of accidents and prevent them in the future. We have added the completion of the SDL and associated checklists to our product team’s launch requirements list. We have simplified tasks as much as possible, presenting the items as statements that require little to no prior context to complete, and we acquire feedback via our  bug bounty  program and incident investigations that help us to improve the checklists as we progress. . When the Component Survey portion of goSDL is complete, two JIRA tickets are created for easy task tracking. One item contains the checklist for the team to complete as they build their feature, and one item is assigned to the product security team, allowing us to track our own review of the feature. . We are using the  Checklist for Jira  plugin to enable the checklist custom fields in JIRA tickets, and the  ScriptRunner for Jira  plugin to create custom a REST API which is then used by goSDL to update the checklist field. . Within the JIRA ticket, there will be checklists that are populated based on the responses to the component survey question. When the majority of items are marked as done, we reach out to the development team to address any outstanding unchecked items (and find out if an item was unclear or if the team is still working on it). . As mentioned earlier, two JIRA tickets are created by the tool. The second is assigned to the product security team. Upon its creation, we receive a Slack message informing us of the new review. . This JIRA review task gives us a heads up of incoming features and their risk ratings. This task item contains all the information we need to perform a security review: . This provides the product security team with context as we work with the development team on the review. . To maintain transparent communication, we use multiple Slack channels to discuss features and reviews with developers. Product security has a triage channel in which anyone can consult our team, and every new feature has its own channel (e.g. “#feat-awesome”). This documents progress and builds knowledge as we work through development processes. . These approaches are vital for successful, secure development. We enable development teams to be thoughtful about the security implications of their features, and we provide a process for that via the checklists. We guide developers to build securely through the completion of the SDL. We maintain channels for transparent, quick, and interactive communication; this encourages participation, while also providing a searchable system of record for all future work. . This system has been an invaluable means of gathering feedback about our security processes, as well as our successes and failures. We gain high-level perspective of the software development trends taking place internally at Slack, as well as areas of particular concern to security. Coupled with external feedback (discussed shortly), we have both internal and external feedback which helps us improve. . We have, on average, between five and ten developers reach out to us each day (often before an SDL or feature is even underway). This gives us a degree of assurance that we are maintaining transparency and availability. Sometimes, developers or teams will reach out with questions about one particular item on a checklist. If multiple individuals reach out, we learn that we should tweak that checklist (e.g. to be more clear or more relevant). We have also received suggestions to add or update more related content for components or checklists. We then create or update content, allowing other teams to quickly benefit from that feedback. . Components and checklists are easy to modify or expand. They are all JSON descriptions, which follow a simple and readable format. We have examples in the Github repository, and adding content is as simple as creating a JSON file. . Developers are smart, and they care about the product. They want to contribute to security, but don’t always have the same security expertise as a more specialized engineer. The checklist approach helps to promote security consideration during the development process, and helps passionate individuals improve their own security knowledge. Here are a few examples of feedback we’ve received from developers about the SDL: . “[The SDL] made me think of security things I normally don’t think about.” . “Having my team see me complete the SDL was useful and made me think through the things I was marking as complete.” . “Upon going over [the SDL], we discovered there were a few things we needed to look at… Wonderful.” . This feedback was unsolicited ( ), and tells us that the process is helping developers as they proceed. . Slack runs a bug bounty program, which allows us to get constant feedback on opportunities for improving our product’s security. The program surfaces some truly clever bugs, and provides an excellent source of evaluation of our security, especially when we find patterns in submissions. We’ve noticed that after we release a new feature, we get more activity on that feature from the bug bounty. This makes the bug bounty an excellent source of external feedback for our SDL implementation. . The number of valid bugs that we receive per month has remained fairly constant, while the number of features released (and SDLs completed) has been gradually increasing. Some months have fewer SDLs than average, but this varies based on when feature work begins and is completed. . When we plot the number of valid bugs found by our bug bounty versus the number of SDLs completed by our engineering team, we see that our ratio of bug bounty bugs to new features is improving. Feature development is accelerating, while our valid bug bounty bug submissions are decelerating. There are other factors at play here, but in general these are desirable trends which point to the success of our SDL process. . First, we owe a big thank you to everyone who has participated in Slack’s ongoing security mission: the developers who build and secure our features, the researchers who submit bugs to our bug bounty, and past and present team members who have helped us build this process! We’ve come a long way, and there is still farther to go. By open-sourcing goSDL, we hope to enable other growing organizations to scale their security. We also hope to learn from their experience; we welcome contributions to the tool, its modules, and its checklists, and are excited to see what pull requests will come in! .  This post is based on a talk from OWASP AppSec USA 2017. You can find a recording of that talk    here   .  .  Additional thanks to Leigh Honeywell, Zach Pritchard and Ari Rubinstein for their work on establishing this process and tool at Slack.  .  We hope you’ve found this information useful. For general information about Security at Slack, please visit    https://slack.com/security   . Also,  we’re hiring ! Slack is used by millions of people every day — we need engineers who want to make that experience as secure and enjoyable as possible.  ", "date": "2018-04-27"}, {"website": "Slack", "title": "Ways we make the Slack iOS app accessible", "author": ["Erica Engle", "Kaya Thomas"], "link": "https://slack.engineering/ways-we-make-the-slack-ios-app-accessible/", "abstract": " Ever needed to  squint  to see some text? How about feeling unsure where a button is because it’s indistinguishable from the background color? Accessible technology means that the product we provide works for  everyone , regardless of what their abilities may be. We want Slack to be the place where work happens and this is one of the major considerations we think about in our development. . In order to create an accessible application, we must break down the status quo of what we deem standard for sizes, colorings, and more. Making a mobile app accessible means providing options that let a user decide what best suits their needs. For example, we could provide a large font for everyone, but that would frustrate users who may benefit from a smaller size. . We’ve done a lot of things to improve the usability of the iOS app, and we picked five to describe here: . Throughout this post we’ll share the tools, websites, and frameworks we use while developing new accessibility features and improving existing infrastructure along the way. . Animated GIFs and emojis are extremely popular in Slack, and help to create rich communication with your teammates. However, for some users, such as those who experience vertigo or epilepsy, seeing these animations can be a jarring or uncomfortable experience, and we wanted to let them disable animations. Also, some users just don’t want to be distracted with animations in their message view. So, how do we tone down the :party_parrot: party? . When a preference is set to disable animations, we post an NSNotification that informs any observers, such as the main chat view, to clear out the current text so we can replace animated emojis and GIFs with the static representation of that emoji or GIF. To create this asset we reference the  FLAnimatedImage framework , using the FLAnimatedImage.posterImage object to check if the preference to freeze animations is on and then passing the contents of the photo data to the poster image to create the static element. (The FLAnimatedImage is valuable for more than accessibility — it’s also what we use to make sure images  are  animated when they should be). . We want users to only have to enable this setting once to have a consistent experience across all their Slack apps. To sync between clients, we use our real-time messaging websocket connection to broadcast changes. Upon receiving the updates, we compare the key stored in NSUserDefaults to the updated preference, and then either update or maintain the state. This works in both directions for the clients, from device to desktop and vice versa. . Squinting while looking at text is annoying — we want to help with that. Whether a user has near or farsighted vision, we’ve got them covered with our font adjustments. . To help with these and other visual impairment issues, users can adjust the scale within the chat view of the app to determine which size is comfortable while watching a sample message dynamically update with each tick. . After a user chooses a setting, we need to update the default scale throughout the app. The flow goes like this: . We use our layout helper that monitors all of our screen scaling, font sizing, and phone-size-dependent boundaries to check if this value has been updated across the app and adjust it as necessary so that the font size is maintained no matter which workspace the user is on. Then if VoiceOver is running, we read the new size aloud (more detail on VoiceOver in the following section). Once the notification about the change has posted we update any areas that contain text, including the main chat view, files view, and text processing manager. . While we do our own scaling within the app, we also want to maintain any iOS settings that a user has customized. In the iOS settings there’s an option to enable Larger Text; if you enable this for your device, and the app is built to support Dynamic Type, the font adjusts in the message view. We’ve taken care to support Dynamic Type in all the areas we perform our own font manipulation, respecting font style by using Apple’s UIFontDescriptor and updating based on the UIContentSizeCategoryDidChangeNotification;The necessary changes are magically (well, programmatically) made to the UI, working with the font chosen first by the OS and second enhanced by our font sizing. . We’re still working to support font changes across the entire app. Currently, menus and sign in don’t support font changes. The sign in views are especially challenging, since we don’t know your Slack-specific settings before you log-in, and need to rely on device settings. Stay tuned for future updates! . One of iOS’s built-in accessibility offerings is VoiceOver, a screenreader that can not only read text but also describe functionality aloud to a user. We use  Apple’s Accessibility API  to ensure users can navigate our app with VoiceOver. VoiceOver knows how to interact with iOS’s default elements and views, but for custom ones – of which Slack’s app has many – it needs help in the form of labels (for descriptions), traits (what type of element, such as a button, link, static text, or image), and values. For example, if there’s a slider on the screen, the label would be “Sound,” the trait would be UIAccessibilityTraitAdjustable, and the value could be “40%”. Traits are what VoiceOver uses to let the user know what element they’re currently focused on and what type of interaction is possible for that element; UIAccessibilityConstants provides a list of all the different traits that are possible to set for elements in your views. . On an element such as a button we can set both an accessibilityLabel, with an NSLocalizedString with the title of the button, and a UIAccessibilityTraits attribute, which in this case we would want to beUIAccessibilityTraitButton. . Each emoji in the picker is a button that can be selected to react to a message. The label is the name of the emoji — for example, the following emoji’s alias is :smile: — and the hint allows the VoiceOver user to know what will happen when they select the emoji. . Our designers here at Slack think deeply about color to make sure text on a background is legible, buttons are easily discovered, and objects can be differentiated. Since all of the interfaces need their own palettes, like our channel header shown above, they crafted a makeover that maintains a more uniform color set and differentiation between items. The designers audited all the colors used in the apps and reduced them to a more manageable number so that they were more consistent and to ensure proper contrast was always possible, while still keeping enough colors for each feature’s design vision. . The correct contrast ratios allow a user to distinguish text on a background, and with these new colors we’re able to use a higher ratio for better separation between elements. . To calculate a contrast ratio, we take the designed assets and enter them into color analyzing tools. Some websites our designers use for calculating the contrast ratio for your images can be found here: . Contrast ratios 4.5:1 and above are required by the Web Content Accessibility Guidelines 2.0 Level AA; we try to target a contrast ratio of 7:1, which meets  Level AAA standards  for normal text. . For example, timestamps in messages used to use  #ACADB1 , which had a contrast ratio of 2.2:1 when compared to the white background. We changed it to  #717274 , which has a much higher ratio of 4.8:1. . If you look at the before and after of the Message Input above (as well as the Channel Header at the beginning of this section), you can see the improvement in readability of the placeholder text: “Message #random” is almost totally transparent before, but after our changes the “+” matches the input’s borders and is more uniform with the rest of the Message Input element. The emoji button is also easier on the eye, and you can definitely tell you’re tapping a button instead of a smiling ghost on the screen. Rather than looking like several different components of different colors, the simplified palette makes the Message Input a singular object that has an improved contrast to the background. . The design team here at Slack has a huge impact on how we develop with accessibility in mind, and we are thankful for their insight to make sure we can reach these standards. . Accessibility elements have a bonus value for us as they’re great for testing. Since we can simulate different flows such as message posting with our end-to-end tests, we can use these identifiers to ensure each step is progressing as it should. Is the text in the message input matching the text that was posted? Can we identify the link a user should be able to tap on? Where is the username? We can use these elements because they change less often than other identifiers, and if a trait changes or text is in the wrong place, the tests will catch it instantly. . We use an automated testing library called  Kif  to implement our functional tests. It allows us to write UI tests that can be launched directly on the view controller without the overhead of having to log into the app, navigate to the view controller, and then the test. Usually functional UI tests are run in the UI test target, XCUITest. Unfortunately XCUITest comes with a lot of overhead so one of the reasons we chose Kif is because it’s built on top of XCTest, the unit testing target, which makes running tests much faster. . Here’s an example of a test that looks at the cells in our emoji picker view and ensures that the search bar and certain emojis are labeled correctly. . From the start of a project to feature complete, we make accessibility a key factor in our decision making. Promoting a culture of user diversity awareness helps foster creative and empathetic product thinking. . These enhancements don’t just benefit our users; they also assist our fellow coworkers and developers as accessibility standards set the bar for quality and maintainability within the user interface. . Apple integrates a great deal of tooling into their developer utilities that enables engineers to make sure their own products are compliant with accessibility standards. However, it’s up to the developer to expand functionality beyond these scopes if necessary, as we’ve done with tweaking font sizing and disabling animations across our clients. . Here is a list of some tools and tips on how to improve accessibility for your app: . At Slack, we continue to better understand how our users interact with our app in the way that best suits their abilities. If you’re interested in working with us on this, we would love to have you  join ! ", "date": "2018-06-19"}, {"website": "Slack", "title": "Streamlining Your Workflow: Debugging for HHVM", "author": ["Amy Shan"], "link": "https://slack.engineering/streamlining-your-workflow-debugging-for-hhvm/", "abstract": " Digging through a large codebase to locate a bug can feel incredibly overwhelming, especially for a junior developer. While some might argue that well-placed print statements are just as efficient, getting to know the ins and outs of a debugging interface can save a significant amount of head-scratching and help you navigate tricky bugs more efficiently. . I’m currently a rising senior at UC Berkeley working as an intern on the Slack Platform Services team, where we aim to improve the experience for those developing apps on Slack through our web and events APIs. Many of my features have little to no visual component, and the variability of user input is high — this is the nature of working on APIs. As such, writing thorough unit tests is critical to each feature’s success, but that also means much of my time is spent debugging code. . Here at Slack, we use  Hack  on  HHVM , which is especially useful when writing strongly-typed code to support variable payload structures in API calls. What follows is a guide to fully utilizing some key debugging tools for HHVM in VSCode that I’ve found most helpful in my day-to-day workflow. This advice is extensible to most other debuggers and languages as well. Regardless of your own personal development workflow, using a debugging interface will help you save time and code smart. . Recently, I was working on adding a new event type through the  Slack Events API  that would allow apps and bots to be notified when a private channel is deleted. I created a unit test to check that users_filter_by_channel_membership would return the correct users to receive the notification — in this case, the users who were present in the private channel at the time it was deleted. The seemingly straightforward test failed after I pushed it to our Continuous Integration environment. Luckily, I was able to use the debugger to quickly identify and fix the problem. In the sections that follow, I’ll walk through the most useful aspects of the VSCode HHVM debugger that helped me in this process. .  Breakpoints  are the backbone of debugging. Setting a breakpoint on the line a test fails allows you to pause execution at that moment and utilize the debugging toolbar to step into the function and explore its behavior more closely. . In this example, I set the breakpoint on line 73 by clicking the margin to the left of the line number, where the assertion fails. The Breakpoints menu shows a list view of all the breakpoints set across the codebase and allows you to control the order of execution for your breakpoints by toggling them on and off. Once you’ve set your breakpoints, you can take advantage of the other tools below. . Most of the debugging you’ll need right away can be accomplished by inspecting the  Variables  menu. At any point in the execution of your debugger, you can view a dropdown of the variables in scope with their respective values. . Here, you can see that $ret[‘user_ids’] is empty, but the expected return should include the user_id corresponding to User 2. This is the most comprehensive ad hoc view of variables, but if you want to single out specific variables, the Watch or Debug Console REPL provide a more targeted approach to monitoring values. . Right beneath the Variables menu is  Watch , a space for you to define a list of variables or expressions and watch their values changes through multiple lines of execution. If you know your test is failing because there’s a mismatch in the expected and actual value of a variable, adding its expression to the Watch section lets you keep an eye on it through multiple runs of your unit test. That way, you’ll know exactly which line the variable receives an incorrect value. . In this case, we care about the discrepancy in $ret[‘user_ids’] so we want to keep track of the relevant user_ids and dig deeper to see why 1005 is not being returned despite User 2 being a member of the private channel when it was deleted. . The  Debug Console REPL  provides an interactive way for you to evaluate expressions beyond what the Debug sidebar can offer. REPL stands for read-eval-print loop and is a simple interface that can take in single user inputs (an expression) and evaluate them. You can still see variable values by just typing in $variable_name, but you can also compose SQL queries to view the state of the database before and after a line of code. . Here, I ran a SQL query against the channels_members table that keeps track of the relationship between users and their channel memberships to see why both User 1 and User 2 were being returned by users_filter_by_channel_membership. As it turns out, both User 1 and User 2 have a timestamp value for their date_deleted attribute. users_filter_by_channel_membership only returns users currently present in the channel (or date_deleted = 0), which explains why User 2 is not being returned! . Lastly, we have the  Call Stack , which shows the path your code takes all the way up to the line that’s currently executing. This tool is especially helpful when you’re navigating a large codebase and aren’t familiar with all the call sites for any given function. Use this stack as your guide as you follow the test’s execution, saving you from having to place print statements in all the possible call sites the function could have been called from. While it’s not being used in this example, it was immensely helpful in helping me pinpoint that users_filter_by_channel_membership was the culprit for why private channel deletion events weren’t being sent to the proper users. . This unit test was set up to mimic the way event dispatches occur in the existing implementation, where the user memberships are first deleted alongside the private channel before users_filter_by_channel_membership attempts to find everyone in the channel who should receive the dispatch. Because the SQL query constructed within users_filter_by_channel_membership did not consider the scenario in which we might be searching for already deleted users, I had to add an additional argument called $date_deleted that would allow me to specify a time range within which the user was deleted. . Getting all these tools to work together, especially if you’re working in an environment or language that isn’t popularly supported, can be tricky to get right. If you use Hack, you’re in luck! Slack uses synced cloud environments for local development, and the HHVM debugger for VSCode was originally developed to be compatible with this workflow. It also easily hooks up to local unit tests which don’t have external service dependencies. . To get started,  install the Hack extension for VSCode . I’ve found that version 3.25 of HHVM or higher works best with this extension. Create or open the existing .vscode/launch.json file and include the following (with some tweaking to make the paths specific to your workspace) in your configuration: . There are other options like  Nuclide  and Facebook’s  command line debugger , but I found that the VSCode extension was the easiest to integrate into my existing workflow. Additionally, the HHVM debugger for VSCode has ongoing community support plus resources to help you  get started . . Whether you’re a new grad or a senior engineer, learning how to take full advantage of the information debuggers provide will bring clarity to failed tests and help you fix bugs more efficiently. If you want to put the skills you learned in this article to the test,  come work with us ! ", "date": "2018-07-25"}, {"website": "Slack", "title": "Unified Cross-Platform Performance Metrics", "author": ["Liang Shi"], "link": "https://slack.engineering/unified-cross-platform-performance-metrics/", "abstract": " With the fast pace of life today, people expect information to be available to them at the speed of light, regardless of which device or apps they’re using. Through the power of social media, their experiences — both good and bad — are instantly shared with the world, and many of those tweets, comments, and reviews are about apps being slow. As an app developer looking at this feedback, I ask myself: . To answer all these questions, performance metrics are essential to translate subjective impressions to quantitative measurements that app developers can continuously monitor and improve with each iteration. . In order to reflect the most frequent user actions and their perception of the app’s responsiveness, we picked a set of performance metrics to capture users’ experience, including how long it takes to launch the app, how long it takes to switch to a channel, and the frame rates as they navigate the app. We transformed formerly distinct performance indicators on each client app to a set of unified metrics. We will discuss  WHY  we need cross-platform metrics, with a deep dive into  HOW  we implemented the launch time metrics, and  WHAT  we learned along the journey. . Due to the fast-paced nature of Slack iterations, we were tracking performance separately for each client platform. On Desktop, Android, and iOS, there were different metrics vocabularies and definitions. For instance, we measured app launch time on both iOS and Android, but we called it cold_start_time_authenticated on iOS and cold_start on Android. The launch times were tracked with various start and end points. Each platform sent distinct types of properties in the metadata. This made it complicated to understand what exactly was included in the performance metrics and made it hard to make apples-to-apples comparisons. . Additionally, each platform was gathering ad-hoc performance logs along the way — some out of sync and not maintained anymore — resulting in multiple metrics tracking the performance of similar processes with slight variance. For data analysts and product managers, it created a maze of metrics that was difficult to navigate, with no guarantee they were looking at the right performance stats. They also had to keep track of the list of exceptions and edge cases on each platform to be aligned with a shared goal. . We need common metrics vocabularies and definitions across platforms for many reasons: . Internally, we want to be able to collect accurate results to help make business and product decisions. They provide us with aligned benchmarks and objectives for measurable goals. We also want them to be the north stars for performance targets that help us understand the impact of product changes on app performance. . Externally, our customers are cross-platform. They start the morning by checking notifications on the phone, come into the office and work on the Desktop app during the day, and finish a couple of messages on mobile apps on their commute home. It’s crucial to provide a consistently good app performance for them despite the devices or platforms they use. We need the metrics to be reliable, so we can show customers the performance status and trend of our apps across devices, to identify the focus area of their expectation, and to demonstrate the progress of continuous improvements. . For the metrics to be clearly defined and easily actionable, we need a clear protocol for the platforms to follow. We defined a shared interface for a batch of cross-platform performance metrics, including launch time, channel switch time, and frame rate stats. We use  Thrift  for the performance metrics specification, which provides us with a unified and language-neutral interface with strongly typed metadata. For the next iteration, we are exploring options towards using  Protocol Buffers , which have better support on Swift. The specification serves as an agreement on the structure and data types of the metrics and their parameters. It makes data aggregation, formatting, and monitoring simple for the data analysts. A single data pipeline not only prevents repeating the work for each platform but also makes it possible to compare the performance data on the Desktop, iOS, and Android side by side. We can easily slice and investigate the stats to generate insights and encourage cross-team collaboration in performance improvements. . In brief, there are three key advantages to having unified cross-platform metrics: . Now let’s look at a case study of the app launch time and how we benefited from united performance metrics across Desktop, iOS, and Android client apps. . First impressions matter when it comes to both people and apps. It’s essential to bring the most relevant and updated information to the user during each app launch. To ensure this, we needed to accurately track the current launch time for users on various devices. . How do we use cross-platform metrics to help us tackle launch time tracking? Each platform has different launch processes and implementations of network requests, data persistence, and UI rendering during app launch. We needed these metrics to echo the perspective of the users, regardless of their platform, to help us meet their expectations on app launch speed. We identified two metrics in the launch process that apply to all the platforms:  Time to Visible (TTV)  and  Time to Usable (TTU) . . Time to Visible (TTV) is defined as the time from app launch to when locally cached content is displayed. From a Slack user’s point of view, it’s the time taken to show the first meaningful content  —  usually messages  —  so they can start from where they left off last time. Similar to the notion of  First Meaningful Paint  for web browsers, we wanted to capture the user experience of how fast they can start reading actual content in Slack, which captures their perception of the app launch speed. . For TTV, the app client has full control of this portion of app launch time. As cached data is available locally, the time to reach TTV is unaffected by backend or network performance. Even if the user is offline during app launch, the app is still able to render what was loaded before. For our users, they will see the conversation from where they left off. However, they might not be looking at the most recent content yet. . Time to Usable (TTU) is defined as the time from app launch to the moment when the  most recent  content has been displayed on the screen. At this point, all content from the server is up to date, and the latest messages are available for the user to make decisions and contribute to the discussion. . The time to reach TTU could be affected by network condition, API delay, response size, etc. However, TTU reflects user experience when they are informed by the most recent messages and can take actions based on what they see, hence the term ‘Usable’. We don’t explicitly check whether the UI is interactive since the user might not choose to scroll or type messages right away. Since the content is up-to-date and all rendered on screen, the interactions are implied. . The example above is recorded on an iPhone 5S device with a slow network connection to demonstrate TTV and TTU. We kick off the request to fetch new content as soon as possible, so the delay between visible and usable is usually shorter. It’s tested on a slower device with high network delay to show visible gaps with UI refresh in each step: . It seems simple to track the launch process from the steps above. However, it’s quite complex with multiple factors involved, and we often run into special cases. We want to capture most of these special cases to be able to diagnose potential tracking issues and detect metrics regressions with accuracy and confidence. The scenarios are identified by client apps and marked explicitly in the metadata. We don’t exclude any launch cases, deferring the decision for the data pipeline to split or filter out stats from each case. Here are some examples of special cases we have logged for TTV/TTU: . It’s possible to launch the application to a channel for which there is no cached content (e.g. a fresh install, or opening channels after resetting the cache), in which case a network request is required to populate the conversation. In this case, TTV is the same as TTU since we always need to ping server to download the data. . On mobile, fewer than 1% of sessions have no cached content, but on the Desktop it’s 100% due to no local cache. . It’s possible after pinging the server that the current channel is already in sync, meaning there is no more content to be downloaded. To the end-user, there won’t be an update of the UI after TTV. In this case, we still include the time of the request in TTU, but mark it with a flag. . On mobile, 90% of sessions has no new content to be synced, while all desktop sessions need to pull content from server. . The timing from app launch to TTV/TTU can get interrupted by user actions or the operating system, e.g. the user could decide to switch to another channel, receive a phone call, or lock their phone screen. We flag several categories of user interventions to be able to identify the app launch path. . We want to capture the experience of users waiting for the app to load. If they choose to move away from the message view, the landing channel’s visible and usable time won’t be perceived at launch. These cases need to be marked to be able to filter out or split to avoid introducing noise to the launch time stats. . We track TTV/TTU for both cold and warm launch. A cold launch includes app launch for the first time, after a device reboot, or when the app has been terminated by the OS, meaning the app process does not exist in the system’s kernel buffer cache. A warm launch means that the app resumes from the background or becomes active in the foreground from the existing instance of the app in the memory. . Many actions could trigger the app launch. We tag each launch accordingly: . If the launch happens after an app or app database upgrade, higher TTV and TTU may occur due to the clean local cache. We mark this case so it’s not confused with an actual regression on launch time. . All of the special cases above are logged as common properties for all clients. Despite the difference between platforms and implementation, we track them with the same parameters in the metadata, making it easy to spot any outliers by looking at the distribution of these parameters on an individual platform under the same data pipeline. For instance, we’ve noticed about 8% of iOS launches are triggered by a notification, while that amount is around 21% on Android. One possible reason is the notification area on top of the screen of Android indicating unread notifications, while on iOS there is no lingering indicator after a notification goes away. Comparing the range and distribution of the stats helps the app engineers to identify causes of performance issues and track the process of improvements with common data infrastructure. . Cross-platform metrics measurement requires an extensive collaboration of developers from multiple clients, backend, and data teams. We’ve faced many challenges during the process: . Here are some lessons we’ve learned so far from tackling these challenges. . Working backward from mock analytics and a mock dashboard is super helpful to define the data format and to decide upon a reasonable range of stats. Have an expectation of what is normal and what is not. For example: What range should the metrics stats fall in? What possible values should metadata have? What should the distribution of metadata values be? If anything falls out of range, it could indicate either a tracking error or an actual performance problem to be addressed. . Since this work involves multiple teams on several metrics over a relatively extended period, it’s important to have directly responsible individuals leading the decision-making process. Try to pair up developers from each platform to work on the same metrics at the same time, and minimize the chances of developers joining or leaving the project as this can introduce friction. . It’s essential to track the metrics updates as soon as changes are deployed to tighten the feedback loop. This process will be much smoother with investment in real-time debugging tools on all platforms; it’s impractical to wait for 2–4 weeks for production data to verify the validity of data, especially for the mobile release cycle. You need to be able to detect tracking errors and performance trends both locally and on dogfood or beta builds. . There will be knowledge gaps for client developers to understand how other platforms work: how data is sent, formatted and stored in the data warehouse and the most efficient way to organize, query data and set up dashboards. It’s beneficial to encourage and coordinate knowledge sharing to get on the same page and avoid surprises down the road. . With unified cross-platform performance metrics, application developers can set shared goals for a consistent end-user experience on all client apps. . This isn’t the end of the story for metrics measurement improvements, though; we’re working on automated performance testing and regression detection, along with adding more granularity to metrics during app sessions and on specific user actions. It’s just the beginning for the cross-platform performance metrics to mirror our users’ experience and help us make Slack apps faster. .  Interested in the teamwork described in this article? Stay tuned to our    Engineering blog    for more information of what’s next on performance measurement and optimization. Or even better:    Come and join us   !  ", "date": "2018-07-31"}, {"website": "Slack", "title": "Maximum Warp: Building Migrations for Slack Enterprise Grid", "author": ["Eric Vierhaus", "Todd Wirth"], "link": "https://slack.engineering/maximum-warp-building-migrations-for-slack-enterprise-grid/", "abstract": " Slack Enterprise Grid  lifted off in January 2017 , allowing Slack to power the work behind even the largest and most complex companies in the world. To achieve this, our new product allows administrators to link multiple Slack teams together under one organization. When we set out to build the Enterprise product back in 2015, it was clear we’d need an entirely new data model while continuing to support all of our pre-existing teams. . Slack’s data model is based around collocating all the data for a Slack team on the same MySQL database shard. This works well for spreading load throughout the fleet, but presents challenges for data that transcends the boundary of a single Slack team. During initial team creation we generate a unique 64-bit unsigned integer that’s assigned to the team and provides a means to hash all team-centric data to the same location. . With Slack Enterprise Grid you can connect many — even thousands — of Slack teams together and communicate across those teams with shared channels. How do we fit the team-based database architecture into this design? It turns out, not very easily! . There were a number of problems we needed to address: . We got to work. . How do “cross-team” channels live in a team-centric database model? Making this as seamless as possible meant we needed a new data model for Slack Enterprise Grid. We chose to assign the Organization itself — the “umbrella” around all the various teams — to its own database shard. Since all teams in an organization store a pointer to this “parent” organization we could easily redirect to the team’s local database shard or the organization shard depending on the type of data we wanted to access. Data stored on the organization’s shard is hashed with the organization’s 64-bit ID, just like data stored on the local team shards. . Once we decided that data related to the organization would reside in a different physical place, we had two tasks: sending new messages and related data to the right DB host, and migrating all the old data to its new home. Any team IDs stored in database fields or within flat data blobs would need to be translated to the organization ID. . Additionally, in Enterprise Grid all user records are assigned new organization wide 64-bit IDs as well so that each user has a single ID value regardless which team they access. Any references to old user IDs within the migrated data must be translated to the new ID as well. Since  Direct Messages and Group Direct Messages  (DMs and Groups DMs, respectively) needed to be available regardless of which team you log into within the organization, this channel data needed to be moved from the team database shard to the organization database shard. . Looking ahead, we knew all current and future features in Slack might need to go through this transformation process, so we wanted to build a robust and flexible framework. Instead of a set of one-off scripts we designed a generic set of data  handlers  that each exposed three separate interfaces: validation, transformation and insertion. . We set out to write data handlers for each piece of Slack data that needed migration. Through a careful audit of our backend code, we discovered quite a lot of Slack primitives, including: files, channels, users, custom emoji and other channel-related data. We created a generic interface to each migration type containing sub-data handlers. Then when a new data type is added the developer need only “fill the blanks” for the handler. For example, when migrating custom emoji a developer adds a data handler to validate, transform, and insert the new emoji data on the new shard. . The data handlers, including the master handler, use our asynchronous job queue system (we’ll get more into this in “The Bridge” section). Each and every handler is idempotent, ensuring that if an error occurs during migration we can safely restart an individual handler without fear of data loss or corruption. . After tackling the initial problem of migrating the data, we had to make it scale. How do we migrate legacy data in the new Enterprise model both safely and quickly? Some of our customers had millions of files and messages that needed to be migrated. As we expected, our application code ran much faster than our database hosts could reasonably sustain. . There are many performance characteristics of databases, and we focused overall on CPU utilization, replication lag, thread count and iowait. Although CPU utilization  can be misleading , it provided a decent picture of how much load we were placing on the database. . How do we keep CPU utilization and replication lag down? Enter rate limiting! Slack uses a custom rate limiting system (that’s worthy of its own future blog post) to keep high volume  external  requests from overwhelming Slack’s servers. During Grid migration testing we observed that our own  internal  requests had the potential to overwhelm our systems as well. So we repurposed our rate limiting system to work with Grid migrations, allowing us to control our migration process so that they move at a steady, safe rate. This also kept CPU utilization and replication lag within acceptable limits. . The flexibility of our rate limit system also allowed us to build an internal GUI for our amazing Customer Experience team to manage migration speeds without needing an engineer available. . For deferring work outside web requests, we have an asynchronous task queue which we call our “job queue” system. This is a set of dedicated hosts and application interfaces that allow engineers to enqueue tasks that can run in parallel and outside the bounds of a single HTTP request. Our job queue allows us to offload lengthy operations to specialized, dedicated hosts. When we began working on our Grid migration process, the job queue system was the obvious choice to perform the lengthy data migrations we needed. We worked with our operations team to create a new queue for Grid migrations utilizing a dedicated set of hosts. An isolated queue and worker pool was an ideal choice, so that we could monitor and scale the systems as needed, without affecting the latency or throughput of other jobs. . As part of Grid migrations, we added new capabilities to our job queue system to allow finer grained monitoring. We created a  job queue inspector  and used it to monitor and observe jobs as they progress through the job queue system. The inspector allows us to “tag” our migration tasks with a unique value, or values, which we can later use to observe jobs of a given tag or tags. . Tagging job tasks with a unique name allows us to observe the state of all migration tasks for a specific tag name or multiple named tags easily. We use this process to monitor the total work to be completed for each migration and to raise an alert if any of these tasks fail to complete successfully. The job queue inspector then powers our real-time progress bot and allows us to monitor the migration, moving automatically from one phase to the next. Building this system proved to be a crucial component of Grid migrations and other engineering teams have started employing the same application logic to monitor and observe their tasks in similar ways. . The job queue inspector has allowed us to run Grid migrations on full auto-pilot without the need for human operators. If an engineer wants to observe the process in real time, we built a special internal user interface to monitor: . The job queue inspector gave us the visibility into our Grid migrations process so that we could build automated tools to alert and inform our own teams of their progress. . Not long after our initial beta customers joined their first Grid organizations, we realized that we wanted — and very much needed — a rich set of tools to inform our own teams about the migration process from start to finish. With that in mind, our first thought was to create our own custom integrations that would share the scheduling, approval, migration and validation processes of our Grid migrations within public channels, for Slack employees to monitor and (of course) search. In fact, one of the aspects that makes working at Slack so great is that there are literally millions of messages and thousands of channels, cataloging the entirety of our company’s history, at each of our fingertips at any moment within Slack. We have a culture of transparency and openness, so we posted these messages in public channels, keeping the appropriate people and departments informed along the way. . We used Slack’s APIs and rich  message attachment  features to build and send real-time messages to a series of channels. This is an example of a message we sent when a new Grid migration was scheduled by one of our Customer Success Managers using our  chat.postMessage  API method: . One of the most beloved integrations has become our real-time migration progress bot which monitors and reports on the data that we have migrated during the Grid migration process. After the first message is sent, we use our  chat.update  API to update the existing message with updated progress every minute: . These automated alerts have allowed our Sales and Customer Success teams to inform customers of any changes made during scheduling and provide minute-by-minute progress reports during the migration process itself. . At Slack we use  collectd  and StatsD compatible services throughout our ecosystem to record numerous types of host and application metrics quickly and easily. During development of our project, we added small bits of application logic to record and observe counting and timing data around important and interesting parts of our project code. Our application framework provides a few simple interfaces for collecting these types of metrics. . The collected host and application metric data is pushed through our data pipelines, aggregated and made available to all engineers and staff via a  Graphite  cluster. For our Grid migrations project, and most projects at Slack, we built a custom  Grafana  dashboard with both the metrics we recorded for our feature and other useful metrics pulled from the hosts which would execute our application code. . Visualizing the system in real time is often the best way to spot and diagnose application issues and performance bottlenecks before they become a customer issue. We use these tools on a daily basis to monitor the health and status of these systems and to optimize our code accordingly. Here’s an example of one such dashboard we created for our Grid migrations: . Building the tools and systems to track our Grid migrations from start to finish has allowed us to build confidence, identify issues and continue to improve the process of migrating our customers to Enterprise Grid. . After iterating on the above systems on many test migrations, we took the plunge and migrated Slack’s own internal teams to Grid. We perfected the systems and added automatic data validation and consistency checks post-migration. Since late 2016, we’ve successfully migrated many of Slack’s largest customers to Grid, all with minimal downtime and no data loss. Making Grid migrations happen continues to be a team effort — many thanks go out to the hard working folks on the Slack Enterprise team for pulling this off. You can find more information about Slack Enterprise Grid and the customers using it at  https://slack.com/enterprise . .  \t\t\t\tCan you help Slack solve tough problems and join our growing team? Check out all our engineering jobs and apply today!\t\t\t\t Apply now  \t\t\t ", "date": "2017-07-31"}, {"website": "Slack", "title": "Evolving API Pagination at Slack", "author": ["Michael Hahn"], "link": "https://slack.engineering/evolving-api-pagination-at-slack/", "abstract": " At Slack, the size and scope of the data we expose via our APIs has changed dramatically since the product first launched. Endpoints that were designed around the expectation that they would, in the most extreme cases, return several hundred records, are now returning hundreds of thousands of records. To handle this rapid growth, we’ve had to rethink how we paginate data — from no pagination, to offset pagination, to a new cursor-based pagination scheme. In this post, we’ll go over the pros and cons of various pagination strategies and discuss how these influenced our latest pagination scheme. . There isn’t a single right answer to how you should paginate your data. The answer depends on the type of application you’re building, how fast your dataset is growing, and the experience you’re trying to deliver to the end user. At Slack, our primary concern is giving developers the ability to retrieve the entire contents of a specific dataset. For example, a task management application may periodically call  users.list  to retrieve all of the users on a team and enable assigning a task to a co-worker. Another application may use  channels.history  to retrieve all of the messages in a channel to run through sentiment analysis. . These use cases define our requirements for a pagination scheme: . With these requirements in mind, let’s look at various types of pagination that are popular in the community. . Using offsets to paginate data is one of the most widely used techniques for pagination. Clients provide a parameter indicating the number of results they want per page, count, and a parameter indicating the page number they want to return results for, page. The server then uses these parameters to calculate the results for the specific page being requested. Implementing this with an SQL-like database is very straight forward: . The response from the server would be: . Where total is the total number of items on all pages, page is the current page and pages is the total number of pages available. . This type of pagination has several advantages: . However, there are several drawbacks that don’t make this strategy a fit for our pagination needs: . Cursor-based pagination works by returning a pointer to a specific item in the dataset. On subsequent requests, the server returns results after the given pointer. This method addresses the drawbacks of using offset pagination, but does so by making certain trade offs: . To understand these tradeoffs, let’s look at how we would implement this for the users.list example: . Let’s assume we want to paginate from the most recent user, to the oldest user. For the first request, we’d select the first page: . Where limit is equal to count plus one, to fetch one more result than the count specified by the client. The extra result isn’t returned in the result set, but we use the ID of the value as the next_cursor. . The response from the server would be: . The client would then provide next_cursor as cursor in the second request: . Again, limit is equal to count plus one. By setting the limit to one more than the count requested by the client, we’ll know we’re at the last page when the number of rows returned is less than count. At that point, we’ll return an empty next_cursor which tells the client there are no more pages to be fetched. . The response from the server would be: . With this, we’ve addressed the drawbacks of offset based pagination: . Even with its trade-offs, cursor-based pagination meets all of the requirements we outlined for our pagination strategy. .  Relay  is a framework for retrieving and caching data from a GraphQL server for a React application. It handles pagination in a really interesting way by allowing the client to paginate forward and backwards from any item returned in the collection. . Relay enables this pagination by defining a  specification  for how a GraphQL server should expose lists of data called Relay Cursor Connections. . The spec outlines the following components: . Using all of these together in a GraphQL query for listing users on a team might look like this: . While the above might seem verbose, it allows the server to implement a robust pagination scheme: . Unlike the previous pagination types we went over, Relay Cursor Connections don’t enforce an underlying database pagination scheme. In fact, because the cursor is an encoded value, we could implement both the offset and cursor-based strategies we defined above with the Relay semantics. Unfortunately, we wouldn’t be able to adhere to the full Relay Cursor Connections spec in a backwards compatible way, but the idea of granting the server flexibility by using an encoded cursor is something we wanted to replicate in our system. . After evaluating the pros and cons of the various types of pagination, we settled on a new cursor-based strategy with some similar principles to Relay. . Requests that support this new pagination will take two new fields: . Responses will leverage our existing  response metadata  and include a new field: . While this is less robust than the Relay model, it focuses on doing one thing well: forward pagination through an entire dataset in a backwards compatible way. . The piece that we’re most excited about in this new scheme is the flexibility we now have by using an opaque string value as the cursor. Similar to the Relay implementation, we’re using Base64 to encode information on how to retrieve the next set of results within the cursor. This allows us to technically implement different underlying pagination schemes per endpoint, while giving a consistent interface to consumers of the API. . For example, if we look at a truncated response from the users.list endpoint which now uses this new scheme: . The client would take the next_cursor value of dXNlcjpXMDdRQ1JQQTQ= and pass that as the cursor argument in a subsequent request to retrieve the next set of results. On the server side, the cursor will be decoded to: user:W07QCRPA4. The server then knows to use the user id of W07QCRPA4 as the starting point to fetch the next set of results. . At the same time, we have existing endpoints like  files.list  which are using offset pagination. If we wanted to add support for our new cursor strategy, while still supporting the existing offset pagination, we could do so without changing the underlying data fetching logic. In this case, we would just return cursors which encode the offset value. For example, the next_cursor after fetching the first 10 files could simply be the encoded version of offset:10, or b2Zmc2VoOjEw. This makes it trivial for us to add support for the new scheme to existing endpoints that are already implemented in a specific way. . Both the offset and cursor-based pagination strategies we discussed assumed that the data we were paginating through lived within the same table. As the size of our datasets continue to grow, this won’t always be something we can rely on. With this new pagination scheme, we’re able to handle these cases by encoding multiple cursors within a single cursor returned to the client. . For example, teams on  Enterprise Grid  contain information at the organization level on one shard and the team level on another shard. If we wanted to paginate through a dataset that spanned both of those shards, we could return an encoded version of: team_last_id:&lt;last-id-on-team-shard&gt;,org_last_id:&lt;last-id-on-organization-shard&gt;. We can then use those two ids as cursors when querying the respective shards. . We’re looking forward to being able to continue to improve the performance and reliability of our APIs with this new cursor-based pagination scheme. While everyone’s pagination needs are different, we hope this can serve as a reference point for other developers creating or evolving their APIs. As a final note, hindsight is always 20/20, but if you’re wondering whether or not you should paginate an endpoint that returns a list of data, no matter how small the dataset seems, we recommend you do  . . If you want more information on how to develop against the new pagination, check out our  developer documentation . .  Want to help build APIs to power the future of work? Come    join us   !  ", "date": "2017-08-16"}, {"website": "Slack", "title": "Localizing Slack", "author": ["Scott Sandler"], "link": "https://slack.engineering/localizing-slack/", "abstract": " Localization is so easy! .  …said no one ever.  . This week, we launched French, German, and Spanish localization in Slack. I’d like to share some of the lessons learned, tooling, and processes we put in place to complete this project and build localization into our ongoing workflow. . The first step in localizing Slack was to prepare the strings in our codebase for localization. While mobile platforms provide clear frameworks for localization that keep strings separate from code, our web and desktop codebase’s strings were embedded in HTML templates and business logic. We needed to create frameworks for representing strings in each programming language we used, and then touch every string in the code to implement the new helpers. We referred to this as “wrapping strings”, since each string needed to be wrapped in a block or function call. . We chose the  ICU MessageFormat  syntax to represent strings. After comparing it to  gettext , we appreciated the power of its  select  and  plural  blocks to deal with some of our more complex strings, as well as the flexibility and robust implementations for  JavaScript  and  PHP . We created ICU helpers for our templating languages,  Handlebars  and  Smarty . While ICU was more powerful than gettext, it was also not directly supported by any  Translation Management System  (TMS) we could find, which meant we needed to turn off most of the TMS’s built in parsing and validation of translations and do that ourselves. . We debated whether to use named keys for each string in the code (like Android’s strings.xml), or use the English string as the key (like iOS’s NSLocalizedString). We chose the latter to keep the code more readable, using a hash of the English as a key. This made string wrapping much simpler as well, but did mean that changing the English string even slightly would require a re-translation. . Here’s an example of a string before localization from our notifications e-mail template: . The  {t}  block serves two purposes . We needed to do this for about 20,000 strings across 2,000 files, which was a massive undertaking. We decided to ask the entire Web Engineering team to help implement these changes. The i18n team held “string jams” where we kicked off with a presentation on how to wrap strings (Keep complete sentences together! Provide context comments with examples for translators!) and sat with each Engineering team for a couple of days to review their code and answer questions. This served as an excellent training exercise to get everyone familiar with how to write localization-friendly code going forward. We invited attendees to a channel in Slack which proved invaluable for discussions, pull requests, and questions in the following months. . One of the most helpful things at this stage was  pseudolocalization , which accents each character yielding a visibly different but still legible string. Since we didn’t have translations yet, enabling this mode allowed engineers and QA to make sure that every string in a given view was ready for localization and that the locale setting was being properly applied. We implemented this in each language, adding some smarts to keep things like html tags, placeholders, and emoji unmangled. We also added extra tilde (~) characters to each word to make them 35% longer, simulating longer words from other languages to identify inflexible UI elements. . We set ambitious requirements for our launch: Slack should have a consistent voice in each language with high quality translations, localization should be built into the workflow of every team, and all new features should be translated at release. To address the first point, we hired a full-time translation team who wrote a glossary and style guide for each language and worked alongside contractors to translate all of the words. We developed a set of tools and processes in service of the other two points. . Slack has robust  linting  tools to enforce code style conventions and prevent common mistakes. We augmented these tools to validate ICU syntax of each source string (there can be a lot of curly braces to get right), and ensure the correct parameters are passed in. We created checks so that every user-visible HTML element’s strings are wrapped for localization. Coupled with training and code review, these tools help avoid localization regressions during development. . Slack’s web codebase is updated and continuously deployed more than 100 times every day. We built additional tooling to ensure that new feature releases and copy changes won’t cause users to see English strings in an otherwise translated experience. . For feature releases, Slack uses a  feature flag  system, which allows us to get new features into the code early and keeps them disabled for most users with conditional blocks. Features can be enabled in the development environment only, or just for our own Slack team, or rolled out to a percentage of teams. We wrote code to identify the set of conditional statements that each string is inside of in order to determine whether that string would be visible to users in production. . This was particularly challenging as the move to  React  on the frontend carried with it a new syntax, so code to parse if statements had to be written for PHP, Smarty, JavaScript, Handlebars, and React / JSX. However, the benefits are significant. This tooling allows us to add a check so that a feature can’t be enabled for users until all the strings are translated. We also augmented this tooling to build a dashboard displaying all the strings related to a feature, along with their translation status. We created a Slack channel where engineers and PMs can post a link to the dashboard to help translators prioritize and communicate about progress. . Sometimes, we just need to make a small change to an English string, such as updating punctuation, capitalization, or using synonyms. For changes that don’t materially impact the meaning of the string, we wanted an easy way to keep the previous translation until the new string was translated. Because we use the hash of the English string in the code as a key, this isn’t trivial. We came up with a flag on our {t} blocks called “fallback_hash”, which let engineers specify the hash of a previous version of a string, and made sure all the tooling understood this option. . There were many other challenges we had to solve to meet our quality bar. Here’s a summary of some of the more interesting ones: . Localizing Slack has been a massive effort. It took almost exactly a year to complete, and nearly everyone in the company supported the effort in some way. Looking forward, we continue to evolve as an engineering organization. We want to ensure we can support people all over the world — in additional languages — to make their working lives simpler, more pleasant, and more productive. .  Want to help Slack solve tough problems and join our growing team?    Check out all our engineering jobs and apply today   .  ", "date": "2017-09-13"}, {"website": "Slack", "title": "LibSlack: The C++ Library at the Foundation of Our Client Application Architecture", "author": ["Tracy Stampfli"], "link": "https://slack.engineering/libslack-the-c-library-at-the-foundation-of-our-client-application-architecture/", "abstract": " Slack ships its client application on many different platforms — we currently support Mac, Windows, Linux, iOS, Android, and Windows Phone. Initially these different platform applications were developed independently at Slack, with separate teams and codebases for each of the mobile platforms and the desktop. About a year ago, Slack formed a new team, LibSlack, to develop a cross-platform C++ library to encapsulate common functionality and integrate it into client apps. LibSlack is currently in use in the Slack iOS and Android apps, with a plan to expand to other platforms. . Sharing common code between different platforms has many benefits but also presents challenges. There have been some bumps in the road and changes of direction as we developed the early versions of LibSlack, and as we tried to figure out ways to make it most useful to the client applications. This post covers the problems we are trying to solve, some challenges we have faced, and the tools and techniques we have used to solve them. . Slack’s initial approach of developing each platform client independently made sense at the time — it allowed platform engineers to quickly develop in the native languages for each platform, without any dependencies on other projects or platforms. But it also meant that any time a new feature was added it had to be implemented multiple times, once for each platform. In addition, the same bug could arise on different platforms and have to be fixed multiple times, and there was inconsistent behavior of the same feature between the platforms — for example, lists of users had a different sort order in different clients. Moving shared code into a platform independent library can greatly reduce development effort and improve consistency and quality across platforms. . Which parts of the application code should be shared across platforms? Where is native code necessary, and where can common platform-independent code be used? We still want Slack to have a great native look and feel, so the UI layer should remain platform code and take advantage of platform APIs. In addition, we want a base platform layer to take advantage of native functionality such as networking support, background tasks, and efficient threading. For example, LibSlack’s threading library is implemented using GCD on iOS and thread pools on Android, allowing us to better integrate with platform thread management. The code best suited to move into the cross-platform library is the business logic and data handling. These areas can be managed in platform-independent code, and keeping this complicated logic in a separate library enables us to have consistent behavior across platforms. Thus, our plan is to have a delicious LibSlack sandwich — a platform UI layer on top, LibSlack logic in the middle, and a platform layer on the bottom to access native capabilities. . Data caching is one of the main focuses of LibSlack. Initially, Slack clients assumed they could download a complete picture of all team information when the client was launched. This worked when Slack teams were small and most users connected through reliable desktop network connections, but as the teams using Slack grew in size, and more people wanted to use their Slack clients in conditions with less reliable network connections, it became untenable to download all information about the team when the app launched. . To handle large teams, Slack clients are switching to  lazy loading  as much information as possible, getting only the data that is necessary for the current view, and  saving a partial cache of information . This improves startup times and behavior on poor networks, but it leads to more complex logic to determine what data will be needed and when. Unreliable connections make cached data even more valuable, since the cache allows the app to retain functionality when it is offline. This means Slack clients now need to determine the most important information that the user will need to use the app, and fetch it proactively. As the logic for what data is fetched and cached becomes more complicated, LibSlack will play an important role by taking over this responsibility for the clients. . LibSlack aims to unify the logic to determine what information needs to be fetched and saved, when cached information is out of date, what data should be evicted from the cache, and who needs to be notified when updated information is received. In the future, LibSlack will power improvements to the offline behavior of Slack clients by proactively fetching data, maintaining a queue of offline actions, and executing them when the network becomes available. Our goal is to make Slack clients function as well in spotty network conditions as they do in reliable ones, and ensure that the user’s actions are never lost because their network dropped. . One of the first decisions we faced in creating a cross-platform library to use with all Slack clients was which programming language to build it with. After much discussion and experimentation, we chose modern C++ as the core language for LibSlack development. C++ has support across compilers and toolchains for all major platforms. In addition, modern C++ has added performance enhancing features like move semantics, lambdas, return value optimization, and smart pointers to aid in memory management. There is an ecosystem of libraries like  Boost  that make C++ more powerful and easier to use. We’re currently using C++ 11 due to the need to support the compiler used for our Windows Mobile client, but we intend to update to C++ 14 shortly, and keep updating to make use of the most recent language features. . In order to incorporate LibSlack into Slack client applications, we need to generate headers for the LibSlack APIs in the languages used on each platform. To create those headers we currently use  Djinni , an open source project from Dropbox. Djinni takes an interface description file and creates C++, Java and Objective-C headers based on that description file. C++ types like unordered_set&lt;T&gt; are translated into NSSet in Objective-C and HashSet in Java. Djinni creates interfaces that are implemented in LibSlack and called by the clients. It also creates interfaces that are implemented in the clients to provide native functionality needed by LibSlack, like threading and networking. Once Djinni has generated the headers, either the C++ or client-side code implements the interfaces. . Aside from creating the headers, Djinni also handles memory management across the boundary between C++ and other languages, and handles string and other data translation to and from C++. But there are some drawbacks as well — marshalling calls across the boundary between languages can have performance costs, and Djinni does impose some restrictions on API design (like no support for lamdas or blocks). Also, Djinni does not currently support Javascript, which will be needed to integrate Libslack with the desktop client. To generate those headers we may need to extend Djinni or write our own header generation scripts. . Creating a cross-platform C++ library for native apps can lead to development and debugging challenges, such as difficulty debugging when combining C++ with native code, and issues of integration and syncing releases of the library to releases of the clients. . For LibSlack, we addressed these issues in a few ways. We initially built a reference client application for easier development, using React Native for the UI and Objective-C or Java to connect to LibSlack. This simple app exercised basic LibSlack functionality and allowed us to do quick prototyping and testing of features in development. Eventually we improved our integration with the client applications to the point that we were able to drop the use of this app and debug features directly in the Slack clients. . Our initial approach to syncing releases with the Slack client applications involved shipping a binary of LibSlack each week, which clients then integrated. But we found that this system created a lot of work for Libslack engineers to validate releases each week, and often required hot-fixes to update the binary and address bugs found after clients integrated it. We have since moved to a continuous integration system where LibSlack code is updated daily, so we can catch and address issues earlier. The mobile platforms also now build LibSlack from source. This enables client and LibSlack engineers to easily set breakpoints and step into the LibSlack code to debug issues, which was not possible with binaries. To ensure that LibSlack does not increase client build times, we are using  ccache , a cross-platform compiler cache that stores previous compilation output per file and allows it to be used again in future builds. . To debug customer issues, we have also added logging and analytics to LibSlack. The combination of logging, performance metrics, and stack traces from Fabric/Crashlytics help us to track down issues in the wild. . One advantage of LibSlack is the ability to provide a highly tested codebase that can be more reliable than separate platform implementations. Because LibSlack does not contain any UI code, it is highly testable. While we are not practicing strict  TDD , we do require unit tests when checking in new functionality and we currently have around 70% code coverage with unit tests. These unit tests are run on 5 platforms for every PR that is opened, and PR’s are gated so they cannot be merged until all unit tests have passed. . In addition, we have added integration tests to exercise the APIs LibSlack provides to clients, to make sure our tests are representative of how the code is actually used by our Slack client applications. Once again, we are gating LibSlack PR’s on the passing of these integration tests on all of our client platforms. . Finally, we are working towards having automated performance testing of LibSlack on-device, to catch any performance regressions before they get out in the wild. . There will always be a need for native development in the client Slack apps. Not only for UI and native capabilities, but prototyping can often be done more quickly and easily in the clients directly. However, by creating a highly reliable, scalable, cross-platform library to share as much common code as possible, we are able to increase efficiency of development, increase quality, and unify Slack client behavior to make it more dependable for customers on all platforms. . If you are a developer with experience creating cross-platform applications,  come join us ! ", "date": "2017-09-27"}, {"website": "Slack", "title": "Rebuilding slack.com", "author": ["Mina Markham"], "link": "https://slack.engineering/rebuilding-slack-com/", "abstract": " In August, we released a major redesign of  slack.com , and we want to give you a peek behind-the-scenes. Rebuilding our marketing website was a massive project that took careful coordination across a variety of teams, departments, and agencies. . We implemented a redesign while overhauling all the under-the-hood code. Our aim was to address a few goals at the same time: deliver a consistent rebranded experience while tackling critical improvements to site architecture, code modularity, and overall performance and accessibility. This would afford us a new foundation for several important company initiatives, including  internationalization . .    Slack.com (L-R: January 2017, August 2017)  . The old slack.com shared many code and asset dependencies with our web-based Slack client. One of our earliest goals was to decouple the website from the “web app” in order to streamline and simplify our codebase. By including only what we need to run slack.com, we are able to increase site stability, reduce developer confusion and create a codebase that is easier to iterate on. A fundamental part of this effort was the creation of our new UI framework, called Spacesuit. . The Spacesuit framework consists of class-based, reusable components and utility classes used to standardize our marketing pages. It allowed us to reduce our CSS payload, in one case by nearly 70% (from 416kB to 132kB). . Some other interesting data points: . Our CSS is organized based on the  ITCSS philosophy  and uses  BEM-like  naming conventions. Selectors are named using a single-letter prefix to indicate the type of style the class represents. The prefix is followed by the name of the component and any variation applied to it. For example,  u-margin-top--small  represents a utility class that sets margin-top to the small value set by our variables. Utility classes such as these are an essential part of our system as it allows our devs to fine tune pieces of UI without having to rewrite a lot of CSS. In addition, spacing between components is one of the tricker parts of creating a design system. Utility classes such as  u-margin-top--small  let us create consistent spacing and eliminate the need to reset or undo any spacing already applied to a component. . The new site uses a combination of Flexbox and CSS Grid to create responsive layouts. We wanted to utilize the latest CSS features, while also ensuring that visitors with older browsers received a comparable experience. . At first we tried to implement our layout with a traditional 12-column grid using CSS Grid. That approach ultimately didn’t work because we were limiting ourselves into a using a single dimensional layout when Grid is meant for two. In the end, we discovered that a column-based grid  wasn’t actually needed . Since Grid allows you to create a custom grid to match whatever layout you have, we didn’t need to force it into 12 columns. Instead, we created CSS Grid objects for some of the common layout patterns in the designs. . Some of the patterns were pretty simple. . Others were more complex, which really showcased Grid’s abilities. . Before our Grid implementation, a layout like the one above required lots of wrapping, and sometimes empty, divs to mimic a two-dimensional grid. . With CSS Grid, we’re able to remove the extra markup needed to simulate a grid, and simply create one natively. Starting with Grid lets us use less markup, in addition to making sure the markup we use is semantic. . At first we used Modernizr to detect Grid support, however that resulted in flashes of unstyled layout while the library loaded. . We decided that addressing the jarring experience of the layout shift was a higher priority than backwards compatibility. The compromise was to use CSS Grid as an enhancement and fallback to Flexbox and other techniques when needed. . Instead of using a library to detect Grid support, we went with CSS feature queries. Unfortunately, feature queries aren’t supported in every browser. This means that any browser that can’t handle the  @supports  rule will not get the CSS Grid layout, even if that browser supports Grid. So IE11, for example, will always use our Flexbox-based layout even though it supports some Grid features. . We use some features of Grid that aren’t currently fully supported in all browsers, the most notable being percentage-based  grid-gap . Although support for this has been implemented in some versions of Safari, we still needed to anticipate its absence. In practice, a Grid object is styled as follows: . Any browser that doesn’t meet the query requirements will use our flexbox fallbacks instead. . Once we had responsive layouts, we needed equally adaptable typography. We created  Less mixins  to help us fine-tune our typesetting. Typeset is a mixin that acts as single source of truth for all typography settings. For each type style, a new line is created inside the mixin that contains the name or purpose of the style, followed by a list of settings for each style. They are, in order:  font-family , min and max  font-size  (in rems by default),  line-height ,  font-weight , and any text-transforms, such as uppercase. For clarity, each type name is prefixed with  display-as-  to make its purpose plain. . Here’s a simplified version of the mixin: . See it in action: . The logic for this mixin takes a parameter, such as  display-as-btn-text , and extracts the settings from the list at the index indicated for each property. In this example, the  line-height  property would be set to 1.3 because it is the 4th indexed value. The resulting CSS would be .  Alice Lee  provided us with some beautiful illustrations, and we wanted to make sure we showcased them in the best possible light. Sometimes it was necessary to display a different version of an image depending upon the viewport width. We toggled between retina vs. non-retina assets, and made image adjustments for specific screen widths. . This process, also known as  art direction , is accomplished by using the  picture and   source  elements with  Picturefill  as a polyfill for older browsers. Defining characteristics, like device size, device resolution, orientation allows us to display different image assets when the design dictates it. . With these tools, we were able to display the best possible version of an asset based upon query parameters we set. In the above example, the main hero image needed a simpler version for a smaller viewport. . This technique allows us to specify which image asset is shown for a particular media query, plus if retina and non-retina assets are needed and available. The end result is greater art direction throughout the site. . Another major goal was to ensure that low-vision, screenreader and keyboard-only users could navigate the site with ease. While starting from a clean codebase, we were able to make many impactful improvements to color contrast, semantic HTML and keyboard accessibility with little additional effort. Additionally, we were able to work in some new features for a more accessible experience. We added a  skip link  before the navigation so that users could bypass the menu if desired. For a better screenreader experience, we added an  aria-live region  and helper functions to announce form errors and route changes. In addition, interactions are keyboard accessible with noticeable focus states. We also strived to use clear, descriptive alt text. . There are always more wins to be had for better performance, maintainability and accessibility. We are refining our site telemetry to better understand where the bottlenecks lie and where we can make the most impact. We’re proud of the progress we have made; progress that will surely serve us well as we look to create a more pleasant experience for our customers around the world. .  \t\t\t\tCan you help Slack solve tough problems and join our team? Check out all our engineering jobs and apply today!\t\t\t\t Apply now  \t\t\t ", "date": "2017-10-11"}, {"website": "Slack", "title": "Growing Pains: Migrating Slack’s Desktop App to BrowserView", "author": ["Charlie Hess"], "link": "https://slack.engineering/growing-pains-migrating-slacks-desktop-app-to-browserview/", "abstract": " Recently Slack on the desktop has been going through an awkward adolescence. Instead of flailing limbs and pitch squeaks, ours has manifested in ways rather more grim: inexplicably failing to render content, reloading during  common operations , and error screens that aren’t actionable. The only silver lining has been being on the receiving end of some absolutely savage burns: . Kinda seems like that something is “writing a desktop chat app in JavaScript”.  pic.twitter.com/bOls7WS8n8  . — Matt Diephouse (@mdiep)  August 11, 2017  .    . In all seriousness, the experience some customers have had leaves us with a pit in our stomach, and we’ve been working tirelessly towards a more mature version of the app, dubbed Slack 3.0. The good news is that it’s  available on our    beta channel    now . Before we dig into the specifics of 3.0 —   why it was necessary and how we got there — we need to cover a little bit of Slack history. . You’ll sometimes see us refer to “the webapp” vs “the desktop app;” what does this mean? How do they relate to one another? A picture might clear this up: . The  desktop  app is a host for some number of guest pages. The guest pages are like browser tabs pointed at slack.com, which we call the  webapp . Although the webapp is on its own quest for modernity, this post is about the  Electron  container around it. . You might think there’s not much to embedding a web page, but like the 1990 classic  Tremors , there’s a lot happening underground. Support for multiple workspaces is the main customer-facing feature, but much of our codebase is devoted to creating a layer of native integration that most folks don’t notice until it’s gone: . With that distinction made, let’s talk about why we needed an overhaul. . We host pages using an Electron feature called  webview . You can think of the webview as a specialized iframe with concessions made for security: it runs out-of-process and lets you avoid polluting the guest page with Node. Although we (and others in the Electron community) have found it to be a spawn point for  bugs , until recently it was the only secure way to embed content. Since it’s implemented in Chromium and imported wholesale into Electron, we can’t tinker with it as easily as other APIs. And since it’s used only by Chrome  extensions  — not the tabs themselves — issues filed against it can  languish . Besides renderer crashes during drag &amp; drop and a litany of focus issues, the worst problem we faced was that sometimes, after a webview was hidden, it would  not render  content the next time it was shown. . Unfortunately for Slack, the webview was the linchpin of the app. There’s a view for each workspace and switching between them is a visibility toggle. . Should be fine, right…? . In hindsight, we spent more time than we should have trying to work around the problem on our end. We considered trade-offs no responsible engineer should face: should users sometimes see a blank page or always have idle CPU usage? . Together, Slack and Sketch only take up 100.000% of my CPU. I have no complaints. . — daniel.pizza 🍕 (@dvdwinden)  September 5, 2017  .    . While we were exploring the boundaries of our creativity, the folks at  Figma  had already abandoned ship and begun on a new strategy for embedding web content. Enter  BrowserView . Their post goes into more detail, but in a nutshell: . What we mean by that is — unlike the webview — you can’t drop a BrowserView into the DOM and manipulate it with CSS. Similar to top-level windows, these views can only be created from the background Node process. Since our app was written as a set of React components that wrapped the webview, and — being React — those components lived in the DOM, this looked to be a full rewrite. But we needed to make haste, since users were encountering problems on a daily basis. So, how did we manage to pull the rug out from under our furniture without moving it first? Were there any design decisions that helped us out? . It turns out there were, or this would be a very short post. There are three parts of our client stack worth mentioning: . Like every webapp written circa 2017, Slack uses Redux. But unlike most Redux apps, Slack sometimes has to synchronize data between stores. That’s because instead of one tidy little process, we’ve got oodles of them. . All Electron apps have a main process that runs Node, and some number of renderer processes that are old-fashioned, card-carrying web pages, complete with a document, a body, and stifling inconsistencies between Mac and Windows. . “How could you possibly need that many processes?” — every Slack customer, to us . This makes the main process’ store the One True Store, and ensures that the others are eventually consistent. With this strategy, there’s no need to shuttle state or get into the serialization game. The only things that cross a process boundary are your actions, and hopefully those are already  FSA-compliant . For us this means that our Redux code is virtually  process-agnostic : the actions can come from any process; the reducers can live in any process; the work gets done all the same. . One oft-expressed critique of Redux is that asynchronous actions — and their side-effects — are a bit of an afterthought. There are  dozens   of   solutions  out there and since none of them are included in Redux, it’s up to you to choose what best fits your app. Slack’s desktop app preaches the gospel of Observable, so  redux-observable  was a natural fit for us. If you’re acquainted with Observables, you may have heard the mantra  Everything is a Stream . And lo, what is a store but a stream of actions? . In redux-observable, you’re given that stream of actions as an input, and you write “epics” (like a saga but more Homeric) that manipulate it. It’s worth noting that the values emitted by this stream are the  actions , not the reduced state. Here’s a contrived example, where we show a sign-in BrowserWindow on startup, if we’re not signed into any workspaces: . This lets us compose sequences of actions, which is sometimes more valuable than looking at the byproduct of the actions (the state). Any objects returned from the stream are automatically dispatched as actions, but nothing says you have to emit an action. Oftentimes we just want to call an Electron API. Redux refers to this as a “side-effect,” but I refer to it as “doing my job.” It becomes  really  powerful when combined with a Redux store in each process, because now we can kickoff main process side-effects from a renderer and vice-versa, in a decoupled way. It’s similar to an event bus or  pub-sub , but across Chromium processes. . How about a more involved example — what if we needed to keep a running total of time spent in different workspaces, to determine which ones were most and least used? This could grow into a mess of timeouts and booleans, but since the stream of actions is an Observable, let’s leverage the  suite of operators  that come with it: . You might be like, “Charlie, that sure looks fancy, but aren’t Observables impossible to debug?” And you’d have been mostly right less than a year ago. But this is JavaScript and in JavaScript, the only const is change.  rxjs-spy  makes debugging (i.e. logging and visualizing) streams as simple as adding a tag. A tagged stream can be monitored, paused, and replayed, right from the console. Testing Observables is a delight too, with the help of the utilities in  RxSandbox  (by our own OJ Kwon): . What we’re doing here is creating a mock stream of actions, and providing it as the input to the epic. We define the stream with a  marble string , which looks odd but is quite simple: any letter represents an action, and a – represents 10ms of virtual time. We can make assertions about the action we expect from the epic, and there’s no need for async or callbacks here — flushing the scheduler runs the clock to completion. . With the exception of LSD, there’s no shorter path to questioning your reality than embarking on a large refactor in a JavaScript codebase. Here a linter is like an over eager sidekick: it means well, but is mostly a distraction.  “Don’t put parentheses there,”  it chides, while the bug slips away. A type-checker, particularly when integrated with an editor, is the Watson you deserve. . Of course there’s an upfront cost — one that we  had already paid  — but that investment saw major returns throughout this project. Much of the work involved rearranging existing features, and a type-checker helped us avoid what would have typically been a long tail of bug fixes. It also makes working with Observables more natural. Never again will you ponder over the output of a flatMap (do I get the array or just one item?), the argument order for a reduce, or the name of that one operator that’s like throttle but starts with a  D … (it’s debounce). When coupled with autocomplete in VS Code, writing JavaScript feels a lot like writing, say, C#. And I mean that in the nicest way possible. . Sometimes, when you haven’t used a workspace in a long time, we take the same approach as Chrome and unload that “tab’s” webContents to  save memory . We still need to show notifications and badges for that workspace, so previously we would navigate to a slim-Slack page that responds to a handful of web-socket messages. Once selected, we stealthily disposed of the intermediate page and spun up the full webapp in its place. . Somewhere along the way, we had a realization: why not run  all  of the slim-Slacks in the main process, instead of each having their own page (and incurring the overhead of a renderer process)? This dovetailed nicely with our effort to make Redux actions process-agnostic: we could just as easily dispatch actions from the main process to update badges or show notifications. All we needed to do was connect to the web-socket from Node, something our colleagues down the hall knew a  thing or two about . . With this change, customers signed in to a lot of workspaces will see a drop in both memory usage  and  number of processes: . So, to wrap it all up: we rewrote most of our Electron app to move from the janky webview to the new-fangled BrowserView. We managed to do it in a relatively short timeframe, thanks to a combination of elbow grease and reasonable choices in our client stack, like: . While it can be tempting to scrap a codebase and go back to green(field) pastures, particularly when faced with a mountain of bugs, this rarely works out for customers. When all was said and done, we reused more than 70% of our code, fixed most, if not all, of the webview’s shortcomings, doubled our test coverage, and substantially reduced memory usage. We think it’ll show in the user experience, but you, dear reader, can be the judge of that. ✌ . If any of this sounds interesting and/or terrifying to you, come  work with us ! ", "date": "2017-10-30"}, {"website": "Slack", "title": "Scaling Slack’s Job Queue", "author": ["Saroj Yadav", "Matthew Smillie", "Mike Demmer", "Tyler Johnson"], "link": "https://slack.engineering/scaling-slacks-job-queue/", "abstract": " Slack uses a job queue system for business logic that is too time-consuming to run in the context of a web request. This system is a critical component of our architecture, used for every Slack message post, push notification, URL unfurl, calendar reminder, and billing calculation. On our busiest days, the system processes over 1.4 billion jobs at a peak rate of 33,000 per second. Job execution times range from a few milliseconds to (in some cases) several minutes. . The previous job queue implementation, which dates back to Slack’s earliest days, has seen us through growth measured in orders of magnitude and has been adopted for a wide range of uses across the company. Over time we continued to scale the system when we ran into capacity limits on CPU, memory, and network resources, but the original architecture remained mostly intact. . However, about a year ago, Slack experienced a significant production outage due to the job queue. Resource contention in our database layer led to a slowdown in the execution of jobs, which caused Redis to reach its maximum configured memory limit. At this point, because Redis had no free memory, we could no longer enqueue new jobs, which meant that all the Slack operations that depend on the job queue were failing. What made this even worse is that our system actually required a bit of free Redis memory in order to dequeue a job, so even when the underlying database contention was resolved, the job queue remained locked up and required extensive manual intervention to recover. . This incident led to a re-evaluation of the job queue as a whole. What follows is a story of how we made a significant change in the core system design, with minimal disruption to dependent systems, no “stop the world” changeovers or one-way migrations, and room for future improvements. . Around this time last year, the job queue architecture could be sketched out as follows, and will be roughly familiar to people who have created or used a  Redis task queue : . The post-mortem of the outage led us to conclude that scaling the current system was untenable, and more fundamental work would be required. . Some of the constraints we identified were: . Each of these problems suggests a variety of solutions, from investing further work in scaling the existing system, to a complete ground-up rewrite. We identified three aspects of the architecture we felt would address the most pressing needs: . We knew that implementing all these potential architectural enhancements would require significant changes in the web app and the job queue workers. The team wanted to focus on the most critical problems and gain production experience with any new system components rather than attempt to do everything at once. A series of incremental changes felt like the most efficient way to make progress towards productionizing the revised system. . The first problem we decided to address is that we couldn’t guarantee write availability during queue buildup. If the workers dequeue jobs at a rate slower than the enqueue rate, the Redis cluster itself would eventually run out of memory. At Slack’s scale, this could happen very quickly. At this point the Redis cluster would be unavailable to accept writes to enqueue any additional jobs. . We thought about replacing Redis with Kafka altogether, but quickly realized that this route would require significant changes to the application logic around scheduling, executing, and de-duping jobs. In the spirit of pursuing a minimum viable change, we decided to add  Kafka in front of Redis  rather than replacing Redis with Kafka outright. This would alleviate a critical bottleneck in our system, while leaving the existing application enqueue and dequeue interfaces in place. . Here is a sketch of the incremental change to the job queue architecture with Kafka and its supporting components in front of Redis. . The first challenge we faced was how to efficiently get a job from our PHP/Hacklang web app into Kafka. Although we explored existing solutions for this purpose, none were well-suited for our needs, so we developed Kafkagate, a new stateless service written in Go, to enqueue jobs to Kafka. . Kafkagate exposes a simple HTTP POST interface whereby each request contains a Kafka topic, partition, and content. Using the  Sarama  golang driver for Kafka, it simply relays the incoming HTTP request into Kafka and returns the success/failure of the operation. With this design, Kafkagate maintains persistent connections to the various brokers and can follow the leadership changes, while offering a low latency simple interface to our PHP/Hack web application. . Kafkagate is designed for: . In the future, we are considering a further optimization of having a Kafkagate service running locally on the web app host to avoid an extra network hop when writing to Kafka. . The next new component in the architecture addresses the need to relay jobs out of Kafka and into Redis for execution. JQRelay is a stateless service written in Go that relays jobs from a Kafka topic to its corresponding Redis cluster. While designing this service, we had to think about the following: . In the earlier system, the web app (written in PHP and Hack) would JSON encode a representation of the job when storing it in Redis. Subsequently, the job queue worker (also written in PHP) would decode the job payload for execution. In the new system, we relied on JQRelay (written in Go) to decode the JSON encoded job, examine it, and then re-encode in JSON and write it to the appropriate Redis cluster. Sounds simple enough, right? . It turns out that both the golang and the PHP JSON encoders have some unexpected quirks related to escaping characters that caused us some heartache. Specifically, in Go,  &lt;,   &gt; , and  &amp;  characters are  replaced with equivalent unicode entities  by default, and in PHP,   /  characters are “escaped” with a    by default . Both of these behaviors resulted in issues where the JSON representation of a data structure would differ between the two runtimes, a situation that didn’t exist in the original PHP-only system. . When a JQRelay instance starts up, it attempts to acquire a  Consul  lock on an key/value entry corresponding to the Kafka topic. If it gets the lock, it starts relaying jobs from all partitions of this topic. If it loses its lock, it releases all resources and restarts so that a different instance can pick up this topic. We run JQRelay in an EC2 auto-scaling group, so that any failed machines are automatically replaced into service and go through this lock flow. When combined with this Consul lock strategy, we ensure that all Kafka topics used by the job queue have exactly one relay process assigned to them, and failures automatically heal themselves. . JQRelay relies on Kafka commit offsets to track jobs in each topic partition. A partition consumer only advances the offset if the job is successfully written to Redis. In the event of a Redis issue, it retries indefinitely until Redis comes back (or the Redis service itself is replaced). Job specific errors are handled by re-enqueuing the job to Kafka instead of silently dropping the job. This way we prevent a job-specific error from blocking all progress on a given queue, but we keep the job around so that we can diagnose and fix the error without losing the job altogether. . JQRelay respects the rate limits configured in Consul when writing to Redis. It relies on the Consul watch API to react to rate limit changes. . Our cluster runs the 0.10.1.2 version of Kafka, has 16 brokers and runs on i3.2xlarge EC2 machines. Every topic has 32 partitions, with a replication factor of 3, and a retention period of 2 days. We use rack-aware replication (in which a “rack” corresponds to an AWS availability zone) for fault tolerance and have unclean leader election enabled. . We set up a load test environment to stress our Kafka cluster before rolling it out to production. As part of load testing, we enqueued jobs to various Kafka topics at their expected production rate. This load testing allowed us to properly size our production Kafka cluster to have sufficient headroom to handle individual brokers going down, cluster leadership changes and other administrative actions, and to give us headroom for future growth of the Slack service. . It was important to understand how different Kafka cluster failure scenarios would manifest in the application, e.g. connect failures, job enqueue failures, missing jobs, and duplicate jobs. For this, we tested our cluster against following failure scenarios: . In all these scenarios, the system functioned as we expected and we hit our availability goals. . We used our load test setup to identify an optimal throttle rate for safe data migration across brokers. In addition, we experimented with using a lower retention period during migration (since we don’t need to retain a job after it has successfully executed). . With 1.4 billion jobs flowing every day, we would prefer to selectively migrate partitions, instead of topics, across brokers. This is planned as part of future work. . Rolling out the new system included the following steps: . Adding Kafka to the job queue was a great success in terms of protecting our infrastructure from exhaustion of Redis memory. Let’s walk through the scenario where we have a queue build up again: In the old system, if the web app sustained a higher enqueue rate than the job queue dequeue rate, the Redis cluster itself would eventually run out of memory and cause an outage. In the new system, the web app can sustain its high enqueue rate as the jobs are written to durable storage (Kafka). We instead adjust the rate limits in JQRelay to match the dequeue rate or pause enqueues to Redis altogether. . In the broader picture, this work has also improved the operability of the job queue, with configurable rate limits and durable storage for when job enqueues outstrip our execution capacity — finer-grained tools than we had at our disposal before. Clearer client semantics will help our application and platform teams make more confident use of the job queue. And the infrastructure team has a foundation for continued improvements to the job queue, ranging from tying JQRelay’s rate limiting to Redis memory capacity to the larger goals of improving the scheduling and execution aspects of the system. .  Even with new systems like this in place, we are always looking for ways to make Slack more reliable. If you’re interested in helping,    get in touch   .  ", "date": "2017-12-07"}, {"website": "Slack", "title": "Android UI Automation: Part 1, Building Trust", "author": ["Valera Zakharov"], "link": "https://slack.engineering/android-ui-automation-part-1-building-trust/", "abstract": " Developer-driven automated testing is vital to the ongoing health and quality of a codebase. It does, however, come with a noticeable cost. This is especially true of UI testing — an area often avoided by even the most test-friendly of developers. In this two-part blog post, we explore how the Slack Android team made authoring and maintaining UI tests part of the development cycle. . In Part 1, we place UI testing in context of the larger quality landscape and outline an approach that helps produce tests stable enough to run continuously. . Let’s start with acknowledging the fact that many of us have been in the situation where a one-line bug fix required hours of struggle with an automated test. Why go through the effort if you can just merge the code and continue adding more features? . If you have ever refactored legacy code with no test coverage, you may remember the “I too like to live dangerously” feeling as you pulled the trigger to merge. A robust automated test suite (including UI tests) allows a you to move quickly with confidence, rather than feeling like you’re sitting at a high-stakes table in Vegas. . This effect is especially critical as your team grows and complexity rises. Unless you’re building a throwaway prototype, don’t make the mistake of deferring testing to a “better” time. There is no better time than now. . Further, authoring UI tests is a systematic way to exercise the code that you are about to unleash onto other developers and users. From the standpoint of code design, test-driven development is not as much a validation method as a design principle that forces you to break up your code into smaller modules — a natural forcing function for a cleaner, more maintainable codebase. . Ultimately, teams where developers are responsible for testing their own code gain a significant advantage that only grows with time. . While most developers agree that unit-testing their code is a good idea, the sentiment is not as positive about UI testing, which is often pushed to dedicated QA teams. A  recent small-scale survey  distributed to the Android development community suggests that  ¾  of respondents write some kind of tests when working on features that involve UI. Of these developers,  all  did some kind of unit testing, but  37%  did not add UI tests for the feature. Here are the top concerns that were cited by respondents when asked — “What prevented you from authoring UI tests?” . We distributed the same survey to the Slack Android team and, to our relief, the results showed a different picture. . What compelled Slack’s Android engineers to write, not only unit tests, but also UI tests? Was it the vast amounts of time they gained from not having to read email? Was it the glory of solving impossible problems? Was it the threat to remove Coconut LaCroix from the fridge? Or, perhaps, we found a UI test strategy that works for us. . Our strategy comes down to reducing the need for UI testing, while keeping the remainder of UI tests stable by making them targeted and hermetic. Let’s dive into these in detail. . No post about testing is complete without the famous geometric shape of testing: . As you move up the pyramid, “larger” tests (i.e. ones that involved interactions between more components) become more difficult to author, maintain and execute. A developer should, therefore, prefer to get the majority of coverage from smaller unit tests, while supplementing the suite with a smaller set of integration and end-to-end tests. . The idea is simple, but in practice, it is complicated — the Android framework invites a developer to bundle business logic with UI interactions in the Activity/Fragment classes. Without adding additional layers to the architecture, you may end up with code that is only testable through UI and, following the path of least resistance, your testing may end up inverted: . Like every other problem in computer science, this can be solved with  another layer of indirection . Is it worth introducing this layer for the sake of decoupling business logic from UI? We absolutely believe so. Of the many MV* (MVC, MVP, MVVM, MVI, MVWTF, …) patterns out there, we chose to go with a variant of MVP, with a bi-directional contract between the Presenter (pure business logic) and View (encapsulating simple UI interactions). . Which one should you go with? There are many takes on which pattern works best. The  Google Samples Android Architecture  repository includes a collection of samples and is a great place to start looking into the subject. With the recent introduction of  Android Architecture Components , the Android team is also providing guidance and framework support for more testable architectures. Find a pattern that best fits your code base and go with it. . At this point, an astute (and lazy!) developer might ask — if I can test all complex business logic through unit tests, do I even need UI tests at all? Our answer to that is a resounding yes because it helps you avoid this situation: . By validating whether the UI can be inflated on the actual Android OS and that business and UI layers are glued properly, UI tests fill the middle (integration) layer of the test pyramid. Lastly, you do not want to miss out on the opportunity to  watch UI tests execute in parallel . A good rule of thumb is to have at least one test per screen that launches the activity and performs a few simple UI interactions/checks. . Remember that one of the third most cited concerns from the Android developer survey above was: “Lack of trust in results”. Can UI tests actually be stable or will we have to live in continuous Christmas mode? . In reality, most UI tests end up being unstable because developers forget to follow two best practices in testing: test only what you need to test and keep full control of test state. . Let’s look at an example. If your test is meant to validate functionality of the settings screen, it should not navigate through the login process, open the navigation drawer, press the overflow button, and only then start the actual test interactions related to settings. That would make the test more difficult to author, slower to execute, and more likely to fail in code paths that are unrelated to the functionality under test. . Instead, the test should launch the settings activity directly. On Android that’s easy to do by including this Junit4 rule in your test class: . But there’s a catch — if you do this without setting up application state, you’ll likely see a login activity or some other setup screen instead of settings. In our project, we solve this by using a combination of  Dagger  (a popular dependency injection framework) and  Mockito  (a framework for easily creating test doubles). . Note: For legacy reasons, we are still on Dagger 1. If you’re starting from scratch, refer to this  sample with Dagger 2 . . Having the ability to swap in a test double for any object in the graph, we can now simulate a logged-in state by swapping out the component of our application that is responsible for signaling such state. . This leads us to the next section where we further discuss state. . In relation to testing, the word hermetic means that the test is fully isolated from non-deterministic state and interactions. The randomness can creep into the test through data, background operations and interactions with other applications. . We are now able to launch our screen directly without navigating through the rest of the application, but will it even show up given the fact that we did not go through our normal setup flow? That’s a trick question — chances are, the application would simply crash because the data model required for inflating the UI is not there. . There are multiple ways in which test data can be provided to the UI under test: . Our recommendation for functional UI testing is to use the “Higher Level of Abstraction” method. We use a layer of classes called data providers, which retrieve data from local and backend sources and provide it in a user-friendly  Rx Observable  to business logic components. This approach reduces the footprint of what’s being tested, thus making the test even more targeted. It also comes with an important side-benefit of reducing non-determinism caused by background operations used during data fetching, which we’ll talk about shortly. . In the case of the MVP pattern, a reasonable alternative would be to swap in a fake Presenter (an even higher level of abstraction). We believe this approach goes too far. It is true that the Presenter should already be unit-tested, but in a functional UI test, we want to cover the integration boundary between the View and the Presenter. . We’ve now launched a screen and injected an initial set of fake data to be displayed in the UI. This, in itself, is already a win — we can test that the Activity can successfully inflate its view hierarchy. But it is likely that you would want to go further and actually interact with the screen to test a few basic operations. This is where the UI testing story often becomes sad. The reason is multithreading — Android tests run on the instrumentation thread, while the UI is processed on the UI thread, leading to race conditions between test interactions and UI state.  Espresso  helps by synchronizing all operations with the UI thread and taking care of a few basic background mechanisms like AsyncTasks. But most complex applications use  Executors  or  Rx  to schedule background work and Espresso does not handle synchronization with these out-of-the-box. . A common anti-pattern at this point is to sprinkle a test with sleep calls or some other forms of sleep-and-retry mechanisms. If we were to identify the most common source of flaky UI tests, this would be it. If you want developers to believe in UI testing, forget about sleep (in the tests, of course). . Thankfully, there are better ways. One approach would be to use Espresso’s  IdlingResource  mechanism to synchronize background operations, but this route can be complex and  error prone . . A simpler and more resilient approach is to get rid of asynchronous background work altogether. Remember those useful DataProviders that we replaced with test doubles to feed data into our app? We can replace the background work with instantly-returning operations that get processed on the instrumentation test thread: . One strength of the Android operating system is its ability to facilitate interactions between applications through intents. At the same time, this brings another source of non-determinism to testing. For example, a particular part of your application may send an intent to the photo-gallery app when it needs the user to pick an image. Testing this interaction presents two problems. First, instrumentation UI tests cannot interact with the UI of external processes (a security limitation of the OS). You could work around this by using  UiAutomator , but here’s the second, more fundamental problem — interacting with a dependency that you’re not fully in control of breaks the hermetic seal. Thankfully, these are exactly the problems that  espresso-intents , a mockito-like framework for Android intents, was designed to solve. Here’s a sample: . Some developers go a step further and isolate the current activity under test from other activities within the application. With this approach, if your test triggers a transition to another activity, there is no need to set up the state for the destination UI. The astute (and not lazy!) developer may point out that this amounts to behavior verification (as opposed to the  recommended state verification ). That is a good point. We leave it up the reader to weigh the benefits of this approach versus the drawbacks. . With the Targeted and Hermetic approach, our developer-authored UI tests were stable enough to run in CI. We addressed an important concern of trust and got developers to believe in the utility of UI testing. However, there was still a problem — developers did not enjoy the process because it was too cumbersome and time-consuming. In Part 2 of this post, we’ll discuss how we tackled the problem of making UI testing easy, and dare we say, enjoyable. .  Interested in joining our test-friendly engineering team? Check out the open Android and Mobile Developer Experience positions    here   .  ", "date": "2018-01-05"}, {"website": "Slack", "title": "How Slack Supports Junior Engineers", "author": ["Carly Robinson"], "link": "https://slack.engineering/how-slack-supports-junior-engineers/", "abstract": " I joined Slack in late October 2015 as an Associate Application Engineer on the Enterprise team, which set out to build the  recently released Enterprise Grid product . Full Disclosure — this was my first engineering job. . When I arrived at my desk on my first day, I tried to play down how star struck I was feeling. To my 1:00 sat Stewart Butterfield, right amongst the engineers. Behind me was Myles Grant, one of the most senior engineers in the company who, along with founders Cal Henderson, Serguei Mourachov, and Eric Costello wrote the majority of Slack in a legendary four-month sprint after the infamous pivot from Glitch.  It was quickly apparent that I was being paid to work with and learn from a group of engineers who in addition to Slack collectively helped build Facebook, Twitter, Google, Yahoo, Flickr and more — products that have transformed the world.  . I had just hopped on Slack’s rocket ship as the company was taking off on an exponential growth explosion. My emotions oscillated between awe and terror. At that point, it was anyone’s guess if I would be able to handle the job. Impostor Syndrome was breathing down my back.  However, I was able to put together a plan to take advantage of the abundant resources Slack had to offer new employees.  As a result, I’ve grown as an engineer at a similar pace. . A year later, I am now a technical feature lead for Slack’s Enterprise Billing products. In the last six months I’ve had the opportunity to design, build, and ship Slack’s Sales Tax and Dunning Systems, in addition to helping optimize Multi-Team Search for Enterprise Grid. . In sharing my reflections on why I accepted my offer to work at Slack, the steps I’ve taken to optimize  direct and indirect mentorship, abundant opportunities for impact and growth, and a culture of learning  that Slack has kept remarkably intact at scale, I hope you will take away some useful insights for you and your team. . Hiring junior engineers is an investment for any business. I was applying to entry-level positions a little over a year ago. Every engineering organization I interviewed with  promised mentorship.  Makes sense — in an ideal world, all companies would invest in the opportunity to shape a fresh developer into their perfect employee. New talent makes up for inexperience with being green — free of bad habits, an eagerness to impress, a lack of opinion on  spaces vs. tabs.  .  However, in reality, not every tech company has the bandwidth or desire to follow through on this undertaking.  Mentorship is critical to every new engineer and has a significant impact on the careers of women and minorities. As I transitioned into the tech industry,  I knew that a company’s approach to mentorship would affect my ability to thrive and grow as an engineer.  And that’s what I was looking for when I accepted my offer at Slack. . The week Slack invited me to an onsite in October 2015, I had an expiring offer in hand and a few pending final round interviews at other companies. After weeks of tirelessly studying for technical interviews and mentally preparing for life in a notoriously cutthroat industry,   Slack’s more human approach to the interview process came as a welcome relief and surprise.   . In my final round interview in with Slack’s CTO, Cal Henderson immediately asked me a question I dreaded answering. He wanted to talk about my previous career in theater. Whenever someone asked me this, I knew the interviewer was trying to assess   my potential liability  . Hiring a former actress who taught herself how to code is undoubtedly on  the extreme end of the risk spectrum . Offering me a job was a gamble for any company, let alone the recently crowned fastest growing SaaS startup in tech history.  The outcome of this conversation was usually a pretty good gauge of a company’s commitment to mentorship and improving diversity in their engineering organization.  . Thankfully, my answer went over smoothly. Cal and I talked at length about  craftsmanship . He listened to me pitch my thesis that my years of studying ballet at a professional level would, in fact, make me a good engineer. I highlighted the little-known parallels between start-up life and the long hours and love of craft that drives professional actors in the face of endless rejections. I tried to make it clear that my presence in this interview was a culmination of two years of teaching myself to code after work while saving money so I could attend two different  12-week coding intensives . As crazy as it sounded, I had fallen in love with computers and was eager to learn, improve, and build software that had a real impact on the lives of others. . At one point, I could tell that he was entertaining the possibility. . “We’ll grow you as an engineer,” I remember him saying, as though the light bulb emoji had just appeared above his head. . At this point, I knew if I got an offer, Slack was a special place I’d be crazy to turn down. . There is a big difference between a culture of academia and a culture of learning. What do I mean by this? Let’s be real; the tech industry is a workforce highly saturated by graduates with elite degrees. I’d like to limit the scope of what “academia” refers to in this post. Academia is prestigious because of its association with the noble cause of intellectual rigor and reason.  However, too often individuals privileged with an excellent education confuse intelligence with aggressive arrogance.  In an ideal world, the best ideas win, and everyone is happy. But when a workplace environment becomes a battle of the brightest mind, team and individual morale are too often undermined by people overcompensating for weaknesses with a series of tactics that directly or indirectly aim to make their coworkers feel stupid. . Anyone thinking of joining tech is reading these stories on the internet and preparing their mental health armor. I know I was. .  Slack has defied my expectations of what it should be like to work in tech.  While I tend to see the good in people, I have yet to come across an engineer who is not incredibly hardworking, talented, humble, kind, and eager to share their knowledge. When I first started working here, it was apparent that the engineers at Slack were not just hired for their talent,  but for their demonstrated values of craftsmanship, courtesy, and empathy in their work and interactions with others.  . When I’ve made mistakes, not once has anyone gone out of their way to make me feel stupid or incapable — and I think that says a lot in itself. Instead, I’ve been encouraged  to develop a growth mindset.  . In addition to hiring quality people,  Slack has created the infrastructure to make sure their employees are thriving  by continuously iterating on a holistic approach to employee development. In my first few weeks on the job, I researched what Slack had to offer and worked with my manager, mentor, and peers to assess how best to take advantage of these opportunities. Some examples include: . I was able to use this to continue my education through the year by enrolling in a part-time UNIX/Linux systems course at UC Berkley and subsidizing my existing subscriptions to Code School and Treehouse. I built up a library of technical books on Linux, microprocessors, PHP, design patterns, and security and set up informal workshops with more experienced coworkers and mentors to work through the material. . Last year I used my funds to take cooking classes at San Francisco Cooking School and a few non-technical books recommended by Slack’s women’s group and the diversity reading club aimed at cultivating empathy among people with different lived experiences. . All the presentations are recorded so even when I feel too busy to leave my desk; I try to make a point to either watch the recording live or revisit past presentations during my free time. . By providing resources and opportunities for us to learn and make ourselves better engineers and citizens of the world,  Slack has built a positive culture of learning . . Mentorship is a burden that is shouldered by senior engineers, and  the engineering culture  will affect the style and quality of the outcome. A successful mentoring relationship requires equal effort from both the senior and junior engineer.  Clear communication, well-defined technical requirements, and regular meetings will only happen if both people invest.  Luckily for me, it was apparent that Slack had already set expectations for engineering mentors: . Expectations around what it means to be a good mentee are rarely spelled out for junior engineers, but there are a few things I’ve found to be useful in my personal experience. . In the first meeting with my assigned manager, Leah Jones, now Head of Engineering for Enterprise , I tried to be as transparent as possible about my personal communication style, my perceived strengths and weaknesses, and my desire for direct constructive feedback . We worked together to establish an effective plan for supplementing my technical development throughout the year. . Additionally, Eric Vierhaus, a Senior Engineer on the Enterprise team, was assigned as my dedicated mentor. When Eric and I first met, I was very open about what I knew and didn’t know, and solicited his advice in creating an effective plan to expedite my ability to contribute independently. For the first month, Leah and I met twice a week, while Eric and I met once a week.  By creating recurring meetings in my first few months, mentorship was not restricted to whenever I had a question.  Instead, the meetings took on a more free-form structure  that adapted to my weekly progress.  . We set concrete goals for the quarter and reviewed how I was doing every few weeks.  By setting goals together, I was accountable for my success, while Eric and Leah helped brainstorm tactics for how to address weaknesses.  For example, to improve on catching edge cases after the Sales Tax project I led concluded,  Eric and I agreed to experiment with using strict Test Driven Development (TDD) style on the next project I led.  While most people at Slack don’t adhere to strict TDD, Eric encouraged me to test out my theory that TDD would help me grow. . My next project saw dramatic reductions in bugs and a feature shipped with 100% test coverage. While my reflections on TDD could be a post in itself,  weekly meetings and concrete goals enabled my mentor, manager and me to work together to improve my skills and velocity in a positive, constructive, measurable way.  . To succeed at Slack in your first year as an engineer, it’s important to understand what makes an effective code review. When I asked my manager Leah what I should be trying to master in my first few months, her top priority for me was  learning to write a good Pull Request.  Slack ingrained  empathy in pull requests  in me from day one. . Empathy is necessary for both giving and receiving critique. When I receive feedback on my code, I try to keep two things in mind. . Mistakes are learning opportunities.  At times when it might have been easier to give and receive a prescriptive block of code, my mentor Eric and I often took the opportunity to have discussions or short pair programming sessions.  Whether the code comment was about a Slack standard, a better design pattern, or a debugging technique, taking the time to transform a question into a learning opportunity  helped me become a better problem solver while increasing my capacity for independent contributions.  . At the end of the day, a lot of engineering culture plays out in the way expectations around code review are set. Over 94% of junior engineers surveyed at Slack strongly agree with the statement that   “I receive respectful, constructive, insightful feedback on Pull Requests that challenge me to improve my problem-solving skills without making me feel stupid.”   .  By creating and continuously striving for a Code Review culture rooted in respect and empathy, new engineers are empowered to ask questions safely and receive feedback with gratitude.  As a result, instead of feeling like an imposter, I come to work each day feeling supported and able to celebrate small wins with a focus on actualizing my potential. . In a survey I conducted on mentorship at Slack, 95% of engineers identifying as entry-level or junior  strongly agreed with the statement that  “The engineering environment at Slack is a safe space for me to ask questions and grow as an engineer.”   If you factor in that many engineers are introverts, this number is pretty impressive. . Asking questions as a new engineer can be intimidating. Thankfully, Slack as a product mitigates this because most likely someone has already asked and the answer is a search keyword away. . Slack the company has a distinct way of using Slack the product; everyone is encouraged to share technical questions, answers, and discoveries in a public place. The more conversations happen in public channels; the more Slack becomes a single resource for the collective intelligence and decision making processes that accumulate over the history of the company. . Channel names are a labeling mechanism to guide employees to the most appropriate place to find information. Have a question about threads? Probably #devel-threads is a good place to go. Curious about the difference between HHVM and PHP’s Zend Compiler, but are too intimidated by Keith Adams (Slack’s Chief Architect who helped write the HHVM compiler and Hack language at Facebook) to ask him yourself? Search for  “hack from:   @keith   ,”  and you have access to everything he has ever had to say publicly about it. . Can’t find an answer?  Ask in the relevant public channel.  Some of my most memorable learning experiences this year occurred when I asked a question in a big channel. I was overwhelmed with thoughtful, thorough responses, supplemental resources, and offers to meet in person to dig deeper into issues from senior engineers. Moreover, those answers continue to benefit anyone who might have the same question. Think of it as a gift you are paying forward to future developers. . Another perk of being a junior engineer at Slack is Office Hours with engineering leaders. Since I started at Slack, access to top engineers has continued to expand with weekly and biweekly office hours. Over the year I’ve tried to take advantage of these opportunities as much as possible.  What better way to supplement your computer science education than to go and have your questions answered in person by some of the most experienced engineers in the industry.  . Office hours are open ended. Engineers of all levels are encouraged to attend and bring questions. Sometimes, even if I didn’t have anything to ask, I would go and listen to what others had to say. .  While you can’t always control whether you get a great manager or mentor, there are a plethora of opportunities to take charge of your improvement.  . When you are choosing a place to start your career, especially if you identify as a woman or minority, it’s important to have role models and be in an environment where you can be yourself. . I get to work on a billing team that is 100% lady engineers. And not a day goes by that I don’t feel grateful to work at a company where I need more than two hands to count all of the influential, outspoken, intelligent, Senior and Staff level female engineers.  I rarely even think about my gender at work, and that is the way it should be.  . We grow the most when we are pushed outside of our comfort zones. As a new engineer this means working on projects where: . We learn the most when we fail, so the sooner we can take our training wheels off, the better. . As one of the fastest growing start-ups in history, Slack has plenty of opportunities for impact. Additionally, since you will likely be working on features touched by millions of users every day around the world, it’s easy to feel out of your comfort zone. .  In a time when I was very vulnerable to potentially career-crippling imposter syndrome, my love for engineering and desire to improve has only grown thanks to the support of my manager, mentor, and teammates.  . When I interviewed at Slack in October 2015, I was eager to find a place to develop a sound basis for my engineering craft. As someone who spent most of her life training to be in Broadway musicals, becoming a software engineer may have been the wildest idea I’ve ever had. But life is unpredictable, and sometimes love comes to us in unexpected ways. I’m grateful that Slack decided to believe in me, and that my passion for engineering has a place to thrive and grow.  Not to mention, there are a surprising amount of stories similar to mine in our engineering organization.  . If you’d like to learn more about my transition into tech, you can follow me on Medium or Twitter ( @carlyhasredhair ). I’d love to hear from you. . Most importantly, be kind to each other and don’t give up. . P.S.  Apply to Slack ! ", "date": "2017-03-10"}, {"website": "Slack", "title": "Slack Bug Bounty: Three Years Later", "author": ["Max Feldman"], "link": "https://slack.engineering/slack-bug-bounty-three-years-later/", "abstract": " We’ve reached a few big milestones for the Slack Bug Bounty program: it’s our three-year anniversary, and we’ve paid out more than $210,000 in bounties! We want to give a big thank you to all the security researchers who have helped make Slack more secure. In this post we’ll offer a retrospective on our bug bounty program, discuss lessons learned, and offer guidance for researchers. We also hope this information will be useful for anyone else running a bug bounty program, or considering starting one. For more details about our program, you can see our HackerOne profile  here . . We launched our bug bounty program in February of 2014. Slack was a much smaller product (and team) back then, but we were committed to the security of our users. Bug bounties allow organizations of all sizes to incentivize security researchers to report vulnerabilities, and ours has formed an integral part of Slack’s security processes. Our current bug bounty process works as follows: . We’ve collected data over the last three years which has informed the evolution of our program into what it is today. It’s our hope that you find these measurements and observations interesting and useful! . As is the case with many new bug bounty programs, we received a massive influx of reports at our launch. In fact, in the first four months alone we received nearly 1,000 new reports, with over half of those occurring in the second month. Fortunately, we were prepared for these reports, and our response and resolution times did not suffer as a result. . After launch, we’ve had a pretty consistent rate of about 100 inbound reports per month. . Another metric we’re interested in is our response time, as it provides a way for us to judge the quality of our communication with researchers. At our program’s launch, despite the small size of our team and large number of reports, we were able to respond to reports very quickly. Gradually, our mean response time slowed a bit, though it is worth noting that these averages can be heavily affected by outliers. Our median response time was better than the mean, as the mean response time was increased by a small number reports which took a long time to receive a response. We first began using a third party triage service in April of 2015, which helped to decrease and stabilize our mean response time shortly after. We’ve also implemented better tracking of our response times, and our current mean response time is less than 24 hours. . We haven’t seen as many patterns in the value of rewards given out per month. At Slack we reward bugs once they are resolved, so the difficulty and prioritization of a bug-fix can factor into how long it takes to reward. We fixed and rewarded a large number of bugs shortly after the launch of our program, which was roughly proportional to the quantity of reports received. It is worth noting, though, that one severe bug which receives a large reward can also make this metric unpredictable. . When we first launched the bug bounty, Slack had fewer than 20 employees and didn’t yet have a dedicated security team (though we did have engineers well-versed in security). Our engineering and security teams have grown significantly in the last three years, and we’ve been able to deliver consistently improving performance as we’ve grown and matured. . Over the course of the bounty we’ve also improved our own processes, which have in turn increased our response and scaling capabilities. We have allocated more resources to the triage process, and have streamlined our internal bug bounty workflow. . Every new bug provides us opportunities to improve our efficiency, resulting in faster response and resolution times. Three years have taught us a lot, and we look forward to growing our program! . One particular area of focus for us has been decreasing both our mean response and resolution times. . As we expected, we received a massive number of reports at the beginning of our program. For those considering launching a bug bounty program of your own, it is worth preparing for this influx so you don’t find yourself struggling to keep up with new reports. Ensure that you have the resources to handle large numbers of both valid and invalid reports — for the health of your program and the health of your engineers. There are many approaches to handling a high volume of reports — using a third party triage service works particularly well for us. We’ve added individuals from our triage provider to our program on HackerOne, which gives them access to incoming reports. From there, they evaluate the reports and reach out to us if more information is needed. They inform us when a bug is valid and provide a description of the issue, at which point we file an internal item. Once a bug is filed internally, our engineering team can work on the fix. The larger and more popular your product, the more interest and reports you can expect to receive. . Nearly half of the reports we have received are “Not Applicable”, which applies to reports which are not valid issues, not security issues, or outside the scope of our program. “Informative” reports may contain some useful information, but do not impact the security of our customers. “Duplicate” reports contain previously reported issues, and “Resolved” reports contain issues which we fixed as a result of that report. More information on the different report statuses can be found  here . . A triage service has been an invaluable asset in the success of our program. It enables us to handle several hundred reports per year while still responding, reproducing, and fixing bugs in a timely manner. Our security team is able to focus on seeing bugs through to resolution without becoming overwhelmed by the total volume of submissions. . A slightly less quantitative, yet still important, consideration is the overall happiness of both researchers and internal developers. As a security team, we must maintain a balance among different stakeholders in order to keep our product secure. . Keeping researchers happy provides an incentive beyond the monetary rewards for researchers to inform us of vulnerabilities that we can fix. The mean response time metric acts as a surrogate for researcher satisfaction — in general, maintaining rapid communication with researchers lets them know that the security team cares about their reports and takes security seriously. While we are always reasonably timely in getting fixes out, when resolution takes a bit longer than expected we’ve found that keeping researchers informed of this progress tends to increase their patience. . Keeping our engineers happy helps foster a good work environment wherein we all work together to resolve bugs. We communicate quickly and concisely with our engineering team when we receive a vulnerability report in order to prevent teams from being overwhelmed. We group bugs into one of four severity levels when filing internal issues. Using discrete bug severities ensures that all vulnerabilities are resolved in a timely fashion, as dictated by their severity, while allowing engineers to efficiently allocate their time. . Having empathy for both external researchers and internal employees goes a long way towards encouraging long term goodwill for the security team, which is vital for a successful bug bounty program. . With all that said, we’d like to offer some guidance to researchers (both old and new) looking to find vulnerabilities in Slack. These suggestions for where to search and what types of bugs may be most interesting, within the scope of our bug bounty program. . Slack comprises many parts, interacting in complex ways. The list that follows highlights several of them, explaining what they are, what technologies they use, and what types of vulnerabilities are most relevant for each of them. We hope that this will provide more insight into what to look for, as well as which areas will be most lucrative for bugs. Happy hunting! .  What it does:  The Slack applications (web, desktop, mobile) all communicate with this endpoint, as do apps and integrations built using the Slack API. The API contains the core functionality of managing and administering a Slack team, communicating on that team, and more. .  What to look for:  Authentication bypasses, bypasses of permissions, information leakage .  What it runs on:  PHP, MySQL, Apache, HHVM, Cloudfront .  What it does:  This is what you interact with when using Slack in the browser. Many of the requests and features interact with the API and WebSocket endpoints, but there are also features unique to the web experience. .  What to look for:  Web application flaws, XSS, CSRF, auth bypasses .  What it runs on:  PHP, MySQL, Apache, HHVM, Cloudfront, HTML/jQuery/Handlebars .  What it does:  This is what you interact with when using the Slack app on your desktop (Windows/Mac/Linux). Many of the requests and features interact with the API and WebSocket endpoints, but there are also features unique to the desktop experience. .  What to look for:  Authentication bypasses, bypasses of permissions, information leakage .  What it runs on:   Electron  frontend, which communicates with the API/web backend .  What they do:  This is the suite of mobile apps for iOS, Android and Windows Phone. They run native mobile OS code and have connections to the API endpoint and WebSocket connections to the WebSocket endpoints. .  What to look for:  Mobile-app issues, insecure storage of sensitive information, such as hardcoded secrets or tokens .  What it runs on:  Native frameworks for each OS (Java, Objective-C, C#) .  What it does:  Slack passes messages through its Real-Time Messaging servers (the  RTM API ). These are real-time communications that are conveyed via WebSockets to our backend. .  What to look for:  Authentication bypasses, bypasses of permissions, information leakage, messaging arbitrary teams or users .  What it runs on:  Java, Go, WebSockets . In all product areas we are always very interested in bugs which: . We hope you’ve found this information useful. If you’re a researcher we welcome your vulnerability reports at  https://hackerone.com/slack . For general information about Security at Slack, please visit  https://slack.com/security . ", "date": "2017-03-16"}, {"website": "Slack", "title": "TypeScript at Slack", "author": ["Felix Rieseberg"], "link": "https://slack.engineering/typescript-at-slack/", "abstract": " When Brendan Eich created the very first version of JavaScript for Netscape Navigator 2.0 in merely ten days, it’s likely that he did not expect how far the Slack Desktop App would take his invention: We use one JavaScript code base to build a multi-threaded desktop application, routinely interacting with native code, targeting Windows, macOS, and Linux. . Managing large JavaScript codebases is challenging — whenever we casually pass objects from Chrome’s JavaScript to Objective-C just to receive a callback on a different thread in Node.js, we need a guarantee that the individual pieces fit together. In the desktop world, a small mistake is likely to result in an application crash. To that end, we adopted  TypeScript  (a statically typed superset of JavaScript) and quickly learned to stop worrying and love the compiler. It’s not just us, either: In the  2017 StackOverflow Developer Survey , TypeScript was the  third most-loved programming technology . Given how quickly static type checking is gaining traction, we wanted to share our experiences and practices. . In the past, we used  JSDoc  to document our function signatures, using comments to inform code wanderers about the purpose and proper usage of classes, functions, and variables. This isn’t without its problems. Looking at the code, it’s hard to know what a JavaScript promise resolves with. You have to trust that the person who wrote the code documented it correctly and that people who changed it later correctly updated the documentation. In complex systems with countless modules and dependencies, it’s entirely possible to break a function without ever opening the file it lives in. . To improve our situation, we decided to give  static type checking  a shot. A static type checker does not modify how your code behaves at runtime — instead, it analyzes your code and attempts to infer types wherever possible, warning the developer before code ships. . A static type checker understands that  Math.random()  returns a number, which does not contain the string method  toLowerCase() . . To be more explicit, the user of such a type checker can support the system by manually declaring types — to inform both human and machine how the program is supposed to behave. The code below defines an interface for a “user” object and a method that is supposed to get the user’s age. A static type checker is able to analyze this code and warn about typical human errors, like expecting a possibly undefined property to always be there. . Interestingly, the code is not modified at runtime, meaning that a static type checker introduces no overhead for the end user. The above example at runtime looks like vanilla-flavoured JavaScript: . A smart static type checker increases our confidence in our code, catches easily made mistakes before they are committed, and makes the code base more self-documenting. . We decided to use Microsoft’s TypeScript, which combines static type analysis with a compiler. Modern JavaScript is valid TypeScript, meaning that one can use TypeScript without changing a single line of code. This allowed us to use “gradual typing” by enabling the compiler and the static analysis early, without suspending work on critical bug fixes or new features. . In practice, switching the analysis and the compiler on without changing code means that TypeScript will immediately attempt to understand your code. It uses built-in types and type definitions available for third party dependencies to analyze the code’s flow, pointing out subtle errors that went previously unnoticed. Wherever TypeScript cannot understand your code, it will assume a special type called “any” and simply move on. . Our original plan was to slowly port files over, extending our vanilla JavaScript with more concrete type definitions wherever possible — adding interfaces, defining class methods as either private or public, and declaring enums. Along the way, we made two surprising discoveries: .  First , we were surprised by the number of small bugs we found when converting our code. Talking to other developers who began using a type checker, we were delighted to hear that this was a common experience: the more lines of code a human writes, the more inevitable it becomes to misspell a property, assume the parent of a nested object to always exist, or to use a non-standard error object. .  Second , we underestimated how powerful the editor integration is. Thanks to TypeScript’s language service, editors with an autocomplete function can support the development with context-aware suggestions. TypeScript understands which properties and methods are available on certain objects, enabling your editor to do the same. An autocomplete system that only uses words in the current document feels barbaric afterward. Gone are the days we find ourselves on Google, checking  yet again  which events are available on Electron’s BrowserWindow. Plugins are available for  Atom ,  Visual Studio Code ,  Sublime , and nearly every other editor out there. Being able to validate our code without leaving the editor boosted our productivity immediately. . Looking ahead and thinking about code maintenance, we appreciate the ecosystem around TypeScript. As heavy users of React and the Node/npm ecosystem, the availability of type definitions for third-party libraries is a huge plus. Many of the libraries we import are already TypeScript compatible. If definitions do not ship with the module itself, they are likely to be found in the fantastic  DefinitelyTyped  project. React, for instance, does not ship with type definitions, yet a simple  npm install @types/react  installs them with no further configuration required. . TypeScript was such a boon to our stability and sanity that we started using it for all new code within days of starting the conversion. It’s taken about six months to annotate most of the JavaScript in the desktop app code base. . To enforce readability and maintainability, all staged code is automatically checked with  TSLint  as a pre-commit hook — meaning that before Git commits your changes, it will first check whether or not the changed code passes our linting rules. We disallow the use of the “implicit any”, meaning that we now require all of Slack Desktop’s code to explicitly state the type wherever TypeScript cannot automatically infer it. . When the time comes to push a branch, Git first runs the whole codebase against TypeScript’s compiler, which analyzes the whole code base for structural and functional errors and transpiles modern features like async/await into ES2016-compatible code. By the time a Pull Request is opened, we already have the confidence that the structural dependencies within our code are sound. . To us, the benefits of TypeScript dramatically outweigh the downsides — which do exist. Most notable to us is the additional training cost. Developers who have experience using any strongly-typed language usually pick up the syntax within an hour or two, but a file that makes full use of all of TypeScript’s features can look daunting to developers with a vanilla JavaScript background. . The most obvious solution to that problem is to phase features in slowly — you can simply enable TypeScript without changing any code, add some simple type declarations, and save more complex concepts like inheritance, generics, and advanced types (intersection types, mapped types) for either specific modules or a later stage. In the end, our experience is that one can reap a lot of benefits with the most basic use of TypeScript. . At Slack, we strive to be good open-source citizens. To that end, we strive to make the move to TypeScript easier for other developers: Whenever we find gaps, we try to close them. . Most notably, Slack’s own electron-compile allows developers of Electron Apps to write in TypeScript without having to worry about the compilation itself. RxJS, a Reactive Extension library heavily used at Slack, Netflix, GitHub, and many other companies, made the move to TypeScript with Slack’s support. The many small libraries written by our desktop engineers are all slowly gaining TypeScript support (like  spawn-rx ,  electron-spellchecker ,  electron-remote ,  electron-notification-state , or  electron-windows-notifications ). . To get started with TypeScript, check out  the official handbook . If you’re wondering what a small port looks like in practice, consider  taking a peek at the port of spawn-rx . If you’d like to write your first lines of TypeScript for an Electron app, consider the excellent  electron-forge , which implements electron-compile and supports TypeScript out of the box — it even comes with an excellent React/TypeScript template, which is an architecture we in the Slack Desktop Team love dearly. If mixing modern web technologies with native code to build a cross-platform desktop app sounds like an exciting mission to you,  come work with us ! ", "date": "2017-04-12"}, {"website": "Slack", "title": "Rebuilding Slack’s Emoji Picker in React", "author": ["Chris Montrois"], "link": "https://slack.engineering/rebuilding-slacks-emoji-picker-in-react/", "abstract": " Slack is transitioning its web client to React. When Slack was first built, our frontend consisted of established technologies like jQuery and Handlebars. Since then, the community has developed better ways to create scalable, data-driven interfaces. jQuery’s “render and modify” approach is straightforward, but it’s prone to falling out of sync with the underlying model. In contrast, React’s “render and re-render” pattern makes consistency the default. Slack is evolving alongside the industry to improve performance and reliability. . We determined that the best way to introduce React would be to rebuild an existing product feature — that way, we could compare the development process and end result to a known quantity. We wanted a component that was interactive, self-contained, and demanding enough to prove our assumption that React could improve performance. It didn’t take long to find a perfect candidate — the highly used and surprisingly complex Emoji Picker. . This post assumes some knowledge of React. If you’re unfamiliar with it, I’d suggest browsing the  official docs . Briefly, React is a JavaScript library that makes it easy to write declarative, data-driven user interfaces. The API is trim, consisting primarily of a  Component  class that includes a handful of lifecycle methods. Components don’t generate markup on their own. Instead, they render into a DOM-like tree called the Virtual DOM. React can compare two Virtual DOM trees to determine the fewest actions required to transform the first tree into the second. For instance, you could tell React to re-render the entire view with new model data, and it might determine that it only needs to update the text of a few nodes. React is like having an army of gnomes making bespoke DOM updates on your behalf. . React excels in consolidating all the ways a component can change into a single template. For example, consider how you’d use vanilla JavaScript to update Slack’s channel sidebar when a channel becomes unread: . The process is straightforward, but you’d have to write additional handlers to support other channel events like “create”, “join”, “leave”, and “rename”. In contrast, React accommodates all five scenarios with: . Instead of handwriting each DOM update, we get to re-render the entire component and let React figure out how to do it efficiently. This approach streamlines development by trading specialized code paths for generic, one-size-fits-all templates. . Emoji are an integral part of Slack’s UI and the Emoji Picker is an ideal React component. It’s dynamic, discrete, and requires only a few inputs — a list of emoji, preferred skin tone, and the user’s historical emoji usage. And coincidentally, the current Emoji Picker was in need of a performance tune-up due to its unfortunate strategy of rendering every emoji regardless of whether it was in view. Searching involved toggling the visibility of every node that did or didn’t match the user’s search terms. It was performance death by a thousand cuts. New Slack teams start with 1,374 default emoji and it increases as you add customs (as of this writing, Slack’s team has 3,126 total emoji, but some teams have even more!) Rebuilding the Emoji Picker would give us the opportunity to impact everyday Slack usage in a meaningful way. . We chose to develop in  Storybook , a self-proclaimed “UI development environment you’ll ❤️ to use”. It doesn’t replace your style guide, but it does make developing, testing, and reviewing code more pleasant. Storybook allows you to define variations of your component by specifying different sets of properties. For the Emoji Picker, we added a variant for skin tone preference and multiple variants for search. Anyone at Slack — developer or otherwise — can open Storybook and view the enumerated states. . The React Emoji Picker is built from a stateful root component with many stateless children. We adhered to the convention of exporting a single component per file. The general structure is as follows: . There are 2 primary methods for writing stateless components in React: the  PureComponent  class and  functions . Functions are simpler, but they render on every reconciliation which can hurt performance. The React team has plans to optimize functions, but for now it seems best to avoid them. We chose instead to use PureComponent, which includes a predefined shouldComponentUpdate method that prevents updates when passed identical props. . Since React is just a view layer, incorporating it into an established application can be more straightforward than integrating a prescriptive framework. It was important for us not to compromise the new Emoji Picker’s encapsulated design in order to accommodate patterns that exist in Slack — we wanted the component to look like it had been plucked from an end-to-end React app. In order to keep the picker pure, we created a lightweight adapter in our existing module system. The adapter mounts the component, extracts model data, and listens to external signals. This pattern allows us to incrementally port the codebase while we develop new features. . While developing with React was a joy, integrating it into our pre-existing development workflow was not — at least not at first. At the time, Slack’s frontend build pipeline had been developed in-house and had no notion of imports, dependencies, or complex transformations like transpilation. We were, however, determined to take full advantage of JSX syntax as well as the benefits of writing ES2015+ JavaScript. In lieu of a modern build pipeline, we started out by building the Emoji Picker assets locally with Babel and webpack. . We expected that checking in locally compiled code would be painful, but we underestimated just how exasperating the ensuing merge conflicts and dependency management would be. As a result, we’re working on integrating webpack into our development and staging environments, with the goal of seamlessly replacing existing workflows. To accommodate this, we: . Rebuilding the Emoji Picker turned out to be exactly the impetus we needed to rethink our build pipeline to bundle and transpile assets in a more robust and scalable way. . We deployed the new component to a small percentage of teams and observed the results. We measured render speeds across 5 common ways users interact with the Emoji Picker. The React implementation was noticeably faster for most actions. Listed below are the differences in render times for a typically sized team: . The biggest improvement came during “First mount” which dropped 270ms falling from 318ms to 48ms, an 85% decrease. This is largely attributed to  react-virtualized  — a virtual list library — which reduced the number of rendered emoji. The React Emoji Picker renders 85% fewer DOM nodes in the default view. . Perhaps the most surprising change came during “Search (many results)” which gained 27ms increasing from 17ms to 44ms. The legacy picker visually hid emoji that failed to match the search query which means it’s relatively fast when almost everything matches. This downside of this approach is evidenced by the “Search (one result)” and the “Reset search” scenarios which force the legacy picker to perform work on a larger set of emoji. . Rebuilding the Emoji Picker in React resulted in faster rendering and simplified code that’s easier to maintain. We’re extending these gains to the rest of our codebase by transitioning fully to React. We have a lot of work left to do, and we’re excited for the positive impact the transition will have on the everyday experience of our customers. At the same time, we’re building in-house expertise with React so we can help push the platform forward. .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our engineering jobs and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2017-05-24"}, {"website": "Slack", "title": "Flannel: An Application-Level Edge Cache to Make Slack Scale", "author": ["Bing Wei"], "link": "https://slack.engineering/flannel-an-application-level-edge-cache-to-make-slack-scale/", "abstract": " Professor Robin Dunbar, when studying  Neolithic farming villages and primate troupes  in the 90s, theorized that the maximum number of stable relationships we can keep is around 148, known popularly as Dunbar’s number. This upper bound is due to the mental dossier kept on individual’s relationships, but more importantly, the number of cross relationships between everyone else, whose number grows geometrically. Today, your Slack client is the window into your workplace, and teams have grown into the tens of thousands of people, much larger than any primitive village. Slack was architected around the goal of keeping teams of hundreds of people connected, and as teams have gotten larger, our initial techniques for loading and maintaining data have not scaled. To address that, we created a system that lazily loads data on demand and answers queries as you go. . Your Slack client strives to be consistent, compact, and searchable replicas of users, files, and messages that a team shares in real time. It initializes with information designed to expedite immediate use. The Slack server sends a full snapshot of a team’s data via the rtm.start API call with many things a client needs: users, channels, members in those channels, the latest messages, DMs, group DMs, etc. The client then uses this data to bootstrap itself and establishes a WebSocket to the server to get a stream of real-time events on the team. Users start typing and messages flow from your keyboard to the screens of your colleagues. . This system, where a client loads everything on startup, is viable for small teams. When teams get especially large, however, the number of channels, users and bots become unwieldy; startup time and client overhead both suffer as a result. Teams larger than tens of thousands of users would start to notice: . How can we maintain all the features of instant access while minimizing client storage size and load on the server? The key is for the client to load minimal data upfront, and only load channel and user data as needed. As long as the experience is seamless from the user’s point of view, we are free to apply resource-saving tactics like lazy loading and just-in-time loading to the architecture of the application. . This lazy loading is the ideological birthplace of Flannel, an application-level caching service developed in-house and deployed to our edge points-of-presence. Upon client startup, Flannel caches relevant data of users, channels, bots, and more. It then provides query APIs for clients to fetch upon demand. It foresees data that will be requested next and pushes data proactively to clients. For example, let’s say you mention a colleague in a channel: While broadcasting that message to people in the channel, Flannel sees that some clients have not loaded the information about the mentioned user recently. It sends the user data to those clients just before sending the message to save them a round-trip query. . Now, when a user wants to connect to Slack, the following happens: . Immediately after connection, the client still only has bare bones data on the state of its constituent channels, so when the user takes advantage of find-as-you-type autocomplete, the client sends requests to Flannel and renders the results. You can test this out right now in your Slack window: If you open the Quick Switcher ( cmd+k on Mac, ctrl+k on Windows ), your client has little to no knowledge of the state or details of your team. However, as soon as you start typing, within moments, the autocomplete query will have been filled and the dropdown menu should then populate with suggestions. A similar cascade of events happen when you start typing a username in the message bar. . We use consistent hashing to choose which Flannel host a user connects to in order to maintain team affinity: Users on the same team who are from the same networking region are directed to the same Flannel instance to achieve optimal cache efficiency. When new or recently disconnected users connect, they are served directly from the Flannel cache, which reduces impact of reconnect storms to the Slack backend servers. . Flannel has been running in our edge locations since January. It serves 4 million simultaneous connections at peak and 600K client queries per second. . This has been an ongoing effort among the Web Client, Mobile and Backend engineering teams. We’re already using Flannel to increase scalability in Slack, but we have even bigger plans ahead: . Hutterite settlements split up when they grew past the magical 148 number, and many Polynesian tribes sent out exploration parties when their island villages grew too large. This was because they could no longer keep track of everyone they interacted with. Their limiting factor was the amount of information each person had to keep in their mental cache. Flannel resolves similar constraints for Slack clients using central servers, providing information on demand. Features like fanout at edge points-of-presence and pub/sub further reduces the computational load on both the server and the client. With advancements like this in place, Slack evolves past its original design and is able to serve teams with hundreds of thousands of users. . We will continue to look for ways to make Slack faster and more reliable. If you’re interested in helping,  get in touch . ", "date": "2017-06-01"}, {"website": "Slack", "title": "Into the Clouds", "author": ["Sam Wolfand", "Ali Ravanchi"], "link": "https://slack.engineering/into-the-clouds/", "abstract": " At Slack we use push notifications to let you know when someone sends you a direct message, or posts in a channel. As part of our Growth team efforts, we wanted to experiment with using push notifications to inform you of other things happening within your Slack team. While this sounds simple, there were a few technical challenges along the way. . The current push notification system is tightly coupled to the idea that a push notification is triggered by user-to-user interaction. As such, when the idea was proposed for all-purpose notifications — where we could send notifications for any event, not just user messages— we set about trying to decouple this architecture and add to it in a way that maximized extensibility and didn’t break backwards compatibility. . One of the biggest challenges was making sure all our new code paths wouldn’t affect the functionality of the existing clients. On Android, this meant updating our existing payload data model in a way that allowed for flexibility and extensibility. This is what our notification payload model looked like after adding the fields needed to handle these new push notifications: . To get the functionality required out of deep links, we created a “Trampoline” activity that received deep link intents and routed them correctly using a simple custom parser. Here is the flow: . The biggest issue we faced while implementing this logic was that of backwards compatibility. For example, what if the server sends us a Notification Type we don’t yet know about, or doesn’t send one at all? There There are three ways we could handle this: . We went with #2, so if we get a push of an unknown type, we simply throw it away. We could guarantee the types we knew about, so there was no real harm in this. . Next, we needed to modify our existing notification processing logic. Based on the type of the notification, the class has a few responsibilities: . The class logic branches based on the notification type (i.e. pre info type): . After creating the notification processing logic, we thought about how the system should respond to info notifications. . We have decided that info type pushes would be very simple: . The following pseudocode shows our logic for handling these cases: . The rest of the notification design goes deeply into the weeds of the Android-specific system construct, NotificationCompat.Builder(), and we’ll omit that here. There is, however, one important thing to mention: You  must  have unique and consistent notification IDs. If you don’t, you’ll end up creating duplicate notifications and/or canceling notifications will have no effect. . On iOS, there were a couple of considerations that had to be taken into account when handling the new notification payloads. When the app is launched from a push notification, the delegate callbacks are operated on as follows: . DeepLinkHandler is a singleton that is responsible for registering and maintaining our deep link routers. It has no awareness of the app state and only cares about the status of all the routers it handles; you can think of it as a traffic controller. When you login to a team, we set up a deep link router that defines behavior to handle deep links with the Slack scheme. . Our routers conform to a protocol that makes the following methods available to the DeepLinkHandler: . When we ask the DeepLinkHandler if it can handle a URL, it checks to see whether the URL is structured correctly, if we have a matching router (given the structure of the URL), and if the deep link router can handle that URL. For example, our deep link router can handle actions like opening a channel, opening a direct message, or opening a file. . When asking the DeepLinkHandler to handle a URL, it ensures that the router is ready for presenting, and if not, it waits until it is. The router has the context of the app, and will do the necessary operations to get our app into an actionable state. Therefore, it is only ready for the presentation of a deep link when our data stack and UI have been created. This is especially important if we’re routing to a different team than the one you’re active on. The following pseudocode shows the logic our router goes through to determine what to tell the DeepLinkHandler . Once we’re given the go-ahead from the router that it is ready, the DeepLinkHandler calls upon the router to handle the presentation of the URL. . We wanted the ability to track the impact these push notifications had on our users. Each time a notification action occurs (e.g. received or tapped), we send over the pushId, type, and date of the notification to our analytics warehouse. By comparing these events with how many people have subscribed to our push notifications, we can start identifying trends and scrap notifications that users don’t find valuable. . For every push received, UNNotificationServiceExtension (iOS) and NotificationHelper.java (Android) are called; that means we can add a statement to the top of those classes that tracks when a push is received. . Initially, we assumed that we would be able to send the tracking request to the server as soon as a notification came in, but due to flakey network connections we saw some analytics requests get dropped. In the future, we plan to store push notification ids in local persistence until the analytics are caught up. This will allow us to piece together the full picture of whether our all-purpose push system actually had the desired effect. . Once we had a system to in place to deliver certain information to users, it was decided that our first test case would be sending a push notification when a team gained its first member . Looking at early data points, we were excited to see that all our work had paid off, and the experiment was a success! During our initial phased rollout we saw a  7.0%  lift in users invited and a  1.5%  lift in teams sending messages. . This work was an exercise in forethought: Every new field that we added to the push payload had the potential to crash the app, every notification we sent had the potential to annoy our users, and every new piece of analytics gleaned had the potential to draw a clearer picture of the effectiveness of the project. By creating a flexible system, we’re well set up for new kinds of push notifications in the future. .  If you’re interested in changing how people get work done from their mobile device, come check our    mobile engineering job listings   !  ", "date": "2017-06-07"}, {"website": "Slack", "title": "Technical Leadership: Getting Started", "author": ["Brenda Jin"], "link": "https://slack.engineering/technical-leadership-getting-started/", "abstract": " Before I became a software engineer, I thought that the most important part of the job would be coding. I was wrong. The most important (and hardest) part of software engineering is working well with other humans. . “I’ll never become a manager!” I told myself, “That way, I can focus all my attention on computers!” I thought that I could ignore the difficult human parts of my job if I stayed on the technical individual contributor (IC) track for the rest of my career. . “If you want to go fast, go alone. If you want to go far, go together.” . While I was avoiding the human aspect of my job, I’d also wonder, “Why aren’t people listening to my ideas?” This was particularly salient when I first started working on the Platform team at Slack. I wanted to transform the way our APIs used tokens so that they would be more secure, and allow the API development cycle to be more consistent across product development teams. But for months, it was impossible to convince any teammates or product managers that this endeavor was a valuable use of anyone’s time. . After some failed attempts at selling my ideas, and after observing senior engineers on my team get their ideas adopted, I realized the missing ingredient: leadership. I couldn’t keep my nose glued to the keyboard every day. If I wanted to advance, I needed to help other people contribute at the same level as me. I needed to increase my impact through leadership. . This story will explain what I’ve learned about leadership and how to get started on an IC leadership path. . As an engineer at Slack, I came to understand how leadership is different from management. . Managers are responsible for their reports. They focus on team-building through coaching and team structure. They also manage performance in order to support the growth of their team. . Managers are often leaders, but leading is something separate that anyone can do. Leadership is about inspiration. It’s about influencing others without relying on authority. It’s about communicating a vision and empowering others to support that vision. . Before you can lead others, you must be able to lead yourself.  Leading Self  must happen before  Leading Others  and  Leading Organizations . The concept of  Leading Self  can be found in leadership literature across a variety of industries and companies. .  Leading Self  is about personal excellence, because leadership by example is one of the most powerful ways to inspire others.  Leading Self  means performing at your best and being responsible for the quality of your own work. . The five components that drive successful leadership of the self are: Finding Alignment, Becoming an Expert, Sharing, Executing Consistently, and Communicating Effectively. . In order to be excellent at work, you’ll need to understand your team, and ultimately, your company. . Principles are your company’s norms that guide which actions are desirable or undesirable. Many times these principles are not stated outright, and it’s your responsibility to uncover them. These  principles  are your compass. They will help you navigate decisions according to the company’s  goals  and  values . . At Slack, for example, we believe in providing an excellent experience for our users. When a customer reports that a core feature is broken, it is important for me to drop what I’m doing, investigate, and fix the issue immediately. At another company, dropping what I’m doing might be the wrong decision entirely. . Most decisions must be weighed on multiple axes. Do you tackle the tech debt today or lay the foundation for tomorrow? Is it better to squash bugs, build tools, or ship features? There is a limited total amount of time and energy you can devote at work. Being able to align your efforts with the company will ensure that your contributions have maximum impact. . Alignment isn’t just about doing what the company asks of you, either. As a leader, you will have plenty of opportunities to identify problems and potential solutions. But in order to convince other people that these problems are worth solving, you’ll need to understand what’s important to the company and be able to communicate that to others. . Becoming an expert is about honing your skills. Having potential is one thing, but it’s not enough. Leaders must be experts in practice. According to Anders Ericsson, a Professor at the University of Colorado, it takes an average of  10,000 hours of high quality, deliberate practice  over an average of ten years to become an expert. . People often ask me whether the fact that I used to sing opera helps me in my software engineering career. Yes! Through music, I developed a  growth mindset . When practicing an aria, I’d spend hours of rehearsal going through the weakest parts, working them until they sounded as natural as the strongest parts. Software engineering is the same — we need to spend time developing our weakest areas. . There are  no shortcuts  to mastery. Instead, it is something that needs to be intentionally developed. Ask yourself (and those around you): What are my biggest areas for growth? What skills do I need to develop in order to become an expert? . While there may be a lot of skills you want to develop, I encourage you to use the following questions to direct your effort: Does the skill align with the company? Does the skill align with my own personal goals? . Don’t focus on the judgment of “not being there yet,” but rather the deliberate process of gaining knowledge and skills day-by-day. Nobody is born an expert. . After  Leading Self , you’ll begin to  Lead Others  and empower the people around you to contribute their best work. To do that, you’ll need to share knowledge. . Sharing knowledge can be unintuitive after you’ve spent so much of your time mastering a skill. It is instinctual to want to “own” a specialty. Expertise can feel magical when effort is invisible, and you might be inclined to hide your magic in a secret box, tuck it away, and pull it out in times of need. Your expertise will stay magical, because other people won’t know how to do it. . But here’s the thing — when you keep knowledge to yourself, you are not helping other people around you grow, because you are making others depend on you. You are not helping yourself grow, because you are not freeing yourself up to learn new things. You are preventing others from contributing, and you are actively hurting the team. . I used to keep information to myself, not because I wanted to hide it, but because I didn’t realize that I had it. For example, my projects consistently navigated the crossroads that stumped many other projects: kickoff, final milestones, and regression-free releases. I wanted my teammates to succeed, so I started paying attention to things that worked well and earmarking those skills to share with others. It didn’t matter if only my projects executed well — that’s a 1 x  impact that doesn’t scale. What matters is that the whole team is able to execute — that’s a N x  impact. . Instead of hiding your knowledge, share it. Teach other people how to do the things you have learned to do in both 1:1 settings such as mentorship and pair programming, or in 1:N settings such as presentations and documentation. Other people, in turn, will teach the next person. You’ll be free to move on to the next skill that you want to learn. Knowledge is an infinite well — there will always be more. . I remember an early conversation I had with my manager, where I told him that I had done especially well on my most recent project and asked when I could be promoted. His wise response: “You need to demonstrate that you can do this with the same quality —  consistently .” . That’s the difference between luck and leadership — consistency. . What matters is  not  that you can do one thing well, one time. What matters is that you can do it again, and again, and again. . To execute consistently, you’ll need to seek out projects of all sizes and types — small, large, cross-functional, user-facing, and non-user-facing. This will expose you to a variety of challenges and develop your approach to problem-solving. It will show you your weaknesses and sharpen your skills. . Let your manager know which skills you’re looking to build. Keep an eye out for upcoming projects, and proactively let your manager know which ones spark your interest and why. During a project, think about how you can target specific skills to develop while you’re working on it. This will help you maximize your time at work. . Sometimes you will be assigned to a project that you don’t feel passionate about, but which is mission-critical for the team. You’ll need to prove you can do these well, too. . Executing consistently will also develop your personal brand and build trust with your coworkers. This trust takes time and experience to cultivate and develop, and it doesn’t happen overnight. This trust will also stay with you through all of your subsequent endeavors. . Have you ever wondered, “Why don’t people listen to me?” . Many times in the beginning of my career, I asked that same question. Then, one day, when I was ready, my boss gave me a clue: I was being perceived as negative. At first, this feedback hurt my feelings. But it turned out to be transformational in my career. I began working with a voice coach and unlocked the secret to effective communication:  listening . .  Listening is not just about receiving information.  It’s about synthesizing information and its context. It’s about understanding where the other person is coming from and asking clarifying questions to gain an even deeper understanding. Believe it or not, this synthesis is the first step to effective communication, and eventually having your ideas take flight. . 3. As levels go up, manager &amp; engineer tracks require identical communication skills. The real superpower on both sides is enabling others. . — Sarah Mei (@sarahmei)  May 11, 2017  .    . Another aspect of effective communication is repetition in the appropriate contexts. While I used to wonder why people “wouldn’t listen”, I also used to get annoyed when I had to repeat myself. . It wasn’t until later that I started to observe effective communicators at work. They surface information in a variety of channels. Not only do they communicate important information at the right intervals — they also provide an appropriate amount of detail, depending on the audience. . Learning to listen and synthesize information, plus learning how to share it effectively, is a foundational skill to influencing people without authority. You’ll need this influence to rally people around a collective vision. . When I started my career in software engineering, I questioned why my ideas weren’t being applied more often in my company and across the industry. Then, when I took a moment to look up from my computer and engage the brilliant people around me, I realized that I could have much more impact if I was able to inspire others to strive for the quality that I envisioned. . There is so much to learn when it comes to leadership, and I personally have a lot more to learn as well. If you want to become an IC leader, I encourage you to start by  Leading Self . Let me know what you discover! .  Want to solve tough problems in a great environment to practice Leading Self? Check out our engineering jobs and    apply today   .  ", "date": "2017-07-20"}, {"website": "Slack", "title": "Syscall Auditing at Scale", "author": ["Ryan Huber"], "link": "https://slack.engineering/syscall-auditing-at-scale/", "abstract": " If you are are an engineer whose organization uses Linux in production, I have two quick questions for you: . 1) How many unique outbound TCP connections have your servers made in the past hour? . 2) Which processes and users initiated each of those connections? . If you can answer both of these questions, fantastic! You can skip the rest of this blog post. If you can’t, boy-oh-boy do we have a treat for you! We call it  go-audit . .  Syscalls  are how all software communicates with the Linux kernel. Syscalls are used for things like connecting network sockets, reading files, loading kernel modules, and spawning new processes (and much much much more). If you have ever used strace, dtrace, ptrace, or anything with trace in the name, you’ve seen syscalls. . Most folks who use these *trace tools are familiar with syscall monitoring for one-off debugging, but at Slack we collect syscalls as a source of data for continuous monitoring, and so can you. .  Linux Audit  has been part of the kernel since 2.6.(14?). The audit system consists of two major components. The first component is some kernel code to hook and monitor syscalls. The second bit is a userspace daemon to log these syscall events. . To demonstrate what we can do with auditd, let’s use an example. Say we want to log an event every time someone reads the file /data/topsecret.data. ( Note: Please don’t store actual top secret data in a file called topsecret.data ). With auditd, we must first tell the kernel that we want to know about these events. We accomplish this by running the userspace auditctl command as root with the following syntax: . Now,  every  time /data/topsecret.data is accessed (regardless of whether it was via symlink), the kernel will generate an event. The event is sent to a userspace process (usually auditd) via something called a “netlink” socket.  (The tl;dr on netlink is that you tell the kernel to send messages to a process via its PID, and the events appear on this socket.)  . In most Linux distributions, the userspace auditd process then writes the data to /var/log/audit/audit.log. If there is no userspace process connected to the netlink socket, these messages generally appear on the console and can be seen in the output of dmesg. . This is pretty cool, but watching a single file is also a very simple case. Let’s do something a bit more fun, something network related. . Daemon processes ( or rogue netcats, ahem ) usually use the listen syscall to listen for incoming connections. For example, if Apache wants to listen for incoming connections on port 80, it requests this from the kernel. To log these events, we again notify the kernel of our interest by running auditctl: . Now,  every  time a process starts listening on a socket, we receive a log event. Neat! This logging can be applied to any syscall you like. If you want to handle the questions I mentioned at the top of this post, you’ll want to look at the connect syscall. If you want to watch every new process or command on a host, check out execve. .  Supernote: We are not limited to the actions of users. Think about advanced cases like apache spawning `bash` or making an outbound connection to some sketchy IP and what that can tell you.  . So now we have a bunch of events in /var/log/audit/audit.log, but logfiles do not a monitoring system make. What should we do with this data? Unfortunately, there are some properties of the auditd log format that make it challenging to work with: . There are a few existing tools to parse these log events, such as aureport or ausearch, but they seem to be focused on investigations that happen after the fact, as opposed to being used continuously. . We saw a lot of potential uses for the data we could get from auditd, but needed a way to run this at scale. We developed the project   go-audit   as a replacement for the userspace part of auditd, with the following goals in mind: . The first three items on that list probably won’t raise many an eyebrow, but the last one should, so it feels worth explaining. . Some obvious questions we need to address with relation to #4 are, “ Why don’t we want to filter these events on each server and just target the interesting ones? ” and “ Won’t you send lots of useless information? ” . Imagine your servers have the curl command installed ( no need to imagine, yours probably do ). During a red team exercise, the attackers use curl to download a rootkit and then to exfiltrate data. Having learned from this, you start logging every command and filtering everything that isn’t curl. Every time someone runs curl, you generate an alert. . There are some serious problems with doing things this way: . We need something better. What if instead of looking for specific commands, we send everything to a centralized logging and alerting infrastructure? This has some amazingly useful properties: . So here it is friends,   go-audit  . We are releasing this tool as open source, for free (as in love). The Secops team at Slack created the first version of go-audit over a year ago, and we have been using it in production for nearly that long. It’s a small, but important, piece of our monitoring infrastructure. [ I recommend you check out my previous post for more context on how we handle alerting. ] . In the go-audit repository, we have provided extensive  examples  for configuration and collection of this data. Here at Slack, we like rsyslog + relp, because we want to send everything off host immediately, but also spool events to disk if syslog is temporarily undeliverable. You can pretty freely use a different mechanism to deliver these logs, and we look forward to seeing your ideas. . We welcome contributions to this project and hope others will find it useful. This repository was privately shared with a number of external folks over the past year, and some friends of ours are already using it in production. . You may have noticed that I haven’t used the word “security” yet in this post. I’m of the opinion that good general purpose tools can often be used as security tools, but the reverse is not usually the case. Auditd facilitates security monitoring that you’d be hard pressed to replicate in any other way, but go-audit was developed as a general purpose tool. The utility of something like go-audit is immediately apparent to an operations or development person, who can use it to debug problems across a massive, modern fleet. . Let’s revisit the questions at the top of this post. Any company with IDS/IPS/Netflow/PCAP/etc on a network tap can tell you a lot about their network connections and probably answer the first question, but none of those solutions can give you the context about a user/pid/command needed to answer the second. This context is the difference between “someone ran something somewhere on our network and it connected to an IP” vs “Mallory ran curl as root on bigserver01 and connected to the IP 1.2.3.4 on port 1337”. . At Slack, we often say “Don’t let perfection be the enemy of good”. This tool isn’t perfect, but we think it is really, really good, and we’re happy to share it with you today. . Osquery is great. In fact, we use it at Slack. For our production servers, we prefer go-audit because these systems are connected 24/7, which allows us to stream data constantly. With osquery, you are generally receiving a snapshot of the current machine state. If something runs to completion between polling intervals, you might miss it. I think this model works well for laptops and other employee endpoints, but I prefer a stream of data for highly-available machines. . Sysdig is also a fantastic debugging tool, and I’ve used it pretty extensively. The main issue here is that sysdig requires a kernel module be loaded on each machine. Sysdig Falco looks to be useful, but they prefer to have detection running on each endpoint. As mentioned above, we prefer centralized rules that aren’t visible to the attacker logged in to a host. . Auditd has the advantage of having been around for a very long time and living in the mainline kernel. It is as ubiquitous a mechanism for syscall auditing as you’ll find in Linux-land. . We send them to an Elasticsearch cluster. From there we use  ElastAlert  to query our incoming data continuously for alert generation and general monitoring. You can also use other popular large scale logging systems for this, but ( my opinion not the opinion of my employer disclaimer goes here ) I have big fundamental problems with pricing structures that incentivize you to log less to save money. . Short answer: It is highly variable.  Long answer: It depends which syscalls you log and how many servers you have. We log hundreds of gigabytes per day. This may sound like a lot, but as of this writing we have 5500-ish instances streaming data constantly. You should also consider over-provisioning your cluster to ensure that an attacker will have a difficult time DoSing your collection infrastructure. . We have a lot of experience with rsyslog, and it has some nice properties. We highly recommend using version 8.20+, which has some bug fixes we contributed back to upstream. We could have let go-audit handle reliable delivery, but the advantages of doing so didn’t outweigh the benefit of using something that we’ve been using successfully for years. . You should be using canaries to validate data flowing in from each of your servers. Generating events that you expect to see on the other side is a useful way to validate hosts are reporting to you. Additionally, go-audit also has mechanisms to detect messages that were missed within a stream. .  My teammate    Nate Brown   , for making go-audit much better.  Mozilla’s    audit-go    folks, for inspiring this project.  The numerous individuals who reviewed this blog post and gave feedback.  The    Chicago Cubs   , for winning the World Series.  ", "date": "2016-11-22"}, {"website": "Slack", "title": "Data Wrangling at Slack", "author": ["Ronnie Chen", "Diana Pojar"], "link": "https://slack.engineering/data-wrangling-at-slack/", "abstract": " For a company like Slack that strives to be as data-driven as possible, understanding how our users use our product is essential. . The Data Engineering team at Slack works to provide an ecosystem to help people in the company quickly and easily answer questions about usage, so they can make better and data informed decisions: “ Based on a team’s activity within its first week, what is the probability that it will upgrade to a paid team? ” or “ What is the performance impact of the newest release of the desktop app?”  . We knew when we started building this system that we would need flexibility in choosing the tools to process and analyze our data. Sometimes the questions being asked involve a small amount of data and we want a  fast, interactive  way to explore the results. Other times we are running  large aggregations  across longer time series and we need a system that can handle the sheer quantity of data and help distribute the computation across a cluster. Each of our tools would be optimized for a specific use case, and they all needed to work together as an integrated system. . We designed a system where all of our processing engines would have access to our data warehouse and be able to write back into it. Our plan seemed straightforward enough as long as we chose a shared data format, but as time went on we encountered more and more inconsistencies that challenged our assumptions. . Our central data warehouse is hosted on Amazon S3 where data could be queried via three primary tools:  Hive  ,   Presto  and    Spark . . To help us track all the metrics that we want, we collect data from our MySQL database, our servers, clients, and job queues and push them all to S3. We use an in-house tool called Sqooper to scrape our daily MySQL backups and export the tables to our data warehouse. All of our other data is sent to  Kafka,  a scalable, append-only message log and then persisted on to S3 using a tool called  Secor . . For computation, we use  Amazon’s Elastic MapReduce  (EMR) service to create ephemeral clusters that are preconfigured with all three of the services that we use. .  Presto  is a distributed SQL query engine optimized for interactive queries. It’s a fast way to answer ad-hoc questions, validate data assumptions, explore smaller datasets, create visualizations and use it for some internal tools, where we don’t need very low latency. . When dealing with larger datasets or longer time series data, we use  Hive  ,  because it implicitly converts SQL-like queries into MapReduce jobs. Hive can handle larger joins and is fault-tolerant to stage failures, and most of our jobs in our ETL pipelines are written this way. .  Spark  is a data processing framework that allows us to write batch and aggregation jobs that are more efficient and robust, since we can use a more expressive language, instead of SQL-like queries. Spark also allows us to cache data in memory to make computations more efficient. We write most of our Spark pipelines in Scala to do data deduplication and write all core pipelines. . How do we ensure that all of these tools can safely interact with each other? . To bind all of these analytics engines together, we define our data using  Thrift , which allows us to enforce a typed schema and have structured data. We store our files using  Parquet  which formats and stores the data in a columnar format. All three of our processing engines support Parquet and it provides many advantages around query and space efficiency. . Since we process data in multiple places, we need to make sure that our systems always are aware of the latest schema, thus we rely on the  Hive Metastore  to be our ground truth for our data and its schema. . Both Presto and Spark have Hive connectors that allow them to access the Hive Metastore to read tables and our Spark pipelines dynamically add partitions and modify the schema as our data evolves. . With a shared file format and a single source for table metadata, we should be able to pick any tool we want to read or write data from a common pool without any issues. In our dream, our data is well defined and structured and we can evolve our schemas as our data needs evolve. Unfortunately, our reality was a lot more nuanced than that. . All three processing engines that we use ship with libraries that enable them to read and write Parquet format. Managing the interoperation of all three engines using a shared file format may sound relatively straightforward, but not everything handles Parquet the same way, and these tiny differences can make big trouble when trying to read your data. . Under the hood, Hive, Spark, and Presto are actually using different versions of the Parquet library and patching different subsets of bugs, which does not necessarily keep backwards compatibility. One of our biggest struggles with EMR was that it shipped with a custom version of Hive that was forked from an older version that was missing important bug fixes. . What this means in practice is that the data you write with one of the tools might not be read by other tools, or worse, you can write data which is read by another tool in the wrong way. Here are some sample issues that we encountered: . One of the biggest differences that we found between the different Parquet libraries was how each one handled the absence of data. . In Hive 0.13, when you use use Parquet, a null value in a field will throw a NullPointerException. But supporting optional fields is not the only issue. The way that data gets loaded can turn a block of nulls— harmless by themselves — into an error if no non-null values are also present ( PARQUET-136) . . In Presto 0.147, the complex structures were the ones that made us uncover a different set of issues — we saw exceptions being thrown when the keys of a map or list are null. The issue was fixed in Hive, but not ported in the Presto dependency ( HIVE-11625 ). . To protect against these issues, we sanitize our data before writing to the Parquet files so that we can safely perform lookups. . Another major source of incompatibility is around schema and file format changes. The Parquet file format has a schema defined in each file based on the columns that are present. Each Hive table also has a schema and each partition in that table has its own schema. In order for data to be read correctly, all three schemas need to be in agreement. . This becomes an issue when we need to evolve custom data structures, because the old data files and partitions still have the original schema. Altering a data structure by adding or removing fields will cause old and new data partitions to have their columns appears with different offsets, resulting in an error being thrown. Doing a complete update will require re-serializing all of the old data files and updating all of the old partitions. To get around the time and computation costs of doing a complete rewrite for every schema update, we moved to a flattened data structure where new fields are appended to the end of the schema as individual columns. . These errors that will kill a running job are not as dangerous as invisible failures like data showing up in incorrect columns. By default, Presto settings use column location to access data in Parquet files while Hive uses column names. This means that Hive supports the creation of tables where the Parquet file schema and the table schema columns are in different order, but Presto will read those tables with the data appearing in different columns! . It’s a simple enough problem to avoid or fix with a configuration change, but easily something that can slip through undetected if not checked for. . Upgrading versions is an opportunity to fix all of the workarounds that were put in earlier. But it’s very important to do this thoughtfully. As we upgrade EMR versions to resolve bugs or to get performance improvements, we also risk exchanging one set of incompatibilities with another. When libraries get upgraded, it’s expected that the new libraries are compatible with the older versions, but changes in implementation will not always allow older versions to read the upgraded versions. . When upgrading our cluster, we must always make sure that the Parquet libraries being used by the analytics engines we are using are compatible with each other and with every running version of those engines on our cluster. A recent test cluster to try out a newer version of Spark resulted in some data types being unreadable by Presto. . This leads to us being locked into certain versions until we implement workarounds for all of the compatibility issues and that makes cluster upgrades a very scary proposition. Even worse, when upgrades render our old workarounds unnecessary, we still have a difficult decision to make. For every workaround we remove, we have to decide if it’s more effective to backfill our data to remove the hack or perpetuate it to maintain backwards compatibility. How can we make that process easier? . To solve some of these issues and to enable us to safely perform upgrades, we wrote our own Hive InputFormat and Parquet OutputFormat to pin our encoding and decoding of files to a specific version. By bringing control of our serialization and deserialization in house, we can safely use out-of-the-box clusters to run our tooling without worrying about being unable to read our own data. . These formats are essentially forks of the official version which bring in the bug fixes across various builds. . Because the various analytics engines we use have subtly different requirements about serialization and deserialization of values, the data that we write has to fit all of those requirements in order for us to read and process it. To preserve the ability use all of those tools, we ended up limiting ourselves and building only for the shared subset of features. . Shifting control of these libraries into a package that we own and maintain allows us to eliminate many of the read/write errors, but it’s still important to make sure that we consider all of the common and uncommon ways that our files and schemas can evolve over time. Most of our biggest challenges on the data engineering team were not centered around writing code, but around understanding the discrepancies between the systems that we use. As you can see, those seemingly small differences can cause big headaches when it comes to interoperability. Our job on the data team is to build a deeper understanding of how our tools interact with each other, so we can better predict how to build for, test, and evolve our data pipelines. .  \t\t\t\tIf you want to help us make Slack a little bit better every day, please check out our job openings page and apply.\t\t\t\t Apply now  \t\t\t ", "date": "2016-12-08"}, {"website": "Slack", "title": "Making Slack Faster By Being Lazy", "author": ["Scott Schiller"], "link": "https://slack.engineering/making-slack-faster-by-being-lazy/", "abstract": " Software performance is like a series of card tricks: . Whether doing magic with cards or a browser, it doesn’t hurt to have an ace up your sleeve. ♠️ . This two-part series is about our work refactoring part of the Slack desktop client for performance. Topic highlights include avoiding and deferring work, adding smart preloading capabilities, the pitfalls of LocalStorage, and lessons learned while refactoring. . This round of improvements went live in mid-2016, but as performance is a continuous area of focus, there are always more plans in the pipeline. . The Slack desktop client* builds and maintains a data model for the duration of your session. There are many elements to the model, but here are a few: . *  The Slack real-time messaging app works in web browsers, and our installable desktop app also uses a web-based view of the same; hence, “Desktop” is an interchangeable term for both.  . The client starts with an empty model, which gets populated by a combination of local cache (as applicable) and API calls. On startup, the client makes some initial API calls and connects to a Message Server via WebSocket. The WebSocket connection is the transport layer for receiving real-time messages — whether messages sent from other users on the team, or model update messages that the client needs to update state(e.g., a user joins/leaves a channel, updates their profile, uploads or shares a file, and so on.) . Originally, Slack’s browser-based web app could effectively load the entire model up front without any notable performance issues. With most teams having less than 100 people, there were still a relatively small number of channels, message traffic was somewhat low, and client load time was quite fast. . As the size of the average Slack team increased, hotspots impacting performance in various areas became more evident. While it can be convenient to have the world at your fingertips, at some point the client cannot — and should not — know about, or need to render, absolutely everything right away. . The Slack desktop client builds and maintains an array of messages for each channel that you are a member of. The client receives new messages over the WebSocket as they are sent, and it can also  fetch older messages  via the Slack API. . Previously, we would make a channels.history API call and fetch the most recent messages for each channel that you are a member as soon as you opened Slack. For a small team with a limited number of channels, this was convenient; however, this pattern clearly does not scale well: . Building rapidly and for scale are not necessarily at odds with each other, but it’s an interesting balance: building out many features simply and quickly, and refactoring for scale as you grow. Even when scale is considered up front, changes in usage patterns can impact the performance of your app on both backend and frontend — sometimes in interesting, and unexpected, ways. . Refactoring the Slack web app to fetch messages for only the current, active channel (instead of all channels), seemed straightforward enough. . In the past, we needed to load message history for every channel in order to determine what the state of each channel was; i.e., whether you had any unread messages (channel name shown in  bold ), or mentions (channel name has a “badge” overlay with a number on the right.) . A flood of API calls could mean waiting a while for the channel list to be populated with initial state. The more channels you are a member of, the longer the wait in order for the unread / mention state to show. If the channel at the bottom of your list had a mention, it could take quite a while before that state loaded in. Seeing the channels in your sidebar light up one-by-one made Slack feel slow for large teams. . Some time after Slack’s public launch, a users.counts API method was implemented for Slack’s mobile apps. This method provides a simple list of unread and mentions counts for each channel that your client needs to know about. . By moving to users.counts on desktop, we are able to “light up” the entire channel list with unread / badge state in a single request. This improves the perceived performance of the client: as far as the user is concerned, everything looks ready to go immediately. . At this point, we know which channels have unreads and/or mentions, but we do not have any actual messages loaded in the client. It makes sense to fetch messages for the channel currently in view, but no more than that. . We could do nothing further at this point, but there are times when it’s required (or,  optimistic ) to make a channels.history API call, fetch messages for a given channel and add them to the client model. . Common cases where we may call the channels.history API: . Previously, the Slack desktop client would define a “page” of messages as small as 30, and as large as 200 based on the number of channels you are in. The intent was to balance the number of messages fetched and cached by the client. As a recurring theme, this approach scales only up to a point. . While loading 200 messages per channel sounds useful if the user is only in a few channels, it is unlikely that users will regularly scroll back that far. If you don’t need it, don’t fetch it! . We experimented a little, and settled on the ever-magical  42  for a page of history. 42 messages covers a reasonable amount of conversation without going overboard, and is enough to fill the view on a large monitor. Additionally, most users have less than a full page of unreads on a per-channel basis when connecting or reconnecting. Whether browsing or catching up on a busy channel, older messages can always be fetched as the user scrolls back through history. . Thanks to the users.counts API, we know if a channel has unreads and/or mentions. Since you’re more likely to view unread channels first, we could lazy-load that message history in advance. . It is easy enough to queue up channels.history API calls, or perhaps batch them to fetch messages for multiple channels with a single request.  However , we can be a little smarter about this if we know a bit more about your usage habits. . Taking scale into consideration again, we don’t want to necessarily pre-load everything; a client could have a lot of channels with unreads (e.g., you return from a week-long vacation). However, it’s reasonable to expect that unread Direct Messages (DMs) will be seen as high priority and likely to be read before others, given the 1:1 conversation context. Every unread DM counts as a “mention” and gets badged, so it is highlighted in the client. . Given that, we prefetch unread messages with the following priority: . At present, the client will prefetch history for all unread DMs, group DMs, and badged channels because you are likely to be interested in those messages. . Considering very large teams, there is a point where pre-fetching should stop as it can do more harm than good. For example: if you’re a member of 50 channels and returning from vacation where most all have unreads, we do not want to load all of that state up front into the client — it could negatively impact performance (e.g., memory use). Even if you are a fast reader, it is going to take you time to read each individual channel. If there is a lot of volume, you may not even view all of them at once. In this case especially, it is smarter for us to rely on other tricks to stay one step ahead of you. . Not only do we know which channels to pre-fetch, we also learn which channels you view the most often (frequency), and most recently (recency) — hence, “frecency”. Frecency is informed by features like the Quick Switcher, and it becomes smarter during your normal use of Slack. . When we get the list of unread channels, we cross-reference the unreads with frecency data and sort the preloading queue, so the places you’re most likely to visit get loaded first. . For the curious, our engineering group has written previously about frecency as used in Slack:  “A faster, smarter Quick Switcher.”  . Both events and user actions can make great hints for preloading message history in channels. . If you use alt + up/down keys to navigate through the channel list, we can preload one ahead of you as applicable. If you use keyboard shortcuts, there’s a reasonable assumption you’re going to repeat that action. . If at any time you get a new message in a channel, we can pre-fetch history and practically guarantee the channel will be synced before you view it. . The following is a rough comparison of timelines from Google’s Chrome Dev Tools, showing the amount of script, rendering and layout work performed during the load of the Slack desktop client on an active team. . While we of course measure and compare individual metrics like “time to load” and “model set-up”, it’s useful to be able to visualize your app’s lifecycle and compare performance holistically. . The two graphs below are dense with information and will be broken down further, so don’t compare them too closely. At a high level, you can see there is less “noise” post-refactor due to the elimination, avoidance and deferral of work. .  Before optimization: “noisy” flame chart activity  .  After optimization: reduced flame chart activity  . JavaScript-initiated DOM updates inevitably mean the browser has to spend time calculating style changes, and then perform layout work as part of updating the UI. . Before refactoring, there was a notable pattern of “thrashing” involving the channel list. During start-up, the client would repeatedly redraw the channel list with unread state (bold and/or badges) because it would fetch messages and then redraw the channel list for every channel you were a member of, one at a time. . Post-refactor, the channel list’s initial state is fetched by a single users.counts call and updated all at once, eliminating an amount of redundant DOM and style/layout work. . There is a certain fixed cost to “booting” the Slack client itself: We must load and parse our core JavaScript files from our CDN, register our JS modules and get the client started. . It is the repeated channel history fetches and related work following boot, however, which presented optimization opportunities. . Post-refactor, there is a notable reduction of JS activity again thanks to users.counts — we know which channels are unread in a single call, and don’t have to call the API for every single channel (nor update the UI after each API call) to determine what’s new. . While hand-wavy and difficult to reliably test repeatedly, it’s good to have a “healthy-looking” JS memory profile. In this case, graphs show similar patterns of gradual allocation and garbage collection; there is no evidence of “GC thrashing” (i.e., a sawtooth pattern) from hot loops allocating a lot of memory or creating thousands of short-lived objects, etc. This is generally good. . All of these factors — rendering, script activity, and memory use — contribute to improving the bottom line of client load time, start-up and performance over time. Slack also aggregates some performance metrics from live clients (e.g., load times), as well. Performance data helps to inform how our app is behaving in the wild and at scale, above and beyond what we can infer with our own local tests. . Per our own performance metrics, clients got a 10% load time improvement across the board following this work. In the most extreme cases (e.g., our own super-size “stress test” teams), load time was reduced by 65%. . Presenting users with a fast, responsive UI for large Slack teams requires more planning, increased technical complexity and a little trickery, but is a worthwhile investment — whether scaling an existing implementation, or building from the ground up. . This is just the first piece of our journey to increase client performance. . Part 2 will cover lessons learned from this refactoring in more depth—trade-offs with caching, LocalStorage, LZString compression, and more. Stay tuned. . In the meantime —  while you’re here  — we are always looking for people who like digging into the details, and  working on interesting problems . ", "date": "2017-01-06"}, {"website": "Slack", "title": "Introducing Electron to the Windows Runtime", "author": ["Felix Rieseberg", "Anaïs Betts"], "link": "https://slack.engineering/introducing-electron-to-the-windows-runtime/", "abstract": " The Slack Desktop Client is powered by  Electron , the same framework that enables Atom, Visual Studio Code, and Basecamp to deliver delightful desktop apps built with web technologies. . This week,  we launched Slack for Windows 10 in the Windows Store . It is one of the first applications to make use of Microsoft’s “Desktop Bridge”, previously known as Project Centennial. Slack partnered with Microsoft to pioneer a way for Electron apps to run within the confines of the Windows Store and to integrate with Windows Runtime APIs. Given that we conquered new lands, we wanted to share some of our discoveries. . The Windows Store was previously only available to Universal Windows Apps, which are in many ways the evolution of the good old exe. The store isn’t just a way for users to discover applications — it also offers one-click installations and “no files or registry items left behind” uninstallations. On top of that, it integrates deeply with Microsoft’s IT management tools for enterprises and sysadmins who need to manage software on thousands of machines. . In addition, apps built on top of the Universal Windows Platform get to communicate with Windows through a set of APIs found in the Windows Runtime (usually just called WinRT). It’s the home for the most interesting and most powerful aspects of Windows: If you’d like to interact with hardware, lock screens, payments, notifications, or Cortana, you’re talking to WinRT to make it happen. . At the heart of Electron lies Node.js, which can interface with native code through “native addons”. Using dynamically-linked shared objects, written in C or C++, they can be used just as if they were ordinary Node.js modules. The open-source project  NodeRT  uses WinRT’s descriptive metadata files to iterate over the body of APIs, automatically generating native Node addons for each WinRT namespace. In Electron, this allows us to replace custom C++ with JavaScript. . Using NodeRT, we can run the same operation in JavaScript: . Many APIs, like the  Live Tile API , assume that the calling application is a Universal Windows App — and look for the Store’s package identity to distinguish between applications. In short: To access the full range of WinRT, we also need to wrap Electron in a Windows Store application package. . Under the hood, the Windows Store application format AppX is essentially a zip file that contains an application manifest, static assets, and the binary executable. At runtime, Windows puts Win32 binaries into a virtual environment — write operations to the disk or the registry are virtualized, ensuring that the app leaves no trace after uninstallation. . To compile Electron Apps into the Windows Store application format AppX, we contributed heavily to  electron-windows-store , which automates the path from pure Electron app to AppX. In detail, the tool does the following operations: . We also contributed to the development of  electron-windows-notifications , which serves as an excellent example for using NodeRT in Electron apps — and can be used in any app to send rich Windows notifications that deeply integrate with the operating system through the Action Center, the Notification Settings, and Quiet Hours. . We’re very excited about creating an excellent user experience, available on people’s favorite platforms. We’re equally excited about making more APIs and features available to developers working with Electron. If that sounds like an exciting mission to you,  come work with us ! ", "date": "2017-01-18"}, {"website": "Slack", "title": "Weaving Threads", "author": ["Paul Rosania"], "link": "https://slack.engineering/weaving-threads/", "abstract": "  This is a guest post from the Product team working on the newly released Threads feature.  . If you use Slack to work with your team, you might notice a new feature —  Threads . You can use Threads to reply to messages, organize discussions, and manage the many conversations that happen in Slack. As one of our most requested features, Threads has been in development for some time, and I wanted to share a little bit about our design process and what we learned. . At Slack, every product kickoff begins with this question. For Threads, it seemed simple at first: channels can get chaotic when a variety of discussions happen at the same time. We saw threading mainly as a way to provide context, i.e. how can we group conversations in a busy channel, so it’s clear which message someone is replying to? . On paper, our early designs felt good. We built the first prototype in late 2015, and a few versions later, we turned threading on for our own team, Slack Corp. . It was promising at first: everyone started connecting messages in hopes of making conversations easier to follow. And yet, while we’d succeeded in providing context, we didn’t quite get the clarity we’d expected. In practice, Threads made busy channels harder to read, not easier. . With our first version of Threads, we realized two of our initial assumptions were wrong: . Imagine a team meeting with a set agenda. The group may discuss a variety of topics, covering one at a time. Now, imagine a meeting where half the room is moving from one topic to the next, while the other half is still discussing the prior topic — both at full volume. . By displaying all replies in the channel, we had inadvertently encouraged additional noise without solving for the problem we set out to address. . By this point, Threads had become so disruptive that we turned the feature off for our team. But when we pulled the plug, something unexpected happened — people complained. Our team had adapted to an imperfect solution and found ways to use Threads to improve how they worked together. . As we talked to these groups, a few patterns started to emerge: . These learnings feel obvious in hindsight, but they point to a common problem in product design: some features seem so straightforward that you never actually solve the underlying customer problem. Our initial version of Threads sought to help people reply to each other. But as we iterated, we realized that Threads could help people manage and organize conversations, so that they could work more efficiently in Slack. . Armed with these insights, we knew we were on the right track. . We arrived at the version of Threads you see today by making a controversial change: we took replies out of channels. You can still start a thread from any message, but now that discussion happens without cluttering the channel. With this change, people came to understand that starting a thread meant starting a separate discussion, and went back to posting normal messages for most conversations. . It wasn’t all rainbows, though. Some people still preferred to see  every  message, and felt anxious about missing updates. To address this, we made a few important tweaks: . Message threading is one of the most challenging features we’ve built at Slack, partly because it’s rooted in contradiction: we want to declutter and organize our channels, but we also want to stay up to date on everything. Untangling this knot has been a two year journey, and we appreciate everyone’s patience. . But shipping Threads is not the end of this journey. Like everything we build, there are many improvements we’ll need to make in the months and years ahead. Some of this is already in the works, but many ideas will come from you, so please let us know what you think by tweeting us @SlackHQ or typing /feedback in Slack. And if you like tackling challenges like this, we’re always  looking for people to help us make Slack even better . ", "date": "2017-01-19"}, {"website": "Slack", "title": "Making Slack Faster By Being Lazy: Part 2", "author": ["Scott Schiller"], "link": "https://slack.engineering/making-slack-faster-by-being-lazy-part-2/", "abstract": "  This is a continuation of    Part 1   , covering some highlights and lessons learned from a refactor of the way the Slack desktop client fetches messages. In particular: architecture lessons learned in hindsight, regressions found while refactoring, the pitfalls of over-aggressive caching, and the downsides of LocalStorage and client-side compression.  . In hindsight, “metadata” API methods like users.counts, which lets us render the channel list state without having to fetch messages for every channel, would have been great to have from the start. Slack was originally built by a small team that envisioned the product being of interest for teams of “up to 150” users, and desktop clients were performing well for teams of that size. . As team sizes and usage grew, Slack’s mobile apps highlighted the need for more efficient ways to fetch state information. The relatively small memory, CPU, cellular network and power constraints of mobile clients drove demand for a new API method — one that could provide just enough data to display the channel list in a single network request, vs. having to query each channel individually. . With mobile driving more efficient ways to fetch state information, we wanted the desktop clients to take advantage of the same improvements. Refactoring the desktop client to call users.counts was trivial. Modifying it further to lazy-load message history was relatively easy; avoiding and deferring channels.history calls did not involve a lot of work. The majority of time spent following the deferral change was making sure the client continued to behave as expected. . By deferring message history, the desktop client effectively had the rug pulled out from under it — the client no longer had a complete data model at load time. Given that the client was used to having all message history  before  displaying a channel, there were a lot of bugs that cropped up — especially when displaying channels for the first time. . Some of the behaviours affected: . With deferral in place, it became possible to switch to a channel where message history had not yet loaded. A lot of refactoring was needed so the client could properly handle the fetch, display and marking of messages on-the-fly, as well as ensuring the fine details of displaying and scrolling through channels remained unchanged. . At the intersection of Wu-Tang fans and software engineers,  Cache Rules Everything Around Me  has inspired both branch names and commit messages. While a powerful tool, caching is best applied carefully and in specific scenarios. Not unlike “loading the world up front”, simply trying to cache everything can quickly become expensive, unreliable or unwieldy. . In the case of the Slack desktop client, we were initially caching a lot of client data in the browser’s LocalStorage as it seemed logical to speed up load time and prevent redundant API calls. Ultimately, it proved to be more trouble than it was worth; LocalStorage access was slow and error-prone, and given our new approach of lazy-loading message history in channels, it made less sense to keep this ever-growing long tail of data in cache. . LocalStorage has a number of performance problems and other restrictions which we ran into (and ultimately avoided by reducing our usage of it). We also tried some creative workarounds during the process. . Slack was previously storing client model information for users, bots, channels and some messages in LocalStorage. We would spend time loading much of this data immediately from LocalStorage while loading the client, so we would not have to load much from the network. . LocalStorage read/write is synchronous, so it blocks the browser’s main thread while it works. In some cases, the client could be effectively blocked for several seconds while many large key/value pairs were read from disk. In the worst case, fetching this data over the network could be faster than fetching it from LocalStorage. At the very least, the network request always has the advantage of being asynchronous. . LocalStorage typically has a top limit on data usage, ranging between 5–10 MB per domain. There is also a per-key size limit, which varies by browser. . Like a hard drive running out of space, there is a risk that some, but not all key/value pairs could be written successfully to LocalStorage when performing a series of writes. This introduces the possibility of leaving cache in a corrupt, incomplete or otherwise invalid state. This can be largely prevented by catching exceptions thrown on write, and even reading back the data written to LocalStorage if you want to be absolutely certain. The latter introduces another synchronous I/O hit, however, making writes even more expensive. . Once in a blue moon, LocalStorage will fail in strange and spectacular ways. The underlying storage data file on disk used by the browser can occasionally become corrupted by a power outage or hardware failure, leading to NS_ERROR_FILE_CORRUPTED errors when attempting LocalStorage reads or writes. If LocalStorage becomes corrupted in this way, the user may have to clear cookies and offline storage, reset their browser’s user profile, or even delete a .sqlite file from their hard drive to fix this issue. While rare, this is not a fun exercise for a user to have to walk through. . Given the variation of storage size limits for LocalStorage across different browsers, we adopted the  lz-string  library so we could write compressed data to disk. Because lz-string could operate slowly on very large strings, the compression was moved into a web worker so as not to block the browser’s main UI thread. . With the extra step of asynchronous compression before writing and the synchronous cost of I/O, our LocalStorage module needed a cache to prevent repeated slow reads, and a write buffer for data that had not yet been compressed and written to disk. . When data was inbound for LocalStorage, it would go to the write buffer first. If another part of our app asked for the same data from LocalStorage during that time, the buffer served as a cache — preventing slow, synchronous fetches of potentially-stale data that also now required a decompression step. . After a period of user inactivity, the pending write buffer was flushed to disk. When flushing, key values were sent off to the compression module, and ultimately written to LocalStorage when the compressed data was returned. At that point, the buffer was cleared. . If nothing else, the only time we would  really  need to write to LocalStorage was right before the user reloaded, closed their Slack tab or exited the browser (or app) entirely. Our ultimate goal was to persist some data across browser sessions, including quitting and relaunching the browser itself. Even if we attempted a synchronous compress-and-write operation at or before window.unload, there was no absolute guarantee that data would make it to disk. . Compression was a trade-off; it bought us some time while we worked on reducing our use of LocalStorage, at the cost of some performance and code complexity. . Compressing and decompressing data is not free; both cost memory and CPU time. Unpacking compressed data can cause a spike in memory use, and large numbers of new, short-lived objects created by the process can contribute to increased garbage collection pauses. . Complexity increased in a few ways: . Learning from this, we decided it was better to avoid the extra work associated with increasingly-large amounts of data entirely. Instead, we started working toward a lean / lazy client model which can start practically empty, and quickly fetch the relevant data it needs — avoiding cache entirely, if need be. . As teams grew rapidly in size, we found that instead of making clients faster, our use of LocalStorage seemed to cause of some real lag in the Slack UI. There was a cost to the amount of data, and the frequency with which we were doing LocalStorage I/O. The more channels a user was in, the more messages there were, and the more time being spent in synchronous LocalStorage I/O. . Today, we no longer store messages in LocalStorage, but we do continue to store a few other convenience items: text that you typed into the message box but didn’t send, the state of the UI (files, activity or team list opened in the right-side panel), and the like. Dropping messages from LocalStorage — with an eye toward dropping user and channel data — eliminates a significant long-tail use of client resources, in favour of storing only ephemeral data and state created by the local user. . In retrospect, perhaps SessionStorage could have served our needs. We did not make the switch with this project, but it may make sense in a future where we are storing minimal, and mostly temporary, data. While it has similar size limits to LocalStorage, SessionStorage is more ephemeral in nature. It can persist through tab closing/restoring and reloads, but does not survive a quit and re-launch of the browser. We still like to persist a few items (i.e., your UI state) in more permanent storage, but perhaps that could be stored as an account preference on our backend at some point in the future. . Software development often involves a balance of developing simply and quickly for product features, and developing for performance at scale. As you design and build, it’s helpful to identify potential performance hotspots in your architecture for the day when your project — or even just a particular feature of it — suddenly becomes popular with users. ", "date": "2017-02-03"}, {"website": "Slack", "title": "Search at Slack", "author": ["Isabella Tromba", "John Gallagher", "Jason Liszka"], "link": "https://slack.engineering/search-at-slack/", "abstract": " On average,  20% of a knowledge worker’s day  is spent looking for the information they need to get their work done. If you think about a typical work week, that means  an entire day  is dedicated to this task! . To help our users find more time in their day, the Search, Learning, and Intelligence team set out to improve the quality of Slack’s search results. We built a new personalized relevance sort and a new section in search called  Top Results , which presents both personalized and recent results in one view. . Search inside Slack is very different from web search. Each Slack user has access to a unique set of documents, and what’s relevant at the time frequently changes. By contrast, in web search, queries for “Prince,” “Powerball” or “Pokémon Go,” can get millions of hits per day, whereas queries within a Slack team are rarely repeated. . Even though Slack search lacks the aggregate search data that has been put to such effective use in web search engines, it does benefit from some other advantages: . Slack provides two strategies for searching:  Recent  and  Relevant .  Recent  search finds the messages that match all terms, and presents them in reverse chronological order. If a user is trying to recall something that just happened,  Recent  is a useful presentation of the results. .  Relevant  search relaxes the age constraint and takes into account the  Lucene score  of the document — how well it matches the query terms (Solr powers search at Slack). Used about 17% of the time,  Relevant  search performed slightly worse than  Recent  according to the search quality metrics we measured: the number of clicks per search and the click through rate of the search results in the top several positions. We recognized that  Relevant  search could benefit from using the user’s interaction history with channels and other users — their “work graph.” . As a motivating example, suppose you are searching for “roadmap”. You’re most likely looking for your team’s roadmap. If a member of your team shared a document containing the word “roadmap” in a channel that you frequently read and write messages in, this search result should be more relevant than another team’s 2017 roadmap. . By incorporating a user’s work graph into  Relevant  search, we saw a 9% increase in searches that resulted in clicks and a 27% increase in clicks at position 1. . Our team was confident that incorporating additional features — such as the searcher’s affinity to the author of the result and engagement in the channel, as well as certain characteristics of the message itself — would improve the ranking of results for  Relevant  search. To achieve this we settled on a two-stage approach: we would leverage Solr’s custom sorting functionality to retrieve a set of messages ranked by only the select few features that were easy for Solr to compute, and then re-rank those messages in the application layer according to the full set of features, weighted appropriately. . In building a model to determine these weights, our first task was to build a labeled training set. Because of the unique nature of search in Slack, we could not rely on techniques that use click performance of results from repeat queries to judge relevance. We also knew that the relevance of a document would drift over time. So we focused on a labeling strategy that judged the relative relevance of documents within a single search using clicks known as a  Pairwise Transform . Here’s an example: . If a query shows messages M1, M2, and M3, and the searcher clicks M2, then there must have been something  different  about M2 that made it better than M1. Since M2 was a better result than M1, the difference in the corresponding feature vectors, F2-F1, should capture the difference, and this difference in values is given a positive label. Inversely, F1-F2 is given a negative label. There are several strategies for picking the pairs for the pairwise transform, and we tried a few before settling on one. We ended up pairing each click at position  n  with the message at position  n -1 and the message at position  n +1. . One issue we struggled with was people’s tendency to click on messages at the top of the search results list — a message at position  n  is on average 30% more likely to be clicked than a message at position  n +1. Because the position of a message is such a strong indicator of whether or not it was clicked, we found that our initial models were learning to reconstruct the original order of the list. To counteract this effect, we evened out the distribution of clicks by position by oversampling clicks on results lower down in the list. . In addition, by pairing each click at position  n  with the message at position  n -1 and the message at position  n +1, we ensured that our training examples would contain equal numbers of message pairs with clicks in the lower and upper positions. We recognize that this is somewhat unsound since the user might not have even seen the message at position  n +1. For now, this is an acceptable tradeoff, and we are actively pursuing other approaches to this problem. . Using this dataset, we trained a model using SparkML’s built-in SVM algorithm. The model determined that the following signals were the most significant: . Notably, aside from the Lucene “match” score, we have not yet incorporated any other semantic features of the message itself in our models. . Our team released our machine-learned re-ranker for  Relevant  sort on November 30th to 50% of users. For our top-line metrics, we looked at sessions per user, searches per session, clicks per search, and click-through rate among the top 1, 2, and 5 search results. As previously mentioned, we saw significant gains over the existing  Relevant  search — a 9% increase in clicked searches and among searches that received at least one click, a 27% increase in clicks at position 1. . We wanted to make sure our users could find what they’re looking for more quickly without having to worry about sorting by relevancy or recency. The new  Top Results  module solves this problem by showing the top 3 messages from  Relevant  search above the normal  Recent  results. . Every time you run a  Recent  search, we also run the  Relevant  search in parallel. We decide whether to show the  Top Results  section based on some simple heuristics, such as result diversity and quantity. Our initial experiment results show a significant increase in search sessions per user, an increase in clicks per search, and a reduction in searches per session, indicating that the  Top Results  module is helping users find what they are looking for faster. .  Top Results  is just the beginning of really exciting opportunities in making Search in Slack better. Imagine searching for “401k matching” and instead of just receiving relevant messages or files, you also get a list of people in HR that can answer your question, or a list of channels for your query where you might be able to find the information you are looking for, or even a list of commonly asked questions relevant to that topic with links to the channel where each one was answered. We still have a lot of work to do to reduce that 20% of information-seeking time, allowing users of Slack to have a more pleasant, productive experience. .  If you want to help us tackle data-driven product development challenges and help make the working life of millions of people simpler, more pleasant and more productive, check out our    SLI job openings   .  ", "date": "2017-02-08"}, {"website": "Slack", "title": "Reducing Slack’s memory footprint", "author": ["Johnny Rodgers", "Charlie Hess", "Raissa Largman", "Jamie Scheinblum", "Chris Sullivan"], "link": "https://slack.engineering/reducing-slacks-memory-footprint/", "abstract": " Our desktop app is the most widely used and most capable Slack client that we offer. For many of our customers, it is one of just a few apps they keep open on their computer throughout the work day. It allows them to communicate and work with all the teams they belong to: reading and writing messages, composing posts, uploading files, taking calls, and responding to notifications. . However, these capabilities come at a cost: the desktop client can use a lot of memory. This memory footprint increases as the user signs into more teams, as each team runs in its own webview. More memory usage means worse performance, degrading our customer’s experience of Slack and their other applications. . Work is underway to fix the underlying factors affecting client memory consumption, but in the meantime  we’ve built a tiny new Slack client to help address this issue . It is loaded silently in place of the teams that you haven’t looked at in a while. This background client does just a few things: . To achieve this we had to teach our servers to be smarter, and to design a thin client that did just enough work to keep your work day flowing. . At the time of writing, about 36% of active Slack users are signed into more than one team in the desktop app. 17% are signed into 3 or more teams, and 5% are signed into 5 or more teams. Each of these teams is running the full JavaScript web application and maintaining and updating the DOM tree as the state of the team changes. This application can consume between ~130MB (p10) and ~960MB (p99) of memory depending on team activity and UI state. . Our data tells us that most people who are signed into multiple teams have 1 or 2 that they actively pay attention to during their work day while they check the others less frequently. Regardless of usage, each of these teams eats up a full quota of memory. . This got us thinking: what is the lightest weight client we could build that could do everything you needed it to do while it was in the background: maintain your presence, display notifications, and keep your unreads and badges up to date? . We explored several options, including: . Our exploration indicated that the first two options either didn’t make the impact on memory usage that we were targeting, or increased the complexity of implementation for our primary client unacceptably. So we decided on the third option and built a thin client that maintains minimal state and only presents data as transmitted by the server. . The aspect of the project concerned with reclaiming resources from backgrounded views isn’t original. Chrome takes a similar approach to  tab discarding  in order to recover memory from backgrounded tabs, and several popular browser extensions similarly suspend memory-hungry tabs that haven’t been viewed recently. . In order for our new client to be as thin and lightweight as we needed it to be, we had to teach the server to do a lot of work that was previously the responsibility of the client. . For example, the server now needs to know: . And it needs to do all this with full awareness of the impact of various team and user preferences and states on the above. Slack is known for its granular and powerful notification preferences — which is great for customers but meant we needed to migrate a good deal of client-side logic to the server: . We developed two pieces of infrastructure to achieve these new requirements. . The tiny client can utilize these tools to do precisely the things it needs to while running in the background: keep your team sidebar up to date with your unread and badge count, and display notifications as the full client does. The client itself is a single JavaScript file of about 1200 lines of unminified code. It keeps  just enough  model and state data in order to achieve its purposes, and is replaced with the full web application when the team is foregrounded. . Swapping between clients is managed by the desktop application, which monitors team usage and time elapsed since a team was last checked. We measure both which teams you use most often, and those you’ve checked recently, and after a period of inactivity unload those teams that are less likely to be foregrounded. . The minimal client dramatically reduces the memory required to run a team in the Slack desktop application, as shown below. This helps to reduce the overall footprint of the app and release resources back to the operating system to improve performance for the teams you are actively using. . We are actively working on a number of projects to reduce the memory footprint of the full client. These in-flight efforts include: . In the meantime, we hope our tiny new client will ease the burden on our customer’s computers and improve their experience of Slack. .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our engineering jobs and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2017-03-04"}, {"website": "Slack", "title": "Data Consistency Checks", "author": ["Paul Hammond", "Samantha Stoller"], "link": "https://slack.engineering/data-consistency-checks/", "abstract": " Databases. They are the single source of truth for our most critical business data, yet as engineers we tend to overlook tooling with this in mind. . An entire ecosystem of monitoring and administrative tools exist for operating our databases, making sure they replicate, scale and are generally performant. Similarly, a number of tools accompany the databases’ query language from linters and beautifiers to query builders and object mappers. But after our application has written data, there is very little tooling to verify that the data is as expected and remains as such. . This is not entirely different from say putting a cat in a box and spending all your efforts on making sure the air holes stay unobstructed and that the food delivery system is operational but never opening the box to see if the cat is dead or alive. . As we develop software, the business logic we apply in the application layer is directly represented in how our data is defined in the storage layer. That zipcode table you just added for the new address system? What is enforcing that a one-to-many relationship exists between zip codes and addresses? Are any zip codes outside of the 0–99999 range of integers? What if the code is within range but invalid according to the USPS? Does the cat still have four legs or has the plutonium you used in your food delivery system caused a mutation? . In a perfect world, we ensure these relationships by writing bug-free software, but even then, outages exist and it’s not practical for every piece of code that deals with data to check everything about the data each time it runs. A network blip could mean that an UPDATE succeeds on one database but not another. A software bug might cause 1 out of every 10k rows to get inserted with a text field set to an empty string. Even well tested code with good error handling cannot stop inconsistencies from creeping in and if you do this at scale, the problem will be compounded. . Some databases can enforce relationships at the systems level, using foreign keys, constraints, and other similar concepts, but these built-in checks and constraints represent only a small set of possible characteristics that may be modeled in any given system. Likewise, ORMs like ActiveRecord can check data at write time using validations, but these often need to be disabled in performance sensitive code, and can themselves contain bugs. . So why, when we write unit tests, smoke tests, integrated tests, all sorts of tests against our code, do we not do the same for our business data? . We’ve been thinking hard about alternative approaches to this problem, and we’ve codified this into what we’ve called a  consistency check framework . . Building a responsive and customizable framework that can analyze and report on your data in meaningful ways has a number of  proactive and reactive  benefits. Consider the following use cases: . The status of your data reflects the health of the overall system. A database that remains consistent over time says something about the quality and stability of your software. In the same way that good unit test coverage ensures the integrity of your business logic, consistency checks should look at the shape of your data and validate important assumptions about your business objects. But how many consistency checks does one need to write in order to be effective? What sorts of relationships are worth investing time into? . The checks you need depend on your stack, the nature of your data, and the types of bugs you’ve found in the past. They belong with the rest of your business logic in your application codebase. Fortunately, building a foundation for these checks is relatively easy to do. . At Slack, it’s common for engineers to write small one-off scripts to diagnose and resolve issues they’re triaging. These scripts are usually removed once the problem has been fixed, but occasionally they’re kept around in case the problem comes back. . One early example of this was the “no-channel backfill”, a script that checks the assumption that a team must have at least one public channel in order to operate correctly. Team metadata is stored on one set of database servers, channel metadata is stored on another so occasionally network partitions meant we successfully created the team, but not the first channel. . This ad-hoc approach worked, but over time, problems arose. Sometimes an engineer would not be aware of an existing script, and would rewrite it from scratch; this particular script was written three times before we noticed the duplication. Furthermore, each implementation varied slightly so even if you knew they existed, it was unclear which version was the canonical script. . As patterns emerged, these scripts were turned into reusable code that could scan a number of teams instead of just one. In a short while, a framework emerged which supported the ability to scan groups of teams and detect a multitude of problems. Standardizing this behavior and encapsulating knowledge not only prevented duplication, but gave developers an efficient and proven way to diagnose teams and by proxy the underlying data. Engineers began proactively adding checks during the development process and the practice quickly worked its way into our engineering culture. . So what did this process look like? We noticed that all of the ad hoc scripts followed the same pattern: some code to scan some subset of teams, a function that checked a single team for a specific issue, and code to report on the results. So, we made generic versions of the scanning and reporting code, and created an interface for the checking code. . Here’s a simplified example of what a consistency check looks like today: . The function takes a team object as an argument, and returns a data structure indicating whether the check ran to completion (`ok`) and an array of warning messages found (`msgs`). . Once this function is written it’s added to a map of checks, along with a name for the check and a description of what it checks. And then the consistency framework takes over. . Initially, you could run consistency checks from the command line and specify either a single team or scan the whole system for issues. This made it easy to check all data on a team for known problems, meaning many customer issues that used to involve lots of head scratching now just involved running a quick check. We then added options that the developer could specify as arguments to make this run faster — like running only against paid teams or teams created since a given date. . The command line framework was a huge win, but only engineers had access to it, and we wanted the tool to be operable by anyone. So we designed a GUI and hooked it up to our internal administrative system. This allowed our customer experience team to ask questions about teams and their underlying data — giving them the tools to triage complex situations and report the underlying problem to engineers. . We’re now also experimenting with routinely running checks against a random sample of teams as a proactive way of spotting problems before our customers do. . Of course, once we had tools to automatically detect a problem, the next obvious step was to add code to automatically fix it. We’ve done this, but only for some consistency checks because many issues require human intervention before we can make any changes. For example, if a single channel guest is somehow in two channels we want to explain the options to a team admin, instead of just picking a channel to remove them from. And in other cases we’d rather look at a problem and ensure we understand the root cause of each specific inconsistency, instead of just blindly letting a script fix it. . The fixer framework works in much the same way as the check framework. First we defined a mapping of keys to callbacks where the keys are the errors returned from the checking process: . If a matching check is detected the callback is dispatched which is responsible for actually fixing the problem and reporting the results. . Most of what we do as software engineers has been done before. It’s easy to assume that if there isn’t an off-the-shelf solution to a problem then the problem doesn’t really exist, or that you’re thinking about the problem in the wrong way. . But, we all work in different domains, with different constraints. Sometimes the right solution is to build some customized internal infrastructure based on your specific business needs. In this case it’s a tool to check the data for a specific team. In other systems it might be the data for a user’s profile or a category of products in a store. Or maybe you need to write validation code for external data sets you’re importing. . Whatever your business, building tools to routinely check your data is a good practice to make part of your everyday operations. A service like Slack is a combination of running code, servers to run it on, and the stored data on those servers. A problem in any one of those will cause visible problems for our customers. Our consistency check framework has been hugely beneficial to decrease these types of issues and ensure confidence in our product both internally and externally. .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our open positions and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-07-27"}, {"website": "Slack", "title": "The Slack Internship", "author": ["Melissa Khuat"], "link": "https://slack.engineering/the-slack-ternship/", "abstract": " Internships — the training grounds for the professional “ real world”.  . But what exactly does the “real world” entail? Working outside the bubble of the university classroom can be intimidating, especially at one of the fastest-growing companies headquartered in the  oh-so-techy  Silicon Valley. . While interning at Slack, I’ve collected insight into what drives the company’s success; how I can do the best work of my life (to date) while I’m here; and what it means to embody Slack’s core values of  Empathy ,  Courtesy ,  Craftsmanship ,  Playfulness ,  Thriving , and  Solidarity . . Before I dive into my takeaways from this summer, here’s some context: . Twelve weeks sounds like a lot of time until you actually start working through it. Taking charge of my internship experience by actively seeking out ambitious, measurable goals has been  the most important  factor in creating purpose and motivation in my day-to-day work. . At Slack, there is huge emphasis on thriving in oneself and others by living up to your full potential every day. Pursue whatever methods that help you and your team track and prioritize these goals. For me, this meant two different things: (1) creating big picture goals and (2) tracking them, using a spreadsheet to record a plan and maintain weekly measurements for each. Slack has given me more autonomy and freedom to create than I could expect (and probably deserve), so it’s only fitting that I paint my own goals with that same liberal brushstroke. . The transparency inherent in Slack’s product itself also manifests in the company’s structure. Talking to a director, VP, or any other flavor of higher-up is as simple as starting a new Direct Message conversation on Slack. In that same vein, it’s commonly understood that all opinions are taken seriously at the meeting table, no matter who it comes from — even myself, the 21-year-old college student. . While interning at Slack, I’ve been surprised by the willingness of more-experienced engineers to listen to my suggestions and debate my design decisions. . For example, one of my projects was to make search in Slack’s App Directory more relevant for our users: What fields do we allow our users to search for? How can we weigh query fields in a way that matches are considered relevant? Questions like these paved the way for spirited discussions with full-time engineers and product managers. . I worked on the query processing aspect of the project, so the final iteration included my suggestions to make use of Apache Solr’s finer-grained querying capabilities. For example, users can require terms in a search result by prefixing a term with ‘+’, exclude terms in a search result by prefixing a term with ‘-’, or require exact phrase matches by surrounding a query by quotation marks. . I got the chance to add value to a feature used by thousands of users because I knew that my opinions were important to the overall search experience. . As common knowledge would share:  ask questions.  During my first week, I hesitated to ask questions in fear of bothering senior engineers or seeming incompetent; I learned very quickly, however, that it’s much more efficient to ask specific questions after independent research than to dwindle an entire day to one problem. .  Courtesy  plays an immense role in the way that work is done at Slack. Before asking another engineer, designer, or product manager for help, it’s a huge show of respect and empathy for others when you come sufficiently prepared with context surrounding the problem. I’ve learned that meetings are best done with a goal in mind: “I want to learn more about your workflow with tool  X” ; “I want to see if I’m interested in a future in Product Management”; or “I want to learn about your experience as a woman and/or minority in tech”. In all these scenarios, practicing courtesy goes a long way with the quality of answers and feedback you receive. . You’re here to learn. . As an intern, you’re not expected to know everything. Imposter syndrome might bubble up, but place things in perspective: understanding your on-the-job learning style will be more valuable to you in the long run. You’re here to learn. . My introductory weeks at Slack were eventful for the Platform Team, marking milestones such as the release of  message buttons  and the funding of 11 additional startups through the  Slack Fund . As part of the team that allows other developers to create interactive experiences within Slack, I knew I needed to immerse myself in our collective mission to build better developer products and distribution mechanisms for apps. . Influential engineers are champions of sharing knowledge and contributing to team discussions. . To dip my toes in the water, I reached out to members of our team for quick coffee chats, attended various developer brown-bag talks, and shadowed people in roles different from my own. Through these small initiatives, I learned that some of the most influential engineers are champions of sharing knowledge and contributing to team discussions— all while practicing patience and humility. . In the bigger picture, team bonding and interpersonal relationships between engineers, designers, product managers, and engineering managers lead the way to shipping better products, since communication and trust are required to nurture an  idea  to a  design  to an engineered  result . Realizing this early on reduces friction between different team roles and provides motivation for smooth transitions between the various stages of building a product. . Or at least get a better idea of what you want out of your career. If you’re like me and you haven’t written an executable 10-year-plan ( do people do this? ), then identify your core values in their infancy and pivot accordingly. . It’s all an iterative process of self-reflection and questioning: What kind of technical and non-technical challenges do you want to tackle? What kind of personalities do you want to surround yourself with? What impact do you want to make in the grander scheme of things, perhaps even beyond the workplace? Working for a company that aligns with your values will make you happier, and ultimately, more productive in your role. . It’s all an iterative process of self-reflection and questioning. . While at Slack, I’ve been exposed to the feel of a large startup — there’s less bureaucracy and greater breadth in roles, which means the pace is generally faster than, say, a multinational corporation. There’s still a lot of unsolved problems and possibilities to explore, so everyone takes on non-trivial responsibilities. Each day counts. Throughout this summer, I’ve challenged myself to reflect daily in hopes of getting closer to understanding my “fit.” . It’s easy to get wrapped up into the details of day-to-day tasks and projects, but I’ve also realized the value of immersing myself in the workplace culture. . During the time I’ve spent at this company, I’ve met so many incredible people. I’ve heard their stories about where they’ve been in their lives, and they’ve inspired me to think about where I want to go in my own. I’ll remember trips to the food trucks across the street, early morning conversations on the patio, and celebratory high-fives in the cafe after releasing a feature. . When someone asks me about my summer, I’ll first tell them about my team and fellow interns. These people will continue to be my colleagues, friends, and mentors even after I return to school in the fall. They make up the most memorable part of this experience, and I know I’ll continue to go to them for career advice, candid feedback, or playful conversation. . Take on high impact, high visibility projects that you might not know how to tackle initially. . Internships are two to three month journeys meant to push you to think critically outside a classroom context. At the end of the day, you should feel like you’re better prepared to transition into industry — one that’s always changing and presenting new, interesting problems. . Every internship experience is different, but remember that you‘re an asset to the team. Take on high impact, high visibility projects that you might not know how to tackle initially. Run with any ideas that experienced engineers might not spot, especially as someone with a fresh perspective on the codebase. Challenge issues in the workplace that you care about. And of course, most importantly, enjoy the experience every step of the way. .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our open positions and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-08-09"}, {"website": "Slack", "title": "Personalized channel recommendations in Slack", "author": ["Renaud Bourassa"], "link": "https://slack.engineering/personalized-channel-recommendations-in-slack/", "abstract": " Public channels provide much of Slack’s advantages over email: they are searchable, long-lasting, themed conversations that are easy to join and leave. But for users, curating the perfect set of channels can leave them feeling like Goldilocks   — it’s easy to be in too many, too few, or miss critical ones. . A common customer request is for tools to help curate channels as companies grow from 100 to 1,000 to 10,000 people using Slack. This is what motivated us to work on channel recommendations: how can we help users tune their channel list to make staying on top of Slack as easy as possible? This feature is one of the first major initiatives from the newly formed  Search, Learning and Intelligence  group based in  NYC . . Building a channel recommendation system can be seen as an application of  recommender systems . Similar to a movie recommendation service, we want to recommend to users the channels we think are most relevant to them, surfacing these recommendations through Slackbot: . But what does it mean for a channel to be relevant? Unfortunately, there is no easy way to directly measure relevancy, but after trying a few different proxy metrics, we ultimately settled on an approximation of time spent in channel based on read and write activity. To compute these scores, we built a  Spark  pipeline that ingests read and write log data stored in the  Hive  data warehouse maintained by our amazing data engineering team and, for each team, outputs a matrix of user-channel activity where each row corresponds to a channel, each column corresponds to a user and each entry corresponds to our computed activity score. . As a simple example, take a team with three users U1, U2 and U3 and three channels C1, C2 and C3. In reality, we deal with thousands of much larger matrices, each with entries numbering up to the millions, but the concepts are the same. Our example team user-channel activity matrix might look like this: . In the figure above, empty entries correspond to user-channel pairs where the user is not a member of the channel and filled entries correspond to the observed activity scores for user-channel pairs where the user is a member of the channel. Since we want to recommend to the user channels to join where we expect them to be the most active, we want to be able to predict what the activity score would be if the user were to join channels for which we have no observed score. The problem of generating channel recommendations thus becomes the problem of filling in the blank in each team activity matrix and recommending the highest predicted values for each user. . To evaluate our solution, we built a test set from a random subset of the filled entries. We also include a random sample of the empty cells in the set, setting their activity level to zero. This technique, drawn from  One-Class Collaborative Filtering , models both the possibility that the user might not be in the channel because they are not aware of it, or that they just may not be interested in it. We can then evaluate our solution by computing a number of metrics, such as  root-mean-square error or RMSE , between our predicted activity levels and the observed activity levels in our test set. . We chose to approach this problem by first looking at similarities between channels. This is often referred to as  item-based  or item-item collaborative filtering in the literature. . For each team, we first compute a similarity score for each channel pair using  cosine similarity  between their activity rows. Given this similarity measure, channels with subsets of similarly active users are considered highly similar while channels with disjoint subsets of active users have a similarity of zero. It also has the interesting property of being scale invariant, which is important for this problem, since groups of users within a larger team often share a set of channels, each with a different average activity level. . Going back to our example team, to compute the similarity between C1 and C2, we first extract the rows for each channel. . We then compute the cosine similarity between the two. . We repeat this operation for each channel pair in the team to compute the full channel similarity matrix. . Now that we have a measure of similarity between each channel pair in the team, we can use that information to fill our original matrix. To do so, we use   k -Nearest Neighbors  (KNN) regression. That is, for a given user U and channel C, we lookup the  k  channels that are the most similar to C according to our channel similarity matrix. We then perform a weighted average of the activity scores between each of the top  k  similar channels and the user U. . For example, assuming  k =2, to compute how relevant C2 is to U3, we first look at the C2 row in the similarity matrix. In this case, the  k  most similar channels, excluding C2 itself, are C1 and C3 with similarities of 0.89 and 0.0 respectively. We then lookup U3’s activity scores for C1 and C3 in our original activity matrix. In this case, the scores are 0.5 and 0.3. We can then compute the weighted average of the scores given the similarities. . We would thus expect U3 to have a 0.5 activity score in C2. We can then repeat the same approach to fill the remaining blank spaces in our original matrix. . The approach described above performs well on our test set, but we can do better. Going back to our original matrix, because of the scale invariance of cosine similarity, C1 and C2 are considered to be highly similar even though U1 and U2 are half as active in C2 as in C1. When computing U3’s predicted activity score for C2, we did not take this difference in scale into account. This likely leads us to overestimate U3’s predicted activity level in C2. . To mitigate this issue, we not only compute a similarity score between every pair of channels, but we also learn a linear relationship between activity levels in each channel. To do so, we look at the activity scores for the common set of members between the two channels and run a  linear least squares regression  using the scores for the first channel as input and the scores for the second as output. The result is a function of the form Ay =  α  ∙ Ax +  β  which can be used to transform an activity score in channel Cx to an expected activity score in channel Cy. . Going back to C1 and C2, the set of common users for these channels is U1 and U2. . In this case, the output of the least squares regression from activity in C1 to activity in C2 is: . If we were to repeat this process to learn a transform from activity in C1 to activity in C3, we would run into an issue: the system is underdetermined and there is an infinite number of solutions since C1 and C3 only share one common user. To get around this issue we introduce a fake data point in the regression with zero input and output. . In this case, the output of the least squares regression is: . Once we are done computing the activity transforms for every channel pair, we can apply them to the predicted activity levels in our KNN regression. Going back to our prediction for U3’s activity level in C2, we modify the weighted average to take the activity transforms into account. . This new estimate is much closer to the activity level of other users in C2 than our previous estimate. . In fact, by using this simple tweak, we were able to reduce the RMSE on our test set by an additional 35%! . In the end, a Machine Learning model does not make a product feature. In addition to improving the quality of the recommendations, we worked on their presentation. This included building out the infrastructure to index and serve the recommendations, choosing the wording and interaction model used by Slackbot and devising a triggering logic to push the final result in a way that wouldn’t seem too disruptive. Finally, to control the roll out as well as to quantify the success of the feature, we leveraged Slack’s internal experiment and logging systems. After releasing the feature to only 10% of teams, we already measured a 22% click-through rate for these recommendations. . But this is really just the start. We are already working on a larger regression model that leverages the work described above as well as other features to generate better recommendations. As we get more insight from usage of the feature, we hope to be able to continuously improve the quality of the recommendations we provide and, at the same time, we hope to make the work lives of our users simpler, more pleasant and more productive. .  If you want to help us tackle data-driven product development challenges and help make the work life of millions of people simpler, more pleasant and more productive, check out our    SLI job openings    and apply today.  ", "date": "2016-09-06"}, {"website": "Slack", "title": "Taking PHP Seriously", "author": ["Keith Adams"], "link": "https://slack.engineering/taking-php-seriously/", "abstract": " Slack uses PHP for most of its server-side application logic, which is an unusual choice these days. Why did we choose to build a new project in this language? Should you? . Most programmers who have only casually used PHP know two things about it: that it is a bad language, which they would never use if given the choice; and that some of the most extraordinarily successful projects in history use it. This is not quite a contradiction, but it should make us curious. Did Facebook, Wikipedia, WordPress, Etsy, Baidu, Box, and more recently Slack all succeed  in spite of  using PHP? Would they all have been better off expressing their application in Ruby? Erlang? Haskell? . Perhaps not. PHP-the-language has many flaws, which undoubtedly have slowed these efforts down, but PHP-the-environment has virtues which more than compensate for those flaws. And the  options  for improving on PHP’s language-level flaws are  pretty impressive . On the balance, PHP provides better support for building, changing, and operating a successful project than competing environments. I would start a new project in PHP today, with a reservation or two, but zero apologies. . Uniquely among modern languages,  PHP was born in a web server . Its strengths are tightly coupled to the context of request-oriented, server-side execution. . PHP originally stood for “ Personal Home Page .” It was first released in 1995 by Rasmus Lerdorf, with an aim of supporting small, simple dynamic web applications, like the guestbooks and hit counters that were popular in the web’s early days. . From PHP’s inception, it has been used for far more complicated projects than its creators anticipated. It has been through several major revisions, each of which brought new mechanisms for wrangling these more complex applications. Today, in 2016, it is a feature-rich member of the Mixed-Paradigm Developer Productivity Language ( MPDPL ) family [   1   ] , which includes JavaScript, Python, Ruby, and Lua. If you last touched PHP in the early ‘aughts, a contemporary PHP codebase might surprise you with  traits ,  closures , and  generators . . PHP gets several things very deeply, and uniquely, right. . First,  state . Every web request starts from a completely blank slate. Its namespace and globals are uninitialized, except for the standard globals, functions and classes that provide primitive functionality and life support. By starting each request from a known state, we get a kind of organic fault isolation; if request  t  encounters a software defect and fails, this bug does not directly interfere with the execution of subsequent request  t+1 . State does reside in places other than the program heap, of course, and it is possible to statefully mess up a database, or memcache, or the filesystem. But PHP shares that weakness with all conceivable environments that allow persistence. Isolating request heaps from one another reduces the cost of most program defects. . Second,  concurrency . An individual web request runs in a single PHP thread. This seems at first like a silly limitation. But since your program executes in the context of a web server, we have a natural source of concurrency available: web requests. Asynchronously curl’ing to localhost (or even another web server) provides a shared-nothing, copy-in/copy-out way of exploiting parallelism. In practice, this is safer and more resilient to error than the locks-and-shared-state approach that most other general-purpose languages provide. . Finally, the fact that PHP programs operate at a request level means that  programmer workflow  is fast and efficient, and stays fast as the application changes. Many developer productivity languages claim this, but if they do not reset state for each request, and the main event loop shares program-level state with requests, they almost invariably have some startup time. For a typical Python application server, e.g., the debugging cycle will look something like “think; edit; restart the server; send some test requests.” Even if “restart the server” only takes a few seconds of wall-clock time, that takes a big cut of the  15–30 seconds  our finite human brains have to hold the most delicate state in place. . I claim that PHP’s simpler “think; edit; reload the page” cycle makes developers more productive. Over the course of a long and complex software project’s life cycle, these productivity gains compound. . If all of the above is true, why  all the hate ? When you boil the colorful hyperbole away, the most common complaints about PHP cluster around these root causes: . Lest I seem like an unreflective PHP apologist:  these are all serious problems that make defects more likely . And they’re unforced errors. There’s no inherent trade-off between the Good Parts of PHP and these problems. It should be possible to build a PHP that limits these downsides while preserving the good parts. . That successor system to PHP is called  Hack  [   3   ] . . Hack is what programming language people call a ‘ gradual typing system ’ for PHP. The ‘typing system’ means that it allows the programmer to express automatically verifiable invariants about the data that flows through code: this function takes a string and an integer and returns a list of Fribbles, just like in Java or C++ or Haskell or whatever statically typed language you favor. The ‘gradual’ part means that some parts of your codebase can be statically typed, while other parts are still in rough-and-tumble, dynamic PHP. The ability to mix them enables gradual migration of big codebases. . Rather than spill a ton of ink here describing Hack’s type system and how it works,  just go play with it . I’ll be here when you get back. . It’s a neat system, and quite ambitious in what it allows you to express. Having the option of gradually migrating a project to Hack, in case it grows larger than you first expected, is a unique advantage of the PHP ecosystem. Hack’s type checking preserves the ‘think; edit; reload the page’ workflow, because the type checker runs in the background, incrementally updating its model of the codebase when it sees modifications to the filesystem. The Hack project provides integrations with all the popular editors and IDEs so that the feedback about type errors comes as soon as you’re done typing, just like in the web demo. . Let’s evaluate the set of real risks that PHP poses in light of Hack: . Hack provides an option that no other popular member of the MPDPL family has: the ability to introduce a type system after initial development, and only in the parts of the system where the value exceeds the cost. . Hack was originally developed as part of the  HipHop Virtual Machine , or HHVM, an open source JIT environment for PHP. HHVM provides another important option for the successful project: the ability to run your site faster and more economically. Facebook  reports  an 11.6x improvement in CPU efficiency over the PHP interpreter, and  Wikipedia  reports a 6x improvement. . Slack recently migrated its web environments into HHVM, and experienced significant drops in latency for all endpoints, but we lack an apples-to-apples measurement of CPU efficiency at this writing. We’re also in the process of moving portions of our codebase into Hack, and will report our experience here. . We started with the apparent paradox that PHP is a really bad language that is used in a lot of successful projects. We find that its reputation as a poor language is, in isolation, pretty well deserved. The success of projects using it has more to do with properties of the PHP  environment , and the high-cadence workflow it enables, than with PHP the language. And the advantages of that environment (reduced cost of bugs through fault isolation; safe concurrency; and high developer throughput) are more valuable than the problems that the language’s flaws create. . Also, uniquely among the MPDPLs, there is a clear migration path to a higher performance, safer and more maintainable medium in the form of Hack and HHVM. Slack is in the later stages of a transition to HHVM, and the early stages of a transition to Hack, and we are optimistic that they will let us produce better software, faster. .  \t\t\t\tSlack Technologies, Inc. is looking for great technologists to join us.\t\t\t\t Apply now  \t\t\t ", "date": "2016-10-13"}, {"website": "Slack", "title": "Mentorship at Slack", "author": ["Brenda Jin"], "link": "https://slack.engineering/mentorship-at-slack/", "abstract": " It’s a scene familiar to many tech companies: summer rolls around, and the office is filled with interns who bring fresh ideas and energy to the workplace. In their first few days, they’ll typically attend some training sessions. Then, once they get settled in, they work on projects where they can contribute meaningful work in a short period of time. . But there’s much more to an excellent internship program than training sessions and project assignments. Strong internship programs need to proactively help interns level up and contribute successfully to the company. That’s why at Slack, we pair every intern with a mentor. . This summer, I had the privilege of mentoring  Melissa Khuat , a software engineering intern from University of Washington. Melissa  was able to contribute to our code base and participate fully in our workplace . She even gave a few presentations to teach the Platform team about SOLR and our real-time monitoring tool. . Mentorship is an active process where you offer your time, support, and insight. What we, as mentors, get in return is an unparalleled growth opportunity. . When good mentorship happens, both the mentor and mentee get more back than what they put in. . Melissa did all the hard work of setting ambitious goals, committing to them, and making the most out of her experience. As a mentor, my focus was on helping her find the technical and organizational tools to succeed. . When you become a mentor, there are specific things you can do to contribute meaningfully to your mentee’s work experience, self-confidence, and ability to succeed in the workforce. . Let me share some tips and tricks for mentoring interns, based on the lessons that I learned this summer. . You will meet a unique, talented, and inspiring person. Get to know them — ask them about their interests, and find out what makes them tick.  Ask them what they want out of their internship experience, and what they’re looking for in a mentor.  . After Melissa’s internship, she told me how much impact that simple question had on her during our first meeting. Thinking about the answer empowered her to be be proactive about making the most of her internship experience. . College-level interns often don’t have much job experience. Because of that, they might not know exactly what they want or how to respond right away. If your mentee hasn’t thought about these questions before, that’s okay! Asking them prompts them to think about it. Get their preliminary thoughts and let them know that you are available to discuss these topics at any time. . Sometimes your mentee won’t give you direct feedback on what they’re looking for. When you meet, ask how you can help and whether they have questions about their work, the organization, or anything at all. . Some questions will need to be redirected to their manager, the internship coordinator, or someone from another team. In that case, help your mentee determine  who  the right person is, and  empower them to ask that person directly . .  Remember that as a mentor, you also have a huge opportunity to learn and grow.  Pay attention to all the things that you are learning about yourself, working with other people, and your company. It is illuminating and refreshing to see your own company through a fresh perspective. Mentorship is an opportunity to gain new insights into the corporate culture that you have already become deeply familiar with. . Melissa and I didn’t have a recurring weekly meeting, but we met regularly over boba, tea, and coffee to talk about work and life. . Whether you have a standing meeting or impromptu ones, make sure you  meet regularly with your mentee . This builds trust. Trust is further built by mutual understanding, taking the time to listen and connect, and being consistent. .  Trust is the foundation to effective communication.  It paves the way for you to give timely and appropriate feedback to help your mentee navigate their internship experience. Without it, your advice can become meaningless or misunderstood. . In addition to regularly scheduled events, swing by their desk and reach out to see how they’re doing. If your mentee is interested in a particular topic, let them know about relevant meetings, events, or reading materials. . Bring your network to them and introduce them to people, both inside and outside the company. . This goes a long way to help your mentee feel welcomed in the industry. Feeling included and safe leads to confidence, better communication, and a higher quality of work. . Melissa mentioned to me that she had enjoyed classes in Asian American studies at university, so I invited her to an Asian American authors’ book club called  Tea Leaves . Even though she couldn’t make it to the discussion, she told me several times how much she appreciated the invitation. . Mentoring is different from managing. As a mentor, I don’t evaluate performance or determine whether we make a full-time job offer. I don’t determine which projects my mentee works on. I also don’t provide the same type of feedback as a manager — but I’m still responsible for providing feedback. . Feedback can come in the form of praise, advice, and suggestions. . It can be a way to draw their attention to a growth area. Encourage your mentee to think constructively about their performance, communicate effectively with their manager, and proactively identify interests. . In order to provide useful feedback, you will need to understand what your mentee needs. Some mentees need technical guidance. Others need information about how to navigate an organization or get more visibility for their work. .  Learning what your mentee needs requires     active listening    .  You’ll need to hear what your mentee has explicitly identified as a growth area, but also recognize other growth areas they haven’t yet identified. . Remember that you’re part of their support team — along with managers and other coworkers. . Do some work behind the scenes to ensure that important information is communicated clearly to your mentee. Sometimes you’ll need to work with their manager if your mentee is confused about feedback they received. . During one meeting with Melissa, she raised a concern about conflicting messages between the expectations being set by the internship program and her own goals. I encouraged her to speak with her manager directly, which she did. Because she was still unsure after that, I asked him to follow up with her to provide more specific details. . Working closely with their manager is especially important for interns, because they might have questions that you don’t remember having — like what to talk about in a 1:1 with their manager, how to ask for feedback on their performance, what it’s like to be promoted, how the organization is structured. Mentorship can be an unparalleled opportunity to learn more about your company. The exercise of putting yourself in someone else’s shoes will be a wonderful way to reflect on how your company can be more productive for everyone. .  Encourage your mentee to explore.  When your mentee has questions, it’s not always best for you to go and fetch the answer for them. If your mentee has been searching for something and you know the answer right away, then go ahead and share. But if you don’t, this is a wonderful learning opportunity. Show them your thought process as you help them find the information or figure out whom to ask. They will need to do this on their own someday. .  Encourage your mentee to stretch  — make sure they are challenging themselves. If your mentee has done something cool that other people on the team should know about, find the right venue for them to share their knowledge. For example, if your intern builds a great workflow tool, suggest that they update the documentation and give a quick demo or presentation in the appropriate meeting. . Get excited about their work and identify opportunities for them to shine. .  Empower your mentee with choices . It is up to your mentee to respond to feedback. At the end of the day, their career is in their own hands. It is their decision whether to take your advice, even if you’ve been specific and actionable. . Your mentee needs to be empowered to make decisions and mistakes, and to grow on their own. Make sure you give them enough space to do so. . This is the single most important thing you can do.  Be yourself  so that they know that you are a human being.  Inspire them just by being you.  . Let your strengths shine, be honest about what you’re working on and why, share real stories from your life, and be you! There are many things that your mentee will be able to learn just by watching you learn and grow. .   .  The Slack internship is a 12-week program for college students to experience work and life at Slack. During that time, interns get to work on public-facing features while meeting with leaders and executives of the company. Interns have the support of their manager and a designated mentor. Some interns receive a return offer from Slack, inviting them to join the company full-time when they graduate. Check our    jobs page    around late October to apply for the Summer internship program.  ", "date": "2016-10-19"}, {"website": "Slack", "title": "Building Hybrid Applications with Electron", "author": ["Anaïs Betts"], "link": "https://slack.engineering/building-hybrid-applications-with-electron/", "abstract": " Today we’ve just shipped a new version of the  Slack Desktop application for macOS . We built it with  Electron , and, as a result, it’s faster, sports a frameless look, and has a number of behind-the-scenes improvements to make for a much better Slack experience. . There are, of course, different ways to build desktop applications with web technologies. Unlike a 100% in-box approach that some other apps take, Slack takes a hybrid approach where we ship some of the assets as part of the app, but most of the assets and code are loaded remotely. Since there isn’t much information out there about how to do this with Electron, we wanted to dive into a bit more detail about how our hybrid application works. . Originally, the Slack desktop application was written using the  MacGap v1  framework, which internally used  WebView  to host web content inside of a native app frame. While that served us well for a long time (including the retrofitting of multiple-team support), this architecture was starting to show its age. New features such as HTTP/2 are only coming to Apple’s new  WKWebView  view, and moving to this would effectively require a complete rewrite of the application. Furthermore, WebView was tied to the operating system’s version of Safari, meaning that we didn’t have many options when older versions of macOS had an issue in Safari that affected our app. . Separately, when we created the Slack Windows application, we couldn’t use the existing codebase, so we decided to bet on a brand new platform called Electron. . We’ve  written about Electron before , but to summarize, Electron is a platform that combines the rendering engine from Chromium and the Node.js runtime and module system. . Since very early in the development of the Slack Electron app, we’ve had a working macOS version (albeit with many missing features). It was useful for us to be able to share our app with coworkers using macOS, for things like design feedback. So, when we looked into how to modernize the Mac app, moving to a unified codebase across Mac, Windows, and Linux was an easy choice. . Despite being the first production Electron application outside of  Atom , the Slack Desktop application has been kept fairly up-to-date with regards to web technologies. Our app has migrated from a  CoffeeScript  application written with vanilla DOM APIs to a modern  ES6 + async/await   React  application, and we’re currently incrementally moving our app to  TypeScript . . Electron inherits Chromium’s multi-process model — the main application as well as every Slack team that you’re signed into live in a separate process with its own memory space. For us, this means that we can restart individual teams that crash or have other issues without affecting the rest of the app, as well as protection from GPU driver issues via a separate GPU process. . On macOS, these renderer processes are labeled “Slack Helper;” you’ll see one for every team, plus three extra for crash reporting, the GPU, and the process that hosts the team switcher. . While we generally trust the local Slack application to run with full access to the desktop and Node.js, allowing remote content to directly access desktop features and Node.js is insecure — if someone were to Man-In-The-Middle Slack, they would have full control over user computers! To prevent this, we use a feature of Electron ported from Chrome Apps called the  WebView element  (unrelated to Apple’s WebView view mentioned above). Conceptually, this HTML element is similar to an iframe, in that it includes another site inline as a block element. However, it actually creates a separate Chromium renderer process and delegates rendering of content for its hosting renderer, similar to how the Flash plugin host framework works. . Before any navigation occurs, we get a chance to run custom code with Node.js integration enabled, called a “preload script.” This script runs before the DOM is created and before the page has an origin, but gives us access to Electron and Node.js APIs. . One thing that we can do in our preload script is set a key on the window object. This allows us to expose an API to the webapp side of Slack. Since we define this API, we can set up a Security Boundary that only grants the webapp certain methods. . There are a few things that you  must  do in order for this approach to be secure: . Communicating between all of these different processes is Tricky Business. On top of Chromium’s low-level IPC module which lets you send messages between processes, we’ve built a library called  electron-remote . . electron-remote is a pared-down, faster version of Electron’s remote module, using  ES6 Proxy Objects . Using proxies, we create an object which represents the window on a remote renderer, and all method calls get sent as messages to that remote. This lets you accomplish the same things as the traditional remote module, but without the pitfalls of remote event handlers and synchronous IPC. . First, set up the API you want to create in the main window. To make our example easier to understand, we’ll use a global variable: . Next, in our preload script, we’ll actually wire it up: . Now, your web application has access to a new object desktopIntegration which has a bounceDock method: . Being able to access remote objects efficiently makes implementing your webapp’s API much easier. In our case, it allows us to easily send  Redux App Actions  to update our app’s state and by proxy, the UI that depends on that state, to render updates to the badges on the Dock icon, or to update the unreads state on the team switcher items. . You must be careful when using electron-remote to audit your remote objects the same way that you audit your other preload objects — being able to ask another process to do something malicious is just as bad as doing it in-process! . As part of writing the Slack Desktop application, we’ve developed a number of libraries and tools that we’ve open-sourced: . We’ve also spent some time contributing to the Electron project itself, to help improve the framework for developers. . As you can see, the new Slack Desktop app helps our development team have the best of both worlds — the rapid iteration and ecosystem of web development, and the ability (with a bit of C++ and elbow grease!) to access the underlying Mac operating system in ways that websites can’t reach. We’re excited for the future of our Desktop apps, especially all the things we can do to bring together your team’s work together. .  \t\t\t\tIf you want to help us make Slack a little better each day, check out Careers site and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-10-25"}, {"website": "Slack", "title": "What matters to you, matters to us.", "author": ["Ayesha Bose"], "link": "https://slack.engineering/what-matters-to-you-matters-to-us/", "abstract": " At Slack, we’re focused on delivering big, impactful features, but we’re also dedicated to improving our users’ day-to-day experience of our product. In fact, every engineer spends at least an hour per week supporting users alongside our Customer Experience team, as part of our  Everyone Does Support  program. When we receive actionable feedback from our users, we share it into a channel where anyone at the company can see the pain our users are feeling. . If the issue at hand is easy to fix, we do so right away. If the problem is more complicated or requires a more involved solution, we’ll post the feedback in the appropriate team or product channel, where the right people can discuss it in more detail. This process has remained central to our team. Even as we’ve grown, we’re still committed to shipping features quickly and iterating rapidly with the help of new employees. (Read more about what we’ve launched recently on our  main blog .) . One improvement we launched recently was our inline media player, which enables users to play audio and video files that have been uploaded to Slack right within their channels and DMs. Previously, users would have had to download these files to play them. For most teams, this new feature was pleasant but minor, but for teams that work with audio and video files everyday, this saved both time and hassle. . Determined to ship this feature quickly, we knew we had to keep the scope manageable and that meant prioritizing and gaining efficiencies where we could. We started by looking at the native capabilities of the HTML5 audio and video player. Then, we worked with our Analytics team to figure out which file types were most commonly uploaded. . Since the HTML5 player would allow us to immediately support .mp4, .mp3, and .wav (72% of all video and 81% of all audio), we decided to move forward. Although this wouldn’t cover all media files, we knew this could help us get the feature out the door more quickly and deliver a better experience for the majority of our users. . Friends! No longer shall you suffer the indignity of downloading an .mp4. Or .mp3, or .wav — Slack now supports them all. Reload!  #changelog  . — Slack (@SlackHQ)  September 19, 2016  .    . When a new engineer — Jason Norris — joined the team, we used the opportunity to add support for another commonly-requested file type: .mov files. This iteration was a great on-boarding project for Jason as it was both well-defined and provided hands-on experience with various parts of our codebase. Within his first month, he was able to take on, build, and deliver the feature to our customers. With this update, we were able to cover 98% of all video files. . Us again, hi. Basically: Same tweet content as below, but adding .mov files to the list. 'Cause now they work too (most of them)!  #changelog   https://t.co/SkrKn8xkg3  . — Slack (@SlackHQ)  October 10, 2016  .    . Another feature we rolled out more recently is the ability to view PDF files directly in Slack. As with embedded media files, this allows people to view PDFs right in Slack without needing to download them. While this may also seem minor, PDFs are our third most popular type of uploaded files (after images and snippets) and we knew making them easier to view would make a big difference to our users. . PDFs are complex documents — structured into different layers of information, data, and objects, and containing different languages, images, and graphics. To simplify this project, we looked into using external libraries and settled on using  PDF.js . This library provided basic capabilities, including security and reliability, and helped us abstract away the complexities of the project. For our first pass at inline PDF viewing, we intentionally kept our scope narrow: display and text selection support for small PDF files. This clear, achievable set of product requirements allowed one of our new engineers, Xi Ji Guo, to start working on the project within his first few weeks on the team. . Every new engineer at Slack is assigned a mentor to help with their first few months. When Xi Ji faced technical challenges, his mentor, Patrick Kane, was able to guide him in the right direction with frequent code and architecture reviews. Patrick helped get the PDF viewer integrated into the Slack client and into our build system. He was also available whenever Xi Ji hit a snag, and pointed Xi Ji to the right people and in the right direction when he was stumped. . Through both of these features, Jason and Xi Ji learned the full product development process at Slack within their first few months, setting them up for their next projects. They worked with their product manager (that’s me!) to develop the features, debugged issues with our Manual QA team, and helped our Automated QA team set up tests. When we released the feature internally (something we do with all features), they wrote their first internal product announcement in our #released-internal channel. When we were ready to launch, they staged a gradual rollout and worked closely with our Customer Experience team to address any questions that came up. . We focus on the small things because they matter to our customers. Being an engineer at Slack means more than just submitting pull requests or making changes because someone told you to. We’ve created a process and a culture that encourages engineers to stay in touch with our customers and empathize with their problems. We like to say that nothing is someone else’s problem and we expect our engineers to take the attention and care that our Customer Experience team does everyday. What matters to you, matters to us. And we mean it. .  \t\t\t\tIf you want to help us make Slack a little better each day, check out our job openings and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-11-16"}, {"website": "Slack", "title": "On Empathy & Pull Requests", "author": ["Duretti Hirpa", "Mark Christian"], "link": "https://slack.engineering/on-empathy-pull-requests/", "abstract": " At Slack, we believe that empathy is humanity’s most important superpower. For our engineering team, that starts when we sit down at our keyboards. . Developing high-quality software depends on high-quality code review from peers. It helps us spot mistakes, avoid duplicating work, and generally ensure better software with less effort. Code reviews are a critical part of the development lifecycle at Slack. . Good code review culture is rooted in empathy for fellow engineers. What do we mean when we say empathy? Well,  Brené Brown  offers a good starting place — she’s a shame and empathy researcher and professor based in Texas. She defines empathy as having four components: . Empathy is a skill that we have to learn and practice — mastery comes from practice. Ultimately, we want to be warm, be kind, and be  accessible .  This  is the breeding ground for constructive collaboration, and the kind of working style we at Slack embrace. . So, how can we foster empathy and create a more collaborative environment, especially when a lot of our work consists of us typing by ourselves on our computers? Well, what about code review and pull requests? After all, that’s one of the main ways we interact with our fellow engineers. . It’s very hard to do a job of reviewing code if the author hasn’t set you up for success, so try to do the same for your reviewers. Here are some signs of good pull requests: . First and foremost, good pull requests give context to your reviewer. When you’re creating that pull request, a few things are true: . On the other hand… . Basically, your reviewer is  totally missing context , and it is your pull request’s job to give them that context. You have a few options: . Not every pull request needs every single one of those things, but the more information you give your reviewer, the better they will be able to review your code for you. Plus, if someone ever finds a problem in the future and tracks it down to this pull request, they might understand what you were trying to do when they make a follow-up fix. . Give your reviewer all the context they need to get up to speed with your bug so they can be an informed, useful code reviewer. It’s all about getting your reviewer onto the same page as yourself. . Each pull request should represent a single task. It’s okay to fix several little things in a single pull request, but they should be related. When your reviewer looks at the files you have changed, they shouldn’t be surprised. . It’s also a good idea to try to keep your pull requests pretty small — your reviewers are more likely to spot problems in smaller pull requests, and it helps avoid code review fatigue. . What’s the point of code reviewing in the first place? Well, code reviews let us find problems before they’re problems. So, as reviewers, we should take responsibility for problems in code that we reviewed. . Let’s repeat that, because it’s important: you should  take responsibility for problems in code that you approve . If a change gets deployed and causes a bug, it’s your fault, too. Do what it takes to have confidence in the code that’s being reviewed. . If you want people to take responsibility for your code, you better go the extra mile. Fixing a bug is good, but adding a unit test to ensure that it never comes back again is even better. Refactoring code is fine, but keeping the documentation up to date is important. Make sure to cover all of your bases. . Good pull requests are an act of empathy, for both the author and reviewer. . As the author, adding as much context as possible to your pull requests allows you to not only receive a good review, but engender good will to your reviewer. Good pull requests, ones that are focused, with tests and comments, allow you to safely forget context. As a reviewer, good pull requests allow you to be more efficient, and point your energies at reviewing the code at hand as opposed to gathering context. Everyone wins. .  In our next post , we’ll look at the other side of the pull request and talk about what goes into a good code review. Stay tuned! .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our engineering jobs and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-02-17"}, {"website": "Slack", "title": "How About Code Reviews?", "author": ["Mark Christian", "Duretti Hirpa"], "link": "https://slack.engineering/how-about-code-reviews/", "abstract": " Last time, we talked about  empathy and what goes into good pull requests . This time, let’s talk about the other side of the equation: what makes a good code review? . First, it’s important to remember why we’re bothering with pull requests and code reviews in the first place. . Each of those reasons is different, but the same strategies are important for all of them. . Take your time. Relax. Drink some coffee. . Code reviews aren’t a distraction from your job:  they are your job . You are here to help build an amazing product, and ensuring that we all do a good job is an important part of that. . If you’re new to the project, you will probably not feel comfortable reviewing code at first. That’s okay! You’ll probably feel confident saying “not okay” sooner than you will feel confident saying “yes, ship it!”, and that’s okay, too. . Make sure you know your own comfort zone. An approval without any confidence behind it isn’t really worth much. . People who are new to your project might need a helping hand in getting used to your project’s coding style, but in general, reviews shouldn’t be too focused on superficial issues like code formatting. Readability still matters, of course, but try to focus on the big picture. Consider whose code you’re reviewing and try to anticipate what they need. . If you don’t understand the code being changed, go read the current version. If you still don’t understand, ask questions. Make sure you really understand. At the end of all that, if you feel totally out of your depth, that’s okay: pass the review on to someone who has more context, and follow-up on it later so you can learn. . Asking clarifying questions can help the author of the pull request understand better, too. Talking about the changes and bringing up edge cases are some of the most valuable things you can contribute to a code review. . It can be helpful to summarize your understanding in a comment — putting your thoughts into words will help you solidify that understanding, and it will also let your reviewer know that you’re paying attention. . Does this bug seem familiar? Is it touching some code you were in recently? Maybe you can do more than just review their branch. . Share your knowledge. Reference old bug reports. Solve the problem at a deeper layer. When you work with another person on something, you both get better. . If you find yourself writing more than a sentence or two, or fundamentally disagreeing with a big part of the work that was done, or going back-and-forth with the other person, maybe code review isn’t the right place for the conversation. Go talk to them one-on-one so you can get onto the same page. . In general, rather than ambushing someone with a huge list of public comments, consider talking to them privately. This leads into our final topic… . Finally, and most importantly, the awkward, fuzzy part: be empathetic. Remember, there are four components to empathy: . If you see a problem with the code, try to put yourself in their shoes and try to help them see the problem without making them feel like a screw-up. Even experienced developers flub this part sometimes! It’s super hard. Sometimes, you will make mistakes. It’s okay — just work at it. . The way you write is everything. Don’t correct people — ask them for clarification. Assume they know something that you don’t. This is a really important thing!  Assume that your co-workers are smart and are doing a good job.  . Written communication can be tricky — it’s missing a lot of the social clues that let people know what you’re thinking. It’s easy to put people on the defensive, so take the time to try to use empathy words. — “us”, “our”, “we” are much better than “you”, “your”, and “mine”. You’re all on the same team, after all. . Pull requests &amp; code reviews are where code meets people. It’s about communication more than it is about code, so take your time, do a good job, and think of the human being on the other side of the pull request. Good luck! .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our engineering jobs and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-02-23"}, {"website": "Slack", "title": "Distributed Security Alerting", "author": ["Ryan Huber"], "link": "https://slack.engineering/distributed-security-alerting/", "abstract": " How does a company know when it has been hacked? Let’s list some ways, in order of best case to worst case: . At first glance, this list seems to indicate “who notices” is the important bit, but that is not quite right. “When” plays an even bigger part here. Time is important because the longer someone has unauthorized access to your systems, the more damage they can do. When something bad happens, and assuming you’ve enabled logging, the first indication is probably sitting right there in a log. . We have at our disposal software to archive and search all of the data we can possibly imagine. We collect them and tag them and store them and :heart: them, because they help us do things like improve performance and verify that a new feature makes users happy. At Slack, system logs are some of our favorite things. . Logs are also extremely important to our security team. Collecting authentication, system and network logs helps us hunt for anomalies and suspicious activity. Having mountains of data is nice, but to make it useful, we need to be looking at it all of the time. . So we have this data and we know that time is super important. How do we make sure the time between $ bad_thing_happens  and $ we_notice_bad_thing  is as short as possible? A few ideas: . You can hire a great security team and give them unlimited coffee and have them each ` tail ` a log. This may work, but it doesn’t scale. Eventually the volume of data will overwhelm the team. Your team may also experience alert fatigue, where they become desensitized and lose their ability to respond meaningfully to events. A great security team should probably be building things instead of staring at logs. :thumbsdown: . Maybe you employ security operations folks to watch your systems 24/7? You can automate much of the process and equip the team with tools they need for investigation. This is a pretty good strategy, but there are a couple of problems. First, it is expensive. You’d need to staff this operations center with people who can make informed decisions. Second, no matter how well these folks understand the systems they are protecting, they can’t know with certainty that something they detect is a problem without reaching out to others. :neutral_face: . Okay, one more idea. What if you send alerts directly to the person(s) involved and ask them whether the event was good or bad? What if you also do it the moment you detect something strange? This means you will be asking exactly the right person whether they did something within moments of seeing the activity. Now we’re onto something! :thumbsup::tada: . So what does this look like in practice? Let’s walk through the lifecycle of an alert using this idea: . But we have a problem! What if some baddie has taken over your employee’s Slack account? If a bot says “did you do this” and the baddie replies “yes”, this system won’t really work. To solve this problem, out of band communication can be used to confirm the alert. . The above idea isn’t hypothetical, it is exactly how we handle security alerting at Slack. . When a Slack employee receives an alert, they reply to the bot with `ack`, and the bot then sends a 2-factor push to their mobile device for final confirmation. If the baddie types `ack`, the evildoer still won’t have access to the employee’s mobile device to confirm the ack. Additionally, if the baddie just deletes the message, it will automatically escalate to the security team within moments. There are some actions that will escalate regardless of the employee’s response, but most suspicious activity can be handled directly by the employee without involving the security team. . To illustrate these ideas, imagine you are monitoring for any users who run the ` flurb ` command on your servers. In this example, `flurb` is something an operator may run occasionally, but it is rarely needed and commonly used by hackers. . Example 1 (the good case): . Example 2 (the bad case): . Most importantly, this system allows us to watch more things than previously possible by deputizing every person at Slack as part of our security team, meaning we have hundreds of people constantly on watch. . The tools we have used to create some of the bots used by our team are freely available on Slack’s GitHub page here:  python-rtmbot ,  python-slackclient . We also have  node-slack-client , an excellent open source library for node if you’d like to create your own interesting integrations. . This investment in security alerting and monitoring is especially important as we grow, and our solution should help us keep pace with the rapid growth of Slack the company, but this is just one example of how we handle security at Slack. Stay tuned for posts from the security team explaining the tools and techniques we use to keep Slack safer, more secure and more productive. .  Want to help Slack solve tough problems and join our growing team?    Check out all our engineering job   s and apply today.  ", "date": "2016-03-01"}, {"website": "Slack", "title": "The Joy of Internal Tools", "author": ["Greg Pelly", "Mark Christian"], "link": "https://slack.engineering/the-joy-of-internal-tools/", "abstract": " Developers working on the same project often have wildly different workflows, based on how they like to work and the tools they are familiar with. Despite that, there are usually a few common tasks that everyone has to do. . If we compare the flow state of writing software to Mario speeding through a level in Super Mario Bros., then development tools provide the stable bridge above the fiery pit that allows Mario to safely navigate through the complex castle that is his project. . To stay in the flow state, engineers must avoid the pitfalls that come with interactions across the systems that make up a development environment, such as source control, testing, linting, and code review tools. When all goes well, an engineer happily skims across the surface of these tools and repeats the process. As any engineer will tell you, this is sometimes easier said than done. . Over time, engineers here at Slack developed their own individual workflows for getting things done. The  slack-cli  project was born when we decided to take some of our ad-hoc shell scripts and package them together in a tidy way. Offering up a shared set of tools had a number of benefits: . In some cases, this process is not much more involved than sharing common terminal aliases and scripts. In other cases, many individuals have riffed on a tool over time to support complex use cases. Overall, the goal was to reduce friction in a developer’s work day wherever possible. . A good rule of thumb for a development tool is this: if a developer has to stop what she’s doing and open a browser window for the next step in her work, then that task is a good candidate for building tooling around. . Is it because engineers ought to do everything from the command line? No, but the internet is a veritable Candyland for the monkey mind, filled with delightful distractions that are unrelated to your current work. Slack wants to enable employees to do the best work of their lives, and providing a focused environment through tooling reinforces that value. . Having a place for tools to live opened up all kinds of interesting possibilities. Once we had a structure in place that made it easy for people to add new commands, we started to see the tool get used in both expected and unexpected ways. Here are some of the less obvious uses: . Here it is, in all its glory: we created a single command-line executable called  slack . It works very similarly to the  git  command line tools: . This strategy makes it easy for people to add new commands (just create a new script called  slack-whatever ), and it keeps things neatly separated. This lets us avoid stepping on each other’s toes and reduces the cognitive overhead of adding new commands. . The tools are installed in  /usr/local/bin  by a plain old Makefile. This means we don’t have to rely on people having the slack-cli code checked out on their computers, and we don’t have to fiddle with people’s  $PATH s. Once the tools are installed, they just work. . To keep the tools up-to-date, we’ve created … a tool!  slack update-tools  will check out a fresh copy of the slack-cli git repository in your temporary directory, run  make install , and clean up after itself. . Making it easy to update tools has helped ensure that people are generally up-to-date, and pushing out an important change is as simple as posting a “Hey, run slack update-tools because…” message inside the relevant channel on Slack. . We’ve found this pattern so helpful that we’d love to share it with the world. We’ve open sourced something called `magic-cli`, a general version of our tool. Check out the  magic-cli repository on GitHub  to get started. . This repository provides the basic command runner, with support for running subcommands located in the same directory as itself. You can rename the main executable from  magic-cli  to anything you want — if you work for Example.com, you could rename it to  example  and add custom commands like  example-test , and everything will just work. We hope you find it as useful as we have. . This endeavor has created a new social group inside of Slack, allowing for cross-pollination across teams.  slack-cli  gives new hires a low-effort way to contribute, and their fresh perspective is invaluable for helping us remove friction. Such gaps in tooling are often overlooked, and fixing them is important as engineering organizations grow. . Are we really saying that someone can come in on day one and improve the day-to-day development process for the entire engineering team? Yes, we are! Which brings us back to why we’re here at Slack in the first place: to collectively make our working lives simpler, more pleasant, and more productive. .  Do you have a proclivity to write tools to make other developers’ lives more pleasant and productive? Well,    do we have the job    for    you   ! (and    many other jobs on our engineering team    as well)  ", "date": "2016-03-30"}, {"website": "Slack", "title": "Using ES2015 with Electron — introducing electron-compile", "author": ["Anaïs Betts"], "link": "https://slack.engineering/using-es2015-with-electron-introducing-electron-compile/", "abstract": " As part of writing the Slack Desktop application, we created a new library / set of tools that will save other developers writing Electron applications a lot of time and effort. We call it  electron-compile , and this post will describe how to use it and explain how it works. .  Electron  is a platform for writing cross-platform Desktop applications using Web technologies — this means, instead of writing your desktop app in languages like C# or Objective C, and using frameworks like Cocoa or WPF, you write your app once in HTML, CSS, and JavaScript. . Traditionally, this approach has had some significant limitations — if your app wanted to do something that wasn’t implemented by the framework or the confines of what a web browser could do, your options were very limited (or even non-existent!). Electron is one of the first platforms that is extensible — adding new native features to the platform can be done via  npm modules , either written in JavaScript, or via C++ native node.js modules. Having access to the entire npm ecosystem out of the gate means that Electron can do many things that other platforms got stuck on. . Electron works by combining the  Chromium Content Module  from the Chromium project, the codebase that powers the Google Chrome browser, and integrating the  node.js  runtime and standard library, as well as adding  APIs of its own  as built-in node modules. These modules allow you to interact with the operating system in ways that normal browsers don’t allow, such as being able to create Menu Bar menus or show OS File Open/Save dialogs. . Out of the box, Electron understands the same languages that Chrome and node.js understand — HTML, CSS, and JavaScript ES5. Developers however, have created a number of higher-level languages that can be compiled into these base languages. Projects like  Babel ,  TypeScript , and  LESS  allow developers to write code more quickly and correctly, and use features that have perhaps not been fully implemented by browsers. In the Slack Desktop app, we make significant use of ES2015 / ES2016 features, such as  ES2015 modules  and  async/await . . However, we noticed when interacting with the community, that while people were excited with these new features, they also felt a  lot  of frustration trying to get started, because using these features required developers to spend a lot of time creating a “build pipeline” to compile their ES2015 code to something that Electron could understand. In particular, the popular  React Framework  requires an enormous amount of boilerplate build code in order to get it working with Electron. . What if, Electron could just  understand  all of these languages natively instead, without a build step? . To this end, we’ve worked with the Electron community to create  electron-compile , a project that allows Electron to natively “Just In Time” compile a number of popular web languages: . These languages work inside standalone files, and even work inside inline &lt;script&gt; elements. . Using electron-compile is very easy — in your package.json, replace your reference to electron-prebuilt and instead use electron-prebuilt-compile. Install, and you’ll have a version of Electron that automatically initializes the electron-compile library behind the scenes — even your opening file can be written in ES2015. . Similar to BabelJS, electron-compile is configured  via a special dotfile , called .compilerc, which allows you to pass options to the various compiler libraries that electron-compile uses. Here’s an example from our app: . Even if you don’t provide a .compilerc, electron-compile has fairly reasonable defaults which are probably well-suited for getting started. . In order to make Electron natively understand web languages, we need to first intercept the different ways that Electron loads web content. Intercepting JavaScript loaded via node.js’s require statement is fairly straightforward, via the mechanisms that the  node module system  provides. We can see this hook in electron-compile in  require-hook.js  . However, the trickier problem is intercepting content loaded by Chromium, which doesn’t use require to load content. In order to do this, we use Electron’s  protocol module  in a novel way — instead of registering a  new  protocol, we intercept the file: protocol in  protocol-hook.js , so that when Chromium requests local content, we get a chance to instead return the compiled result. . Being able to intercept content gives us the ability to do some useful things — for example, in the original version of electron-compile, we required developers to initialize every process separately. However, we control all content! This means that we  control the HTML  that is loaded — automatically initializing electron-compile is as simple as silently inserting a script tag at the very top of any included HTML file. . Now that we’ve got a way to intercept content, we need a way to cache compiled results, from which we’ll take some inspiration from the Git version-control system. We want to create a table mapping the SHA1s of input contents, to the compiled result. We can then create a write-through cache that persists to disk, where the filenames are the SHA1s of the input file contents. . In order to do this efficiently, we store SHA1s of seen files in a separate cache ( file-change-cache.js ), as well as some file metadata about input files, such as whether this file has a Source Map or if it’s in node_modules, which is a good hint that we shouldn’t be compiling it. . Using the File Change cache means that subsequent fetches of a file’s SHA1 only requires a stat call to check the file size and mtime. This cache also comes in handy later when we build the app for production, which we’ll see later — it gets saved out as part of compiler-info.json.gz. . Initially looking at the directory structure of an electron-compile cache, it seems to be intentionally obfuscated — SHA1s everywhere! There is some reason to the rhyme however — let’s have a look: . The root directory’s SHA1 represents the path to the application — we don’t want two separate applications writing to the same cache. We see compiler-info.json.gz as the only human-readable filename, which contains the contents of the FileChangedCache, as well as the compiler options passed to every compiler, as well as the  versions  of the compilers. . Because different versions or compiler options generate different code, when this input changes, we need to forget about our already-compiled code. In order to do this, we simply calculate a SHA1 based on this compiler option + version information, and use it for the folder name. . Opening any of these files gets gibberish, but in fact, these files are simply gzipped — using gzcat reveals that they are simply JSON, which includes the compiled code as well as the MIME type of the result: . Combining these features of electron-compile, along with our policy to gzip everything, means that in production, electron-compile does less I/O work than even precompiling all assets. . electron-compile is great for development, but we can also use the same framework for production mode — simply by compiling every file in our project, then copying the compile cache into our application. Because we definitely don’t want to be compiling anything in production, we configure the library to work in Read-Only Mode. An important facet of this mode is that it creates dummy versions of the compiler classes in  read-only-compiler.js  that return the same information as the original compilers, using the information stored in compiler-info.json.gz. . Since our FileChangedCache is already seeded with all of the files that are valid files to load along with their sizes, we can use this to determine what files to load, without even opening the original files. Unfortunately, while we’re  this close  to being able to ship just the cache and not the original source which would save space, the node.js module system will path.resolve anything it loads — the original files have to be there (though they can be zero-length). . From a developer-using-the-library perspective, electron-compile provides both a command-line as well as a full API to implement compiling for production, and just as electron-prebuilt-compile provides a wrapper for electron-prebuilt, it also provides a wrapper for the popular  electron-packager  project that will automatically do all the electron-compile work behind the scenes when packaging your project. . We’ve used the electron-compile package quite a bit in both development and production on all of the major desktop operating systems, and we’ve found it to be really useful, especially for quickly prototyping ideas in React. Here’s where you can learn more: .  \t\t\t\tWant to help Slack solve tough problems and join our growing team? Check out all our engineering jobs and apply today.\t\t\t\t Apply now  \t\t\t ", "date": "2016-04-26"}, {"website": "Slack", "title": "A Walkthrough Guide to Finding an Engineering Job at Slack", "author": ["Julia Grace"], "link": "https://slack.engineering/a-walkthrough-guide-to-finding-an-engineering-job-at-slack/", "abstract": " Interviews, eh? Horrid, stressful ordeals that fly by in a sweaty mess and then linger long in the memory. Or, at least, that’s the traditional model. But why on earth would anyone want that? They should be a chance for you to show your best self and to find out if you want to work somewhere. . “Somewhere”, in this case, being Slack. . We’ve put a great deal of effort into designing our interview process so that it is comprehensive and consistent, and are working hard to remove as many points of bias as possible. To date we’ve found it successfully identifies people who will succeed here — those with a high degree of technical competence who also embody Slack’s values: empathy, courtesy, craftsmanship, solidarity, playfulness, and thriving. . First and foremost we look for skilled engineers who are passionate about programming and display a high degree of craftsmanship. We value those who can level up their whole team rather than just themselves, and who have a passion for exploration and inquisitiveness about how things work and what our customers need. People who are highly collaborative and understand the value of a diverse team with different backgrounds, thoughts, ideas, and lived experiences do very well at Slack, as well as those who take personal responsibility for their decisions and get stuff done. . Candidates do their best in interviews when they know up front what to expect, so here’s an outline of our process. We follow the same process for all web engineering candidates, regardless of position or level of experience: .  1. Resume screen   At a high level, we’re evaluating if you’re a good fit for the role you applied for — there are many amazing, talented people, but not all will be a great fit for Slack (and Slack won’t be a great fit for everyone). We aren’t concerned with where, or even if, you went to college as much as your experience and the passion for your work. .  2. A phone call with one of our technical recruiters   This takes around 30 minutes and covers high-level questions about what you’re looking for and why you’re interested in Slack. .  3. A technical exercise   We’d like to get an idea of how you write code in the real world, since we feel this is the best indicator of how you’d write code day to day here at Slack. Granted, the Slack codebase is larger and more complicated than any technical exercise, but we have found the technical exercise to be a good indicator of future performance on the job. There are great engineers at big name companies and at small ones, so this gives everyone a chance to shine independent of where they are now. . This varies by position, but generally you’ll have a week to complete a technical exercise and submit the code and working solution back to us. . Since we don’t do any whiteboard coding during the onsite interview, the technical exercise is one of the best ways we’ve found to evaluate programming competency. . The exercise is graded against a rigorous set of over 30 predetermined criteria. We’re looking for code that is clean, readable, performant, and maintainable. We put significant effort into developing these criteria to ensure that, regardless of who grades the exercise, the score is an accurate reflection of the quality of the work. We do this to limit bias. If there are clear criteria, variations that might impact score but have nothing to do with the candidate (such as if the grader is having a good day) are less likely to influence the outcome. . It is important to note that we go to great lengths to ensure the technical exercise is graded as blindly as possible. For the majority of positions the person grading the exercise has not seen the name or resume of the person submitting it (we are working towards this for all positions). Some positions require candidates to submit a working version of their coding exercise and some candidates choose to host it on their website, so we are exploring ways to mask the URL from graders. . We cannot emphasize enough that the coding exercise is the most important way for us to evaluate your technical skills. If you’re selected to come in for an onsite interview, a portion of that interview will be devoted to discussing this exercise, including the choices and tradeoffs you made. .  4. A phone call with the hiring manager   This usually takes 1 hour and is an in-depth conversation about your background, current technical challenges you face and what you’re looking for in your next role. We welcome and encourage any questions, so come prepared with the list of things you’d like to know about Slack. .  5. An onsite interview   This usually takes around 4 hours. You’ll talk to 4–5 people from the engineering team, each for 45 minutes, followed by 15 minutes with the hiring manager. Our onsite interview focuses on technical and architectural discussions as well as determining how the values we care about at Slack fit with your own. . As mentioned earlier, we won’t ask you to solve algorithm questions or write code on the whiteboard (although we may use the whiteboard to have you draw out how you envision a system being built). Whiteboard coding often skews towards people who have a lot of experience and practice with whiteboard coding, which is not something that’s part of the day to day job at Slack. . To be clear, we often use a whiteboard to hash out ideas, but not to code up a binary search algorithm (or any coding problem).  Interviews are stressful, and when the candidate is asked to do something they don’t normally do and do it in front of someone judging them, it introduces a performance dynamic that can be alienating.  . There is no need to bring a computer to the interview, nor are there any specific subjects you should study up on. We want to get a good idea of how you think about building and debugging complex systems at a high level, which is not something you can necessarily study for. .  Every person you speak with will leave time for questions — remember, you’re interviewing us as much as we are interviewing you.  We want to make sure that Slack is a place you will enjoy working and can thrive in from both your perspective and ours! . Many candidates ask about our tech stack and the interesting engineering challenges we face. As a rapidly growing company the challenges are often moving and changing at a high rate, but here is an overview. . Slack is a distant descendant of a conventional LAMP stack app:  L inux,  A pache,  M ySQL, and  P HP all play important roles in the server-side of Slack. . We’ve extended our database layer to do automatic sharding, failover and caching. We have built a distributed, asynchronous job scheduler and execution engine. We deliver messages and notifications via a custom real-time messaging server and perform intelligent edge-caching in an application-aware way. So far, we have found that each order of magnitude beyond which Slack needs to grow requires creative rethinking of the back-end and front-end, and we know that we have more rounds of this kind of challenge ahead. .  Our current challenges include:  .  Major languages in use include:  . All of our code lives in git and GitHub and we use a variety of tools, some homegrown and some externally built, to manage the commit, test, review, build, deploy cycle in a very automated fashion. We are proud that our developer tools infrastructure allows us to safely and confidently update Slack &gt;50 times a day in production. .  Our careers page  lists all of our open engineering roles — if you spot one that looks like a great fit, apply online (but please don’t apply for multiple roles). We look at all resumes and do our best to respond within 1 week. . Many candidates think they need to find someone currently at Slack to “get their foot in the door.” Rest assured this is not the case; in fact most of our hires have come from people who have applied via our careers page. We take all applications seriously. We care deeply about  diversity at Slack  and when you only hire from your current employees’ networks, you tend to get a homogenous set of candidates. .  \t\t\t\tWe’re working hard to build a good team here at Slack, and, as you can see, have many big, interesting challenges! If they sound exciting to you, join us!\t\t\t\t Apply now  \t\t\t ", "date": "2016-05-04"}, {"website": "Slack", "title": "Calls: Is it you or is it me?", "author": ["Faraz Khan"], "link": "https://slack.engineering/calls-is-it-you-or-is-it-me/", "abstract": " Slack Calls are now in beta, on Mac, Windows, iOS, Android and Chrome. If you haven’t given it a try yet, please do (and let us know how it goes)!  Our help center article on Calls  has more details on the feature. . We wanted to answer the age-old question that we have all asked each other during a voice call when quality degrades: “Is it my connection or yours?”. We wanted a UI that the user can  trust.  So we decided to build in the logic to answer that question for you. It’s much better than a generic “Having network issues” warning. . We utilize a selective forwarding architecture to better support group calls. In this architecture, participants communicate only with the media server and are unaware of the state of each other’s network. In this post, we’ll talk about how we analyze real-time control protocol (RTCP) feedback from each of the users to generate correct feedback to all users about their individual uplink / downlink quality. . We use  Janus  as our selective forwarding unit (SFU) and  WebRTC  for voice chat. WebRTC provides us with standardized RTCP feedback but Janus doesn’t analyze it out of the box. This post is about us adding this support. Most code excerpts that you see are from Janus alongside some of our customizations. . An SFU’s job is to broadcast audio which it receives on the real-time protocol (RTP) layer from each participant to all other participants (this type of architecture is also known as a router server). This is in contrast to a Multipoint Control Unit, or MCU, which mixes the audio from each participant to maintain a single client/server stream.  More details on the differences between these approaches here . . In addition to RTP, there is also the RTCP layer (Real-time Control Protocol), which is used to relay back information about the connection. The topology offers us an advantage: since the server sits in the middle, it can separate out the two legs of the call (upload and download). We can use this to pinpoint a problematic link. . We analyze the RTCP Receiver Reports (RR) that all the receivers of a particular stream send back (which denotes  total loss ) while simultaneously calculating the upload loss of said stream from the sender. Given this, we figure out the  realistic loss  (loss caused  only  by the downlink from the server) on each of the downloads. We use ALL the  realistic losses  gathered by the process and convey them as custom Slack messages called  download_link_quality  and  upload_link_quality . . Now we’ll show you how to analyze the RTCP RR packets that WebRTC receivers send back and use it to figure out the quality of each leg of the call. . According to the  RFC , the RTCP RR packet looks like this: . For simplicity, we assume that all RTCP-RR packets only have a single report block within them (called non-compound RTCP). Generally, they can have many of these RR Report blocks (for multiple streams) but for this example we assume there is only one. Also we assume you are familiar with packet parsing and that code is removed for brevity. To learn more about RTP/RTCP parsing check out section A-2 of the  RFC . . The important part here is “fraction lost” which is a ratio (out of 255) of packets lost divided by the number of packets transmitted in this period. . Okay enough talk! Time to see some code. . To calculate the actual number lost we need to know how many packets the receiver has seen so far. The  ehsnr  (Extended high sequence number) holds the 32-bit sequence number which the receiver has last seen before this report was sent. . The above gets us the actual number of missing packets. We must remember that we have just counted the  total amount  that the receiver has seen missing. It’s unclear whether this is due to  download  loss on the receiver’s side or due to the sender’s  upload  loss. Since we maintain the sequence number spacing from the original sender’s stream, the loss reported here is total loss (sender’s upload loss + receiver’s download loss). We must go deeper. . We calculate  pkts_missed_due_to_upload_loss_since_last_rr  by calculating how many sequence numbers have been missed on the upload side of this stream (code not shown for simplicity). . For download loss &gt; 20% we would transmit an event to the receiver which would make their UI give the appropriate warning. As an example if Faraz is calling Pavel and is suffering from download loss &gt; 20% he’d see the following: . This state is broadcasted to all other users so everyone knows that Faraz’s download is bad and he might not be able to understand what is being said. So during this call Pavel’s side would correctly say: . Why 20%? For two reasons. Firstly the audio codec WebRTC uses (Opus) is able to deal with packet loss &lt; 20% elegantly so people may not even notice it. Opus does this using forward error correction (FEC) which is an ingenious way of encoding information about the previous packets into current ones (so they can be recovered if lost).  There’s evidence  that indicates that user perception of audio quality does not degrade significantly at loss levels of up to 20%. . This is the easy one and is done without using RTCP. Every RTP packet that you receive has a sequence number. We simply maintain a list of all sequence numbers received to date. Every 500ms, we run a loop that calculates  fraction loss  as: . As with download loss, this is communicated to the sender and warns him appropriately above 20%. With the same call if Faraz is suffering upload instead of download loss this time around Pavel would see: . Though we do slightly more with the fraction loss we have calculated here (like pack it into a RTCP RR message back to the sender to increase FEC) sometimes there is little you can do to help. So we do the best we can and put up  accurate  UI warnings alerting users as to why they are experiencing the problem. We have found that this helps users understand the problem instead of “Having network issues” which would leave them more in the dark. . Combating the variability of the internet is a never-ending battle in real-time communication. We can’t guarantee perfect audio quality every time, but hopefully with changes such as these it makes your call experience a bit more pleasant, and your conversations more productive. . We could not have built Slack Calls without amazing open source software, standardized protocols, and freely available research. Here are some resources to learn more about the technologies mentioned in this post: .  \t\t\t\tWe’re working hard to build a good team here at Slack, and, as you can see, have many big, interesting challenges! If they sound exciting to you, join us!\t\t\t\t Apply now  \t\t\t ", "date": "2016-05-11"}, {"website": "Slack", "title": "Making Slack Feel Like Slack", "author": ["Haim Grosman", "Mike Fleming", "Keith Adams"], "link": "https://slack.engineering/making-slack-feel-like-slack/", "abstract": " Systems problems are rooted in impossible dreams. Your file system wants to give you infinite, fast, durable storage. Your garbage collector and your kernel’s virtual memory subsystem both strive, in very different ways, to provide the illusion of infinite, fast, volatile memory. The constraints of physical reality make these hopes impossible to realize in  every  case, but astonishingly many of the common cases can be handled well. . Your Slack client strives to be a consistent, compact, zero latency, searchable replica of all of the files, messages, custom emojis, voice calls, bots, sound effects, etc. that your team is sharing in real time. Since Slack clients run on physical devices, this is impossible, so we must make do. Slack engineers working on the client-side have rummaged around in the systems toolbox to handle problems like: . Writing a high-quality Slack client is  tricky . We’ve done it a few times over now, and are boiling the lessons down into a modest-sized native library called  libslack . With the caveat that libslack is still a work-in-progress, we are expecting to reap the following benefits: . We at Slack find ourselves wrangling a vast and growing body of native client code. People use Slack from the web, from desktop clients for Mac OS, Windows, and Linux, and from native mobile clients for iOS, Android, and Windows Mobile. Some people also connect over XMPP or IRC using gateways that we operate on behalf of the team, and these gateways behave, for all intents and purposes, as alternative ‘clients’ that also happen to be servers for other protocols. Each of these clients is a mostly-separate codebase, with its own features, bugs, roadmap, code idioms, experts in the codebase, and gotchas. While having separate codebases with similar purposes is irritating to the engineering spirit, some of this is just a necessary evil in 2016. To date, platform capabilities have been shifting too fast for any of the Grand Unified Native Frameworks to reliably produce native-feeling, high-performance, feature-rich apps. So we roll up our sleeves and write different clients for different platforms. . This would be fine, of course, if all of the effort in a client went towards doing platform-specific, hardware-constrained things. Writing multi-touch UI code in an iOS-specific way, for example, is a perfectly natural thing. But a lot of the engineering effort in these clients is the sort of platform-agnostic, distributed caching work that we hope to deduplicate in libslack. . To understand libslack, we need a rough model of the interaction between a Slack client and the mothership. The two big client-facing channels are the  REST API , and the  Real-Time Messaging API . The former is a typical set of GET/POST endpoints for app-level state. The RTM API, on the other hand, is WebSocket-based, and is where full-duplex, latency-sensitive conversational stuff (humans sending messages to other humans) actually happens. . Libslack presents an object-oriented interface to a model of a Slack team. From the perspective of a libslack client, the non-local aspects of interacting with a team are entirely sealed off. Libslack’s virtual team is an always-on, always-consistent team as seen by a logged-in user. All of the behind-the-scenes systems work of caching, syncing, and hiding latency is hidden behind this virtual team abstraction. Libslack is about 20kloc of C++ at this writing, and is functional enough to support prototyping of demo clients. We are in the process of migrating the flagship Android and iOS Slack clients to libslack, and plan to use it universally going forward. . With respect to language choice: C++ provides a lowest-common-denominator environment common to all the desktop and mobile environments we are targeting. It also provides the kind of low-level control of resource footprint that is appropriate for libraries that may be linked into unknown, potentially much larger applications, with their own ideas about how to use CPU and memory. . So now, where before we had several different copies of the code talking to the Slack mothership, we now have just one: . Libslack expects to be called from an application language with a lowest-common-denominator of object-oriented features: roughly, classes and dynamic dispatch. We use an open source interface definition language to generate cross-language bindings for C++, Java, and Objective-C. The core object is a SlackAPI, which represents one user’s view of a team. Within libslack, RTM messages are translated into  events , and a SlackAPI manages a pool of worker threads that consume events from a central event queue. Libslack treats these events as cache coherency messages that enable it to update its cached model of the team. . The client subscribes to state updates, first creating views of a SlackAPI, and then registering callbacks to fire when the underlying model state changes. For example, an Objective-C client that displays the channels a user subscribes to might look like: . Compared with directly consuming the REST API and rtm stream, client code benefits from: . What about the web? Many people use Slack through a web browser. Since we have no way to inject native code into a web browser, it is hard to directly apply libslack. . However, the vast majority of people using Slack in web browsers are doing so over  stable, low-latency, high-bandwidth internet connections , like office wifi or wired ethernet. Some of these are internet-distant from Slack’s orbiting mothership, but we have edge capacity near them. . The layer of indirection here initially seems strange, but it allows us to hide the long round-trips between the edge POP and the central data center. The majority of requests can be satisfied from the local POP’s edge cache. The application-aware edge cache is an active proxy, where libslack fits in naturally. . Our desktop clients for Mac OS, Linux, and Windows are conceptually similar to the mobile clients, but leverage the work that has gone into making the web client. . In a libslack-ified world, we could have desktop clients simply talk to an edge-resident caching proxy just as web servers do. However, one of the core limitations of the web client is removed, in that we have the option of installing native code, and using more local machine resources. We’re actively prototyping a system we call ‘Slackd’, which runs the application cache locally, and accesses it over ports from localhost in the desktop apps. This is a similar architecture to the web setup described above, but instead of running the caching proxy in the local edge POP, we’ll be running it right on people’s hot little laptops. The latency-hiding strategy, and protocol spoken between slackd and the web application, are identical, but we now have an even shorter round-trip time. . Integrating all of the above pieces together, we get something like this: . We’ve changed the native mobile client so that its instance of libslack “stacks” onto the edge POP instance. Notice that  all  communication across swathes of the Internet that Slack cannot influence happen to the edge pop, and that in most cases, it is a client version of libslack communicating with a server version of libslack. This gives us the opportunity to experiment with different protocols and wire formats with changes to a single codebase. . libslack is still early in its lifecycle, and the picture we’re drawing here might be incomplete. It does not make sense to release into open source at this time, as its API and basic design have not yet settled. But we remain excited about libslack and its future. It has the potential to consolidate effort not only across client codebases, but also between back-end and client. We’ll keep you posted here as things progress. Finally, if you’re interested in doing this kind of work,  we are looking for you too . ", "date": "2016-06-03"}]