[
{"website": "Ebay-Research", "title": "Mathesis: Elements of Learning and Intelligence", "author": ["John (Ioannis) A. Drakopoulos"], "link": "https://tech.ebayinc.com/research/mathesis-elements-of-learning-and-intelligence/", "abstract": "A formal and interdisciplinary theory of learning and intelligence that combines biology, neuroscience, computer science, engineering and various branches of mathematics to provide a unifying framework, direction and a broader horizon for neural network and machine learning research. Learning and intelligence are essential for the future of ecommerce. The increasingly complex ways that buyers and sellers engage online, a number of existing and emerging issues and opportunities, and the range of applications (including natural language processing, search, computer vision, fraud/authenticity and virtual reality) require ecommerce platforms to be flexible, sophisticated and, ultimately, intelligent. The associated scientific fields have evolved without an underlying theory, driven primarily by empirical and incremental research. This has hindered progress in many ways. The future requires a different approach. Mathesis (Hellenic Μάθησις, “learning”) is an interdisciplinary theory of learning and intelligence that combines natural and formal science, including biology, neuroscience, computer science, engineering and various branches of mathematics. It attempts to unify all of learning under a common framework and provide the missing theoretical basis that can drive research and applications. The theory has been developed over multiple decades and demonstrates the importance of rigor and perseverance, as well as the potency of interdisciplinarity over complexity. Synthesis and coherence are two of the theory’s foundational principles which underpin the larger framework. The theory starts with a simplified definition of learning as optimization and a formal definition of neural networks. It derives axiomatic principles from biology and neuroscience, emphasizes the role of synthesis and its consequences, introduces the concept of coherence, and applies it to learning. Mathesis defines neural networks as compositions of simpler neural structures that may receive input from multiple sources through cross-connections and aggregating functions. The theory discusses locality and receptive fields, provides a critique of convolutional neural networks, and examines their advantages, drawbacks and flaws. The theory initially treats learning as optimization, which is a dramatic but necessary simplification for the gradual expansion of the theory. It considers existing learning paradigms and discusses the computational limitations of learning, as well as its inherently local – and thus coherent – nature. Mathesis considers biology to be imperative for learning and intelligence and borrows biological principles to construct a partial axiomatic basis. It derives an analogous learning theory from cell theory and proposes energy, heredity, homeostasis and evolution as essential concepts and processes in learning systems. It demonstrates the thermodynamic inefficiency of von Neumann architectures (which are fundamentally incompatible with neural networks), and sets the stage for a new computing architecture. Cell theory and our axiomatization imply that learning and intelligence constitute a large-scale synthesis of simpler elements. Synthesis requires recursive structure, synergy, state and the convenience of a neural algebra. It thrives in diversity and leads to the synergy conjecture that divides the space of interactions into three main regions: detachment, synergy and mimesis. Coherence represents the theoretical core of Mathesis and is one of its most fundamental contributions. Coherence underlies all learning phenomena, unifies them under a common framework, explains their structure in a fashion similar to comparative anatomy, and predicts or explains various empirical results and observations in machine learning. It also provides a foundation for the creation of more sophisticated methods, neural structure and architectures. Most of Mathesis is effectively a transition from the current empirical and almost random state-of-the-art into coherent entities. This includes coherent functions, coherent learning calculi, coherent structure, coherent plasticity and growth, and coherent evolution. Coherence provides a scientific definition for generalization in the same way that evaporation and condensation provide a scientific explanation for rain. Locality is shown to be a special case of coherence. The same applies to regularization. Virtually everything in learning relates to coherence because learning is coherence. Common operators can be used to manipulate coherence and define coherent learning methods, such as synaptic, neural or dendritic coherence. Those are applied to parameters, their values, gradients and updates. Most known learning algorithms are special cases of synaptic coherence. Mathesis introduces new local neural structures such as flowers, sunflowers, corollas and striders. Based on the theory, we combine such structures into a larger synergetic model, train them with synaptic coherence, and measure the error rates on mnist and imagenet. The network has achieved a new record on mnist. Our experiments on imagenet are still in an early stage but they have produced some modest gains so far. We will update the evaluation as more results become available. Our next step is to apply Mathesis to eBay data on various domains, starting with computer vision and machine translation, and then progressing to search and search-related problems. The overall synthesis (elements, structure, synergy, diversity), learning parameters and methods that we have used in our experiments are not optimal and may evolve significantly in the coming months and years. The theory is not meant to replace evolution, but rather to initiate, fuel and drive it. As the technological landscape continues to change in both scope and complexity, Mathesis will provide the necessary evolutionary framework.", "date": "2021-01-25"},
{"website": "Ebay-Research", "title": "Finding Desirable Items in eBay Search by a Deep Dive into Skipped Items", "author": ["Ishita Khan"], "link": "https://tech.ebayinc.com/research/titledemand/", "abstract": "When you search on eBay and there are many matching listings, how does eBay figure out which ones to rank at the top? One key ingredient is to determine how well the listing matches the intent of the query. If a user is looking for a “kitchenaid mixer,” results titled “kitchenaid artisan series stand mixer” would be closer to the query intent than those for “kitchenaid mixer dough hook.” However, for the query “kitchenaid mixer attachment,” the latter result would be more desirable. Just as the example illustrates, effectively ranking relevant items in the search result page is one of the core challenges in e-commerce. At eBay, where people can list and describe their items almost any way they want, we have an even more interesting problem to solve. eBay uses machine learning to incorporate many signals into its core ranking algorithm. One of the most important signals is the intent of the item title. In this article, we present a novel approach to predict the intent of item titles in the context of an e-commerce query by looking at the skipped items for a query, which is subsequently used as a feature for the item ranking. Given a user-issued query and an item title, the problem we address in this article is to assign a score to the title that reflects how well the intent of the query matches the title of the item in the result set. Historically, this was done using two features in eBay ranking. The first is a well-known probabilistic relevance model used as a ranking function called BM25, which applies to any query and is based purely on how many tokens (words) are in common between the query and the title. In more detail, it uses TF-IDF, so the BM25 formula uses the frequency of tokens in our inventory. The second feature is a Naïve Bayes method called \"titleRel,\" which is based on behavioral data (clicks). In this work, our goal is to develop a model to replace the Naïve titleRel method, addressing its intrinsic challenges. Contrary to the Naïve approach, we developed a custom parameterized model of user behavior to learn why users skip a result. We leverage behavioral data to find the parameters of this Item Skip Likelihood Estimation (ISLE) model, which is consequently used to learn title intent in the context of a query. Compared to the Naïve titleRel baseline, the new ISLE model shows significantly better accuracy in offline evaluation metrics and lift in A/B test, and is currently in use in eBay in all markets. We have also published this work as an industry track full paper in the 2017 IEEE International Conference on Big Data: What is skipped: Finding desirable items in e-commerce search by discovering the worst title tokens . Since the reasons item titles are skipped vary widely across queries, we build a separate ISLE model for each query. The baseline titleRel had two steps, one offline and the other online. Offline, it examined clicks made on a set of search result pages (SRPs) for the query, and used the clicks to compute the probability that a token is clicked when it appears in an item title. Each token is assigned a probability. At run-time, it takes a title and sums the log of the probabilities of the tokens in the title to get a score for title. The offline computation is done separately for each query, so the token probabilities are query specific. The final title score is the feature. There are some refinements, but that is the basic idea. In summary, the score of a title is the sum of the scores of the tokens, and the score of a token is the log of its probability. The new ISLE model also computes the score of a title using an aggregation function to combine the scores of the tokens, but it mainly differs from titleRel in the way it computes the score of a token. It doesn't use log of the probability of a click, where the probability is directly observed from training data. Instead, it posits that the probability that a user skips over a title on a SRP depends only on the worst token in the title. When searching for a KitchenAid Mixer, it assumes that a title containing the token \"attachment\" is just as bad as one that contains \"attachment\" and \"broken.\" The ISLE score of a token is computed using maximum likelihood. Each token has a probability of being skipped. These probabilities are the parameters to be determined. This model is based on a simple yet strong assumption that the probability of skipping an item title is the same as the probability of skipping its worst token. Again using an example, consider the query \"iphone 7,\" and titles \"apple iphone 7 16gb white \" and \"iphone 7 verizon 16gb screen cracked.\" Although both the results are iPhone 7 phones, the token \"cracked\" in the second title makes it less desirable for the query. In other words, \"cracked\" uniquely stands out as the deciding factor for the relatively higher skip likelihood of the item. Concretely, if a title consists of tokens w 1 , w 2 , .., w M then the probability of skipping the title is: \\begin{equation*} max_{j=1}^{M}Pr(w_j) \\end{equation*} where Pr(w j ) is the parameter to be learned, the probability of skipping the token w j . Then a training set of titles is assembled from historical behavioral data, containing titles that were skipped and not skipped, and the expression for the probability of seeing this training set becomes: \\begin{equation*} { \\prod_{i=1}^{N} (max_j\\lambda_j)^{1-\\delta_i}(1-max_j\\lambda_j)^{\\delta_i} } \\end{equation*} If the total set of tokens used in titles for this query is { w 1 , w 2 , .., w K }, then $\\lambda$ j = Pr(w j ) are the parameters to be learned. For a given query $q$, say $(T_i, \\delta_i)$ is the training set for the $i^{th}$ title, $\\delta_i$ is 1 if the title was clicked and 0 if it was skipped. $N$ is the number of titles for a query. In this data set accumulated over weeks of behavioral data, the same titles are impressed multiple times for a given query. Consequently, the $(T_i, \\delta_i)$ signals appearing multiple times for a query is captured by numSkips and numClicks parameters in the equations below: \\begin{equation}\\label{eq:3} { L = \\prod_{i=1}^{N} (max_j\\lambda_j)^{numSkips_i}(1-max_j\\lambda_j)^{numClicks_i} } \\end{equation} This computation is done using maximum likelihood , where the token skip probabilities are the parameters of the likelihood function that are learned in the optimization process. These token skip probabilities are then converted to click probabilities (1 - Pr(w j ) ) which are stored in a fast lookup table as token weights to be used in the online phase, saving significant online computational time. When a user issues a query, the ISLE model looks up the relevant token weights for that query from the lookup table and uses an aggregation function to compute a score for each item title in the recall set, which is structurally similar to titleRel model's online step where titleRel aggregates by summing. This item desirability score is further normalized using model score distribution from a second lookup table, and the normalized score is used as a feature for the item ranking algorithm. The overall architecture of the ISLE model is shown in the figure above. The Token Weight Computation block shows the offline query-specific token weight computation. Here the ISLE model computes the token weights and also computes the summary statistics to be used for normalization. These are stored in the ISLE Token Scores and ISLE Score Distribution lookup tables. The Title Score Computation block depicts online title score computation leveraging data in the look-up tables. The offline token weight generation itself happens in three phases: the first phase is sampling of user behavioral signals for queries issued over a 6-week time frame. Next, in the modeling phase, token weights are learned by looking at the probabilities of users skipping titles containing the tokens. The third phase is score distribution computation, where distribution statistics of the title scores leveraging this model are computed. This information is used for normalizing title scores when they are used as a feature in the item ranker. In the online title scores computation, the token weights learned during offline modeling are looked up and an aggregation function is applied on these weights to obtain a score for each item for the query. This is followed by a normalization step using the query-specific score distribution statistics computed offline. This score for each item is used as a feature in the item ranking model. The ISLE model described so far uses clicks and skips from user behavioral data to learn token weights. In e-commerce, sale and sale-related behavioral signals are readily available and can be used to augment the click-only model. These signals, while sparser than clicks, are cleaner and better correlate with intent. While we use several user actions that indicate purchase intent at eBay, for illustration purposes we'll consider the add-to-cart and buy events along with clicks to model the token skip probabilities. The augmented click-sale model, modifies the likelihood equation as follows: \\begin{equation}\\label{eq:5} { L' = \\prod_{i=1}^{N} (max_j\\lambda_j)^{numSkips_i}(1-max_j\\lambda_j)^{numAugClicks_i} } \\end{equation} where $numAugClicks^i$ is the weighted counts of clicks and the sale signals in the augmented model (here, $numAugClicks_i$ = $numBuy_i$ X $w_{buy}$ + $numAddToCart_i$ X $w_{AddToCart}$ + $numClicks_i$ X $w_{click}$ ). These weights for each signal are added as additional parameters along with the token skip probabilities for the ISLE model to optimize. Note that since the ISLE model is query-specific, these signal weights are also learned per query. The gradient of log likelihood with respect to $\\lambda$ now becomes: \\begin{multline}\\label{eq:6} \\frac{d(\\log L')}{d\\lambda_j} = \\\\ ~~~~~~~~\\begin{cases} \\sum_{i=1}^{N} \\frac{numSkips^i}{\\lambda_j} - \\frac{numAugClicks^i}{1-\\lambda_j},& \\text{if $\\lambda_j$ is max_j$\\lambda_j$ } \\\\ 0, & \\text{otherwise} \\end{cases} \\end{multline} And the gradient of log likelihood with regard to the sale weights is computed as the following (for buy ): \\begin{equation}\\label{eq:7} \\frac{d(\\log L')}{dw_{buy}} = \\sum_{i=1}^{N} numBuy_i * log(1 - max_j\\lambda_j) \\end{equation} Similar to the click-only ISLE model, the augmented click-sale model described here maximizes the augmented log likelihood function ( L' ) and upon convergence, learns token skip probabilities, where the weights for each of the sale signals are also optimized. The ISLE model is a feature used in our overall ranking algorithm for item search. Accordingly, we evaluated our model against the baseline Naïve Bayes model in three stages: offline evaluation in a synthetic ranking task where both the baseline titleRel and ISLE were used as stand-alone rankers (metrics used here were AUC, NDCG, Average Precision and F-Measure), both titleRel and ISLE as a feature for the overall item ranking algorithm (metric used here was validation error of the overall ranking algorithm), and finally both crowd-sourced human judgment evaluation and A/B testing with live traffic. From offline evaluation of the ISLE model as a standalone ranker, we compare the ISLE model against the titleRel baseline on a ranking task with a data set of 125K randomly selected queries and between 4-10 items for each query of which at least 1 item is relevant. The query item pairs used in this task are human judged with binary relevance labels (1 for relevant and 0 for irrelevant). The relevance labels of the ranked items are compared to evaluate our new model using average-precision@k, f-measure@k, ndcg@k, and auc@k, where the rank k is 5. We observed statistically significant improvement along AUC, NDCG, Average Precision and F-Measure as shown in the figures below (bootstrap confidence intervals shown for each method): Here we compare a click-only model (C-ISLE), clicks augmented with sale signals (CS-ISLE) using a weighted (weights empirically determined) combination of clicks and other sale signals, and a weighted combination of clicks and sales where the weights are also learned in the model (OCS-ISLE). It is evident from the plots that all ISLE models outperform the baseline. While not statistically significant, augmenting the model with an optimized weighted combination of sale based engagement signals (OCS-ISLE) is observed to have higher model performance. This was also observed in the next experiment using ISLE as a feature for the overall item ranking model. The sale-based signals, though sparser than clicks, are much cleaner and stronger for predicting relevance and demand. All three variants of our model are significantly better than the baseline feature. We also achieved 1.73% reduction in validation error in the overall ranker with ISLE as one of the top three features of the ranking algorithm, as shown in the figure below: Finally, we observed significant search relevance improvements in US (0.31%), UK (0.27%) and DE (0.35%) through human judgment-based evaluation. Evaluating the enhanced ranker (with ISLE) through A/B tests, we saw neutral GMB in US and UK, and a significant GMB improvement in DE (0.75%). We built our ISLE model offline on Hadoop using Scala and Spark, which generates models for ~8M queries every week in under 1.5 hours. The reported performance is based on eBay’s commercial search engine serving millions of queries each day. Based on these improvements of the ISLE model over the Naïve Bayes baseline used previously, the ISLE model is currently in use in eBay search for all markets.", "date": "2018-03-23"},
{"website": "Ebay-Research", "title": "The Design of A/B Tests in an Online Marketplace", "author": ["Jason (Xiao)  Wang"], "link": "https://tech.ebayinc.com/research/the-design-of-a-b-tests-in-an-online-marketplace/", "abstract": "A/B testing is at the heart of data-driven decision making at eBay when launching product features to our site. However, the tests must be designed to carefully manage the interaction between the test and control groups. Typically, when developing a test, a part of the live traffic is randomly selected to receive a new feature (test) while another part is given the product status quo (control). The randomization is based on a cookie ID or user ID (provided when the customer logs in). It is assumed that customers’ behavior is independent of each other (not affected by other people’s treatment), so that we can extrapolate from the observed difference test vs. control regarding the effect when launching to all customers. The independence assumption however doesn’t always hold, given the complex dynamics inside our marketplace. In one scenario, the experiment is a new ranking algorithm for auction items that introduces factors like predicted item quality, in addition to the default of time-ending-soonest. Imagine that the algorithm does indeed do a better job and that people in the test engage more with the auction. However, a surprisingly large amount of credit will be claimed by the control group. The reason is that, by the time an auction is close to ending, it is easier to get prominent impressions in control and to convert. The observed lift of auction conversions therefore doesn’t provide a good estimation to the actual impact if we launch the algorithm to site. Another scenario where we face the challenge of test-control interaction is with customer-based randomization. The experiment is about offering price guidance to sellers so that they don’t list their items at an unrealistically high price, which reduces the chance that the item will sell. The hypothesis, in this case, is that sellers will take the advice, and eBay overall will sell enough additional items so that the total GMV ( gross merchandise value ) will at least be the same. A natural design is to randomize by seller ID, i.e. some sellers are placed in the test receiving price guidance while others are in the control receiving no guidance. The experiment can be run for weeks, then the amount sold by the test group is compared to the amount sold by the control group. There are at least two possibilities of interaction with this way of testing: Buyers will shift their purchases from control to test to purchase at a lower price. The experiment will show test selling more than control, but there is no net sales gain for eBay ( cannibalization ). Sellers in the control group monitor price changes of sellers in the test group, if they are competitors. The effect of price guidance will spill over from test to control. Martin Saveski, et al. 1 discussed the violation of the independency assumption in social networks and proposed cluster-based randomization. We have a different context and strategy to mitigate test-control interaction. Buyers often shift purchases from seller to seller, but they are less likely to substitute the intention of purchasing one type of merchandise with another type, e.g. replacing a cellphone with shoes. At the same time, cellphone sellers may be competing with each other, but they won’t care how shoe sellers manage their inventories. So instead of randomizing on the seller, the idea is to randomize by the type of merchandise so as to control both cannibalization and spillover. eBay has various ways to define merchandise types, and we chose the leaf category of listed items for randomization. Examples include Cases Covers & Skins , Men's Shoes:Athletic , and Gold:Coins . If a definition has too fine a granularity, it won't help much to control the interactions (e.g. people easily switch from buying an iPhone 8 to buying an iPhone 8 Plus). On the contrary, too coarse a definition, e.g. Electronics , Fashion, and Collectibles , may diminish the sample size so severely that the experiment becomes too insensitive and even useless. The leaf category provides a reasonable balance. There are total about 10,000 leaf categories on the eBay site. Some of them have very sparse sales, for example Collectibles:Rocks Fossils & Minerals:Fossils:Reproductions . Even if we do an even split of 50% vs 50%, the test and control groups will each only have a few thousand effective samples. Moreover, these samples are often incomparable with regards to GMV or BI ( bought item ). Taking the category DVDs & Movies:DVDs & Blu-ray Discs as an example, it sells nearly twice the number of items as the next most sold category. If an experiment is to measure BI and the Discs category is assigned to test by chance, then there is a valid concern of selection bias. Mathematically let $i$ be the index of a category and $x_i$ be the metric of interest during experiment, the *treatment effect* is measured as $$mean(\\{x_i \\thinspace : i \\in test\\})-mean(\\{x_i \\thinspace : i \\in control\\})$$ The concern is that it will largely reflect the inherent variation among selected categories rather than the actual price guidance effect. Difference in differences is a technique commonly used for a remedy. Let $x.pre_i$ be the value of the metric for a period before the experiment, $x_i-x.pre_i$ is the change over time for category $i$. If price guidance works, we expect on average bigger change in test, so the treatment effect can be measured instead as $$mean(\\{x_i-x.pre_i \\thinspace : i \\in test\\})-mean(\\{x_i-x.pre_i \\thinspace : i \\in control\\})$$ While the idea of using pre-experiment data for variance reduction is intuitive, a better way than difference in differences is to use post-stratification . Thinking of pre-experiment characteristics as covariates, Alex Deng and others 2 studied both building regression models as well as constructing strata and established their connection. In our implementation, the data is fit into a linear regression model $$x_i = a + b*I_i + c*x.pre_i$$ where $I_i$ is the indicator variable which equals $1$ if category $i$ falls in test and $0$ in control. Coefficient $b$ is then the treatment effect: when category switches from control to test, the expectation of $x_i$ increases by $b$. Recall that we put forward category-based randomization, but are worried that the small number of categories and their large variation will make the estimate of treatment effect noisy. We plot the daily BI of test categories vs. control categories below. Notice that the green line is consistently higher than the red, and it is exactly because categories like DVDs & Movies:DVDs & Blu-ray Discs are randomized into the test group. The experiment was started on Feb. 1, but due to a logging glitch, data is missing during Feb. 1-14. The missing data is not relevant to our study. The following graph shows the pre-experiment period Jan. 9-31 and the experiment period Feb. 15-March 22. We are interested in not just the testing result, but also comparing the different analysis methods. For that purpose, we do A/A simulation where categories are randomly divided into test and control with no regard to actual treatment and all three equations are computed. The simulation was run multiple times. Since there is no treatment effect, we expect and do see the mean of simulated lift ( treatment effect divided by control mean) is close to 0. Its standard deviation, however, provides a measure of the noise level. As clearly shown in the above table, it is vastly better to leverage pre-experiment data than directly comparing test and control, yielding over a 10X reduction in the standard deviation. Post-stratification provides a further 12% reduction over difference in differences . In summary, we discussed the issue of test-control interaction when conducting A/B tests in an online marketplace and proposed category-based randomization rather than the traditional user-based strategy. To cope with the selection bias, we advocate for leveraging pre-experiment data as covariates in a regression model. Together it gives us a better design to detect the treatment effect with trust and sensitivity. 1 Martin Saveski, Jean Pouget-Abadie, Guillaume Saint-Jacques, Weitao Duan, Souvik Ghosh, Ya Xu and Edoardo M. Airoldi, 2017, Detecting Network Effects: Randomizing Over Randomized Experiments, in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , p1027-1035 2 Alex Deng, Ya Xu, Ron Kohavi and Toby Walker, 2013, Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data, in Proceedings of the sixth ACM international conference on Web search and data mining , p123-132 Leadership for the development of this method provided by Pauline Burke and Dave Bhoite.", "date": "2018-05-07"},
{"website": "Ebay-Research", "title": "GUI Testing Powered by Deep Learning", "author": ["Yotam Sharan", "Honghao Wang", "Sovan Rath"], "link": "https://tech.ebayinc.com/research/gui-testing-powered-by-deep-learning/", "abstract": "Deep Learning (DL) is revolutionizing the face of many industries these days, such as computer vision, natural language processing, and machine translation, and it penetrates many science-driven products and technological companies, including eBay. These days, DL is taking its first strides in eBay’s Quality Engineering (QE) space, and it has already proven to outperform the best test veteran and industry-grade applications one could find. Current methods of Graphical User Interface (GUI) testing gravitate between Functional Testing (focusing on a system’s external behavior or its elements) to Structural Testing (focusing on internal implementation). These methods are susceptible to changes and usually involve extensive automation efforts. Cross-screen testing, like in the case of desktop Web and mobile Web or mobile App testing, accentuates these risks and costs. Testing across multiple operating systems, devices, screen resolutions, and browser versions quickly becomes a huge challenge that is difficult to execute and govern. Quality risk-control measures, such as coverage-based or usage-based testing, address some of these uncertainties, but only to a certain degree, as it comes at a cost to the overall quality. Testing methodologies of web interfaces are mostly browser-dependent, while mobile app interfaces are platform-dependent, where the GUI and its detailed implementation are validated with test applications that hold interactive capabilities with the GUI under test. These tools may be Selenium WebDriver for testing HTML-based web pages and Espresso Test Framework for testing View-based Android Applications. While product developers wrap-up GUI implementation, quality engineers begin breaking down the screen to its elements, identifying locators for each UI components and writing up large pieces of their code around asserting the elements’ aspects, such as dimension, position, and color, to make sure the GUI implementation matches the design. Even a slight design change or refactoring of product code could end up failing the regression suites and may involve significant re-work for QE to fix the automaton code. Some testing tools, like the ones mentioned above, call for a developer skill set and an intimate knowhow of the hosting platforms. Such prerequisites introduce a technical-proficiency dependency and compels the QE to master multiple test applications, frameworks ,and operating systems, such as TestNG, Selenium, Appium, IOS Driver, and Selandroid, etc. As a result, writing and maintaining test suites and scripts for multiple platforms take considerable time and effort and come at the risk of reducing the test scope. Contemporary developments in DL unleashes efficiencies in GUI testing and in the software lifecycle, potentially. A recent pilot, described below, proved this approach to be realistic and practical. DL simulates the human way of finding errors or anomalies. Humans are driven by past experience and conditioning to make decisions. Machines with the proper application of training or conditioning can detect errors that surpass human precision. We begin our understanding of DL as the subset of a broader class called as the supervised machine learning algorithm. The supervised learning algorithms take a set of training examples called as the training data. The learning algorithm is provided with the training data to learn a desired function. Further, we also validate our learning algorithm by a set of test data. This process of learning from training data and validating against test data is called modeling. Neural Nets (NN) A NN is a group of logically connected entities that transfer a signal from one end to another. Similar to brain cells or neurons that are connected to enable the human brain to achieve its cognitive powers, these logically connected entities are called perceptrons, which allow signals to move across the network to do formidable computations, for example, differentiating a lily from a lotus or understanding the different signals in the traffic. These operations become possible when we expose our NN to a significant amount of data. A deep neural net (DNN) is an addition of multiple layers arranged in the order shown in Fig 2. This mathematical lattice is the core structure that drives autonomous vehicles and other inventive applications, such as eBay’s image search. Fig. 2. NN example, 2 layers deep DL can be utilized to contribute to the efficiency of GUI testing and reduce the churn associated with this work. Process Fig. 3 Process outline The suggested methodology begins with capturing the entire webpage as an image (see Fig. 3). This image is then divided into multiple UX components. The division of UX components or groups of components helps in generating training and test data to feed our model. Once our model is ready, we can test any new UX component across browsers, resolutions, and additional test dimensions by feeding the image of the UX components for the desired specification to the model. Our model would classify whether our test UX component passes the desired quality criteria or not. This process of deciding the particular images into one of the classes (passing or failing the quality criteria) is called Classification. Training data and test data creation We create the training and test data by automated modification of UX components taken from the webpage wireframes. Based on the design guidelines and the test variations, we introduce potential flaws in direct correlation to the design input. These flawed design mockups are manifested as images. Proper labeling of these images ensure proper organization of test data. Once we have a minimal set of images in our arsenal, we are ready to train our model. Modeling Based on the training data and the complexity of our scenarios, different models such as Convolutional Neural Nets (CNNs), Support Vector machines (SVMs) or Random Forests (RFs) can be chosen. Once the model is decided, we can train our model to capture GUI defects. Pursuing the above-mentioned procedure and steps, we implemented our own process for one of the new home page modules called “Popular Destination.” Using the mockups created by the Design team, we generated 10,000 images that included different defects; we have introduced intentional design flaws by modifying images, texts, and layout to simulate the real world scenarios and issues. The following were some of the examples we used for emulating the defects. 1.  Missing images 2.  Layout issues The system provides a classification score between 0 to 1. A score closer to 0 should signify a model prediction of a potential test-case failure, which may imply a certain GUI imperfection was detected by our model. A score closer to 1 could signify a prediction of a test-case that meets its quality criteria. In such a case, we intend to establish a cutoff threshold. A cutoff threshold determines a value below which signifies that the module is having a potential GUI defect. This cutoff varies for different modules. Based on our model, we were able to capture defects with a 97% accuracy. During the process of testing, we were successful to find real production bugs. For example, we captured the UX component with an Internet Explorer 11 browser and found the production issue below, where thin lines appear across circular images in the Popular Destination module against this specific browser version. Automation testing would have never captured it and manual testers would probably need Steve Austin’s bionic eye and a whole lot of time and patience to even notice this artifact in the vast continuum of their test matrix. Fig. 6: Production bug for Popular Destination in IE 11 Key learnings and benefits Our learned lessons and insights came from testing our GUI in eBay’s top two domains: Homepage (HP) team and Advertising (Ads). Both team wanted to have a test tool and methodology that would enable them to conduct Ads testing using new approaches and tools that differ from their traditional validation and verification applications. Traditional approaches and tools come at a high cost to the individual engineer. Ramping up on some test applications can take more than a week and proficiency comes with much longer periods of time. ML calls for a different developer skill set, which deprecates the need to master a great deal of traditional validation and verification techniques and tools, such as Selenium WebDriver or iOS and Android drivers. Traditional approaches and tools come at a high cost to the individual engineer. Ramping up on some test applications can take more than a week and proficiency comes with much longer periods of time. ML calls for a different developer skill set, which deprecates the need to master a great deal of traditional validation and verification techniques and tools, such as Selenium WebDriver or iOS and Android drivers. The new approach eliminated the need for a deep and intimate domain knowledge. A new eBay intern was able to ramp up in a matter of a day or two and start generating test data when training a ML model. Previously, some QE teams would require a few weeks of daily work in order to become familiar with the domain’s specifics and the intricate knowledge of our webpages. The new approach eliminated the need for a deep and intimate domain knowledge. A new eBay intern was able to ramp up in a matter of a day or two and start generating test data when training a ML model. Previously, some QE teams would require a few weeks of daily work in order to become familiar with the domain’s specifics and the intricate knowledge of our webpages. The QE teams witnessed a quick set-up time for ML-based testing when a single engineer was able to prepare test automation to run against their main UX components in a matter of day or two. Usually, the teams invest multiple weeks to achieve such test coverage. The QE teams witnessed a quick set-up time for ML-based testing when a single engineer was able to prepare test automation to run against their main UX components in a matter of day or two. Usually, the teams invest multiple weeks to achieve such test coverage. Our experiment kicked-off the quality assurance process early-on, using design mockups. Training our model with these wireframes allowed us to begin the QE work potentially before substantial development phase even started. Now, implementation details are becoming irrelevant for such QE process. Our experiment kicked-off the quality assurance process early-on, using design mockups. Training our model with these wireframes allowed us to begin the QE work potentially before substantial development phase even started. Now, implementation details are becoming irrelevant for such QE process. Some findings were particularly prominent when it became clear that the defects detected by the model would have been practically impossible to capture by any other means of manual or automated testing. Some findings were particularly prominent when it became clear that the defects detected by the model would have been practically impossible to capture by any other means of manual or automated testing. The model produced a classification score per asserted output. Such results allowed the QE to focus their attention on GUI artifacts with the highest probability of having a fault. The model produced a classification score per asserted output. Such results allowed the QE to focus their attention on GUI artifacts with the highest probability of having a fault. Maintaining test suites and scripts for several platforms take considerable amount of time and effort. It comes at high risk of reducing test scope, when time is of essence. Even a slight design change or refactoring of product code could end up failing the regression suites and may involve significant re-work for QE to fix the automaton code. Our ML process became agnostic to implementation details and less sensitive to the platforms it runs on. Maintaining test suites and scripts for several platforms take considerable amount of time and effort. It comes at high risk of reducing test scope, when time is of essence. Even a slight design change or refactoring of product code could end up failing the regression suites and may involve significant re-work for QE to fix the automaton code. Our ML process became agnostic to implementation details and less sensitive to the platforms it runs on. Teams were excited about the use of innovative techniques and unleashing its potential. It inspired engineers to hone their skills and learn new tools and approaches. Teams were excited about the use of innovative techniques and unleashing its potential. It inspired engineers to hone their skills and learn new tools and approaches. In addition to the benefits listed above, we are also on the hunt for adopting other useful ML-based approaches, enhancements, and optimized processes. Some of the ideas described below are work-in-progress and some are exciting future concepts we are toying with. Attribute-based assertion ML can be applied to detect abstract components, such as images/shapes/text, and extract those components and their respective positions. When focusing on a narrower perspective of ROI (Region Of Interest) we can build an attribute-based assertion to compare with the wireframes. For example, layout-related assertion, such as image position extracted from our ML model, can be checked against predefined characteristics. Such approach would provide a more granular validation mechanism. This work is beneficial for scrutinizing specific elements or areas of elements, contrasted against certain predetermined anchored attributes. Feedback-adapting models As a next step to raise the accuracy of the system, one may train the model by integrating it with new or existing processes. Bug reports can be re-used to teach the model what is a real bug and what is not. Say an issue was closed with a fixed status, this may trigger a learning opportunity for our model and add this data set into its logic. In the case of an issues being closed as “not a bug,” then the system could automatically learn how a misleading issue may look like. When a classification score is assigned with in an inconclusive range (say, a range of 0.4 to 0.7, where it is not certain whether the quality criteria was passing or failing) then human judgment should be applied and fed back into the training process. Packaging ML modalities and creating an open-source service This testing methodology is browser, resolution, and even language agnostic if we use certain OCR libraries, therefore we need not train our models against different browsers, resolutions, or localization constraints. Further enhancements could be done by packing Accessibility and Application Security libraries. Captions and tabbing can be run against Accessibility requirements (by WCAG2.0, for example) as part of the ML modalities. Enhancing our current system into a full-scale service solution would let the service take our training images with other hyper parameters, such as the module name and module parameters, into consideration. In turn, this would allow us to create a full-fledged real-time solution for entire webpages, where each module would be a test case and a collection of modules would be called as a test suite. Test suites would be available for each webpage. The work above could be open-sourced for communities benefit by sharing ideas and libraries that enhance and extend the ML modalities. Such real-time service could become widely available for common use. Enhancements to Software Design Verification DL-powered GUI testing could be expanded into additional testing fields in software design verification techniques, where the machine develops an understanding about the relationships between businesses and people. Software design subjects, such as visual functionality, usability, accessibility, and others, could be captured by the same ML paradigm when enhanced with additional modalities. Some software functionality matters may manifest in a graphic manner, like in cases of slow-loading ads or poor implementation of user-control widgets. Usability has become an increasingly important factor, when Apple’s Human Interface Guidelines or the Android User Interface Guidelines may accept or reject certain an application’s availability in their stores. While User Experience Research (UER) is limited by time, resources, and quality of human feedback, DL can compute the highest score for effective Usable Designs, cutting down time and costs. Multimedia Accessibility Testing (especially for complying with Web Content Accessibility Guidelines 2.0) might be done by the use of recurrent neural networks (RNNs), which holds the power to understand natural language and extract a relationship between UX components and their descriptions provided by the developers. The current days of manual and automated GUI testing gradually become ineffective when contrasted with an ML-based solution. By following the recipe explained above, both stakeholders and engineering teams who deal with UX components would gain from one of the latest technology stacks the world has to offer. By applying this DL-fueled, human-like expertise on prevalent alternatives, we can now finally scale the existing labor-intensive and skill-expensive methodologies.", "date": "2018-06-28"},
{"website": "Ebay-Research", "title": "Explainable Reasoning over Knowledge Graphs for Recommendation", "author": ["Dingxian Wang", "Canran Xu", "Hua Yang", "Xiaoyuan Wu"], "link": "https://tech.ebayinc.com/research/explainable-reasoning-over-knowledge-graphs-for-recommendation/", "abstract": "Incorporating knowledge graphs into recommender systems has attracted increasing attention in recent years. By exploring the interlinks within a knowledge graph, the connectivity between users and items can be discovered as paths, which provide rich and complementary information to user-item interactions. Such connectivity not only reveals the semantics of entities and relations, but also helps to comprehend a user’s interest. However, existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path. We have developed a new model named Knowledge-aware Path Recurrent Network (KPRN) to exploit knowledge graphs for recommendation. Our new model, Knowledge-aware Path Recurrent Network (KPRN), can generate path representations by composing the semantics of both entities and relations. By leveraging the sequential dependencies within a path, we allow effective reasoning on paths to infer the underlying rationale of a user-item interaction. Furthermore, we design a new weighted pooling operation to discriminate the strengths of different paths in connecting a user with an item, endowing our model with a certain level of explainability. We conduct extensive experiments on two datasets about movie and music, demonstrating significant improvements over state-of-the-art solutions, Collaborative Knowledge Base Embedding and Neural Factorization Machine. Incorporating knowledge graphs (KGs) into recommender systems has attracted increasing attention in recent years. By exploring the interlinks within a knowledge graph, the connectivity between users and items can be discovered as paths, which provide rich and complementary information to user-item interactions. Such connectivity not only reveals the semantics of entities and relations, but also helps to comprehend a user's interest. However, existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path. In this article, we summarize our paper “ Explainable Reasoning over Knowledge Graphs for Recommendation ,” accepted by the AAAI 2019 conference. We contribute KPRN to exploit knowledge graphs for recommendation. KPRN can generate path representations by composing the semantics of both entities and relations. By leveraging the sequential dependencies within a path, we allow effective reasoning on paths to infer the underlying rationale of a user-item interaction. Furthermore, we design a new weighted pooling operation to discriminate the strengths of different paths in connecting a user with an item, endowing our model with a certain level of explainability. We conduct extensive experiments on two datasets about movies and music, demonstrating significant improvements over state-of-the-art solutions, Collaborative Knowledge Base Embedding and Neural Factorization Machine. Figure 1. Illustration of a KG-aware recommendation in the music domain. Extra user-item connectivity information derived from a Knowledge Graph endows recommender systems the abilities to reason and explain. Taking music recommendations as an example (Figure.1), a user is connected to \"I See Fire,\" since she likes the song \"Shape of You\" sung by the same singer Ed Sheeran. Such connectivity helps to reason about unseen user-item interactions (i.e. a potential recommendation) by synthesizing information from paths. Running Example: {Alice, Interact, Shape of You}^{Shape of You, SungBy, Ed Sheeran}^{Ed Sheeran, IsSingerOf, I See Fire}=>{Alice, Interact, I See Fire}. Clearly, the reasoning unveils the possible user intents behind an interaction, offering explanations behind a recommendation. How to model such connectivity in KGs, hence, is of critical importance to inject knowledge into a recommender systems. Our new solution, KPRN, not only generates representations for paths by accounting for both entities and relations, but also performs reasoning based on paths to infer user preference. Specifically, we first extract qualified paths between a user-item pair from the KG, each of which consists of the related entities and relations. We then adopt a Long Short-Term Memory (LSTM) network to model the sequential dependencies of entities and relations. Thereafter, a pooling operation is performed to aggregate the representations of paths to obtain a prediction signal for the user-item pair. More importantly, the pooling operation is capable of discriminating the contributions of different paths for a prediction, which functions as the attention mechanism (Chen et al. 2017; Neelakantan, Roth, and McCallum 2015). Owing to such attentive effect, our model can offer path-wise explanations, such as why \"Castle on the Hill\" is recommended since you have listened to \"Shape of You,\" which is sung and written by Ed Sheeran. We conducted extensive experiments on two datasets to verify our method. In this section, we elaborate on our proposed method, as illustrated in Figure 2. Before introducing our proposed method, we first formally define the knowledge graph and the user-item data, and describe how to combine them in an enriched knowledge graph as the inputs of our model. Figure 2. Schematic overview of our model architecture. Background A Knowledge Graph (KG) is a directed graph whose nodes are entities ${E}$, and edges ${R}$ denote their relations. Formally, we define KG as ${KG}=\\{(h,r,t)| h,r\\in {E}, r\\in {R}\\}$, where each triplet $(h,r,t)$ indicates a fact that there is a relationship $r$ from the head entity $h$ to the tail entity $t$. The user-item interaction data is usually presented as a bipartite graph. In particular, we use ${U}=\\{u_{t}\\}_{t=1}^{M}$ and ${I}=\\{i_{t}\\}_{t=1}^{N}$ to separately denote the user set and the item set, where $M$ and $N$ are the number of users and items, respectively. Following that (Chaudhari, Azaria, and Mitchell 2016), we represent the interaction between a user and an item with a triplet $\\tau=$($u$, interact, $i$), if there is an observed interaction (e.g., rate, click, and view feedbacks), where the interaction is a pre-defined relation. We merge the item set and the entity set through string matching: ${I}\\subseteq{E}$, ${G}=\\{(h,r,t)|h,r\\in {E}',r\\in {R}'\\}$, where ${E}'={E}\\cup{U}$ and ${R}'={R}\\cup\\{\\text{interact}\\}$. For consistency, the KG in the rest of paper denotes the combined graph ${G}$, including both the original KG and user-item data, unless otherwise noted. The triplets in the KG clearly describe direct or indirect (i.e. multiple-step) relational properties of items, which shall constitute one or several paths between the given user and item pair. We explore these paths in order to achieve comprehensive reasoning and understanding for a recommendation. Within ${G}$, we formally define the path from the user $u$ to the item $i$ as a sequence of entities and relations: $p=[e_{1}\\xrightarrow{r_{1}}e_{2}\\xrightarrow{r_{2}}\\cdots\\xrightarrow{r_{L-1}}e_{L}]$, where $e_{1}=u$, $e_{L}=i$; $(e_{l},r_{l},e_{l+1})$ is the $l$-th triplet in $p$, and $L$ denotes the number of triplets in the path. We elaborate on the construction of paths in the Path Extraction section of the paper . Next, we will use a realistic example to show the sophisticated relationships (i.e. paths) between a user and an item behind their possible interactions, which inspires us to model the high-level semantics of a path compositionally by considering both entities and (multiple-step) relationships. Examples: Consider the music recommendation shown in Figure 1, where the “listen to 'Castle on the Hill'” behavior of user Alice can be referred by the following paths: $p_{1}=\\text{[Alice}\\xrightarrow{\\text{Interact}}\\text{Shape of You}\\xrightarrow{\\text{IsSongOf}}\\div\\xrightarrow{\\text{ContainSong}}\\text{Castle on the Hill]}$; $p_{2}=~\\text{[Alice}\\xrightarrow{\\text{Interact}}\\text{Shape of You}\\xrightarrow{\\text{SungBy}}\\text{Ed Sheeran}\\xrightarrow{\\text{IsSingerOf}}\\text{Castle on the Hill]}$. $p_{3}=~\\text{[Alice}\\xrightarrow{\\text{Interact}}\\text{Shape of You}\\xrightarrow{\\text{InteractedBy}}\\text{Tony}\\xrightarrow{\\text{Interact}}\\text{Castle on the Hill]}$; These paths from the same user Alice to the same item, \"Castle on the Hill,\" obviously express their different multiple-step relations, and implies various compositional semantics and possible explanations of the listen behavior. In particular, $p_{1}$ and $p_{2} $ infer that Alice may prefer songs that belonging to the album $\\div$ and the songs sung by Ed Sheeran , while $p_{3}$ reflects the collaborative filtering (CF) effect: similar users tend to have similar preferences. Therefore, from the view of reasoning, we consume the connectivity along all paths to learn compositional relation representations, and weighted pool them together for predicting the interact relationship between the user and the target item. Task Definition: Our task can be formulated as follows: given a user $u$, a target item $i$, and a set of paths ${P}(u,i)=\\{p_{1},p_{2},\\cdots,p_{K}\\}$ connecting $u$ and $i$, the holistic goal is to estimate the interaction by: $ \\hat{y}_{ui}=f_{\\Theta}(u,i|{P}(u,i)), % \\hat{y}_{ui}=\\Space{P}(\\tau|\\Set{P}), % \\hat{y}_{ui}=\\Space{P}((u,\\text{interact},i)|\\Set{P}), $ where $f$ denotes the underlying model with parameters $\\Theta$, and $\\hat{y}_{ui}$ presents the predicted score for the user-item interaction. Distinct from embedding-based methods, we can explain $\\hat{y}_{ui}$ as the plausibility score of the triplet $\\tau=(u,{interact},i)$ inferred by the connectivity ${P}(u,i)$. Modeling: KPRN takes a set of paths of each user-item pair as input and outputs a score indicating how possible the user will interact the target item. As illustrated in Figure 2, there are three key components: (1) the embedding layer to project three types of ID information: the entity, entity type, and the relation pointing to the next node into a latent space, (2) the LSTM layer that encodes the elements sequentially with the goal of capturing the compositional semantics of entities conditioned on relations, and (3) the pooling layer to combine multiple paths and output the final score of the given user interacting the target item. Embedding layer Given a path $p_{k}$, we project the type (\\eg person or movie) and specific value (\\eg Peter Jackson or The Hobbit II) of each entity into two separate embedding vectors, ${e}_{l}$$\\in$${R}^{d}$ and ${e}'_{l}\\in{R}^{d}$, where $d$ is the embedding size. In real-world scenarios, it is common that the same entity-entity pairs may have different semantics due to different relations connecting them. Such differences may reveal the diverse intents about why a user selected the item. As an example, let ({Ed Sheeran, IsSingerOf, Shape of You}) and ({Ed Sheeran, IsSongwriterOf, Shape of You}) be the triplets in two paths referring a user's preferences. Without specifying the relations, these paths will be represented as the same embeddings, regardless of the possibility that the user only prefers songs sung by Ed Sheeran, rather than that written by Ed Sheeran. We hence believe that it is important to explicitly incorporate the semantics of the relations into the path representation learning. Towards this end, each relation $r_{l}$ in $p_{k}$ is represented as an embedding vector ${r}_{l}\\in{R}^{d}$. As a result, we obtain a set of embeddings for path $p_{k}$, $[{e}_{1},{r}_{1},{e}_{2},\\cdots,{r}_{L-1},{e}_{L}]$, where each element denotes an entity or a relation. LSTM layer With the embedding sequence to describe a path, we employ LSTM models to explore the sequential information and generate a single representation for encoding its holistic semantics. Such a long-term sequential pattern is crucial to reason on paths connecting a user and item entities to estimate the confidence of the ''interact'' relation. At the path-step $l-1$, the LSTM layer outputs a hidden state vector ${h}_{l-1}$, consuming the subsequence $[e_{1},r_{1},\\cdots,e_{l-1},r_{1-1}]$. Simultaneously, we concatenate the embedding of current entity $e_{l-1}$ and relation $r_{l-1}$ as the input vector: ${x}_{l-1}={e}_{l-1}\\oplus{e}'_{l-1}\\oplus{r}_{l-1}$ where $\\oplus$ is the concatenation operation. Note that for the last entity $e_{L}$, a null relation $r_{L}$ is padded to the end of path. As such, the input vector contains not only the sequential information, but also the semantic information of the entity and its relation to the next entity. Consequently, ${h}_{l-1}$ and ${x}_{l-1}$ are used to learn the hidden state of the next path-step $l$, which could be found through LSTM . Having established the representation of path ${p}_{k}$, we aim to predict the plausibility of $\\tau=(u,\\text{interact},i)$. Towards this end, two fully-connected layers are adopted to project the final state into the predictive score for output, given by: \\begin{align}\\label{equ:path-pred} s(\\tau|{p}_{k})={{W}}_{2}\\text{ReLU}({{W}}_{1}{p}_{k}), \\end{align} where ${W}_{1}$ and ${W}_{2}$ are the coefficient weights of the first and second layers respectively. Bias vectors are omitted form simplicity, and the rectifier is adopted as the activation function. Weighted pooling layer A user-item entity pair usually has a set of paths connecting them in a KG. Let ${S}=\\{s_{1},s_{2},\\cdots,s_{K}\\}$ be the predictive scores for $K$ paths, ${P}(u,i)=\\{p_{1},p_{2},\\cdots,p_{K}\\}$, connecting a user-item pair $(u,i)$. The final prediction could be the average of the scores of all paths, which is formulated as, \\begin{align}\\label{equ:mean-pooling} \\hat{y}_{ui}=\\sigma(\\frac{1}{K}\\sum_{k=1}^{K}s_{k}). \\end{align} Nevertheless, prior studies suggest that different paths have varying contributions to model user preferences. Inspired by previous work~\\cite{reasonchain,ACF}, we design a weighted pooling operation to aggregate scores of all paths. Here the pooling function is defined as follows, \\begin{align}\\label{equ:per-path-score} g(s_{1},s_{2},\\cdots,s_{K})=\\log\\left[\\sum_{k=1}^{K}\\exp\\left(\\frac{s_{k}}{\\gamma}\\right)\\right], \\end{align} and the final prediction score is given by, \\begin{align} \\hat{y}_{ui}=\\sigma(g(s_{1},s_{2},\\cdots,s_{K})), \\end{align} where $\\gamma$ is the hyper-parameter to control each exponential weight. Such pooling is capable of distinguishing the path importance, which is attributed by the gradient: \\begin{align} \\frac{\\partial g}{\\partial s_{k}}=\\frac{\\exp(s_{k}/\\gamma)}{\\gamma \\sum_{k'}\\exp(s_{k'}/\\gamma)}, \\end{align} which is proportional to the score of each path during the back-propagation step. Moreover, the pooling function endows the final prediction more flexibility. In particular, when setting $\\gamma\\rightarrow 0$, the pooling function can degenerate to max-pooling; whereas, it can degrade to mean-pooling by setting $\\gamma\\rightarrow\\infty$. We conduct a case study on exploring the utility of the weighted pooling operation in the Case Studies section of the paper . Learning: we treat the recommender learning task as a binary classification problem, where an observed user-item interaction is assigned a target value $1$, otherwise $0$. We use the pointwise learning methods to learn the parameters of our model. In particular, the negative log-likelihood is adopted as the objective function, which is defined as follows, \\begin{align} l=-\\sum_{(u,i)\\in{O}^{+}}\\log\\hat{y}_{ui}+\\sum_{(u,j)\\in{O}^{-}}\\log(1-\\hat{y}_{uj}), \\end{align} where ${O}^{+}=\\{(u,i)|y_{ui}=1\\}$ and ${O}^{-}=\\{(u,j)|y_{uj}=0\\}$ are the positive and negative user-item interaction pairs, respectively. We conduct $L_{2}$ regularization on the trainable parameters $\\Theta$, which is omitted here for simplicity, to avoid overfitting. We performed experiments on two real-world datasets (movie item recommendation: MovieLens-1M and IMDb datasets, named MI, and music recommendation: KKBox) to evaluate our proposed method. We aimed to answer the following research questions: RQ1: Compared with the state-of-the-art KG-enhanced methods, how does our method perform? RQ1: Compared with the state-of-the-art KG-enhanced methods, how does our method perform? RQ2: How does the multi-step path modeling (\\eg the incorporation of both entity and relation types) affect KPRN? RQ2: How does the multi-step path modeling (\\eg the incorporation of both entity and relation types) affect KPRN? RQ3: Can our proposed method reason on paths to infer user preferences towards items? RQ3: Can our proposed method reason on paths to infer user preferences towards items? We process the datasets as: if a user rates a movie or has an interaction record with a song, we set the user-movie or user-song pair as the observed positive feedback with the target value of $1$, and $0$ otherwise. For each dataset, we hold out the $80\\%$ and $20\\%$ interaction history of each user randomly to construct the training and test sets. For each positive user-item interaction pair in the training set, we conducted the negative sampling strategy to pair it with four negative items that the user has not interacted with. During the test stage, the ratio between positive and negative interactions is set as $1:100$, namely, $100$ negative items are randomly sampled and pair with one positive item in the testing set. Path Extraction: In practice, it is labor intensive and infeasible to fully exploring all connected paths over the KG. Especially, the number of paths grows exponentially the length of path, where millions of interlinks will be generated. As suggested in prior efforts (Sun et al. 2011; Shu et al. 2018), truncating all paths at a certain length and disregarding remote connections are sufficient to model the connectivity between a user-item pair. Moreover, as pointed out by (Sun et al. 2011), paths with length greater than six will introduce noisy entities. Therefore, we extract all qualified paths, each with length up to six, that connect all user-item pairs. RQ1:  Our method KPRN substantially outperforms MF (Rendle et al. 2009), NFM (He and Chua 2017), CKE (Zhang et al. 2016) and FMG (Zhao et al. 2017) hit@$K$ and ndcg@$K$, achieving the best performance. RQ2: We set up another method KPRN-r without considering relations in paths. The performance of KPRN-r decreases on both datasets. This justifies our intuition that specifying different relations is of importance to capture the path semantics, especially when the same entities are involved. RQ3: The desirable property of KPRN is to reason on paths to infer the user preferences towards target items and generate reasonable explanations. This is because our model captures the higher-level semantics from these key factors: entity, entity type, and relation. To demonstrate this, we show an example drawn from KPRN on a movie recommendation task. We randomly select a user, whose ID is u4825 in MovieLens-1M, and select the movie \"Shakespeare in Love\" from her interaction record. We then extract all the qualified paths connecting the user-item pair and present the subgraph in Figure 3. We have several observations. Collaborative filtering effect plays a pivotal rule to recommend the movie \"Shakespeare in Love\" to the user, since the interaction behaviors from other users (e.g., u940 and u5448) are involved in two paths. In particular, the path containing u5448 offers the high contribution score of 0.356 to infer the user's interest. Collaborative filtering effect plays a pivotal rule to recommend the movie \"Shakespeare in Love\" to the user, since the interaction behaviors from other users (e.g., u940 and u5448) are involved in two paths. In particular, the path containing u5448 offers the high contribution score of 0.356 to infer the user's interest. The target item is connected to what u4825 has watched before {Rush Hour, Titanic, and Fantasia} by the shared knowledge entities, such as actor {Tom Wilkinson} and director{James Algar}. This shows that KPRN is capable of extending user interests along KG paths. The target item is connected to what u4825 has watched before {Rush Hour, Titanic, and Fantasia} by the shared knowledge entities, such as actor {Tom Wilkinson} and director{James Algar}. This shows that KPRN is capable of extending user interests along KG paths. Analyzing these three paths jointly, we find that different paths describe the user-item connectivity from dissimilar angles, which can be treated as the evidence why the item is suitable for the user. Specially, we can offer path-wise explanations such as {Shakespeare in Love is recommended since you have watched Rush Hour acted by the same actor Tom Wilkinson} or {since it is similar to Titanic that you watched before}. This case demonstrates KPRN's capacity of providing informative explanations. Analyzing these three paths jointly, we find that different paths describe the user-item connectivity from dissimilar angles, which can be treated as the evidence why the item is suitable for the user. Specially, we can offer path-wise explanations such as {Shakespeare in Love is recommended since you have watched Rush Hour acted by the same actor Tom Wilkinson} or {since it is similar to Titanic that you watched before}. This case demonstrates KPRN's capacity of providing informative explanations. Figure 3. Visualization of three paths with prediction scores for the user of u4825 in the MI dataset. The prediction scores are normalized for illustration. The details are discussed in our paper “ Explainable Reasoning over Knowledge Graphs for Recommendation .” In this work, we exploit knowledge graphs to construct paths as extra user-item connectivity, which is complementary to user-item interactions. We propose a knowledge-aware path recurrent network to generate representation for each path by composing semantics of entities and relations. By adopting LSTM on paths, we can capture the sequential dependencies of elements and reason on paths to infer user preference. Hopefully, this article gave you some insights into why these tips are important. Please read “ Explainable Reasoning over Knowledge Graphs for Recommendation ,” AAAI 2019 for more details. This work is supported by eBay, Search Science Shanghai Director Hua Yang, Manager Wu Xiaoyuan, and Intern Zhang Mohan. Chaudhari, S.; Azaria, A.; and Mitchell, T. M. 2016. An entity graph based recommender system. In RecSys. Chen, J.; Zhang, H.; He, X.; Nie, L.; Liu, W.; and Chua, T.-S. 2017. Attentive collaborative filtering: Multimedia recommendation with item- and component-level attention. In SIGIR, 335–344. He, X., and Chua, T. 2017. Neural factorization machines for sparse predictive analytics. In SIGIR, 355–364. McCallum, A.; Neelakantan, A.; Das, R.; and Belanger, D. 2017. Chains of reasoning over entities, relations, and text using recurrent neural networks. In EACL, 132–141. Neelakantan, A.; Roth, B.; and McCallum, A. 2015. Compositional vector space models for knowledge base completion. In ACL, 156–166. Rendle, S.; Freudenthaler, C.; Gantner, Z.; and SchmidtThieme, L. 2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI, 452–461. Shu, Z.; Yang, J.; Zhang, J.; Bozzon, A.; Huang, L.-K.; and Xu, C. 2018. Recurrent knowledge graph embedding for effective recommendation. In RecSys. Sun, Y., and Han, J. 2012. Mining heterogeneous information networks: a structural analysis approach. SIGKDD 14(2):20–28. Sun, Y.; Han, J.; Yan, X.; Yu, P. S.; and Wu, T. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB 4(11):992– 1003. Yu, X.; Ren, X.; Sun, Y.; Gu, Q.; Sturt, B.; Khandelwal, U.; Norick, B.; and Han, J. 2014. Personalized entity recommendation: a heterogeneous information network approach. In WSDM, 283–292. Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W. 2016. Collaborative knowledge base embedding for recommender systems. In SIGKDD, 353–362. Zhao, H.; Yao, Q.; Li, J.; Song, Y.; and Lee, D. L. 2017. Meta-graph based recommendation fusion over heterogeneous information networks. In SIGKDD, 635–644.", "date": "2019-01-24"},
{"website": "Ebay-Research", "title": "Interactive Visual Search", "author": ["M. Hadi Kiapour", "Shuai (Kyle) Zheng", "Robinson Piramuthu"], "link": "https://tech.ebayinc.com/research/interactive-visual-search/", "abstract": "Interactive visual search with user feedback helps buyers find the perfect item and while enjoying the exploratory journey. In our previous article, “ Seven Tips for Visual Search at Scale ,” we discussed visual search where a user query is an image, and the results are shown to the user based on visual similarity to that query image. One could consider this as a single iteration of the search process. This is great when the user has the picture of the exact product and finds the right match in the result set in terms of price, size, condition, shipping, etc. It is still possible that the exact product is not in the result set for reasons such as product out of stock. What if (i) the user knows the exact product but does not have the picture or (ii) the user has an image of a product but wants one with a few variations from the query or (iii) the user wants to explore the beautiful product landscape? An interactive approach is natural in such scenarios where the user gives feedback after each result set. Such feedback could be based on items in the result set that had a click-through from the search result page. Results on the next (unseen) page could be updated based on images that experienced a click-through. We present an approach for interactive visual search. Although the same could be true for text or speech or multimodal search, we limit our discourse to image queries. The aim of this material is to keep it simple. More technical details can be found in our paper “ Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback ,” that was presented at WACV 2019 . Watch the 5-minute video at the end of this article as an alternative. The scope of this article does not include personalization, but includes only user feedback during the search session for a single quest. However, extension to personalization is straightforward, since our approach could be generalized easily. We represent each image by a compact signature in the form of a series of numbers represented as a vector a.k.a. embedding . Similarity is then measured by proximity based on this vector. This is illustrated in Figure 1. This vector could be generated by a deep neural network, i.e. the deep neural network maps the input image to a vector. One way to train such deep neural networks is to train it using triplets and predict if they have the desired order. The resulting embedding is called “triplet embedding.” Valid triplets are constructed such that they are made up of three images (A, P, N), where A is the anchor, P is a positive sample, and N is a negative sample. The definition of positive and negative samples could be based on whether they have a specific attribute in common or not. For example, in Figure 1, (A, B, C) is a valid triplet for brand and (A, C, B) is a valid triplet for fastening type. Figure 1. Each image is mapped into a vector using a deep neural network. These vectors are used to measure similarity of pairs of images. Closer pairs are more similar than those farther apart. In the above example, products A, B have the same brand, and products A, C have buckles. Pair (A, B) should be closer than (A, C) when we want to measure brand similarity. Similarly, pair (A, C) should be closer than (A, B) when we want to measure similarity based on presence on buckles. If our vector is of dimension 1, as shown here, there is no way to capture both of these two conditions (brand, buckles) of similarity simultaneously. However, “similarity” is subjective. For example, as in Figure 1, we may be interested in measuring the similarity of images A, B, and C based on brand or based on buckles. A single mapping function cannot be used to measure similarity universally. Thus, there is a need that the mapping function adapts to the desired conditions or constraints. One way to achieve this is to use “ Conditional Similarity Networks ” (CSN) which was published in CVPR 2017 . CSN learns a mask vector for each condition. The underlying vector is common for all conditions and is modulated by the mask vector by element-wise multiplication. This modulation operation is illustrated in Figure 2. Figure 2. Illustration of modulating an embedding by a conditional mask. Values in the mask vector lie between 0 and 1 with a total sum of 1. The original embedding is shown on the left. The mask is determined by the selected condition A or B as shown in the middle. The original embedding is modulated by element-wise multiplication with the selected mask, as shown on the right. Modulation operator is depicted by the “o” symbol. “ Study the past, if you would divine the future ” - Confucius Consider the scenario where the user does not have the exact product image, but knows what it looks like. Our goal is to show a collection of images to the user where the user picks the best image that shares the most aspects of the desired product. Once this feedback is received, a new set of images is shown to the user. This new set awaits user response. This process is repeated until the user finds the desired product. This is similar to the popular “ 20 Questions ” game. A single iteration is shown in Figure 3. Figure 3. During each iteration, the user picks the image that is closest to the desired image. Our algorithm constructs the best set of images shown to the user such that the desired image appears in as few iterations as possible. Our algorithm does this in two steps. The first step involves a form of visual search to get an initial set of candidates. The second step ranks these images based on how informative they are. We use reinforcement learning to train how to rank based on information content derived from user feedback in all past iterations. Figure 4. The quest starts with an initial query. Each search iteration results in showing the user a result set which could be 1, 2, or more images. As part of feedback, the user may choose to select an image or not. This feedback is used to generate the next result set, and the process continues until the user finds the desired product. This is similar to the popular “ 20 Questions ” game. For simplicity, assume that the inventory consists of only a single image per product. We can construct a graph as in Figure 5, where each node is a product. Nodes are connected if they have at least one shared attribute. For example, nodes corresponding to two products from the same brand may be connected. These connections have weights proportional to their similarity, which is based on the distance between their embeddings. Note that these weights depend on the condition of similarity (for example, brand, buckles). Figure 5. Our setup is also novel in the sense that the entire user feedback process can be simulated, and performance can be evaluated automatically. In fact, this helps us train our deep neural network for reinforcement learning. Given a graph of inventory where each node is an image of a product (for simplicity, assume one image per product), we can randomly sample two nodes, where one is the initial query and the other is the image of the desired product. Our goal is to go from one to the other in as few steps as possible. Typical approaches requiring such user feedback construct training and validation sets using actual user feedback by crowdsource. This could result in subjective and inconsistent labels. Also, this is an expensive procedure that results in a limitation in the size of the data set. Evaluation of the approach has further complexities related to the repeatability of the experiment. This is easily addressed by simulation. Our goal is to reach the desired product in a minimal number of steps. We train a CSN based on triplets generated from the available attributes in the data set. This creates rich embeddings for each attribute (as in Figure 1). It is important to note that we create triplets based on coarse attribute labels (for example, “are A and B both purple?”) which are easily available instead of the expensive relative attribute labels (for example, “is A more purple than B?”). Figure 6 shows that even coarse labels can produce rich embedding comparable to those achieved by expensive relative attribute labels. We achieve this by using CSN and making some modifications to it (i) restrict mask vector so that all elements add up to 1 (ii) discourage large values for the magnitude of embedding vector (iii) apply global consistency constraints by considering overlap of attributes between pairs. These modifications result in about 3% absolute improvement in accuracy. See our paper for technical details. Figure 6. The visualization of an embedding using t-SNE for the attribute “closure” on UT-Zappos50k data set. Note that this captures similarity based on shoe style very well even though it was trained to predict “closure.” We were able to learn high-quality embedding with just the binary attributes (for example, “do A and B have the same brand?”) instead of the expensive relative attributes (for example, “is A more purple than B?”). Our modifications to CSN improves absolute accuracy by about 3%. As shown in Figure 4, we use nearest neighbor search (think of visual search) to sample top candidates, and then use reinforcement learning (RL) to rank them and pick the top few (say, two) to be shown to the user. As mentioned in Figure 5, we can simulate the entire procedure for interactive visual search since we can randomly select initial query and final target. RL is very effective for such cases. We use Deep Q-Network (DQN) as in “ Playing atari with deep reinforcement learning ”, NeurIPS Deep Learning Workshop 2013. DQN learns to predict the Q-value for a given set of actions from a given state. The Q-value for a given state and action is the maximum expected long-term future reward for that action from that state. For instance, in the game of chess, Q-value could be the probability of winning the game when we make a specific move (the action) from the current configuration of the chess pieces on the board (the state). Note that the short-term reward could be to take out the opponent’s pawn even though this may not necessarily increase the long-term future reward of winning the game. Thus, Q-value is a measure of the quality of action at a given state. Note that it is dependent on both the state as well as the action. The same action may not be optimal at a different state. In our case, an “action” is selection of an image from the candidate set of images from the sampler. “State” consists of relative embeddings of images with regard to the embedding of the latest query that generated the candidates. As in Figure 7, the best image has the largest estimated Q-value (as indicated by the green checkbox). The DQN consists of three fully connected layers and ReLU as nonlinearity. This is illustrated in Figure 8. As discussed in the previous section, CSN learns a mask that adapts to the condition for similarity. Thus, the sampler can produce candidates per masked embedding. A separate DQN predicts Q-values for the candidate sets from each sampler, as in Figure 9. CSN was trained first, and then DQN was trained using Huber loss (a combination of piecewise linear and squared loss) based on the expected and observed rewards of picking the top images for user feedback. Note that user feedback is simulated. The best image will be the closest to the target image (known during simulation) among all the queries picked so far. Figure 7. Simplified view of Deep Q-Network showing inputs and outputs. The sampler gives a candidate set of images. The goal of DQN is to pick the top images from this candidate set based on the estimated Q-values. Here, “action” is selection of an image and “state” is defined by the candidate set of images from the sampler. We define the relative embeddings of images with regard to the embedding of the current query image to be the state variables. Also see Figure 8. Figure 8. Our Deep Q-Network contains three fully connected layers with ReLU as non-linearity. Also refer to Figure 7. Figure 9. Complete architecture to train the neural networks. This figure shows loss layers for both CSN and DQN. We use ResNet-18 as a backbone for the CSN. The architecture for DQN is shown in Figure 7. The sampler produces nearest neighbors based on the masked embedding. Huber loss is used to train the DQN based on the expected and observed rewards of picking the top images for user feedback. Figure 10. Sample navigation using our approach. Each row shows the traversal initiated by a query shown on the left. The desired products are highlighted in green boxes at the right. Although not immediately obvious, consecutive products share specific attributes. See Figure 10 for a qualitative illustration of navigation for various input queries. Every selected image has at least one common attribute with the previous one. The average agreement of human annotators with our simulated approach was about 79%. We observed a reduction in number of steps by 11.5-18.5% when we use our DQN approach, compared to competing hand-engineered rules to rank. The reduction is 17-27% when compared to nearest neighbor. See our paper for technical details. We presented a scalable approach for interactive visual search using a combination of CSN (for conditional masked embedding), DQN (no hand-engineered rules), and simulation (train without human-in-the-loop and at scale). Our approach can be easily extended to multimodal data as well as personalization. Improvements in Embedding No need for expensive relative attribute labels No need for expensive relative attribute labels Modified CSN Modified CSN ~3% absolute improvement in accuracy (UT Zappos 50K, OSR) ~3% absolute improvement in accuracy (UT Zappos 50K, OSR) Improvements in Navigation Learn to rank triplets and select candidates using DQN Learn to rank triplets and select candidates using DQN 11.5-18.5% reduction in number of steps when compared to competing hand engineered rules to rank 11.5-18.5% reduction in number of steps when compared to competing hand engineered rules to rank See our paper for technical details, or watch the short video below. Interactive Visual Search from eBay Newsroom on Vimeo . This is collaborative work with Bryan Plummer who was our summer intern in 2017 and the primary author of our paper .", "date": "2019-01-22"},
{"website": "Ebay-Research", "title": "ModaNet: A Large-scale Street Fashion Dataset with Polygon Annotations", "author": ["Shuai (Kyle) Zheng", "Fan Yang", "M. Hadi Kiapour", "Robinson Piramuthu"], "link": "https://tech.ebayinc.com/research/modanet-a-large-scale-street-fashion-dataset-with-polygon-annotations/", "abstract": "Searching for an ideal dress or pair of shoes sometimes could be challenging, especially when you do not know the best keywords to describe what you are looking for. Luckily, the emerging smart mobile devices provide an efficient and convenient way to capture those products of interest in your photo album. The next natural thing is letting an ecommerce app like eBay figure it out for you. Understanding clothes and broad fashion products from such an image would have huge commercial and cultural impacts on modern societies. Deploying such a technology would empower not only the fashion buyers to find what they want, but also those small and large sellers to have quicker sales with less hassle. This technology requires excellence in several computer vision tasks: what the product is in the image (image classification), where it is (object detection, semantic image segmentation, instance segmentation), visual similarity, how to describe the product and its image (image captioning), etc. Recent works in convolutional neural networks (CNNs) have significantly improved the state-of-the-art performance of those tasks. In the image classification task, ResNeXt-101 method has achieved 85.4% in top-1-accuracy 1 in ImageNet-1K ; in object detection, the best method 2 has achieved 52.5% mAP in the COCO 2017 benchmark for generic object detection; in semantic image segmentation, the top-performing method 3 has reached 89% mIOU in PASCAL VOC leaderboard for the generic object segmentation. Due to unique challenges in street fashion images, including wide variations in appearance, style, brand, and layering of clothing items, one remaining question is how well those object detection and semantic image segmentation algorithms perform on the street fashion dataset. By understanding the pros and cons of existing algorithms for object detection and semantic image segmentation on street fashion dataset, we would be able to provide the technology to eBay customers. Figure 1. Examples of Annotations in ModaNet dataset. These images contain pixel-level annotations for each product type. Yamaguchi et al. 4 created a street fashion dataset called the Paperdoll dataset with a few hundred pixel-wise clothing annotations based on super-pixels. We are introducing a new dataset called ModaNet, which is built on top of the Paperdoll dataset and adds large-scale polygon-based fashion product annotations, as shown in Figure 1. Our dataset provides 55,176 street images, fully annotated with polygons on top of the 1 million weakly annotated street images in Paperdoll. ModaNet aims to provide a technical benchmark to fairly evaluate the progress of the latest computer vision techniques that rely on large data for fashion understanding. The rich annotation of the dataset allows measurement of the performance of state-of-the-art algorithms for object detection, semantic segmentation, and polygon prediction on street fashion images in detail. Figure 1 shows a snippet from ModaNet. The ModaNet dataset provides a large-scale street fashion image dataset with rich annotations, including polygonal/pixel-wise segmentation masks, bounding boxes. It consists of a training set of 52,377 images and a validation set of 2,799 images. This split ensures that each category from the validation set contains at least 500 instances, so that the validation accuracy is reliable. It contains 13 meta categories, where each meta category groups highly related categories to reduce the ambiguity in the annotation process. The 13 meta categories are bag, belt, boots, footwear, outer, dress, sunglasses, scarf and tie, pants, top, shorts, skirt, and headwear. All images are annotated by human annotators. Annotators have been trained for 2 weeks before starting the annotating, and their annotation quality accuracy reached 99.5%. During the annotation process, the annotators conducted two tasks: (1) skip the images that are ambiguous to annotate, and (2) provide polygon annotations for individual objects of interest in the image and assign a label from a predefined set of 13 meta categories. The goal of object detection in ModaNet is to localize each fashion item from the image and assign a category label that can be further used for visual search or product recommendation. We chose three most popular object detectors to evaluate their performance on the ModaNet dataset: Faster RCNN , SSD , and YOLO . Both SSD and YOLO are single-stage, real-time detectors. Faster RCNN is the representative work for the two-stage approach, which aims to give more accurate results. Specifically, in our experiments, Faster RCNN uses Inception-ResNet-v2 as its backbone network, while we chose Inception-V2 for SSD and YOLO v2 network for the YOLO detector. As shown in our experimental results, we find that more effort should be put into developing detectors that can better handle small and highly deformable objects for fashion. Figure 2. Semantic image segmentation results on the ModaNet dataset. The first column contains output from DeepLabV3+. The last column contains ground truth annotations. Semantic image segmentation provides more detailed localization information. We considered several most representative approaches to evaluate on the ModaNet dataset. These approaches are: Fully Convolutional Neural Networks (FCNs), Conditional Random Fields as Recurrent Neural Networks (CRFasRNN), and DeepLabv3+. FCNs methods use a VGG network as its backbone network. We adapt the VGG network with batch normalization, which has obtained higher top-1 accuracy in the ImageNet-1K dataset. We also adapted the CRFasRNN module on top of the FCNs, and we obtain higher accurate results than FCNs. For DeepLabV3+, we take the publicly available TensorFlow implementation and ImageNet pre-trained Xception-65 model, and fine-tune on the ModaNet (see Figure 2). We find that DeepLabv3+ performs significantly better than the alternative approaches across all metrics. This shows the importance of the backbone network as well as the careful design in the CNN modules for semantic image segmentation. We also find that CRFasRNN helps to get a better shape of some objects like “outer” and “pants,” but performs poorer in small objects such as “sunglasses,” as show in Table 1. Table 1. F-1 score per category of evaluated semantic segmentation approaches. One immediate application of semantic image segmentation is to predict the color attribute name given a street fashion photo. We develop a prototype based on the models trained on the ModaNet dataset. We first conduct the semantic image segmentation and then predict the color attribute names by mapping the mean RGB values for each segment to a fine-grained color namespace. This gives interesting results as shown in the Figure 3. Figure 3. Color attribute prediction using semantic image segmentation. Mahajan,et al. Exploring the Limits of Weakly Supervised Pretraining. ArXiv, 2018. Peng, et al. MegDet: A Large Mini-Batch Object Detector. CVPR, 2018. Chen, et al. Rethinking Atrous Convolution for Semantic Image Segmentation. CVPR 2018. Yamaguchi, et al. Retrieving Similar Styles to Parse Clothing, IEEE PAMI, 2014. Ren et al., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015. Liu et al., SSD: Single Shot MultiBox Detector. ECCV 2016. Redmon et al., You Only Look Once: Unified, Real-Time Object Detection, CVPR 2016. Abadi et al., TensorFlow: A system for large-scale machine learning. CoRR abs/1605.08695, 2016. Zheng et al., Conditional Random Fields as Recurrent Neural Networks. ICCV 2015. Long et al., Fully convolutional networks for semantic segmentation. CVPR 2015. Szegedy et al., Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. AAAI 2017. Deng et al., ImageNet: A Large-Scale Hierarchical Image Database. CVPR 2009.", "date": "2018-11-05"},
{"website": "Ebay-Research", "title": "Beyond Logos and Patterns: How We’re Training eBay’s AI to Understand Brands", "author": ["M. Hadi Kiapour", "Robinson Piramuthu"], "link": "https://tech.ebayinc.com/research/beyond-logos-and-patterns-how-were-training-ebays-ai-to-understand-brands/", "abstract": "We’re researching how to recognize brands using computer vision by training our AI to look beyond logos and iconic patterns. Think of your favorite brands: How do you recognize them when you are shopping? Maybe it’s that iconic swoosh on your favorite sneakers or that distinctive plaid on your handbag or that apple on your phone. Brands express themselves in various visual forms. At eBay, we’re researching how to recognize brands using computer vision . We’re training our AI to look beyond logos and iconic patterns to dial in on the unique characteristics that brands use to create specific items. We’ve compared our deep learning model to human understanding in an experiment to validate visual perceptions of brands. As your brain evaluates a shoe or a bag, it’s taking in and processing all sorts of information—from the style to the pattern to the fabric. It makes a decision with its best hypothesis based on a variety of factors and insights learned over time. These are some of the elements that make up brand recognition. And when you’re shopping, this recognition is one of the key steps to finding the perfect item since brand encapsulates such rich information. We set out to understand how to identify brands visually by targeting unique designs, patterns, stitching or hardware. We also wanted to understand how deep networks distinguish between similar products and to compare our analysis to human perception. We investigated how our deep learning models build internal representations of brands and we examined how those representations vary over products. This allowed us to further understand the classification path that AI uses. We are using these representations to analyze visual embodiment of brands at large scale and to find the key characteristics of a brand’s visual expression in a single product, a brand as a whole and across categories. It’s important to note that one of the reasons eBay’s computer vision models are so powerful is because we are training the models on varying qualities of images—from professional or stock photos to amateur, dimly-lit photos with complicated backdrops. Our dataset consists of 3,828,735 clothing products from 1,219 brands spanning across a wide range of clothing types. These are real-world ecommerce images from our catalog. For every product, we collect an image, title and a set of attributes from which we extract the brand information. After training the model on a given image, we get outputs for probabilities of what brand is the most likely featured in the image. Then, we follow where the neurons at each layer are focused to make their decision through “attention” maps. Our goal is to visualize and interpret the deep model’s decisions in order to explain the visual characteristics of fashion brands. For instance, we saw that our AI was recognizing the three-stripes signature on Adidas products instead of the logo. As part of our work to understand brands, we’ve also analyzed our AI at the neuron level to get insights into different visual indicators for fashion brands. In our deep learning model, neurons collect and process information by classifying unique characteristics and assigning it to the most likely brands. We’ve created attention maps to interpret the visual characteristics of a brand present in a single image and model the general design direction of a brand as a whole. Even more exciting, we observed that certain neurons diverged into experts or generalists when learning over time. Some neurons leaned toward specialties and became decision makers while other remained generalists. For example, once a neuron learned to identify a color like purple and a pattern like paisley, it was more likely to be called on to identify purple and paisley characteristics in the future. This is important to answer which decision-makers in our neural networks make certain judgments and helps us come closer to answering the all-important question of why certain neurons in deep learning models become decision-makers and gain more authority over time. Our work analyzes the deep neural network model to understand and characterize the types of neurons that help predict brand from a single image. Brand prediction beyond logo is an example of narrow AI—or a place where AI is more efficient than humans at a specific task. Taking a step back, AI technology should be explainable since we use it to make big decisions. As we look to the future, this application of AI and our understanding of these neurons is paving the way for answers to specific and important questions that will help reduce bias, sharpen personalization and ultimately to the improve our recommendations. By understanding brand characteristics, we can further cater the shopping experience to individuals and serve them up a truly tailored experience where everything they see is personalized to them. eBay is not affiliated with or endorsed by Vera Bradley or Adidas. For a more in-depth look into our computer vision efforts, read our latest Tech Blog .", "date": "2018-11-05"},
{"website": "Ebay-Research", "title": "Going the Distance — Edit Distance 3", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/research/going-the-distance-edit-distance-3/", "abstract": "How do you normalize Edit Distance? Some simple ideas to get useful numbers about the changes in your text. One thing is absolutely true: the absolute number that is Edit Distance (see our previous post) is not very useful in most MT cases. A change of 5 words in a sentence of 20 words is an Edit Distance of 5 (and is 25%). A change of 2 words in a sentence with 4 is an Edit Distance of 2 (and is 50%). So, 5 words looks like much more than 2 words, but 25% is less than 50%. That is why we need to use percentages or relative numbers, because the change effort needs to be placed in context, related to the length of the text. I used word counts below because they are easier to visualize. Let’s talk about naming: TAUS is calling this Edit Density, which is a fine name. I used to call it “Percent Edit Distance” or “Percentage of Change”. It is easy to slip into saying an “Edit Distance of 30%”, but strictly the “pure” Edit Distance is in words or characters (or operations of change applied to words or characters) and it is not a percentage. TAUS presents it as number of edits per 100 characters, which is a percentage. Now that we see that we need a percentage, comes the question: the Edit Distance should be divided by… what? Should be a percentage of what? You may hear this question as “how should we normalize Edit Distance?”. There are various statistical definitions for normalization, but what we want to do with Edit Distance is simply called scaling: we want to bring the values into the same range, so that we can compare them. It is a simple form of normalization. There are three possibilities: Divide by the initial (MT) count In MT, this means that you are calculating based on the number of words that the posteditor started working with. Divide by the final (PE) count In MT, this is the postedited word count, the number of words that the posteditor ended with. Based on the maximum between both initial and final The numbers are between 0 and 1 One could argue that using the MT count lets you know the costs before the work is done, while the others require the work to be finished to get the number. I think that this is traditionally why the industry uses source word counts, because you know the count before you work on it. Then you can give your price to the client, and the translators knows how much they will make at the beginning. But it doesn’t necessarily have to be that way. Translators many times struggle with projects that took much more effort than they initially estimated. I can’t think of a reason to use the PE count. If you have one, please share. The best metric is to use the maximum between both MT and PE. Placing the results between 0 and 1 mean that your percentage is between 0 and 100%, which is very convenient to create charts. Here is how this works. Let’s take our previous example of “Roses are sometimes red > Violets are blue and you are sweet”. There were 7 changes in this transformation, the Edit Distance is 7 words. You can see the % based on MT getting a high score of 175%. But something different could happen: The % based on the PE could go high, or the MT could go really high to 400%. Meanwhile, the % on Max is always well behaved, between 0 and 100%. So, let’s use Max. What do you think? You can find the first two articles in this series at Going the Distance — Edit Distance 1 and Going the Distance — Edit Distance 2 . If you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2019-08-28"},
{"website": "Ebay-Research", "title": "Measuring Success with Experimentation", "author": ["Tianlin Duan"], "link": "https://tech.ebayinc.com/research/measuring-success-with-experimentation/", "abstract": "Tips from eBay's Experimentation Science team on how you can best leverage A/B tests to measure the success and health of your product. During a recent internal product conference, I had the honor of sitting on the Product Health & Opportunity Sizing panel. Along with four amazing product owners, we discussed the importance of opportunity sizing, and shared tips, tools, and challenges when measuring the success and health of products. As the Experimentation Science team, we have the great pleasure of working with product teams all across eBay to understand the impact of their products. In this blog, we would like to share with you some of our insider tips on how you can leverage experimentation to guide your product development journey. Step #1 towards effectively measuring the success of your product? Have a clear definition of success. Often times when product owners approach us to discuss the feasibility of an experiment, they already have a product or feature change in mind or already in development and at least a vague idea or vision of what impact this change might have. The very first thing to ask at this early stage would be, what is your definition of win, or what does success look like for this product/feature. This definition should cover both the potential business impact and how it might change user behavior. We currently put a lot of emphasis on the first part, and less so on the latter, but the two actually go hand in hand. Try to frame your definition of success in terms of the engagement you would like to see from the users, and then describe how that positive behavioral change may lead to desired business outcome. In sum, when building your business case, form a user story first. With a good user story comes a strong hypothesis and naturally follows your set of success metrics. Success is rarely defined by a single goal. Success can mean increasing conversion and decreasing friction; it can encompass both short-term win and long-term impact; it may also include keeping potential cannibalization to other parts of the site at an acceptable level. Each of these aspects of success corresponds to one or more metrics that have their own expected movement in response to the testing feature, and together they help you see the whole picture of the impact of your product. Brainstorming and doing your homework on all these components of success will prepare you for the upcoming test. You will know what might happen and what to look for. When finalizing the design of your experiment, we work with you to translate your clear and thorough definition of success into metrics that serve various purposes: some measure the direct impact of the testing feature, some measure actions that are a few more steps from the change, and others provide additional insight into your product. Next we’ll dive into three main categories of metrics that we focus on in experimentation context: primary success metric, guardrail metrics, and monitoring metrics. Let’s consider an example. Say you are testing different designs of the Search bar to improve its prominence. By drawing out the User Flow Diagram, you define a desired user path as noticing the more prominent design → perform a search → find the item they want or perform a new search until then → showing purchase intent by clicking on Add to Cart or Buy It Now → complete the purchase. This is the metric that measures the most direct impact of your product, and is thus by definition a product metrics, not a business one. It is also the metric that Touchstone, our experimentation platform, uses to determine the duration (or days remaining) for your test, and what you should focus on when interpreting the results of the test. In the Search Bar prominence example above, the expected immediate next action after users in test are exposed to the new design (definition of “treated”) is “perform a search”, which translates into the number of searches. If the number of searches per treated sample is higher in Test than Control, then you’ve realized your definition of success — making the Search Bar more prominent. These are metrics that measure the overall impact on the site or the business, and thus also the metrics you don’t want to break. They are generally business metrics that are usually a couple steps away from the immediate action your feature triggers, and thus they are noisier and take longer to reach statistical power. Business metrics include metrics such as Purchases, Bought Items (BI, number of items purchased in a transaction), and GMB (gross merchandise bought) fall into this category. In our example (or in every experiment that we run), the ultimate goal is to drive more purchases, have more purchasing users, and make more revenue eventually. But notice how many steps we have between performing a search and making a purchase? Chances are, if the new design is indeed working and we observed a statistically significant lift in the number of Searches, that exciting little lift might have already diminished before users reach the last step, completing a transaction. If you chose business metrics, especially BI or GMB, as your primary success metric, you will very likely be disappointed when the experiment ends after weeks in noise without a clear launch signal. These are metrics that measure the indirect impact of your feature and/or provides additional insights into how your feature is impacting user behaviors. Product metrics measuring events or user actions between the immediate next step (measured by your primary success metric) and end-of-funnel transactions (measured by your guardrail metrics), or cannibalization of your feature on related features on site all fall into this category. In our example, metrics like SRP (Search Result Page) to VI (View Item page) conversion and purchase intent metrics such as Buy It Now or Add to Cart clicks make good secondary metrics, and you may also benefit from tracking and comparing click shares on different components of the Global Header (where Search Bar lives) to understand how the more prominent search bar is affecting these coexisting features. Hopefully the above categorization of metrics provides some inspiration on how you want to measure the success of your experiment. Try identifying your primary, secondary, monitoring, and guardrail metrics for your next experiment, and if you’re new to the process or have questions about it, reach out to your experimentation team for help. Often times we jump right into the discussion on how to measure the success of your product, but we also need to think about whether that success is measurable. There are three important pieces of information that can help us answer that question: the baseline performance of your success metric, the expected lift as a result of your product/feature change, and your treated percentage. In this section, let’s deep dive into treated percentage. Treated percentage represents the percentage of users that would actually experience the product change you have in mind. For example, if you’re testing some changes on the checkout success page, fewer than 10% of our users who come to the site might actually have the chance to complete a purchase and really see the change. If you’re testing something on the Home Page, the treated percentage would be much higher. Having a decent treated percentage is key to make sure your definition of success is actually measurable. Here’s an example: let’s say you have one test group and a control and you’re hoping to test it on US Desktop, which usually has the least concern about traffic. Having a 70% treated percentage would let you identify a 1% lift on purchase and BI within the minimum test duration required by our Experimentation Platform. A 10% treated percentage would extend that time to months. A 1% treated traffic? Over a year! In case you wonder, it takes longer than 5 years to measure a 1% lift in GMB in this case. This is not to mention that we rarely see a 1% or larger lift in these business metrics, meaning that these numbers are, in fact, an underestimate of the actual duration your test is going to take. If you already know you will be dealing with small treated percentages, here are a couple options you should consider: If your treated percentage is extremely small, like <5%, don’t run an A/B test to measure business impact. Use your best product judgment to launch it. As you’ve seen above, it’s going to take months, if not years, to measure a 1% lift that rarely happens, so what usually happens is that the test would run for a month and end up having all metrics in the noise and underpowered. If your treated percentage is extremely small, like <5%, don’t run an A/B test to measure business impact. Use your best product judgment to launch it. As you’ve seen above, it’s going to take months, if not years, to measure a 1% lift that rarely happens, so what usually happens is that the test would run for a month and end up having all metrics in the noise and underpowered. If you have a relatively small treated percentage, like 10%, be our friend and do yourself a favor by choosing a product metric, especially one that measures the direct engagement with the feature, as your primary success metric. Also, A/B test in this case will only be efficient and helpful if you’re testing a high-impact change — if you’re expecting a 0.1% lift, even engagement metrics can take months to reach statistical power. If you have a relatively small treated percentage, like 10%, be our friend and do yourself a favor by choosing a product metric, especially one that measures the direct engagement with the feature, as your primary success metric. Also, A/B test in this case will only be efficient and helpful if you’re testing a high-impact change — if you’re expecting a 0.1% lift, even engagement metrics can take months to reach statistical power. From a company’s perspective, there is no doubt that revenue matters a lot. In eBay’s business context, GMB has always been a key metric that finance teams track and monitor, as it should be. It's a separate discussion about whether it’s the money or the users (buyers and sellers) that matters more for a company’s health, growth, and success, but in both experimentation and product development context, we would like to argue that the users and their behaviors are much more measurable and provide much more actionable insights for your product development journey. GMB is the most noisy metric among all and measures purchase behavior, which is the very end of the funnel. Remember that the more steps between the event measured by your primary success metric and your actual product change, the more noise you have in the data and often the longer the duration. If you don’t want to wait 6.5 years to get a read on your test, resist the temptation to define success as “boosting GMB by 1%.” A related case is driving conversion. One question to always ask is, what exactly is “conversion” in your case? Conversion is the general movement of users moving one step further down the shopping funnel, and thus could mean quite different things depending on the context: conversion for a personalized module on the homepage might mean users showing interest in the module and clicking to land on a VI page, while conversion on a VI page might mean a Buy It Now click or other actions showing purchase intent. Note that Conversion never meant GMB or revenue. It is widely accepted in the industry as “the percentage of users who perform a desired action.” If this desired action is making a purchase, then the Conversion Rate can be a percentage of users who completed at least one purchase during the test period, but not the average GMB generated by a user or the average GMB per session. The good news is, if defined properly, conversion rate can be a great candidate for a primary success metric, and it usually requires less duration to reach statistical power given its nature as a binary metric. Hopefully by now you have a much better sense of how to define the success of your product and feature, whether that version of success is easily measurable, and how to leverage different types of metric to get the whole picture when measuring success with experimentation. Thank you for reading and please feel free to leave comments, questions, or suggestions below! Have fun experimenting.", "date": "2019-06-12"},
{"website": "Ebay-Research", "title": "Going the Distance — Edit Distance 1", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/research/going-the-distance-edit-distance-1/", "abstract": "What is Edit Distance? How could it be used to measure quality? Find out the basics about this simple metric used for Machine Translation. Edit Distance has always been a great metric to measure the changes made to something, according to at least one person (me). In our MT world, we are usually talking about the MT output as a starting point, and a final version of the target language as the end point. This target language can be a post-edited version created from the MT output, or it can be a purely human translation created from scratch, without seeing the MT output. The Edit Distance measures the changes from starting point to end point. Edit Distance can be a metric for quality: if your starting point is of good quality, it needs few changes to get to the end point. So a low Edit Distance is good. It could also be seen as an indicator of productivity: if you need to make few changes, your work will tend to be faster than if you have to make lots of changes. Edit Distance is probably at the core of nearly all known MT quality scores such as BLEU, TER and others. All of them compare an initial version to a final version and measure what changed. But these metrics are adding more complex features to try to be closer to human evaluations. Edit distance may just be the simplest of all metrics. One word about all scores: whatever score you like, that score is usually much better when used to compare things than to provide absolute verdicts. Are you comparing multiple MT engines? Or different versions of one MT engine? You may get reliable results saying that engine X is better than Y or that version 2 is better than 1, or that Neural is better than Phrase-based. Or you may find out that Transformer s is the best movie ever is a better technology than other NMTs. But the absolute statement “this MT is good”, based on any score, has been usually harder to trust. Now that we know where we are, let’s go back to simple, and take a look at Edit Distance. The core of Edit Distance calculation is an algorithm called Levenshtein distance , which finds the minimum number of changes (additions, deletions or substitutions) to change something into something else. This is how it works: Let’s say you want to change characters. If I want to change Rose > Violet (you know where this is going): Change R to V = V ose Insert an i = Vi ose. Don't do anything to o . Change s to l = Viol e Don't do anything to e . Add a t = Violet . The Edit Distance is 4 for a total of 4 operations, or 4 characters. Funny enough, Violet to Rose is also 4: Violet > Riolet > Rolet >Roset > Rose. (And if you thought that “Edit Distance being the same in both directions” was really something “funny enough,” you may be a fellow nerd.) Now let’s change words : Roses are sometimes red > Violets are blue and you are sweet Change Roses to Violets = Violets are sometimes red Don't do anything to are . Remove sometimes = Violets are sometimes red Change red to blue = Violets are blue Add and = Violets are blue and Add you = Violets are blue and you Add are = Violets are blue and you are Add sweet = Violets are blue and you are sweet The Edit Distance is 7 for a total of 7 operations, or 4 words. The calculation is simple, but you see that there are already two variations in how we can calculate. So one question already is here: What am I gonna watch after Game of Thrones ended? Should we use characters or words? What is your take on this? You can find the first two articles in this series at Going the Distance — Edit Distance 2 and Going the Distance — Edit Distance 3 .", "date": "2019-08-08"},
{"website": "Ebay-Research", "title": "Kubernetes Secrets: A Secure Credential Store for Jenkins", "author": ["Vasumathy  Seenuvasan", "Ravi Bukka"], "link": "https://tech.ebayinc.com/research/kubernetes-secrets-a-secure-credential-store-for-jenkins/", "abstract": "At eBay, we containerized Jenkins to provide a continuous build infrastructure on Kubernetes Clusters to power the ecommerce marketplace experience. Our goal was to leverage the capability of Kubernetes secrets, for managing the Jenkins credentials. eBay.com is powered by applications that are built with Jenkins as the opinionated choice of build-machinery. We run Jenkins instances as a cloud native workload on the Kubernetes Clusters with server-agent mode, all of which are managed by the eBay cloud team. The Jenkins servers provided to eBay development teams have a default credential plugin installed. The credentials stored on these Jenkins instances are used for various purposes, such as connecting to GitHub and third-party APIs. The credential plugin stores the user credentials, as well as the encryption key, on the disk in an encrypted format. These credentials are very sensitive in nature and need to be stored safely and securely. However, storing the key and the credential on the same disk poses a risk. Jenkins instances are also generally shared across a team with ADMINISTER privileges , which increases the chances of team members knowing each other’s credentials. Initially, we considered setting up a Vault 1 store, which is generally used in the community to secure the credentials in Jenkins. However, that would be a vendor lock-in. To remain vendor-neutral, we chose a different strategy. However, eBay has its own proprietary key management platform, which integrates with Kubernetes cluster’s control-plane and mounts secrets in a secure manner to the containers leveraging Kubernetes API standards. Since we are running Jenkins as workloads on Kubernetes, a simple solve was to standardize by using Kubernetes Secrets. Another benefit of this approach is that the same secrets can also be backed up to the in-house key management solution. The challenge here was to integrate the Jenkins application with this proprietary key management platform. To solve the challenge, we developed a new plugin - Jenkins Credentials Secrets Plugin - which replaces the default credential plugin and is now available as an eBay open source project. This article explains the design and process of a more secure way of storing Jenkins credentials as secrets on Kubernetes Clusters. We have enabled this plugin to store credentials as Kubernetes Secrets on ~6000 Jenkins instances at eBay. We have also migrated all the credentials currently stored in these instances to Kubernetes secrets with ZERO down time for the end users - eBay application developers. 1.     The Jenkins master runs as a container in Kubernetes Cluster. 2.     The Jenkins master has a Kubernetes service account mounted to it. Only credentials under the Jenkins Global domain are supported by the plugin at this moment. When a credential is created by the user from a Jenkins UI or API call, a Kubernetes Secret is generated in a namespace (as specified by the plugin user) in the cluster with the required information from the credentials. The Secret specification’s “data” will hold the credential’s “sensitive” information. The Secret will have labels and annotations in the spec to store the below details. The secret name is generated as “sec”  + “-” + “UUID” + “-” + “JENKINS_INSTANCE_NAME” Example : sec-<<UUID>>-myCI. We use UUID because the Kubernetes Secret object’s naming convention does not support all the characters allowed by Jenkins credentials (e.g. whitespaces and underscores). The credentials ID stored by the user is captured in the Secret’s labels, as shown in the above chart. We also capture other credential information, like description and type, and this list can be extended to attributes added in the future as well. The core component of this approach is the “Secrets-to-credentials converter” module. On Jenkins start up, the plugin gets the list of secrets that have the Jenkins name label selector. This is necessary so only the secrets belonging to the Jenkins in question are pulled. Converters for all of the below Jenkins supported credentials are implemented. Username with password Docker Host Certificate authentication Kubernetes Service Account OpenShift OAuth token OpenShift Username and Password SSH Username with Private Key SSH Username with Private Key and Passphrase Secret file Secret text Certificate The credential type of a secret is identified from the label (jenkins.io/credentials-type), and the corresponding converter implementation is invoked to convert a secret to a credential. This credential is added to the Jenkins credentials Map offered by the Jenkins credentials plugin. This makes the approach transparent to the users, as they just see the same credential information on Jenkins UI, and all the secret conversion happens in the background. ➔ Create: When users add credentials in the Jenkins UI, secrets will be created on the corresponding Kubernetes cluster in the namespace provided, and credentials will no longer be stored in `credentials.xml` on the disk. ➔ Read: On Jenkins startup, credentials of the particular CI are loaded from the Kubernetes cluster (using label selector “jenkins.io/ci-name”). ➔ Update: When a credential is updated, the corresponding secret is updated (using label selectors “jenkins.io/ci-name” and “jenkins.io/credential-id”). ➔ Delete: When a credential is deleted, the corresponding secret is deleted (using label selectors “jenkins.io/ci-name” and “jenkins.io/credential-id”). Username Password credential Type: The rest of the credential type YAML specs are available in the source code repository here: https://github.com/eBay/kube-credentials-plugin/tree/master/credentialspecsamples Base Credentials plugin - 2.1.19 Kubernetes plugin (dependency) - 1.1.4 We have open sourced this project, and the git repo can be found here: https://github.com/eBay/kube-credentials-plugin We welcome any PullRequests (PR) or github-issues on this repo. We have listed to-do items on the repo and welcome PRs to address them. 1 Vault is a product from HashiCorp: https://www.vaultproject.io/", "date": "2020-07-27"},
{"website": "Ebay-Research", "title": "Going the Distance — Edit Distance 2", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/research/going-the-distance-edit-distance-2/", "abstract": "If you change a sentence, should you see the characters or words that changed? Edit Distance is back to help you figure this out. Before we go any further, Edit Distance is, by definition, an absolute number, which is the number of operations for change. It is not a relative number. Can you do anything with absolute numbers? Absolutely! Spellcheckers look for words in a dictionary that have the smallest Edit Distance to the word that has been misspelled (or is it mispelled?). The smallest is 1 character, an absolute value. But most applications in MT will need a relative number, proportional to some form of length. The Edit Distance can be measured in characters or words. Which one should we choose? Some arguments in favor of calculating in characters could be: It can be used for any language, including Asian languages. It can be used for German, where a compound word could be equivalent to several English words, for example, throwing the word-based calculation off a little. This is more of an “against-words” argument than an “in-favor-of-characters” one. This is more of an “against-words” argument than an “in-favor-of-characters” one. It better represents minor changes to words such as adding an “s” for a plural. Change Rose to Rose s = 1 character operation Change Rose to Roses = 1 word operation Change Rose to Rose s = 1 character operation Change Rose to Roses = 1 word operation Some arguments against characters could be: Changing one Asian character is not the same as changing one non-Asian character. They carry more meaning and there are less Asian characters in a sentence compared to its equivalent in English, for example. So, you can’t really compare character distances across all languages. They carry more meaning and there are less Asian characters in a sentence compared to its equivalent in English, for example. So, you can’t really compare character distances across all languages. Some languages, such as Japanese, will have Asian and non-Asian characters in the same sentence. They would need to have different weights. And a reordering of one word from MT to PE: MT: The seller voluntarily refunded the buyer. > PE: The seller refunded the buyer voluntarily . The seller voluntarily refunded the buyer. The seller refunded the buyer voluntarily . The seller voluntarily refunded the buyer. The seller refunded the buyer voluntarily . The change of position of the word “voluntarily” means two word operations: the deletion of the word where it was initially, and the addition of the word where it is now. For 2 changes out of 6 words, we get 33%. But for characters, it looks like this: 1. The seller v oluntarily refunded the buyer. 2. The seller vo luntarily refunded the buyer. 3. The seller vol untarily refunded the buyer. 4. The seller volu ntarily refunded the buyer. 5. The seller volun tarily refunded the buyer. 6. The seller volunt arily refunded the buyer. 7. ... 11. The seller voluntarily refunded the buyer v . 12. The seller voluntarily refunded the buyer vo . 13. ... 22. The seller voluntarily refunded the buyer voluntarily . There was a lot of change, 22 out of 42 characters, more than 50% of the characters were moved. Does this look more like two changes of words or more like changing over 50% characters? So, certain changes are better represented as word changes. Some other possible arguments that could be made in favor of words : The “unit of attention” of a translator is a word and not a character. Nobody changes characters, they change words. Translators think of the meaning of the whole word and then apply a change in meaning, which may be just a character. So, the “effort of PE” is a thinking effort in words. Translators think of the meaning of the whole word and then apply a change in meaning, which may be just a character. So, the “effort of PE” is a thinking effort in words. It is easier to think of changing 2 words out of 5 than 17 characters out of 85. This image below shows the difference in Words – Chars looks like for a sample of about 1000 segments. The average % edit distance in words was 44% and for chars was 28%. The numbers I have seen usually seem to be around the edit distance per characters being smaller by about 35 to 40% of the edit distance value for words. If 44-28 = 16, then 16 is about 36 % of 44. Some automatic metrics are based on words, such as BLEU and TER . Other metrics, such as CharacTer and chrF++ have words and characters working together to produce scores. And some metrics are calculated to ignore changes in position, such as Position Independent Word Error Rate (PER), and BLEU to some extent. These are all attempts to better represent the changes. We should just calculate both edit distances (by characters and by words) for a while, until we get better numbers that help us choose one or the other in each situation. This list of arguments for one calculation or the other is by no means exhaustive. What others can you think of? There is one more choice about Edit Distance: how should we normalize the Edit Distance ? You can find the first two articles in this series at Going the Distance — Edit Distance 1 and Going the Distance — Edit Distance 3 . If you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2019-08-21"},
{"website": "Ebay-Research", "title": "Seven Tips for Visual Search at Scale", "author": ["Fan Yang", "M. Hadi Kiapour", "Robinson Piramuthu", "Qiaosong Wang"], "link": "https://tech.ebayinc.com/research/tips-for-visual-search-at-scale/", "abstract": "We present seven tips for visual search at scale, based on our KDD 2017 paper titled \"Visual Search at eBay.\" We had the pleasure of being part of the panel session on visual search at ODSC West 2018 on Nov. 2, 2018. In this article, we summarize our presentation, which was based on our paper “ Visual Search at eBay , ” presented at KDD 2017 . Imagine that you are in a store that looks like a large warehouse, with numerous aisles that are identified by a unique positive integer. eBay has over 1B live listings at any given time. So, if eBay were such a store, it could very much look like the infinite stockroom in the 1999 movie Matrix . You have a page from a printed catalog of plumbing parts and are on a quest to find one of them for which you do not have a model number. You seek a store assistant and point at the plumbing part on the page. The assistant directs you to few aisles, say aisles 183, 5276, 14098. Without this information, you would have to walk through every single aisle and compare the picture of the plumbing part with each item on the aisle. This comparison task gets very complex if parts look very similar. In this case, you would pay attention to every single detail such as color, shape, size, brand, packaging, etc. If the aisles were organized by the type of items, you would sample a few from each aisle and then spend more time at relevant aisles. This is a very complex task, where if you miss the item of interest, you may never find it in a single pass through all the aisles. Algorithmic search essentially simplifies this process when it comes to large scale search. As mentioned in our KDD paper, we train a neural network to predict leaf category from a given image (see Figure 1). These leaf categories are much like the different aisles in our “warehouse.” This neural network-based classifier acts like our store assistant who identifies the potential aisles. We use a top few potential leaf categories selected based on probabilities predicted by the softmax layer in our neural network. Figure 1. Listings are organized by a hierarchy of categories where the terminal entries are called “leaf categories.” These labels can be used to train a deep neural network to predict leaf category, given an image. Once we are at an aisle, we need to know how to compare two images (the query and an item on the shelf). We represent each image by a compact signature in the form of a series of numbers represented as a vector . This signature is also extracted by the same neural network using weak supervision. We extract a binary vector (made up of 1's and 0's) by training the network with a sigmoid layer to predict top leaf categories. It is best to use as much supervision as possible at all steps. Since we have a lot of diverse data for a leaf category, we train the network to predict the leaf category. You can look at “ Exploring the Limits of Weakly Supervised Pretraining ,” ECCV 2018 to appreciate the power of pretraining a large network and then transferring it to a different task. This weak supervision along with data augmentation (such as crop, flip, rotation) during training helps the network to discount the background and focus more on the important parts of the object that matter, giving rise to a compact semantic signature. These signatures can be easily compared. In the case of binary signature, as in our paper, we can count the number of matched bits based on Hamming distance . Matched items from the selected leaf categories can be ranked based on this similarity. The more matched bits the better. Key challenges we face in large scale visual search for commerce include Variation in image quality and composition Large inventory to compare Inventory is heavily fine-grained Need simple architecture and models for easy maintainability In the following, we summarize tips to build a large-scale visual search system. A lot of these tips could be applicable to non-visual or multimodal systems. 1. Understand the data, and use stratified sampling Data is principal in determining the strategy for the entire procedure. The quality of image can introduce complexities and needs to handled carefully. It is important to understand both the images in the inventory as well as the images typically uploaded by users during visual search. Figure 2 depicts a visualization of a subset of the eBay handbag inventory. It has a mix of complexities based on background as well as attributes of the handbag. Figure 3 highlights another situation where the same object can be captured so many different ways when it comes to camera pose. It is critical to use a diverse and representative set of images when we train our neural network so that these situations can be handled by the neural network. Figure 2. Visualization of images of handbags on eBay using t-SNE. Notice that about 40% of the images have a difficult background, slightly over 10% have a white background. User-uploaded images have even more variations, such as rotation. Figure 3. An eBay item titled “Ferrari Puma~Red Leather Sport Shoes Sneakers~Men’s Size 8,5 (Women’s Size 10).” Same item, very different pictures! The size of the training set is determined by factors such as the number of labels we want to predict, the diversity of data within each label, memory and compute constraints imposed by the training infrastructure, and the time budget to train such a system. When we create the training set for our neural network, we use a stratified sampling over leaf category, sellers, condition, brands, etc., and finally remove duplicates, in order to make sure we capture the richness in diversity of data. 2. Data augmentation is critical, specifically rotation Data augmentation is a critical step in training neural networks when the training data does not capture all variations that are likely to happen in real use case. When a user takes a picture of an object using a mobile phone, it is very likely that the object is not properly zoomed in/out, is cropped out, is rotated, is blurred, etc. Data augmentation (Figure 4) produces variations in data synthetically, especially when these variations are more likely to happen, but are not present in the training set based on eBay listings. It is not very likely that the object is rotated much in eBay listings. However, it is normal in user uploaded images. So, we observed that rotation is an important operation in data augmentation. Rotation is often ignored in data augmentation but is important for visual search. Figure 4: Data augmentation allows us to synthetically create more variations from a single sample. Here is a selected few data augmentation operations. Rotation is often ignored, but is very important. 3. Extract the semantic signature with as much supervision as possible As mentioned in the introductory text, it is very important to use as much supervision as possible. This helps in training the classifier to focus on informational content and discount other non-informational regions. It is best to leverage large, diverse data with low acquisition cost for strong supervision (such as leaf category prediction) when labels are not available for the actual task (measure similarity between pairs of images). 4. Analyze the entropy of signature This step is usually ignored in many system designs for large information retrieval systems. It is critical to assess if we effectively pack information within a given capacity of signature. For example, if we use 8 bits to represent the binary signature, we could represent up to 2 8 unique concepts. In the optimal case, each bit takes the value of 1 with a frequency of 50%. We could calculate the entropy of the system to get the effective bit length and compare with the actual bit length. It is good to allow some slack to account for redundancy in the system, in case some bits are affected by noise and perturbations (Figure 5). Figure 5. Optimal bit occupancy is vital for optimal information representation. This picture is from our paper and corresponds to ImageNet . 84.1% of bits are active (=1) on 45% to 55% of data. It is good to have some redundancy in the system so that it is not exactly 50% for all bits. 5. Within-class variance is important when labels are coarse We use coarse leaf category labels instead of product IDs to train the neural network. This is partly because leaf categories, although coarse, are more readily available. Several items in categories such as clothing and furniture do not have product IDs. Typical classification systems aim for minimal within-class variance. The ideal case is when variance is 0. Here, all samples from a class collapse to a single point (see Figure 6). For example, all samples from athletic shoes will be collapsed to a single point. But, there are numerous unique products that fall under the leaf category “athletic shoes,” and we want to be able to find them using signature similarity. So, we claim that when labels are coarse and fine-grained search is desired, between-class variance should be high, but within-class variance should also be high. This can be measured by looking at the entropy of the signature, as discussed in the previous tip. Figure 6. The recommendation for classifiers is to have small within-class variance and large between-class variance. However, we argue for a better fine-grained search, and it is important to have large between-class variance, but also large within-class variance. Fine-grained matching is not possible when the points in each cluster collapse to a single point. This figure shows five classes from ImageNet, where samples from the same class belong to the same type (equivalent to product), plotted using binary signatures. Within-class variance becomes more important for large scale visual search for commerce, where we use coarse class labels (leaf category) and aim for fine-grained search (product). 6. Process of elimination for speed and precision The process of elimination is very powerful when tailored for high speed and precision. For example, if the input image contains an athletic shoe, there is no point to search inventory for dress, table, computer. We showed in our KDD paper that it is very effective to use a strong classifier to predict the top few potential partitions (leaf categories/aisles) to reduce the search space and also to improve precision (dress signature will not get confused with shoe signature). 7. Absolute vs. cumulative top-k partitions We use a strong classifier, a deep neural network trained using strong leaf category labels, to predict the top partitions (leaf categories/aisles). When confidence of top prediction is high, there is no need to search other partitions. However, when top prediction is uncertain, it is best to include other competing partitions. So, we recommend to use cumulative top-k for better precision and absolute top-k only for those situations where exact match is desired even at higher cost. See Figure 7 for details. Figure 7. Assume that we use top-5 predicted partitions (leaf categories) to narrow down the search space by the process of elimination. We recommend using top-5 based on cumulative score rather than absolute score. Use absolute score only when precision is not as important as recall, such as in the scenario to find the exact match at any cost. You can find details in the KDD paper . We show three scenarios in this figure with a score threshold of 0.9. A cumulative score uses more categories (as shown by gray cells in a, c) when the confidence is low and uses few categories when confidence is high (as shown by gray cells in b). We presented seven tips above. In addition, we recommend minimizing humans in the loop (including for evaluation), retrain models periodically to cope up with shift in data distribution, and keep the number of models to a minimum for easy maintainability. Hopefully, this article gave you some insights into why these tips are important. Please read “ Visual Search at eBay ,” KDD 2017 for more details.", "date": "2018-11-29"},
{"website": "Ebay-Research", "title": "Building a Product Catalog: eBay's 2nd Annual University Machine Learning Competition", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/research/building-a-product-catalog-ebays-2nd-annual-university-machine-learning-competition/", "abstract": "Participating universities will structure listing data to help solve a real-world ecommerce challenge. After last year's success, eBay is once again hosting a machine learning competition on an ecommerce dataset of eBay listings. This challenge is open to college and university students, and the winning team* will be offered a 2021 summer internship with eBay. We invite students to start using our dataset to solve a real-world ecommerce challenge. There are many datasets out there, but the primary focus has been recommender systems, price estimation, computer vision, Natural Language Processing (NLP), and more. None have been at a scale pertaining to mapping unstructured items to well-cataloged products. Like last year, we sincerely hope that making this real-world dataset available will entice students to explore the ecommerce domain further and come up with novel approaches to solve complex problems that can positively impact our platform and services. Problem The question we invite students to address is how to identify two or more listings as being for the same product by putting them into the same group. We call this Product Level Equivalency (PLE). That is, if a buyer purchased two items from two different listings in a single group, and assuming the items were in the same condition, they would assess that they had obtained two instances of the same product. PLE is defined over manufacturer specifications. That is, offer specific details such as condition, or item location are to be ignored. For example, a broken phone and a new phone with the exact same specifications (make, model, color, memory size, etc.) are considered to be Product Level Equivalent, while a golden and a gray phone of otherwise the same make and model are not considered Product Level Equivalent. The objective is thus to produce a clustering of the listings according to PLE. More mathematically, let L be the set of all listings. A clustering C is a partition of L into disjoint subsets: Ideally, all listings in each C i are Product Level Equivalent, and listings from different clusters are not Product Level Equivalent. The measurable objective, evaluation, submission format, and other details are available on EvalAI. Data The data set consists of approximately 1 million selected unlabeled public listings. We also provide an Annexure document that describes the columns and parsing logic. Approximately 25,000 of those listings will be clustered by eBay using human judgment (“true clustering”). These clustered listings will be split into three groups: a) Validation set (approximately 12,500 listings), b) Quiz set (approximately 6,250 listings), c) Test set (approximately 6,250 listings). The validation set is intended for participants to evaluate their approach. Anonymized identifiers and cluster labels will be provided to the participants. We will release the validation set along with the main dataset. The quiz data is used for leaderboard scoring. The test set is used as a factor to determine the winner. For the quiz and the test datasets, neither the listing identifiers nor the cluster labels will be provided to the participants. Hosting The challenge will be hosted on the open-source platform EvalAI. College and university students will submit their entries through EvalAI, which will be evaluated for leaderboard scoring. Please checkout the EvalAI challenge page for more details. Timelines Dates are subject to change, but expected deadlines will be: August 24th, 2020 – Challenge begins. Access to the dataset is granted. We start accepting submissions through EvalAI and begin the evaluations. February 1st, 2021 – Challenge ends. February 22nd, 2021 – We announce winners. Participation Criteria and Prize Teams (no more than 5 members per team) must only include students who are interested in an internship. Assuming eligibility criteria are met, members of the winning team will be offered an internship for Summer 2021 at eBay Inc. eBay’s internship program is a combination of real work experience plus a robust program giving interns exposure to various business verticals, executives and networking opportunities. The internship will also be an excellent opportunity for students to put their ML models into real use. Further details on the participant eligibility criteria, internship prize eligibility criteria, official contest agreement, and rules for the competition, as well as other details, are available as part of the official contest rule package. See eBay Contact details below to receive the official contest rule package. eBay Contact To find out more about how to participate in the challenge and receive the official contest rule package, please reach out to MLChallenge@ebay.com. *Teams should be no more than five members", "date": "2020-08-25"},
{"website": "Ebay-Research", "title": "Building a Product Catalog: eBay's University Machine Learning Competition", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/research/building-a-product-catalog-ebays-university-machine-learning-competition/", "abstract": "Trade has played a critical role in the history of humanity and yet, data from ecommerce, the modern form of trading, has received limited attention from academia. We at eBay want to change that. At eBay, we use state-of-the-art machine learning (ML), statistical modeling and inference, knowledge graphs, and other advanced technologies to solve business problems associated with massive amounts of data, much of which enters our system unstructured, incomplete, and sometimes incorrect. The use cases include query expansion and ranking, image recognition, recommendations, price guidance, fraud detection, machine translation, and more. Though most of the above use cases are common among other technology companies, there is a very distinctive and unique challenge that pertains only to eBay — making sense of more than 1.3 billion listings, of which many are unstructured . Currently, we use our in-house machine learning solutions to approach this problem, but we also want to grow our community and future technologists that haven’t had access to this type of data. By working with universities, we hope that it will pique academic curiosity within ML, spur more research in the ecommerce domain powered by a real-world ecommerce dataset, and help us improve our platform. To support this idea, eBay is hosting a machine learning competition to structure listing data, in other words, producing a product catalog. We are very excited to partner with students at the following universities (list below), which now can start using a subset of our public listing data to help solve a real-world ecommerce challenge. We have more than 40 students from these universities participate as a team or at individual capacity. There are a number of teams competing from: NYU Stanford University at Buffalo The University of Texas at Dallas There are plenty of datasets out there, but the primary focus of those have been recommender systems, price estimation, computer vision, Natural Language Processing (NLP), etc. None have been at a scale pertaining to mapping unstructured items to well-cataloged products. We are using the EvalAI open source platform for hosting the challenge. Our main challenge page has all the relevant details. The question we want to address is how to identify two or more listings as being for the same product by putting them into the same group. We call this Product Level Equivalency (PLE) . That is, if a buyer purchased two items from two different listings in a single group, and assuming the items were in the same condition, they would assess that they had obtained two instances of the same product. The measurable objective, evaluation, submission format, and other details are available on EvalAI . The dataset consists of 1 million selected public data from unlabeled listings. Approximately 25,000 of those listings will be clustered by eBay using human judgment (“true clustering”). These clustered listings will be split into three groups: a) Validation set (approximately 12,500 listings), b) Quiz set (approximately 6,250 listings), c) Final submission set (approximately 6,250 listings). The validation set is intended for participants to evaluate their approach. Anonymized identifiers and cluster labels will be provided to the participants. The quiz data is used for leaderboard scoring. The final submission set is used to determine the winner. For the quiz and the final submission dataset, neither the listing identifiers nor the cluster labels will be provided to the participants. The challenge began on October 11, 2019. The partnered university teams can post their submissions anytime through EvalAI. The evaluation and leaderboard scoring will commence on or about November 8, 2019. The competition will run for about five months and end on or about March 4, 2020. We expect to announce the winning team on March 25, 2020. Students of the winning team will be offered an internship for Summer 2020 at eBay (subject to eligibility verification checks). The 12-week internship will take place at eBay’s San Jose, CA, headquarters and will be fully paid, including furnished summer housing. eBay’s internship program is a combination of real work experience plus a robust program that gives interns exposure to various business verticals, executives, and networking. The internship will also be an excellent opportunity for students to put their ML models into real use. From concept to creation, this challenge was an entirely voluntary effort from people across various disciplines. What started as a hallway conversation eventually ended up into a small group of likeminded enthusiasts. We formed an Operating Committee (OC) and met weekly to brainstorm ideas. Gradually the plans were put into motion, and now we are launching it. It has been an incredible journey, and I was fortunate to be part of the below team that made it happen. Engineering and Research — Roman Maslovskis, Uwe Mayer, Jean-David Ruvini, Anneliese Eisentraut, Akrit Mohapatra, Bennet Barouch, Pavan Vutukuru, Sathish Shanmugam, and Jon Degenhardt Program Management — Roya Foroud Legal — Brian Haslam, Brad Sanders, Sonia Valdez, and Kai Weingarten Recruitment — Cindy Loggins Comms — Melissa Ojeda We would also like to thank the EvalAI team for quickly responding to our numerous queries. And finally a shoutout to our senior leadership ( Mohan Patt and Ron Knapp ), who have been supporting this idea from the get-go. We sincerely hope that making this real-world dataset available will entice universities and students to explore the ecommerce domain further and come up with novel approaches to solve complex problems that can have a positive impact on customers and sellers alike. If you are a university student, researcher, or professor and would like to participate in future programs, please feel free to reach out to us.", "date": "2019-10-16"},
{"website": "Ebay-Research", "title": "Towards Agile AI", "author": ["Jean-David Ruvini"], "link": "https://tech.ebayinc.com/research/towards-agile-ai/", "abstract": "In this article, we propose a set of better practices, designed by and for eBay ML scientists, for facilitating weaving ML modeling into the cyclical Agile process flow. As artificial intelligence becomes more prevalent in our daily lives, machine learning (ML), its core enabler, is consuming a greater share of software development efforts across industries. Therefore, more machine learning tools, methods, and products are developed using the principles, processes, and tools of Agile methodologies like scrum, kanban, and lean. However, ML modeling (the tasks involved with identifying and implementing an appropriate machine learning algorithm; selecting the data, metrics, training; tuning the features and algorithm; and then producing the target model) is often conducted by data scientists who are not familiar with software engineering or Agile approaches and who have difficulty harmonizing their research activity with Agile project management, time boxes, or engineering configuration management. This article proposes a set of better practices, designed by and for eBay ML scientists, for facilitating weaving ML modeling into the cyclical Agile process flow. One important element of the Agile methodology is the Definition of Done (DoD) for “shippability” and keeping each modicum of incremental work shippable at all times. The DoD is a list of requirements, or acceptance criteria, to which software must always adhere in order to be called complete and accepted by an end user customer, team, or consuming system. However, standard acceptance criteria, such as unit test coverage, code reviewed, or functional tests passed, are inappropriate to ensure ML modeling quality, as they fail to address essential success criteria of the modeling task. Indeed, the absence of quantifiable requirements in modeling quite often leads to misunderstandings, sometimes to frustration (on all sides), when for example an engineering scrum master asks a data scientist “when will you complete your research?” We argue that well-known ML best practices can be very helpful in enabling Agile modeling if specified as requirements from the very beginning of a project and repeated throughout the entire life cycle of an ML model, from problem definition all the way through deployment to production, maintenance, refactoring, and end-of-life. More precisely, we believe that specifying and agreeing upfront on requirements elicits a discussion around their achievability which, in turn, naturally leads to an iterative mindset, a core tenet of Agile. Figure 1 highlights six important phases of ML modeling and their acceptance criteria. The rest of this article describes these requirements in more detail. Figure 1 . The six phases of ML modeling and their acceptance criteria. Before you start. At the beginning of an AI project, before any technical work has started, it is critical to get clarity on the business or technical problem for which the ML model will be applied and how the accuracy of the model’s predictions relate to the overall objective. More precisely, we have observed that answering the following questions before a sprint starts helps communicate precise qualitative requirements for the performance of the model: What business problem are you trying to solve? For which business measurement are you optimizing? Increased net revenue? Increased transaction rate? Increased market share in a different category? Acquiring new profitable, high-spending buyers or frequent shoppers? What are your scientific evaluation criteria? How does your scientific optimization correlate with your business optimization? What is your baseline? What is the industry baseline? What is the minimum viable performance you must achieve to declare this iteration of your model a success at the end of a time box? The answers to the last two questions are particularly critical. Understanding how the scientific metrics correlate with business metrics allows you to quantify the return on investment (ROI) of each measured increment of improvement of the scientific metrics. What would be the business impact of a model with an “accuracy” of 80%? 90%? 95%? Clarifying the current baseline helps define the minimum success bar. In a competitive market, it is important to understand how other companies perform compared to your current feature or service. If you are iterating on an existing model, you need to clarify how much better the new model must perform. If not, you must still quantify the minimum performance needed to reach a satisfactory level for success of your effort. While it is obvious that data quality is paramount for ML modeling, two aspects of data preparation are often overlooked: how the data is sampled and how it is split into training, validation, and test. Data sampling: don’t forget the body and the tail. Critically, the data used for training, tuning, and evaluating an ML model should be as close as possible to the production data and its distribution. In particular, attention must be paid to the head, body, and tail of the distribution of interest. Evaluating a model only on the head of the distribution is a common pitfall of ML modeling. Note however that some scenarios (unbalanced classes) require re-sampling the training data and purposefully training the model on a different distribution. Furthermore, evaluating a model on old test data should be avoided as they run the risk of rewarding old models for being outdated and punishing newer model for being current. And of course, seasonality and other time series patterns in data should accounted for when sampling data. Data splitting: no déjà vu! Any ML scientist knows that training, validation, and test data should not overlap in order to ensure a reliable estimation of the performance of the model on future unseen data. However, it is sometimes overlooked that real life data may contain identical or near duplicate samples. While this may be due to the nature of the underlying distribution governing the data, this deserves special attention to make sure that duplicate samples are not dominating the validation and test data and are not biasing the estimation of the performance of the model. To summarize, the following two questions must be addressed in the data preparation phase: Did you sample separately from the head, body, and tail of the distribution so that you can evaluate your model on each of these? Does your training data overlap with validation data or test data? Use industry standards! While the target metrics should have been identified in the Problem Definition phase, it is important to crystalize them before starting the training and evaluation phase, for two reasons. First, to ensure that industry standards are used. Table 1 lists some of the most commonly used ML metrics. While it is sometimes justified to create a new metric, standard metrics can be effectively used in a wide range of settings. Second, to ensure that the metrics used to evaluate the model and the loss function used to train it are consistent. In summary, the requirements for the metrics definition phase can be formulated as: Are you using industry standards? Are your metrics and your loss function consistent? Accuracy, Precision and Recall, F1, ROC curves, Precision and Recall curves. Regression Root Mean Squared Error,  Maximum Absolute Error Probability Distribution Estimation Log loss scores such as Negative Log Likelihood, Cross Entropy, KL Divergence. Ranking nDCG, DCG, AUC, Kendal Tau, Precision @k for various low values of k Language generation BLEU, TER, WER, ROUGE Table 1. Some of the most common machine learning success measurements. The training phase of an ML model preparation is mostly about hyperparameter tuning, the task of identifying the parameters of the learning algorithm that result in the best model. Hyperparameters should be tuned on the validation data only, not on the test data. Ideally, the test data should be used only once, to confirm that the model provides consistent performance on unseen data. There are two frequent reasons that performance is inconsistent. The most common cause is overfitting the training and validation data, which can be prevented using well-known techniques, such as removing features, adding training data, early stopping, etc. The second-most common cause is that the test data and the validation data are not within the same distribution, and one of them is not representative of production data. In the latter case, the data preparation phase must be revisited. Note that if error analysis is performed using the test data, the test data must be discarded (or added to the training or validation sets) and a new set should be generated to avoid overfitting the test data. In all cases, having a good synthetic data generator or a frequent feed of redacted production data for testing are invaluable. The requirements of the training phase can be summarized with one question: Did you tune your hyperparameters on the validation set only? It’s all about the baseline. We highlighted in the Problem Definition section the importance of identifying the strongest possible baseline. Of course, beating the baseline and achieving minimum viable performance is a requirement of the Evaluation phase. However, aiming initially at modest improvements over the baseline and iteratively publishing shippable models through successive refinement, is an invaluable and key benefit of the Agile methodology. Statistical significance. If the improvement over the baseline is small, statistical significance testing should be performed to ensure that the apparently superior performance of the model is not due to chance or noise in the data and is likely to be observed in production on unseen data. Student’s t-test, Welch’s t-test, and the Mann-Whitney U test are examples of well-known tests for regression; McNemar’s test and the Stuart-Maxwell test for classification. Confidence Score: “a reasonable probability is the only certainty” (E.W. Howe). If it is required that your model outputs a confidence score for its prediction, it is important to ensure that the score is a well-calibrated probability that means that the confidence score matches the true correctness likelihood. This consists of ensuring that when the model is p% confident about its prediction (say 80% confident), it is actually correct p% of the time. Confidence scores can be calibrated using a validation set. Don’t forget operating constraints. Finally, it is critical to ensure that the model also meets operating requirements early on. Examples of operating constraints include inference latency, throughput and availability, expected CPU, GPU, TPU, memory, SSD, HDD, and network bandwidth. To summarize, the following requirements must be met in the Evaluation phase: Do you exceed your baseline and reach minimum viable performance? Is your result statistically significant? Is your confidence score well calibrated? Do you meet the operating constraints? “ML models need love, too” (J. Kobielus 1 ). The task of model building does not end with a specific model being handed over for production deployment. It is important to establish and enforce a maintenance plan that ensures pro-active refresh of the model (as opposed to waiting until some metrics go down). Besides, formalizing such a plan forces a dialog between the modeling Agile team and the engineering Agile team and facilitates weaving modeling into the Agile work process. Before handing a model to production, the following questions must be answered: How will you monitor performances? How often will you retrain the model? Good times come and go but good documentation is forever! While the Agile manifesto favors “working software over comprehensive documentation,” ML modeling is not as self explanatory or reproducible as standard code and needs to be documented appropriately. In particular, we believe that the following should be archived and documented: The code used to sample the data (training, validation, and test). How to reproduce and operate the model. The test data. Hopefully, we have convinced you that specifying upfront clear and quantifiable requirements for each phase of the ML modeling process fosters model quality, quick iterations, better communication, and closer collaboration between the ML scientists and their partners, namely the business and the Agile engineering team responsible for building the inferencing engine and deploying the model in production. We intentionally kept these requirements simple and actionable, in the spirit of the Agile manifesto to favor “individuals and interactions over processes and tools.” However, acceptance criteria are just one of the tools that the Agile methodologies advocate. And if you have experience with extending some of these tools to ML modeling or data science, we would love to hear from you! Acknowledgements The author would like to thank all the co-workers that have been involved in the design of these best practices: Alex Cozzi, John Drakopoulos, Giri Iyengar, Alan Lu, Selcuk Kopru, Sriganesh Madhvanath, Robinson Piramuthu, Ashok Ramani, and Mitch Wyle. Special thanks to Robinson Piramuthu and Mitch Wyle for their careful review of the draft of this article. 1 J. Kobielus. “Machine learning models need love, too”. https://www.infoworld.com/article/3029667/machine-learning-models-need-love-too.html", "date": "2019-10-30"},
{"website": "Ebay-Research", "title": "Congruent Numbers Part I", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/congruent-numbers-part-i/", "abstract": "Usually my posts have a connection to eBay, but this time I’m writing about a recreational math problem that caught my attention. One of the most famous facts of mathematics is that a triangle with sides of length 3, 4, and 5 is a right triangle, which means it can be drawn with a horizontal base and a vertical side, forming a right angle. Since the area of a triangle is one-half base times height, this triangle has area . Is there a right triangle with area 5? Of course there is, since a right triangle with a base of length 1 and height 10 has area 5. But the third side (the hypotenuse) has length , which is not an integer. If you require all three sides of a triangle to be integers, then there is no right triangle with area 5. A brute-force enumeration verifies this, and easily shows that 6 is the smallest, and the next few after 6 that are the area of a right triangle are 24, 30, 54, 60. So asking if there is a right triangle with integer area appears to be of little interest. But what if you allow rational sides? It’s possible for the height and width to be non-integer fractions, but have integer area. And in fact there is such a triangle with area . The equation shows that a triangle with sides , and is a right triangle. The area is . An integer that is the area of a right triangle having all sides of rational length is called congruent. What numbers are congruent? Detecting them has something in common with detecting primes. There is a fast test to see if a number is prime or composite, but it’s a lot harder to obtain direct evidence that a number is composite by producing its factors. Similarly, there is a fast test to see if a number is congruent, but it is hard to find the side lengths , , that demonstrate directly that is congruent. The reason it can be hard to find the sides is that they can be fractions with a lot of digits. For example, the sides of a right triangle with area are I found tables on several web sites containing the side lengths for all congruent numbers less than 1000, but I believe they are all copies of the same table, because they all have an identical glitch at which I will explain shortly. In this posting and the next I will explain my efforts to compute the sides of a triangle with area . I got interested in this problem because I wondered how it was done. I wanted to compute sides for . I wanted to check if the existing table can be improved. I found the existing table can indeed be improved, as I now explain. The rational sides demonstrating that is congruent are not unique. For example, 6 is the area of a triangle with sides (3,4,5) and also . I’ll always try to find the example with the smallest denominators. So I prefer (3,4,5) (denominator of 1) to (denominator of 70). After I wrote my own code to generate a table of side lengths, I discovered that the web table doesn’t always give sides with the smallest denominator. Two examples are below, where the triangle sides are , , . How big (how many digits) are the numerator and denominator of the sides? I used the web table to plot the number of digits needed for the denominator of the length of the hypotenuse against the area . The plot is below. The vertical axis is which is the number of base-10 digits in the denominator . Note that prime values of (blue) generally require more digits than composite numbers do. The straight line suggests that the number of digits needed for the denominator increases linearly with . In other words, the size of is exponential in . In this section I explain in detail a search method for finding the sides for the triangle of area that can easily find the fractions for , which are A brute-force exhaustive search for would find the fractions and by trying all with integers , having up to digits (and similarly for ). That is possibilities, where . This search would take a long time, . There are some simple tricks that make it much faster to find the fraction by brute force. Going back to sides for the congruent number , Rewrite the equation to have a common denominator. The numerators satisfy A triple of such integers is called a pythagorean triple. This link between congruent numbers and Pythagorean triples holds in general, as summarized below: So from rational numbers , , and you get integers and that are Pythagorean, meaning is a square. I’ll always assume that any factor common to all of , and has been removed, in other words that . Also note that if is congruent, then multiplying the sides of its triangle by the integer shows that is congruent. And conversely, if you have a triangle showing that is congruent, divide the sides by to get a triangle for . So I only need to consider square-free . Here are the first few square-free congruent numbers. Note that in each case the denominator of is the product of the denominators of and . A proof that this is always so is in the final post of this three-part series. Pythagorean triples can be written in the form (1) The proof of this can be found on the web and in most number theory textbooks under the name Euclid’s Formula, but I also give a proof in the final post of this series. The formulas can be rewritten as (2) So there is a 1-1 correspondence between , , and , . Specifically, starting with , , you rewrite with a common denominator to get , , and then use ( 2 ) to get , . Conversely, given and use ( 1 ) to compute , , , and then get using (3) Finally, reduce to get , similarly for . This means that a brute-force search can be done using only two integers and (with instead of four integers , , , and . What is their size? If etc. have digits, then etc. have digits and , have digits, so brute force is now . Huge but smaller than . Here is a table of the first few square-free congruent numbers represented as and : Note that and are always of opposite parity (that is, one is odd and the other even). As I’ll explain in the final post, once you remove any common factors from then and must be of opposite parity. This is the glitch in the web table I mentioned earlier. For , the web table gives , , which are both odd. If you use them to generate you will find they all have a common factor of 2. If you remove this factor and recompute , you get , with . A major improvement in exhaustive search can be made by noting the special form of and . You can see the pattern in the following table of and , where they are written in partially factored form. For entries in the table, is a square, or a square times a divisor of , and similarly for . As shows, both and can have divisors. Stated more formally, and where . Since this is true for all numbers in the table above, it’s plausible that it’s true in general, and I’ll show that in the final post. This property means you only need to search up to the square root of and . In other words, you only need to check values of and similarly for , so exhaustive search takes steps when . This  can be done in less than a minute, at least on my laptop. I won’t bother to give the code, because although it easily computes the sides for it can’t get the side lengths for more challenging areas such as . In the next post I’ll explain an improved algorithm using elliptic curves and give code for the SageMath system that can solve . Powered by QuickLaTeX", "date": "2016-01-28"},
{"website": "Ebay-Research", "title": "eBay’s New Approach to Managing a Vast Service Architecture", "author": ["Hanzhang Wang", "Chirag Shah", "Sanjeev Katariya"], "link": "https://tech.ebayinc.com/research/ebays-new-approach-to-managing-a-vast-service-architecture/", "abstract": "Learn how eBay's architecture knowledge graph was developed; the benefits eBay has received from it; and the use cases we see now and in the future for this approach. Governing and understanding the vast ecosystem in a service architecture is challenging – and with over 3,000 service application clusters in its production system, this is particularly true for eBay. Each application evolves independently with different features and development methods. Efficient development can be inhibited by lack of documentation and not having proper knowledge about internal customers. eBay’s vision – known as INAR, Intelligent Architecture – is to build sustainable service architecture by providing automated visibility, assessment, and governance Intelligence. In this pursuit, we developed a new approach to model and process the application ecosystem using a knowledge graph. A knowledge graph is a commonly used term whose exact definition is widely debated. Basically, a knowledge graph is a programmable way to model a knowledge domain using subject matter experts, interlinked data, and machine-learning algorithms. For eBay, the application/infrastructure knowledge graph is a heterogeneous property graph that improves architectural visibility, operational efficiency and developer productivity, eventually allowing customers to have a better experience when visiting the site. This article will explain how the eBay architecture knowledge graph was developed; the benefits eBay has received from it; and the use cases we see now and in the future for this approach. The Intelligent Architecture vision is aimed at addressing three key challenges of a service architecture: Blindness : It can be difficult to observe architectural issues such as inappropriate dependencies for software and/or hardware; or to envision the eBay infrastructure and ecosystem with customized search. This is an issue because popular software and services evolve frequently and become monolithic, resulting in redundant services and duplicated functions. Ignorance : Lack of measurability for service architecture or technical debts (additional rework that is required when you take an easier upfront approach that is worse in the long run) can prevent you from developing the metrics you need to improve operational efficiency. As business management guru Peter Drucker famously said, “If you can’t measure it, you can’t improve it.” Primitiveness : Diagnostic, engineering and run-time automation is not present. Consequently, artificial intelligence cannot be applied for IT operations, making it difficult to detect anomalies in operations. It was apparent we needed a clearer understanding of our ecosystem if we were going to serve the needs of our 183 million buyers. Our goal was to provide better visibility, provide pattern/anomaly detection, and automate and enhance IT operations. That led us to the idea of using a knowledge/property graph. The graph was constructed using real-time metrics, business features, and operational metadata. Ultimately, the purpose of this graph is to connect data sources and break the boundaries between isolated management domains. Here is a depiction at a high level: One of the first steps in developing a knowledge graph is to calculate the best application metrics and applied machine learning algorithms to automatically cluster the applications. We developed metrics that measured the popularity of applications based on real-time traffic flows and run-time dependencies. We calculated metrics for all eBay clusters and used techniques called K-means and canopy clustering to cluster all services and based on their popularity scores. This allowed us to organize the ecosystem into different categories, such as how active they are. We discovered that 77% of the clusters are labeled as low-activity. One of our goals for using a knowledge graph was to improve developer productivity and enable them to retrieve the information they needed more efficiently. Currently, developers have to go through many tools to receive the information they need. To improve productivity, we built a complete batching system which fetches data from different sources and builds a knowledge graph automatically. We also built an intelligent graph search that dynamically generates a query to explore the knowledge graph, including service metrics and intelligent layering. The following data schema was designed at application(pool)-level, and the boxes with bold or black borders are enabled as the very first “baby” step: By connecting cloud-native data, hardware, people, code and business, we gained better visibility of the ecosystem. The visualization provides rich information in a way that can be quickly understood and acted upon. In the following service dependency example: we randomly picked 18 services and visualize them by one of the default methods. The edge thickness represents edge properties (volumes). Node size represents the behavior metrics. The different colors represent teams or organizations (yellow, for example, is one domain team). The POC is adopted by the eBay dependency system “Galaxies” and now, the graph schema is extended as follows: We calculated metrics and intelligent service laying in more than 3,000 eBay production clusters. Three senior architects manually validated the initial results of the popularity metrics and automatic clustering. The results were surprising and informative. About 10% of the high-activity applications are running under an incorrect availability zone, which can impact operational performance and uptime. For eBay, the knowledge graph has become an important tool(galaxies) that allows us to provide customizable visualization, application metrics, intelligent layering, and graph search. The system provides top-down and bottom-up view of the application, along with the dependencies and increased accuracy; enrich data to enforce application compliance; governance with clear ownership details; and operational performance recommendations. Moving forward, we plan to enhance the graph to support site anomaly detection (an initial work ) by presenting suspected events on the graph with full causality details of each incident. We also plan to extend this graph to include service API metadata, which will enable service layering, recommendation and clustering. The knowledge graph promises to become a critical tool for understanding our ecosystem and meeting customers’ expectations for continually faster and better service.", "date": "2019-11-15"},
{"website": "Ebay-Research", "title": "Congruent Numbers Part II", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/congruent-numbers-part-ii-2/", "abstract": "The method of my first post is too slow to find the side lengths for a triangle of area , which has many digits: In the 1984 book by Koblitz, Introduction to Elliptic Curves and Modular Forms , computation of the side lengths for is credited to the mathematician Don Zagier. Now, however, it can be computed quickly by anyone with access to the SageMath system. The Python code given below computes the values for in less than one minute on my Mac laptop. This post has three parts: I derive equations that produce triangle side lengths for congruent numbers; I give Python code that implements the equations; and I present two tables of congruent numbers. The first table is for , and is a variant of the web table. The second is an incomplete table for constructed using the Python code. Exhaustive search is too slow to find the sides of the triangle with area ; finding them requires a new idea. That idea is to reduce the search to a problem in elliptic curves (the same idea used to prove Fermat’s Last Theorem!). An elliptic curve is the next level of complexity after a quadratic curve. A quadratic curve is ; an elliptic curve is . For the congruent number problem, Koblitz’s book Introduction to Elliptic Curves and Modular Forms shows the connection between a congruent and the elliptic curve . I summarize it here. The connection involves adding points. For a conic section like a circle each point can be represented by , and two points , with angles , can be added to get a third point with angle . In a formula with a similar formula for . So the formula for is a rational function of , where is our special addition, not vector addition. In the same way, two points on an elliptic curve can be added, and there is a formula for the sum. If and then the formula for (I don’t need ) is If is a point of this simplifies to In that equation , and are rational numbers. I need a formula in terms of their numerators and denominators. So write (1) Then Next, use Koblitz page 47, which says that if is the sum of a point on with itself, then you get rational sides with area for sides , with so Or since , This gives an algorithm that can find the sides’ lengths for area , and is fast enough to work for . Use SageMath to find a point on . Next use equation ( 1 ) to define , , , and the equation after that to get and . And use the equations directly above to get the side lengths and . Here is the code that implements the algorithm of the previous section. Here is a table with the side lengths of congruent numbers. Like the web tables, it gives and rather than the side lengths , , but I give and in factored form. In addition it differs from the web table at , , , and . The changes at and give fractions with a smaller denominator. The change at fixes the glitch wherein both and are both odd. The change at adds a second solution with the same denominator. I remarked before that there are multiple values of , (or equivalently , , ) for each , and that I would pick the one with the smallest . This doesn’t always select a unique triangle. For example has both and , both with . Other such examples are And now, here’s the table. Also available as a text file. One of the motivations for writing the code in this posting was to try computing the side lengths for . Of the 358 square-free congruent numbers between 1000 and 2000, the code was able to find sides for all but 50. I’d be thrilled to hear about more sophisticated algorithms that can compute these missing entries.  Here are the entries I was able to compute, also available as a separate file. Powered by QuickLaTeX", "date": "2016-02-10"},
{"website": "Ebay-Research", "title": "Congruent Numbers Part III", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/congruent-numbers-part-iii/", "abstract": "Before demonstrating the claims made in Part I and Part II on this topic, let me mention two simple facts about integer solutions to . First, if some number divides both and , it also clearly divides . So and have a common factor if and only if , , and do. Or saying it another way, and are relatively prime if and only if , , and are. From now on I always assume that this is true for , , and . The second fact is that and have opposite parity, that is, one is even and the other odd. It’s clear that and can’t both be even, since then they would have a common factor of 2. To see that and can’t both be odd, note that the square of an odd number is so the square of an odd number is congruent to 1 modulo 4. This means that if and are both odd, the left side of is congruent to 2 modulo 4. But then the right-hand side is even, so is of the form and is congruent to 0 modulo 4, contradiction. With these two facts out of the way, let me show that a Pythagorean triple is always of the form (1) It is easy to check that if , and can be written this way in terms of and then . But where does the formula come from, and why are there always such and ? A Pythagorean triple can be thought of as a point on a unit circle: (2) Each point corresponds to a single point on the -axis, as in the left half of the figure below. The slanted line connects to a point on the -axis. The key fact is that each pair corresponds to a single number . And is rational if and only if is rational. This can be seen from explicit formulas linking and . To get those formulas consult the right half of the figure where the triangle with sides , is proportional to the triangle with sides . So The second equation shows that if and are rational then so is . And the last two equations show the converse. But they show more. Writing in reduced form (i.e. and relatively prime) and combining with ( 2 ) shows Equating numerators suggests that and . But there is a subtlety: this is guaranteed only if and are relatively prime. For example , but there are no and with and . The small print below gives two ways to turn the suggestion into a proof. To see that the relative primality of and implies the relative primality , suppose that a complex integer divides both of . If is a real integer, then it divides both and , contradicting their relative primality. Similarly if is purely imaginary. That means I can assume that . By conjugating, it follows that and so both and divide . Since these are distinct, the real integer divides which is again a contradiction. The final detail is that although , they could be associates, that is . But that would imply so which after taking norms becomes , and that contradicts the statement from the beginning of this post that and are of opposite parity. Formula ( 1 ) gives an easy way to show why there isn’t a triangle with integer sides and area 5. If then that formula shows that there is a pair , with and When this has no integer solution because three of the factors must be 1, the other 5. Since is the largest of the four factors, it would have to be 5 and , which is clearly impossible. Since the numbers , , , and are used to compute the sides , and , you can remove any common factors if necessary since that won’t change the side lengths , , . So I will always assume that , or using a common shorthand . Proposition: If represents a square-free congruent triangle and , then . Proof: By contradiction. Suppose . Then some prime divides . First suppose so but . Then , , and are integers so . It follows that one of or is even. This means that one of , is divisible by 4, the other by 2, so . Since It follows that , but and is square free. This is a contradiction, so . Similarly, if is an odd prime, and divides , , and but not , then , contradiction. So . I use this to give a shortcut in the exhaustive search over and . You only need to consider a subset. In the lemma below, the parity of is , i.e. odd or even. Lemma: For a square-free congruent triangle, if then and , are of opposite parity. And such pair of generates a triangle with . Proof: First assume . If and have the same parity, then from equation ( 1 ) all of , and are even, contradicting the proposition. If and have a common factor, so do , and , so that is not possible either. For the other direction, note that since and have different parities, then is odd and is even so . Thus you only need to consider an odd prime . If then so so and similarly . It follows that in exhaustive search, you only need to consider , that are relatively prime and of opposite parities. Note: this is the glitch in the web table I mentioned earlier. For , the web table gives , , which are both odd. If you use them to generate , remove the common factor of 2, and then recompute , you get , with . Next, I’ll verify the important speedup, which is that and are almost squares. Proposition: If , are relatively prime and generate a square-free congruent number, then and where (in particular, and ). Proof: Combine equation ( 1 ) and to get If a prime has then or . If the latter, but since , is impossible, and so is impossible so . Therefore every factor of is either a square or a factor of . It follows that where each factor of divides . If any factor of is a square then incorporate it into after which . Similarly . Since , so . We observed earlier that in our tables, the denominator of times the denominator of equals the denominator of . Here is the proof that it is true in general. Repeating the equations: Even though , and can have a common factor in which case and . Lemma: With the usual assumption that , then . Proof: First check that so that . If not, then there is a with and . From and (3) it follows that (or perhaps ). But and implies , contradiction. Next from ( 3 ) every prime factor of must divide either or . Start with and . Then consider the prime factors of one-by-one. For each factor , remove it from either the numerator and denominator of or from . After the removal process becomes and becomes and since and are completely reduced, , and so . Since each factor of was removed, so and thus . Finally, write The last equation shows that . If not, then it writes in a more reduced form, contradicting the first part of the proof. I previously showed so it follows that . And the first part of the proof showed that . Powered by QuickLaTeX", "date": "2016-03-03"},
{"website": "Ebay-Research", "title": "A Human-centric Approach for Evaluating Visual Search Models", "author": ["Michal Romi", "Michael Ebin", "Chantal Acacio"], "link": "https://tech.ebayinc.com/research/a-human-centric-approach-for-evaluating-visual-search-modelsnew-blog-post/", "abstract": "Part of our mission within Core AI at eBay is to develop computer vision models that will power innovative and compelling customer experiences. But how can we compare several visual search models and say which of them works better? This article will describe a method that is tackling this problem directly from the eyes of the users. Since our main objective is to create compelling customer experiences, this article will describe a method that is tackling this problem directly from the eyes of the users. We are using a fixed set of n randomly selected user loaded images that will serve as our query images for both models during this evaluation. These images were not part of the training set that consists of eBay’s active listings, but are reflective of the true images our buyers use to search for eBay products. For each query (i.e. anchor image) we call a model, obtain the top 10 results per anchor image, and then collect 10X n images per model output for our evaluation dataset. Once we have the evaluation dataset, we upload these images to FigureEight (i.e. crowdflower), a crowd tagging platform that we use to collect responses on how well the output of a model compares to the anchor image given (see Figure 1). Figure 1. FigureEight demo. Since images are extremely subjective to evaluate, we decided to incorporate dynamic judgments in order to establish a confidence score for every pair of questioned images. We start by asking three people the same question and reviewing their responses. If they all answer the same, we keep this answer. If they answer differently, we will ask two more people (totaling up to five) to ensure a high confidence of this response. Our evaluators are also being tested while answering these questions. There are test questions, handpicked by our team, that every evaluator must go through in order to qualify as a valid labeler. Their accuracy on these test questions will be linked to their trust score . They must score at least a 70% on the test in order to be accepted to complete this task. In addition to the pre-test, there are test questions distributed throughout the task that could result in their trust score falling below our designated threshold of 0.7, which would result in these labelers being removed from the task. The overall confidence score per each answer is calculated by the level of agreement between labelers and their assigned level of trust. For example, if there were two types of answers selected for the same question, we will take the answer that has a higher confidence score overall.  Only questions that have a confidence greater than or equal to 70% are being evaluated (see Figure 2). Figure 2. Confidence score This process is done in order to obtain a score per each of the models we are evaluating so we can do a fair evaluation between them and decide which one users might prefer. We are using DCG (Discounted Cumulative Gain), which is a standard metric for ranked results (see Figure 3). Figure 3. Discounted cumulative gain. The weights we are using are described in the following table. Once we have all the answers from the crowd, we can assign the relevant numbers to the formula and accumulate the total score per each model. A model with a higher score means a model that produced more relevant search results per this 10X n evaluation set and thus will be the chosen one.", "date": "2020-03-23"},
{"website": "Ebay-Research", "title": "Collocations: The Secret Web of Language", "author": ["Jose Sanchez"], "link": "https://tech.ebayinc.com/research/collocations-the-secret-web-of-language/", "abstract": "Imagine this. You are a beginning English learner. You enjoy the methodical approach, so you tackle the language systematically, memorizing lists of irregular verbs, spelling norms, and syntactic rules. No conversational practice, no watching movies. You want to get the theory right first. One day, you think you have mastered it. You are a walking grammar book. You after 6 months of English studies Then you realize you have been so engrossed in your studies that you skipped lunch, so you ask a passer-by: Excuse me, sir. I am heavily hungry . Could you point me to the nearest swift-food restaurant? Which he greets with a baffled stare. What went wrong? You studied the standard rules of English, but there is a part of the language (of any language) that will never fit in that tidy set of axioms — collocations . Collocations — a vast n-gram web that connects all words in a language. According to the Oxford English Dictionary , collocations are pairs or groups of words that are habitually juxtaposed, such as strong coffee or heavy drinker . As such, they are the final touch foreign learners (or say, machine translation systems) need to “speak properly.” You can communicate without knowing them, but you will sound pretty weird to the native ear. In a wider sense, collocations are a sort of lightweight idioms , but they differ from idioms in a couple of ways: Their meaning is transparent — you can guess it the first time you see them (which you can’t with proper, metaphorical idioms, such as kick the bucket or a piece of cake ). There are vastly more collocations than idioms. (The largest explanatory collocation dictionary in existence covers only 1% of all possible vocabulary.) Most importantly, collocations don’t follow clear rules. There is no apparent reason why we should say a burning thirst and not a blazing thirst , except that most people say the former and not the latter. In a way, these whimsical word patterns are like an unexplored realm at the edges of grammar — a lush rainforest with all sorts of curious and apparently random species. At the edge of the forest, the human language learner and the MT system both face the same problem — how to chart it? Playing Linnaeus to the human language “biosphere” is no trivial task, but fortunately there is help — massive computational power applied to vast sets of texts (linguistic corpora) is producing resources for us all: Humans: Collocation (aka Combinatory) Dictionaries, some of them online: The Online Oxford Collocation Dictionary (also with a convenient app ) The Diccionario de Colocaciones del Español Zanichelli´s Dizionario delle collocazioni The French-Canadian Dictionnaire des Cooccurrences The Online Oxford Collocation Dictionary (also with a convenient app ) The Diccionario de Colocaciones del Español Zanichelli´s Dizionario delle collocazioni The French-Canadian Dictionnaire des Cooccurrences Machine Translation Systems: Statistical Language Models , that is, tables assigning a likelihood score to sets of words based on their frequency. MT Systems usually produce several “candidate” translations for a given source. Each of them is checked against the Language Model table, and the one with the highest score (the most frequent expression in the language) is used. The work with collocations is far from over. For MT, the challenge is finding enough corpora. (Except for a few — such as English, French, and Spanish — most languages don’t have enough online texts to create accurate models.) For human learners, there is the additional problem of analyzing and describing the vast amount of data in terms useful to the language student. The good news is that here, as in other areas, human linguists and MT systems can leverage each other’s efforts. Every new language model provides helpful data that can be used by the next generation of dictionaries, and every dictionary throws new light on the relationship patterns between words that MT can incorporate. Meanwhile, language students will do well heading to the pub every once in a while for some conversational practice. Want to learn more? Go academic — Check out Mike Dillinger’s and Brigitte Orliac’s paper on Collocation extraction for MT . Go deeper — Learn about collocations’ extended family: the phrasemes . Go pro — Find your own collocations: Juan Rowda’s article on MT tools (the “Concordance Tool” section) tells you how. And if you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2016-04-25"},
{"website": "Ebay-Research", "title": "The Biggest Dictionary in the World", "author": ["Jose Sanchez"], "link": "https://tech.ebayinc.com/research/the-biggest-dictionary-in-the-world/", "abstract": "By now, we are all familiar with the use of machine translation to produce versions of a text in different languages. Did you know, though, that scientists are also working on producing a massive multilingual dictionary with minimal human intervention? And when I say massive, I mean over 9,000 language varieties – in other words, all the languages in the world . The project, named Panlex , started in 2004, and there is nothing comparable to it. Google Translate , for instance, covers a meager 103 languages. So how does Panlex work? The math is complicated, but the logic isn’t. Example: Let’s say you want to translate the Swahili word ‘nyumba’ (house) into Kyrgyz (a Central Asian language with about 3 million speakers). You are unlikely to find a Swahili–Kyrgyz dictionary; if you look up ‘nyumba’ in PanLex, you’ll find that even among its >1 billion direct (attested) translations, there isn’t any from this Swahili word into Kyrgyz. So you ask PanLex for indirect translations via other languages. PanLex reveals translations of ‘nyumba’ that, in turn, have four different Kyrgyz equivalents. Of these, three (‘башкы уяча’, ‘үй барак’, and ‘байт’) have only one or two links to ‘nyumba’. But a fourth, ‘үй’, is linked to it by 45 different intermediate language translations. You look them over and conclude that ‘үй’ is the most credible answer. One of the beauties of this system is that it works as a semantic web, with no central hub. Most multilingual projects (including MT) have to rely on a pivot language (often English) in order to harvest translations between two minority ones (say, Basque and Quechua ). With the Panlex model, this is not necessary; translations are validated without having to go through the English “funnel.” I like this example of technology working for the little (language) guy. And there is more. Panlex’s sponsor, The Long Now Foundation , has some other interesting stuff in the works, like the 10,000 Year Clock or the Rosetta Project , all centered on making us take the really long-term view on the world. If you are local to the San Francisco Bay Area, you are in luck — you can explore their events at the Blog of the Long Now . And if you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2016-05-03"},
{"website": "Ebay-Research", "title": "A Surprising Pitfall of Human Judgement and How to Correct It", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/a-surprising-pitfall-of-human-judgement-and-how-to-correct-it/", "abstract": "Algorithms based on machine learning, deep learning, and AI are in the news these days. Evaluating the quality of these algorithms is usually done using human judgment. For example, if an algorithm claims to detect whether an image contains a pet, the claim can be checked by selecting a sample of images, using human judges to detect if there is a pet, and then comparing this to the results to the algorithm. This post discusses a pitfall in using human judgment that has been mostly overlooked until now. In real life, human judges are imperfect. This is especially true if the judges are crowdsourced. This is not a new observation. Many proposals to process raw judgment scores and improve their accuracy have been made. They almost always involve having multiple judges score each item and combining the scores in some fashion. The simplest (and probably most common) method of combination is majority vote: for example, if the judges are rating yes/no (for example, is there a pet), you could report the rating given by the majority of the judges. But even after such processing, some errors will remain. Judge errors are often correlated, and so multiple judgments can correct fewer errors than you might expect. For example, most of the judges might be unclear on the meaning of “pet”, incorrectly assume that a chicken cannot be a pet, and wrongly label a photo of a suburban home showing a child playing with chickens in the backyard. Nonetheless, any judgment errors remaining after processing are usually ignored, and the processed judgments are treated as if they were perfect. Ignoring judge errors is a pitfall. Here is a simulation illustrating this. I’ll use the pet example again. Suppose there are 1000 images sent to the judges and that 70% of the images contain a pet. Suppose that when there is no pet, the judges (or the final processed judgment) correctly detect this 95% of the time. This is actually higher than typical for realistic industrial applications. And suppose that some of the pet images are subtle, and so when there is a pet, judges are correct only 90% of the time. I now present a simulation to show what happens. Here’s how one round of the simulation works. I assume that there are millions of possible images and that 70% of them contain a pet. I draw a random sample of 1000. I assume that when the image has a pet, the judgment process has a 90% chance reporting “pet”, and when there is no pet, the judgment process reports “no pet” 95% of the time. I get 1000 judgments (one for each image), and I record the fraction of those judgments that were “pet”. That’s one round of the simulation. I do the simulation many times and plot the results as a histogram. I actually did 100,000 simulations, so the histogram has 100,000 values. The results are below in red. Since this is a simulation, I know the true fraction of images with pets: it’s 70%. The estimated fraction from the judges in each simulation is almost always lower than the true fraction. Averaged over all simulations, the estimated fraction is 64%, noticeably lower than the true value of 70%. This illustrates the pitfall: by ignoring judge error, you get the wrong answer. You might wonder if this has any practical implications. How often do you need a precise estimate for the fraction of pets in a collection of images? But what if you’re using the judgments to determine the accuracy of an algorithm that detects pets? And suppose the algorithm has an accuracy rate of 70%, but the judges say it is 64%. In the machine learning/AI community, the difference between an algorithm that is 70% accurate vs 64% is a big deal. But maybe things aren’t as bad as they seem. When statistically savvy experimenters report the results of human judgment, they return error bars . The error bars (I give details below) in this case are So even error bars don’t help: the actual accuracy rate of 0.7 is not included inside the error bars. The purpose of this post is to show that you can compensate for judge error. In the image above, the blue bars are the simulation of the compensating algorithm I will present. The blue bars do a much better job of estimating the fraction of pets than the naive algorithm. This post is just a summary, but full details are in Drawing Sound Conclusions from Noisy Judgments from the WWW17 conference. The histogram shows that the traditional naive algorithm (red) has a strong bias. But it also seems to have a smaller variance, so you might wonder if it has a smaller mean square error (MSE) than the blue algorithm. It does not. The naive algorithm has MSE 0.0037, the improved algorithm 0.0007. The smaller variance does not compensate for the large bias. Finally, I can explain why judge error is so important. For a typical problem requiring ML/AI, many different algorithms are tried. Then human judgment can be used to detect if one is better than the other. Current practice is to use error bars as above, which do not take into account errors in the human judgment process. But this can lead the algorithm developer astray and suggest that a new algorithm is better (or worse) when in fact the difference is just noise. I’m going to assume there are both an algorithm that takes an input and outputs a label (not necessarily a binary label) and also a judge that decides if the output is correct. So the judge is performing a binary task: determining if the algorithm is correct or not. From these judgments, you can compute the fraction of times (an estimate of the probability ) that the algorithm is correct. I will show that if you can estimate the error in the judgment process, then you can compensate for it and get a better estimate of the accuracy of the algorithm. This applies if you use raw judge scores or if you use a judgment process (for example, majority vote) to improve judge accuracy. In the latter case, you need to estimate the accuracy of the judgment process. A simple example of this setup is an information retrieval algorithm. The algorithm is given a query and returns a document. A judge decides if the document is relevant or not. A subset of those judgments is reevaluated by gold judge experts, giving an estimate of the judges’ accuracy. Of course, if you are rich enough to be able to afford gold judges to perform all your labeling, then you don’t need to use the method presented here. A slightly more subtle example is the pet detection algorithm mentioned above. Here it is likely that there would be a labeled set of images (a test set) used to evaluate the algorithm. You want to know how often the algorithm agrees with the labels, and you need to correct for errors in the judgment about agreement. To estimate the error rate, pick a subset of images and have them rejudged by experts (gold judges). The judges were detecting if an image had a pet or not. However, what I am really interested in is the error rate in judging whether the algorithm gave the correct answer or not. But that is easily computed from the gold judgments of the images. In the introduction, I mentioned that there are formulas that can correct for judge error. To explain the formula, recall that there is a task that requires labeling items into two classes, which I’ll call positive and negative. In the motivating example, positive means the algorithm gives the correct output for the item, negative that it is incorrect. I have a set of items, and judges perform the task on each item, getting an estimate of how many items are in the positive class. I’d like to use information on the accuracy of the judges to adjust this estimate. The symbols used in the formula are: The first formula is This formula is not difficult to derive. See the aforementioned WWW paper for details. Here are some checks that the formula is plausible. If the judges are prefect then , and the formula reduces to . In other words, the judges’ opinion of is correct. Next, suppose the judges are useless and guess randomly. Then , and the formula makes no sense because the denominator is infinity. So that’s also consistent. Notice the formula is asymmetric: the numerator has but not . To see why this makes sense, first consider the case when the judges are perfect on negative items so that . The judges’ only error is to take correct answers and claim they are negative. So an estimate of by the judges is always too pessimistic. On the other hand, if , then the judges are optimistic, because they sometimes judge incorrect items as correct. I will now show that the formula achieves these requirements. In particular, if then I expect and if then . To verify this, note that if the formula becomes . And if then the last inequality because and . Up until now the formula is theoretical, because the precise values of , and are not known. I introduce some symbols for the estimates. The practical formula is In the introduction, I motivated the concern with judgment error using the problem of determining the accuracy of an ML algorithm and, in particular, comparing two algorithms to determine which is better. If you had perfect judges and an extremely large set of labeled data, you could measure the accuracy of each algorithm on the large labeled set, and the one with the highest accuracy would clearly be best. But the labeled set is often limited in size. That leads to some uncertainty: if you had picked a different labeled set you might get a difference answer. That is the purpose of error bars: they quantify the uncertainty. Traditionally, the only uncertainty taken into account is due to the finite sample size. In this case, the traditional method gives 95% error bars of where The judge gave a positive label times out of , is the estimated mean and the estimate of the variance of . I showed via simulation that if there are judgment errors, these intervals are too small. The corrected formula is where The formula shows that when taking judge error into account, the variance increases, that is, . The first term of the equation for is already larger than , since is it divided by a number less than one. The next two terms add additional error, due to the uncertainty in estimating the judge errors and . The theme of this post is correcting for judge errors, but I have only discussed the case when the judge gives each item a binary rating, as a way to estimate what fraction of items are in each of the two classes. For example, the judge reports if an algorithm has given the correct answer, and you want to know how often the algorithm is correct. Or the judge examines a query and a document, and you want to know how often the document is relevant to the query. But the methods presented so far extend to other situations. The WWW paper referred to earlier gives a number of examples in the domain of document retrieval. It explains how to correct for judge error in the case when a judge gives documents a relevance rating rather than just a binary relevant or not. And it shows how to extend these ideas beyond the metric “fraction in each class” to the metric DCG , which is a common metric for document retrieval. Analyses of the type presented here always have assumptions. The assumption of this work is that there is a task (for example, “is the algorithm correct on this item?”) with a correct answer and that there are gold judges who are expert enough and have enough time to reliably obtain that correct answer. Industrial uses of human judgment often use detailed guidelines explaining how the judgment should be done, in which case these assumptions are reasonable. But what about the general case? What if different audiences have different ideas about the task? For example, there may differences from country to country about what constitutes a pet. The analysis presented here is still relevant. You only need to compute the error rate of the judges relative to the appropriate audience. I treat the judgment process as a black box, where only the overall accuracy of the judgments is known. This is sometimes very realistic, for example, when the judgment process is outsourced. But sometimes details are known about the judgment process would lead you to conclude that the judgments of some items are more reliable than others. For example, you might know which judges judged which items, and so have reliable information on the relative accuracy of the judges. The approach I use here can be extended to apply in these cases. I close this post with the R code that was used to generate the simulation given earlier. Powered by QuickLaTeX .", "date": "2017-05-04"},
{"website": "Ebay-Research", "title": "Relation Embedding with Dihedral Group in Knowledge Graph", "author": ["Canran Xu", "Ruijiang Li"], "link": "https://tech.ebayinc.com/research/relation-embedding-with-dihedral-group-in-knowledge-graph/", "abstract": "eBay researchers recently published a paper about a method for KG relation embedding using dihedral group. Experimental results on benchmark KGs show that the model outperforms existing bilinear form models and even deep learning methods. Large-scale knowledge graphs (KG) play a critical role in the downstream tasks such as semantic search, dialog management, and question answering. In most cases, despite its large scale, a KG is not complete due to the difficulty to enumerate all facts in the real world. The capability of predicting the missing links based on existing dataset has been one of the most important research topics for years. A common representation of a KG is a set of triples (head, relation, tail), and the problem of link prediction can be viewed as predicting new triples from the existing set. A popular approach is KG embeddings, which maps both entities and relations in the KG to a vector space, such that the scoring function of entities and relations for ground truth distinguishes from false facts. The standard task for link prediction is to answer queries (h,r,?) or (?r,t). In this context, recent works on KG embedding focusing on bilinear form methods are known to perform reasonably well. The success of this pack of models resides in the fact they are able to model relation (skew-) symmetries. Furthermore, when serving for downstream tasks such as learning first-order logic rule and reasoning over the KG, the learned relation representation is expected to discover relation composition by itself. One key property of relation composition is that in many cases, it can be non-commutative. For example, exchanging the order between parent_of and spouse_of will result in completely different relation (parent_of as opposed to parent_in_law_of). We argue that, in order to learn relation composition within the link prediction task, this non-commutative property should be explicitly modeled. Applied researchers from eBay recently published a long paper in Association for Computational Linguistics (ACL) 2019 to tackle this problem. In this paper, the researchers proposed DihEdral to model the relation in KG with the representation of dihedral group. The elements in a dihedral group are constructed by rotation and reflection operations over a 2D symmetric polygon. As the matrix representations of dihedral group can be symmetric or skew-symmetric, and the multiplication of the group elements can be Abelian or non-Abelian, it is a good candidate to model the relations with all the corresponding properties desired. Details of the paper can be found in here. A dihedral group is a finite group that supports symmetric operations of a regular polygon in two dimensional space. Here, the symmetric operations refer to the operator preserving the polygon. For a K-side (K∈Z+) polygon, the corresponding dihedral group is denoted as DK that consists of 2K elements, within which there are K rotation operators and K reflection operators. A rotation operator Ok rotates the polygon anti-clockwise around the center by a degree of (2πm/K), and a reflection operator Fk mirrors the rotation Ok vertically.  The element in the dihedral group DK can be represented as 2D orthogonal matrices: OK(m)=[cos⁡(2πmK)−sin⁡(2πmK)sin⁡(2πmK)cos⁡(2πmK)]FK(m)=[cos⁡(2πmK)sin⁡(2πmK)sin⁡(2πmK)−cos⁡(2πmK)], where m∈{0,1,⋯,K}. The elements of D4 is visualized in the following figure, with each subplot representing the result after applying the corresponding operator to the square of ‘ACL’ on the upper left corner. We propose to model the relations by the group elements in DK. We assume an even number of latent dimensions 2L. More specifically, the relation matrix takes a block diagonal form R=diag[R(1),R(2),···,R(L)], where R(l)∈DK for l∈{1,2,⋯,L}. As a result, the score for a triple (h,r,t) in bilinear form can be written as a sum of these L components h⊤Rt=∑l=1Lh(l)⊤R(l)t(l). This model is able to learn the following properties for a KG: Relation symmetry and skew symmetry. A relation R is symmetric if (t,R,h) is true when (h,R,t) is true. A relation R is skew-symmetric if (t,R,h) is false when (h,R,t) is true. Relation inversion. R1 is the inverse of R2 if (t,R2,h) is true when (h,R1,t) is true. Relation composition. If R3 is the composition of R1 and R2, then when both (h,R1,t1)  and (t1,R2,t2) are true then (h,R3,t3) is also true. That means, R3=R1R2. Moreover, dihedral group is able to learn a non-Abelian relation composition, which means it can distinguish the order of the composition for R1 and R2. To train the relation embedding with dihedral group, we proposed two optimization methods. The first one utilizes the commonly used Gumbel-softmax trick to train discrete values. This model is called Dk-Gumbel. The second one parametrizes the group elements by binary variables, which are optimized by a straight-through estimator, and the model is named as Dk-STE. We performed experimental studies on 5 public KG datasets: WN18, WN18RR, FB15K, FB17K-237 and YAGO3-10. Here's the result: We observe that the proposed model performs similarly to or better than the state-of-the-art models. We proposed a method for KG relation embedding using dihedral group. By leveraging the desired properties of dihedral group, relation (skew-) symmetry, inversion, and (non-) Abelian compositions are all supported. Our experimental results on benchmark KGs showed that the model outperforms existing bilinear form models and even deep learning methods. To see more details, please read our paper.", "date": "2020-05-14"},
{"website": "Ebay-Research", "title": "Faster E-commerce Search", "author": ["Roberto Konow"], "link": "https://tech.ebayinc.com/research/making-e-commerce-search-faster/", "abstract": "The search engine plays an essential role in e-Commerce: it connects the user's need with a set of relevant items based on a query. This is not a simple task; millions of queries per second need to be processed over possibly billions of items, and it is expected that every query will be executed in just a few hundred milliseconds using limited resources. In this article, we show how we improved eBay's search engine efficiency by over 25%, inspired by a technique coming from web search. This article is an adaptation of our original publication at the SIGIR e-commerce workshop. This work was done by eBay's Search Backend Engineering team members Vishnusaran Ramaswamy, Roberto Konow, Andrew Trotman, Jon Degenhardt,  and Nick Whyte. Search engines are implemented as large distributed systems where there is a limited budget in CPU and memory that every machine can use. Any improvement in efficiency could potentially be translated into a reduction in hardware, networking, and operating costs. Tremendous research and engineering efforts have gone into addressing performance challenges: reduction of memory requirements by improving data compression, reduction of CPU usage by implementing early termination techniques, and the use of massively parallel execution engines are just a few of the techniques that have been extensively studied in the past. In this article, we focus on one technique originally designed to improve data compression and reduce the size of the data that is loaded into main memory: document id reordering . Before we go into more details, let us explain some basic concepts about the search engine’s main data structure: the Inverted Index . The inverted index is an old and simple, yet very efficient data structure that is at the heart of most search engines and is used to support various search tasks. From a collection of documents, an inverted index stores for each unique term $t$ (or word) a list of postings . Each posting stores pairs $\\langle d,w(t,d) \\rangle$, where $d$ is a unique document identifier (doc_id) and $w(t,d)$ is a relevance measure of the term $t$ with respect to document $d$ (often the number of times term $t$ occurs in document $d$). These posting lists can be extended to store additional information such as the positions of the term within the document. Posting lists are usually stored sorted by doc_id and processed one document at a time. To help with compression, doc_ids are often stored difference encoded—the value stored in the list is the difference (or d-gap ) between this id and the preceding id. The list of doc_ids $\\langle d_1, d_2, d_3,\\dots d_n \\rangle$, is a strictly monotonically increasing sequence. These integers can be decreased in size by calculating the differences between each document id and the preceding document id, resulting in a list of d-gaps  $\\langle d_1, d_2 - d_ 1, d_3 - d_2, \\dots , d_n - d_{n-1}\\rangle$. These d-gaps are further compressed using variable-width integer codes such as variable byte encoding , PForDelta , QMX , or similar. In practice, search engines can assign doc_ids in a number of different ways: at random, based on document similarity, in the order documents are indexed (collection order), based on a global measure of quality such as page rank, or for web documents, URL order. Others have noted that document reordering not only reduces index size, but can also decrease query processing time. This has motivated several authors in the past to study the doc_id assignment process in such a way as to optimize compression and query latency. Query processing involves a number of steps such as query parsing, query rewriting, and the computation of complex machine-learned ranking functions that may include hundreds or thousands of features derived from the documents, the query, and the user. To rank efficiently, it is common to separate the query processing into multiple stages. The first stage is responsible for identifying which documents must be ranked, and the subsequent stages rank those documents. In the first phase, a simple and fast retrieval filtering such as Boolean operators are often used. A conjunctive Boolean query of the form \"Last AND Jedi'' requires an intersection calculation, whereas a disjunctive query of the form \"Episode OR VIII'' requires a union calculation. Union queries are solved by linearly traversing all postings lists for all terms in the expression and returning all documents containing either term. Efficiently resolving intersection queries requires complicated traversal of the postings lists and has been examined for decades . It has been proven that the optimal intersection algorithm for two sets of length $m$ and $n$ with $m \\leq n$ is $O(m\\log\\frac{n}{m})$. The most popular algorithm for solving intersections is Set Versus Set (and also known as Small Versus Small). A popular e-commerce search technique to improve precision is to constrain a query to a particular set of categories in a category tree. This can be done automatically by a trained classifier, or manually by the user. For example, the results of the query iphone case can be constrained so that all the resulting items belong to the category \"Cell Phones & Accessories Cases, Covers & Skins.\" These categories also form natural partitions, clustering items according to popular search dimensions. Document reordering is a well-studied technique in web search. Most prior work has focused on reordering documents to achieve better compression. The approach is to perform text clustering on the collection to find similar documents and then assign doc_ids to minimize the d-gaps in the posting list. Fabrizio Silvestri explored a much simpler idea that takes advantage of an implicit organization of the documents in the Web. In his work, he proposes to assign doc_ids sequentially according to the document URL and showed that this simple technique achieved competitive results when compared to text clustering techniques. In essence, he roughly clustered on topic, because different sites and different parts of the same sites are usually topically related. Our approach is motivated by the work of Silvestri—we present a simple but non-optimal document id reordering technique. It takes advantage of the structured nature of documents in e-commerce, specifically that items that are for sale are usually classified and categorized before being listed. E-commerce search is a much more structured and constrained scenario than web search. In e-commerce, much of the document content is given by the item properties, such as price, brand, model, category, color, and so on. It is natural to consider these features as filtering components of a search, and it is consequently common practice to generate posting lists for those kind of features. For example, by generating posting lists for each instance of the feature \"brand\" (i.e brand:apple, brand:samsung, etc) the user can easily constrain their search to just the items made by a given manufacturer—and indeed they expect to be able to do so. Category is a particularly interesting property of e-commerce items (and queries), because it is not only used to divide the inventory into distinct types of products, but it is also commonly used to improve the precision of the results. Given a user query, the search engine can constrain the results to just those from the most relevant category. This can be done in an automatic way by training query to category classifiers, or by allowing the user to manually select a category. Figure 1 shows an example user query \"star wars\" being constrained to the category \"art\" on ebay.com. Figure 1: User category constrained query on ebay.com If the search engine creates posting lists for each category, the first stage of query processing can be improved significantly, since it can perform a direct Boolean intersection between the (possibly term expanded) user query and the posting list for the given category. Since this cuts down the size of the recall base, it increases the efficiency of the search engine, but since it restricts to the most likely category, it also removes noise from the results list, increasing precision. And this is the motivation for document id reordering based on item category. We re-order the collection so that the doc_ids are assigned in such a way that items belonging to the same category are given contiguous doc_ids. This reduces the size of the d-gaps in posting lists, which leads to better compression. However, this is not the only benefit. Because posting lists are sorted by doc_id, it creates implicit category \"divisions\" within each posting list. Figure 2 illustrates this. On the top left, the collection of documents is shown in \"collection order,\" where the distinct shades of gray represent different categories. The top right gives example posting lists for words ($w_1,w_2,w_3$). The bottom left of the figure shows the collection category reordered where, for example, collection ordered $d_2$ becomes category ordered $d_3$. The bottom right shows the effect of document reordering on the posting lists; they are now implicitly divided by category. Figure 2 : Document reordering diagram. Different grat levels represent different categories. On the left, we show a sketch of how documents get new doc_id assignment. On the right, the effect on posting lists. This document reordering scheme not only helps compression, but also decreases latency: as the d-gaps are smaller, the decompression of the integers is faster because, in general, a smaller number of operations is required to decode a smaller integer. Equally, since similar documents are stored together, fewer skips are needed. It is obvious that when a query is category constrained, the results must lie within a consecutive part of the postings list. We implemented this idea and conducted our experiments using eBay's search engine. We selected 12 million random items from our dataset and constructed two indexes: Random Index: documents were assigned doc_ids at random. Reordered Index: documents were sorted by category and then assigned doc_ids. To evaluate the performance of our document reordering technique, we use two sets of queries: General Queries: approximately 3 million queries from production logs of one eBay data center. These queries included user-issued queries as well as system-issued queries (such as those from the eBay public APIs). No country-specific filtering was performed, so queries are in many languages. User Queries: a random sample of 57,058 queries from ebay.com exactly as entered into the search box by ebay.com users. 3.2% Smaller For the purpose of this work, we constructed a special index that uses variable byte encoding to compress doc_ids. We see a reduction of 6.1% in the average number of bytes required to represent a doc_id using this encoding scheme. This represents a 3.2% space reduction of the complete index. Table 1 presents a summary of the results. It shows that the number of d-gaps equal to 1 has increased by 70%, that the average d-gap has decreased by 67.5%, and that the average number of bits required to represent d-gaps is reduced by 28%. In practice, the actual size reduction in the index will depend on the integer encoding scheme. Random Reordered Change d-gaps = 1 $890 \\times 10^6$ $1,510 \\times 10^6$ +70% Avg. $\\log_2(d-gaps)$ 5.73 4.11 -28% Avg. d-gaps 1966 639 -67.5% Avg. Bytes/d-gap(vByte) 1.30 1.22 -6.1% Index Size 29.67 28.72 -3.2% Table 1 : Space savings and bits per d-gap obtained before and after applying document reordering. 26.9% Faster! The first experiment considered throughput using the General Queries—it's a mirror of the production environment. We computed the average number of queries per second (QPS) that could be resolved when the CPU was held at 80% utilization (the other 20% is used in eBay for processes such as index maintenance). We found that on average, the Reordered Index could process about about 30% more queries per second than the Random Index. Table 2 shows the average search time per query (in milliseconds) at the mean, median, 95th percentile, and 99th percentile. In all cases, we see a substantial improvement; the mean latency improved by 26.9% while 95th percentile latency is almost halved when compared to the random document Reordering. Random Reordered Latency Reduction Mean 22.43 16.4 26.9% Median 4.35 3.21 28.3% 95th Percentile 57 30.2 47% 99th Percentile 375 224 40% Table 2: Comparison of random doc_id assignment versus category doc_id reordering. Mean, median, 95th, and 99th percentiles of query processing times in milliseconds for general queries. We also evaluated the impact of applying category constrains to the queries. The results are shown in Table 3. The left side shows the latency (in milliseconds) when category constraint is not used. In this case, the Reordered index improved mean query latency by 47% and the 95th percentile by 41%. The right shows the effect of enabling category constrain on the queries. There the mean query latency has reduced by 55% when the Reordered Index is used, and a similar effect is observed for the 95th percentile. Clearly both unconstrained and category constrained queries are materially improved. Without Category Constraint With Category Constraint Random Reordered Latency Reduction Random Reordered Latency Reduction Mean 26.8 14.3 47% 18.9 8.4 55% Median 5.9 3.5 42% 3.2 1.7 48% 95th Pct 85.8 50.8 41% 61.6 27.6 55% Table 3: Execution time in milliseconds for user queries with and without category constraint enforcement. Categories naturally cluster both document terms and query terms. Items satisfying a multi-term AND query will tend to come from a small number of categories. Ordering posting lists by category will put documents satisfying these queries near each other both in posting lists and in the forward index . This should improve CPU cache hit rates and even improve the inherent efficiency of the posting list processing algorithms. The latter effect would result from having larger clusters of posting list entries that are either matched or skipped than in a random assignment of the documents. Query expansion is likely to compound these effects, especially in an e-commerce environment. As in most search engines, we make extensive use of query expansion to improve recall and precision. Rewritten queries often form a complex Boolean expression involving many posting lists and nested AND/OR constructs. Expansions involve not only text keywords, but also the structured meta-data associated with products. For example, the term \"black\" may expand to \"color:black\" and \"samsung\" to \"brand:samsung.\" To examine these effects, we used the CPU usage profiling tool perf , while processing a portion of a General Queries collection, to analyze and identify the exact locations where the improvement was more noticeable. We observed the functions performing the task of iterating (and skipping) through posting lists was consuming about 5% of the total CPU time, and we observed a noticeable improvement especially in these parts. We also saw improvements in the doc_id variable byte decoding section. Finally, we analyzed the effect of how cache misses were affected by the document reordered index. We observed a 7% reduction in overall cache misses and a 13% reduction in last-level cache misses (LLC). These numbers show that that document ordering by category yielded a significant improvement in overall CPU cache hit rates. These numbers are consistent with our hypothesis for the improvements in latency. Additional analysis is still needed to quantify the effects on posting listing processing. The probability that any given cluster of posting list entries will be referenced by a query is far from uniform in the General Queries collection, and more likely following something like a zipfian curve. This should reduce the number of CPU cache entries filled with posting list data during processing of a query, and thus reducing the CPU cache working set size for the portion of query processing dedicated to processing posting lists. The reduction in CPU cache working set size for posting lists allows a larger amount of CPU cache to be used for other functions performing query processing work, which improves the CPU cache hit rate for those other functions. The above discussion focuses on the recall component of query processing. As mentioned earlier, document reordering also better co-locates forward index data for items satisfying the recall expression for a query. Forward index data is used extensively in the ranking component of query processing. As such, this also has potential to improve CPU cache hit rates. Our results show that document id ordering on category reduces mean latency per query by 27% and 99th percentile latency by 45%. Latency improvements are seen both with and without category constraints applied. It also reduces the index size by 3.2%. For more details you can find our SIGIR e-commerce workshop paper here . We thank Prof. Alistair Moffat and Prof. Gonzalo Navarro for their help and comments during the development of this project.", "date": "2018-01-18"},
{"website": "Ebay-Research", "title": "Natural Language Processing and eBay Listings", "author": ["Junling Hu"], "link": "https://tech.ebayinc.com/research/natural-language-processing-and-ebay-listings/", "abstract": "You’ve heard before on this blog about the difference between products and items on eBay: the former uses a well-defined structure to describe product information, the latter allows a seller to enter free-form text for describing what’s for sale. In order to help buyers find what they’re looking for, how can we extract relevant information from these unstructured item titles and make them comparable to products? Natural language processing (NLP) can be used in this context. In a paper titled “Bootstrapped Named Entity Recognition for Product Attribute Extraction” , we present a named entity recognition (NER) system for extracting product attributes and values from listing titles. These titles pose some unique challenges for NLP: They’re relatively short Often they’re just a list of nouns without any grammatical structure They contain abbreviations and acronyms, and even typographical errors There is no contextual information that could help in identifying product attributes We combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBay’s fashion categories , our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identify novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and find n-gram substring matching to work well in practice. We presented our work (*) at the international conference on Empirical Methods in Natural Language Processing (EMNLP) this July. (*) Duangmanee Putthividhya and Junling Hu, “Bootstrapped Named Entity Recognition for Product Attribute Extraction”, Proceedings of EMNLP-2011, July 2011. -Junling Hu Principal Data Mining Lead", "date": "2011-08-22"},
{"website": "Ebay-Research", "title": "Personalized Search at eBay", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/personalized-search-at-ebay/", "abstract": "In this blog post and a succeeding one, I will discuss personalization at eBay. Specifically, I’ll talk about how search can be personalized. First I’ll investigate whether buyers actually differ in their preferences for the items they’d like to see in their search results. Next I’ll discuss what preferences have the widest variation from buyer to buyer. Then I’ll talk about how to implement personalization, and finally I’ll explain the theory that underpins the implementation. Some people argue that personalization is actually not desirable. I’ll address that at the end of my last posting. To make this concrete, I’ll focus on what eBay calls format : is the item being offered for sale via an auction, or is it a fixed-price item with a buy-it-now button? Of course some users buy more auctions than others. But does this indicate a preference? Perhaps not. Perhaps users simply search for the best deal, and if a user buys more auctions it’s not because they prefer auctions, but simply because they happened to purchase a string of items where the best deal was an auction. The following graph suggests that the “just looking for the best deal” theory is not tenable. Users who bought 1 item in the past year did not have much taste for auctions. They only purchased an auction 25% of the time. On the other hand, heavy buyers who purchased at least 600 items in the past year bought mostly auctions – over 70% of their purchases were auctions. So it seems that users do have different preferences. Infrequent buyers prefer fixed-price items, while frequent buyers prefer auctions. But before discarding the “looking for best deal” hypothesis, I want to consider some alternate possibilities. First is the question you should always ask – is the data statistically significant? Given how consistently the curve moves upwards, it seems unlikely that we’re seeing an artifact due to noise. But I can check this rigorously. I’m measuring a binary variable – one that takes on two values, auction or fixed-price. Statistics textbooks tell us that the noise (as estimated by the standard error) is √ pq/n . The leftmost point represents 14 million buyers. So p =.25, q =.75 and n≈14000000. The noise is about 0.0001. I can be confident that the 25% is very accurate. Here’s a more subtle alternate explanation. Suppose everyone has similar auction preferences. But suppose infrequent buyers tend to purchase electronics, which has fewer auctions, and frequent buyers are more likely to buy antiques, which have a high fraction of auctions. So the curve might be consistent with buyers having similar auction preferences, and simply be a reflection of the type of items that different buyers purchase. I can test this alternate explanation by focusing on a single category, for example Jewelry and Watches. This is a category with a high proportion of auction listings. The plot shows the same pattern as before: infrequent buyers prefer fixed price, heavy buyers prefer auctions. The curve is shifted upwards, because Jewelry is a high-auction category. But there’s a clear difference in preferences. The same pattern holds in a low-auction category like Computers & Networking. The curve is shifted downward, but again more experienced buyers have a stronger preference for auctions. So to summarize, the most likely explanation of the graphs I’ve shown is that some buyers have a much stronger preference for auctions than other buyers. But I want to dig deeper. Suppose I look at buyers with similar buying experience, for example all those who bought 8 items in the past year. Do they have different format preferences? To answer that question, I need to be more precise about what it means for users to have similar format preferences. I’ll assume that buyers chose an auction with probability p . One way that might happen is that just before buyers make a purchase, they mentally throw dice in their head, and depending on the outcome say “I feel like an auction today”, or “I feel like buying fixed price today”. OK, that’s not too realistic, but there are other scenarios where it will appear that buyers are choosing an auction with probability p . For example their decision might depend on the inventory available, and that could vary randomly. Or more likely, their decision depends on factors that I don’t have available, such as how soon they want to receive the item (for an auction, you have to wait for bidding to end). In any case, imagining that buyers have some propensity for auctions that is captured as a probability is a simple but plausible model. A consequence of this model is that I can predict how many auctions a user will buy. Recall that I’m focusing on those who bought 8 auctions. Assuming that all buyers have the same auction propensity p , the chance that k of the 8 purchases are an auction is the familiar formula In this formula n = 8. In Jewelry & Watches, about 75% of the purchases are auctions, so p = 0.75. The top histogram below labeled Predicted was generated using this formula. Most users are predicted to buy 6/8 = 0.75 auctions. But many are predicted to buy 8 auctions. In our model they buy an auction with probability p , and so there’s random variation. They buy 6 on the average, but some buy 8 and others only 2. The bottom histogram gives the actual numbers. They are nothing like the prediction. This suggests that the hypothesis of all users have the same auction preference is false. Here’s another example in the Computers & Networking category. Again, the count of auctions vs. fixed price is nothing like what would be predicted if all users had a similar auction preference. Here’s a summary of my reasoning. I focused on users with similar buying experience by looking at those who bought 8 items in the past year. I observed how many bought 0/8 auctions, how many bought 1/8, etc. If all buyers had similar auction preferences, that preference would be 75% (in Jewelry), and I could predict how many would buy 0/8, 1/8, etc. But the predicted values and observed values are totally different, so I conclude that buyers do not have similar auction preferences. Let me move on to a new topic. Users have widely varying preferences for auctions. Are there other attributes of items that vary widely? I can repeat the analysis above to find out. I will examine 3 different attributes: item condition (new vs. used), whether the seller had an eTRS (eBay Top-Rated Seller) rating, and whether the item had free shipping. In each case I will compare the predicted and observed plots, and observe by how much they differ. First item condition: The two histograms are very different, so condition appears to be a good attribute for personalization. Next is a graph for items sold by trusted sellers. This time the histograms are similar. True, the tails are a bit larger in the observed histogram, but there is nowhere near the variation there was for item condition. Finally, here’s free shipping. The observed histogram is not too different from what would be expected if everyone had a 44% preference for eTRS. So I conclude that personalizing search results on format and condition will probably be much more useful than personalizing on eTRS or free shipping. Now that I’ve established that buyers have different preferences, I need to figure out how to estimate a buyer’s personal preferences. I will use a technique called Empirical Bayes . Like all things Bayes, it has some controversy. Brad Efron, the former chair of the Stanford Statistics department has said: The suggestion here, made explicit in the final section, is that after 50 years of underuse, we are poised for an avalanche of empirical Bayes applications Meanwhile across the Bay, the late statistician David Blackwell, who was chairman of the Berkeley statistics department, has been quoted as such: He [David Blackwell] noted that he didn’t believe in empirical Bayes and showed that it didn’t make sense when applied to a single inference. I will ignore the controversy and plunge ahead. First, here’s an extremely simple way to implement personalization: If a user has bought more than 10 auctions, I will assume I have enough information to estimate their auction preference, and compute it as k / n where k is the number of auction purchases and n is the total number of purchases. Otherwise I make no assumptions. This is clearly unsatisfying. Why 10? Do I really know nothing if they’ve purchased 9 items, but suddenly at 10 items I believe their purchase history is the whole story? As a rule of thumb, hard cutoffs like this do not perform as well as a smooth transition. And empirical Bayes is a perfect tool for eliminating the hard cutoff. It gives a principled way of estimating a buyer’s propensity (probability) of purchasing an auction. In the next posting, I’ll explain how to use Empirical Bayes to derive an estimation formula. For now, I’ll just explain how the formula applies to a typical buyer, Mr. X. The first step is to aggregate buyers who are similar to Mr. X. This peer group is then summarized by two numbers, a and b . I’ll explain how to compute them later. These numbers encode the aggregate estimate of the auction propensity, f A = a /( a + b ). I write this as f , since this probability estimates the fraction of auctions. The next step is to record X’s purchases. If he has made n purchases, and k were in auction format, then his personal estimate is f P = k / n . Empirical Bayes gives a compromise between these two numbers, via the formula f = ( a + k )/( a + b + n ). I’ll next check whether this formula is reasonable. Suppose there were no purchases, so that k = n = 0. Then the formula becomes f = ( a + k )/( a + b + n ) = a /( a + b ) which is the aggregate estimate. This makes sense. On the other extreme, suppose X has made many purchases so that k and n are large. Then the Bayes estimate becomes f = ( a + k )/( a + b + n ) ≈ k / n which is the user’s personal preference. So again the formula makes sense: it gives the aggregate value initially, and then as X’s purchases increase, it becomes more like his personal value. I’ll now work through an example. Suppose I’ve bought 8 items in the Computers & Networking category in the past year. I’ll use as my peer group the buyers who have also purchased 8 items in Computers & Networking. The a , b numbers are a =0.95, b =4.64. These numbers encode the behavior of my peer group, and I can use them to compute the average fraction of auctions purchased by my peers via , f A = a /( a + b ) = .95/(.95 + 4.64) = .17. The empirical Bayes formula says that my auction propensity is f = ( a + k )/( a + b + n ) = (.95+ k )/13.59. Here’s a plot of what the formula predicts for different values of k , the number of purchases that were auctions. The empirical Bayes estimate is a compromise between the aggregate value and my personal value. Here’s the plot with n = 4 and n = 16 added to n = 8. As n gets larger, the Empirical Bayes estimate gets closer and closer to the personal estimate. Here’s a second example in the Jewelry & Watches category. Again with 8 purchases, the numbers a and b are a = 0.75, b = 0.80 and so f A = a /( a + b ) = .75/(.75 + 40.8) = .48. The plot is below: Again, the Empirical Bayes estimate is between the aggregate and personal. You might be wondering why the aggregate isn’t more simply represented by a single number f A , rather than decomposing it into f A = a /( a + b ). The reason is that a and b encode not just the average number of auctions in your peer group, but also the variation in that group. When a and b are large, buyers in the peer group cluster closely near f A (left plot below). When a and b are small, buyers have a large spread (right plot). This is consistent with f = ( a + k )/( a + b + n ). When a and b are large the left graph shows that the peer group is very consistent in their auction preference, and so we should be skeptical of a user whose personal preference is very different from f A . And indeed, the formula f = ( a + k )/( a + b + n ) shows that k and n have small influence. When a and b are small the right graph applies. It shows that peer group has no strong preference, and so k and n are more important. I’ve actually already shown you an example of this. In Computers & Networking, a and b are large, and the Bayes compromise is roughly midway between the personal and aggregate. But in Jewelry & Watches, a and b are small and the compromise is much closer to the personal line – see below. In my next posting I will explain where the formula f = ( a + k )/( a + b + n ) comes from, and how to compute the numbers a and b.", "date": "2011-10-24"},
{"website": "Ebay-Research", "title": "Personalized Search at eBay, part II", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/personalized-search-at-ebay-part-ii/", "abstract": "In the first part of this blog posting, I talked about how to estimate a buyer’s propensity to purchase an auction over a fixed price item. I gave the Empirical Bayes formula f = (a+k)/(a + b+ n) for the auction propensity f which is a compromise between the buyer’s shopping history k/n and the propensity of his peer group a/(a+b) . In this posting I will explain where the formula comes from, and how to compute the numbers a and b . Since the method is called Empirical Bayes, it won’t surprise you to learn that it uses Bayes Theorem The specific form I’ll use is In this formula, p is the probability of buying an auction, n is the number of purchases that a shopper has made, and k is the number of those purchases that were auctions. The left-hand side is what I’d like to know: the probability (propensity) of an auction given that the buyer has previously purchased k/n auctions. As usual, the reason to use Bayes formula is that it relates the unknown left-hand side to the computable right-hand side. The pleasure of Bayes formula is the first term on the right-hand side. This is the textbook formula we’ve seen before: The pain of Bayes formula is the second term, the prior Pr( p ). This is the estimate of p before (prior to) learning there were k / n auction purchases. Much time has been spent debating what value to use for the prior. This is where empirical Bayes comes in. I will use the data itself to estimate Pr( p ). I do it by assuming that Pr( p ) follows a Beta probability distribution and all I need to do is specify the two parameters a and b . Why this distribution? It’s general enough to fit most data, but simple enough to make explicit calculations possible. I’ll do those calculations in a moment. The following figure from Wikipedia shows the first point. By varying the parameters a and b (α and β in the figure) you can get a large range of distributions. Now for the details of how to compute a and b . The probability of purchasing k / n auctions is If I don’t know a precise value for p , but do know it varies via some probability distribution Prob( p ) then p k is an average over those p , expressed as the integral As I mentioned above, I’ll assume that Prob( p ) is a beta distribution Now plug this value of Prob( p ) into the formula for p k which I’ve written as p k ( a , b ) to emphasize the dependence on a and b . If you’re following along in detail, you’ll see that I did some simplifications after the plug-in. The important thing is this: the value of p k depends on the known k , n and the unknown a , b , and my job is to pick those a , b so that is minimized. In other words, I want to make the computed p k as close as possible to the observed fraction, which I’ve written with a hat. For example p 2 ( a , b )  is the computed fraction of people in the peer group that bought 2 auctions assuming the prior is Beta( a , b ), while p 2 with a hat is the observed fraction of users who bought two auctions. Here’s an illustration. Suppose the peer group is users who’ve purchased 21 items. Then the best values of a and b are 1.16 and 2.22 which gives this beta distribution: Once I have a and b I can compute the p k ( a , b ) and see how closely they approximate The match is excellent. The red predicted curve has only two parameters and yet it has a very good fit to all 22 points. Now that I’ve explained how to compute a and b , I can explain where the formula f = ( a + k )/( a + b + n ) comes from. Use Bayes formula and substitute the beta distribution B a,b for Pr( p ) as follows: I’ve been a little pedantic and used â to show that it is an estimate of a derived from the peer group (and similarly for b ). The calculations above show that when you plug the formula for B a,b ( p ) into Bayes formula you get out another beta distribution! This is the explanation for my earlier statement that beta was chosen to make explicit calculations possible. My goal is to estimate the auction propensity for a buyer who has purchased k / n auctions. What I have so far is a probability distribution for this propensity, specifically a beta distribution. If I want a single probability, the natural choice is the average of that beta distribution. The average of Beta( a , b ) is a /( a + b ). So if I want to get a single number for the auction propensity, I take the average of which is ( a + k )/( a + b + n ). This is the formula that I have used many times in this posting. So much for the calculations. I want to end by discussing whether personalization is desirable. Some have argued that it isn’t a good idea to show users only what they’ve seen before. I think these arguments are often attacking a straw man. A good personalization system doesn’t eliminate all variety, it merely tailors it. Suppose our Mr. X has purchased 20 items, all of them at fixed price. But his friend Ms. Y has done the opposite: all her purchases were made by auction. I find it hard to believe that anyone would seriously argue that X and Y should see the same search results. Just as bad as showing X a pile of auctions would be to never show him any auctions. A good personalization system should be nuanced. Methods like those shown in this posting provide principled estimates of user’s preferences. But how these preferences are used requires some finesse.", "date": "2011-11-04"}
]