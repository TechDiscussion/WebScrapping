[{"website": "Yahoo", "title": "A New Chapter for Omid", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/180867271141/a-new-chapter-for-omid", "abstract": "  yahoodevelopers : .  By Ohad Shacham, Yonatan Gottesman, Edward Bortnikov Scalable Systems Research, Verizon/Oath  .  Omid , an open source transaction processing platform for Big Data, was born as a research project at Yahoo (now part of Verizon), and became an Apache Incubator project in 2015. Omid complements Apache HBase, a distributed key-value store in Apache Hadoop suite, with a capability to clip multiple operations into logically indivisible (atomic) units named transactions. This programming model has been extremely popular since the dawn of SQL databases, and has more recently become indispensable in the NoSQL world. For example, it is the centerpiece for dynamic content indexing of search and media products at Verizon, powering a web-scale content management platform since 2015.  . Today, we are excited to share a new chapter in Omid’s history. Thanks to its scalability, reliability, and speed, Omid has been selected as transaction management provider for  Apache Phoenix , a real-time converged OLTP and analytics platform for Hadoop. Phoenix provides a standard SQL interface to HBase key-value storage, which is much simpler and in many cases more performant than the native HBase API. With Phoenix, big data and machine learning developers get the best of all worlds: increased productivity coupled with high scalability. Phoenix is designed to scale to 10,000 query processing nodes in one instance and is expected to process hundreds of thousands or even millions of transactions per second (tps). It is  widely used in the industry , including by Alibaba, Bloomberg, PubMatic, Salesforce, Sogou and many others. . We have just released a new and significantly improved version of Omid (1.0.0), the first major release since its original launch. We have extended the system with multiple functional and performance features to power a modern SQL database technology, ready for deployment on both private and public cloud platforms.  . A few of the significant innovations include: .  Protocol re-design for low latency   . The early version of Omid was designed for use in web-scale data pipeline systems, which are throughput-oriented by nature. We re-engineered Omid’s internals to now support new ultra-low-latency OLTP (online transaction processing) applications, like messaging and algo-trading. The new protocol, Omid Low Latency (Omid LL), dissipates Omid’s major architectural bottleneck. It reduces the latency of short transactions by 5 times under light load, and by 10 to 100 times under heavy load. It also scales the overall system throughput to 550,000 tps while remaining within real-time latency SLAs. The figure below illustrates Omid LL scaling versus the previous version of Omid, for short and long transactions.  .  Throughput vs latency, transaction size=1 op  .  Throughput vs latency, transaction size=10 ops  .   Figure 1.  Omid LL scaling versus legacy Omid. The throughput scales beyond 550,000 tps while the latency remains flat (low milliseconds).   .  ANSI SQL support  . Phoenix provides secondary indexes for SQL tables — a centerpiece tool for efficient access to data by multiple keys. The CREATE INDEX command is on-demand; it is not allowed to block already deployed applications. We added Omid support for accomplishing this without impeding concurrent database operations or sacrificing consistency. We further introduced a mechanism to avoid recursive read-your-own-writes scenarios in complex queries, like “INSERT INTO T … SELECT FROM T …” statements. This was achieved by extending Omid’s traditional Snapshot Isolation consistency model, which provides single-read-point-single-write-point semantics, with multiple read and write points.  .  Performance improvements   . Phoenix extensively employs stored procedures implemented as HBase filters in order to eliminate the overhead of multiple round-trips to the data store. We integrated Omid’s code within such HBase-resident procedures, allowing for a smooth integration with Phoenix and also reduced the overhead of transactional reads (for example, filtering out redundant data versions).  . We collaborated closely with the Phoenix developer community while working on this project, and contributed code to Phoenix that made Omid’s integration possible. We look forward to seeing Omid’s adoption through a wide range of Phoenix applications. We always welcome new developers to  join the community  and help push Omid forward! ", "date": "2018-12-06"}, {"website": "Yahoo", "title": "Announcing OpenTSDB 2.4.0: Rollup and Pre-Aggregation Storage, Histograms, Sketches, and More", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/181461332311/announcing-opentsdb-240-rollup-and", "abstract": "  yahoodevelopers : .  By Chris Larsen, Architect   .  OpenTSDB  is one of the first dedicated open source time series databases built on top of Apache HBase and the Hadoop Distributed File System. Today, we are proud to share that version 2.4.0 is now available and has many new features developed in-house and with contributions from the open source community. This release would not have been possible without support from our monitoring team, the Hadoop and HBase developers, as well as contributors from other companies like Salesforce, Alibaba, JD.com, Arista and more. Thank you to everyone who contributed to this release! . A few of the exciting new features include: .  Rollup and Pre-Aggregation Storage  . As time series data grows, storing the original measurements becomes expensive. Particularly in the case of monitoring workflows, users rarely care about last years’ high fidelity data. It’s more efficient to store lower resolution “rollups” for longer periods, discarding the original high-resolution data. OpenTSDB now supports storing and querying such data so that the raw data can expire from HBase or Bigtable, and the rollups can stick around longer. Querying for long time ranges will read from the lower resolution data, fetching fewer data points and speeding up queries. . Likewise, when a user wants to query tens of thousands of time series grouped by, for example, data centers, the TSD will have to fetch and process a significant amount of data, making queries painfully slow. To improve query speed, pre-aggregated data can be stored and queried to fetch much less data at query time, while still retaining the raw data. We have an Apache Storm pipeline that computes these rollups and pre-aggregates, and we intend to open source that code in 2019. For more details, please visit  http://opentsdb.net/docs/build/html/user_guide/rollups.html . .  Histograms and Sketches  . When monitoring or performing data analysis, users often like to explore percentiles of their measurements, such as the 99.9th percentile of website request latency to detect issues and determine what consumers are experiencing. Popular metrics collection libraries will happily report percentiles for the data they collect. Yet while querying for the original percentile data for a single time series is useful, trying to query and combine the data from multiple series is mathematically incorrect, leading to errant observations and problems. For example, if you want the 99.9th percentile of latency in a particular region, you can’t just sum or recompute the 99.9th of the 99.9th percentile. . To solve this issue, we needed a complex data structure that can be combined to calculate an accurate percentile. One such structure that has existed for a long time is the bucketed histogram, where measurements are sliced into value ranges and each range maintains a count of measurements that fall into that bucket. These buckets can be sized based on the required accuracy and the counts from multiple sources (sharing the same bucket ranges) combined to compute an accurate percentile. . Bucketed histograms can be expensive to store for highly accurate data, as many buckets and counts are required. Additionally, many measurements don’t have to be perfectly accurate but they should be precise. Thus another class of algorithms could be used to approximate the data via sampling and provide highly precise data with a fixed interval. Data scientists at Yahoo (now part of Oath) implemented a great Java library called  Data Sketches  that implements the Stochastic Streaming Algorithms to reduce the amount of data stored for high-throughput services. Sketches have been a huge help for the OLAP storage system  Druid  (also sponsored by Oath) and  Bullet , Oath’s open source real-time data query engine. . The latest TSDB version supports bucketed histograms, Data Sketches, and  T-Digests . . Some additional features include: . Try the releases on  GitHub  and let us know of any issues you run into by posting on GitHub issues or the  OpenTSDB Forum . Your feedback is appreciated! .  OpenTSDB 3.0  . Additionally, we’ve started on 3.0, which is a rewrite that will support a slew of new features including: . Please join us in testing out the current  3.0  code, reporting bugs, and adding features. ", "date": "2018-12-27"}, {"website": "Yahoo", "title": "Introducing HaloDB, a fast, embedded key-value storage engine written in Java", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/178262468576/introducing-halodb-a-fast-embedded-key-value", "abstract": "  yahoodevelopers : . By Arjun Mannaly, Senior Software Engineer  . At  Oath , multiple ad platforms use a high throughput, low latency distributed key-value database that runs in data centers all over the world. The database stores billions of records and handles millions of read and write requests per second at millisecond latencies. The data we have in this database must be persistent, and the working set is larger than what we can fit in memory. Therefore, a key component of the database performance is a fast storage engine. Our current solution had served us well, but it was primarily designed for a read-heavy workload and its write throughput started to be a bottleneck as write traffic increased.  . There were other additional concerns as well; it took hours to repair a corrupted DB, or iterate over and delete records. The storage engine also didn’t expose enough operational metrics. The primary concern though was the write performance, which based on our projections, would have been a major obstacle for scaling the database. With these concerns in mind, we began searching for an alternative solution. . We searched for a key-value storage engine capable of dealing with IO-bound workloads, with submillisecond read latencies under high read and write throughput. After concluding our research and benchmarking alternatives, we didn’t find a solution that worked for our workload, thus we were inspired to build  HaloDB . Now, we’re glad to announce that it’s also open source and available to use under the terms of the Apache license. . HaloDB has given our production boxes a 50% improvement in write capacity while consistently maintaining a submillisecond read latency at the 99th percentile. .  Architecture  .  HaloDB  primarily consists of append-only log files on disk and an index of keys in memory. All writes are sequential writes which go to an append-only log file and the file is rolled-over once it reaches a configurable size. Older versions of records are removed to make space by a background compaction job.  . The in-memory index in HaloDB is a hash table which stores all keys and their associated metadata. The size of the in-memory index, depending on the number of keys, can be quite large, hence for performance reasons, is stored outside the Java heap, in native memory. When looking up the value for a key, corresponding metadata is first read from the in-memory index and then the value is read from disk. Each lookup request requires at most a single read from disk. .  Performance    . The chart below shows the results of performance tests with real production data. The read requests were kept at 50,000 QPS while the write QPS was increased. HaloDB scaled very well as we increased the write QPS while consistently maintaining submillisecond read latencies at the 99th percentile.  . The chart below shows the 99th percentile latency from a production server before and after migration to HaloDB. .  If  HaloDB  sounds like a helpful solution to you, please feel free to use it, open  issues , and  contribute ! ", "date": "2018-09-19"}, {"website": "Yahoo", "title": "Apache Pulsar graduates to Top-Level Project", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/178450976346/apache-pulsar-graduates-to-top-level-project", "abstract": "  yahoodevelopers : . By Joe Francis, Director, Storage &amp; Messaging . We’re excited to share that The Apache Software Foundation  announced today  that Apache Pulsar has graduated from the incubator to a Top-Level Project.  Apache Pulsar  is an open-source distributed pub-sub messaging system, created by Yahoo in June 2015 and submitted to the Apache Incubator in June 2017. . Apache Pulsar is integral to the streaming data pipelines supporting Oath’s core products including Yahoo Mail, Yahoo Finance, Yahoo Sports and Oath Ad Platforms. It handles hundreds of billions of data events each day and is an integral part of our hybrid cloud strategy. It enables us to stream data between our public and private clouds and allows data pipelines to connect across the clouds.   . Oath continues to support Apache Pulsar, with contributions including best-effort messaging, load balancer and end-to-end encryption. With growing data needs handled by Apache Pulsar at Oath, we’re focused on reducing memory pressure in brokers and bookkeepers, and creating additional connectors to other large-scale systems. . Apache Pulsar’s future is bright and we’re thrilled to be part of this great project and community. .  P.S. We’re hiring! Learn more  here .   ", "date": "2018-09-25"}, {"website": "Yahoo", "title": "Introducing Oak: an Open Source Scalable Key-Value Map for Big Data Analytics", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/178045294111/introducing-oak-an-open-source-scalable-key-value", "abstract": "  yahoodevelopers : . By Dmitry Basin, Edward Bortnikov, Anastasia Braginsky, Eshcar Hillel, Idit Keidar, Hagar Meir, Gali Sheffi . Real-time analytics applications are on the rise. Modern decision support and machine intelligence engines strive to continuously ingest large volumes of data while providing up-to-date insights with minimum delay. For example, in  Flurry Analytics , an Oath service which provides mobile developers with rich tools to explore user behavior in real time, it only takes seconds to reflect the events that happened on mobile devices in its numerous dashboards. The scalability demand is immense – as of late 2017,  the Flurry SDK was installed on 2.6B devices and monitored 1M+ mobile apps . Mobile data hits the Flurry backend at a huge rate, updates statistics across hundreds of dimensions, and becomes queryable immediately. Flurry harnesses the open-source distributed interactive analytics engine named  Druid  to ingest data and serve queries at this massive rate.  . In order to minimize delays before data becomes available for analysis, technologies like Druid should avoid maintaining separate systems for data ingestion and query serving, and instead strive to do both within the same system. Doing so is nontrivial since one cannot compromise on overall correctness when multiple conflicting operations execute in parallel on modern multi-core CPUs. A promising approach is using  concurrent data structure  (CDS) algorithms which adapt traditional data structures to multiprocessor hardware. CDS implementations are thread-safe – that is, developers can use them exactly as sequential code while maintaining strong theoretical correctness guarantees. In recent years, CDS algorithms enabled dramatic application performance scaling and became popular programming tools. For example, Java programmers can use the  ConcurrentNavigableMap  JDK implementations for the concurrent ordered key-value map abstraction that is instrumental in systems like Druid.  . Today, we are excited to share  Oak , a new open source project from Oath, available under the Apache License 2.0. The project was created by the Scalable Systems team at Yahoo Research. It extends upon our earlier research work, named  KiWi .  . Oak is a Java package that implements OakMap – a concurrent ordered key-value map. OakMap’s API is similar to Java’s ConcurrentNavigableMap. Java developers will find it easy to switch most of their applications to it. OakMap provides the safety guarantees specified by ConcurrentNavigableMap’s programming model. However, it scales with the RAM and CPU resources well beyond the best-in-class ConcurrentNavigableMap implementations. For example, it compares favorably to Doug Lea’s seminal  ConcurrentSkipListMap , which is used by multiple big data platforms, including  Apache HBase ,  Druid ,  EVCache , etc. Our benchmarks show that OakMap harnesses 3x more memory, and runs 3x-5x faster on analytics workloads.  . OakMap’s implementation is very different from traditional implementations such as  ConcurrentSkipListMap. While the latter maintains all keys and values as individual Java objects, OakMap stores them in very large memory buffers allocated beyond the JVM-managed memory heap (hence the name Oak - abbr. Off-heap Allocated Keys). The access to the key-value pairs is provided by a lightweight two-level on-heap index. At its lower level, the references to keys are stored in contiguous chunks, each responsible for a distinct key range. The chunks themselves, which dominate the index footprint, are accessed through a lightweight top-level ConcurrentSkipListMap. The figure below illustrates OakMap’s data organization. .  OakMap structure.   . The maintenance of OakMap’s chunked index in a concurrent setting is the crux of its complexity as well as the key for its efficiency. Experiments have shown that our algorithm is advantageous in multiple ways:  . 1.  Memory scaling.  OakMap’s custom off-heap memory allocation alleviates the garbage collection (GC) overhead that plagues Java applications. Despite the permanent progress, modern Java GC algorithms do not practically scale beyond a few tens of GBs of memory, whereas OakMap scales beyond 128GB of off-heap RAM.  . 2.  Query speed.  The chunk-based layout increases data locality, which speeds up both single-key lookups and range scans. All queries enjoy efficient, cache-friendly access, in contrast with permanent dereferencing in object-based maps. On top of these basic merits, OakMap provides safe direct access to its chunks, which avoids an extra copy for rebuilding the original key and value objects. Our benchmarks demonstrate OakMap’s performance benefits versus ConcurrentSkipListMap: . A) Up to 2x throughput for ascending scans. . B) Up to 5x throughput for descending scans.  . C) Up to 3x throughput for lookups. . 3.  Update speed.  Beyond avoiding the GC overhead typical for write-intensive workloads, OakMap optimizes the incremental maintenance of big complex values – for example, aggregate  data sketches , which are indispensable in systems like Druid. It adopts in situ computation on objects embedded in its internal chunks to avoid unnecessary data copy, yet again. In our benchmarks, OakMap achieves up to 1.8x data ingestion rate versus ConcurrentSkipListMap. . With key-value maps being an extremely generic abstraction, it is easy to envision a variety of use cases for OakMap in large-scale analytics and machine learning applications – such as unstructured key-value storage, structured databases, in-memory caches, parameter servers, etc. For example, we are already working with the Druid community on  rebuilding Druid’s core Incremental Index  component around OakMap, in order to boost its scalability and performance.  . We look forward to growing the Oak community! We invite you to explore the project, use OakMap in your applications, raise issues, suggest improvements, and  contribute code . If you have any questions, please feel free to send us a note on the Oak developers list:  oakproject@googlegroups.com . It would be great to hear from you! ", "date": "2018-09-13"}, {"website": "Yahoo", "title": "Open-Sourcing Panoptes, Oath’s distributed network telemetry collector", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/178738044351/open-sourcing-panoptes-oaths-distributed-network", "abstract": "  yahoodevelopers : . By Ian Flint, Network Automation Architect and Varun Varma, Senior Principal Engineer    . The Oath network automation team is proud to announce that we are open-sourcing  Panoptes , a distributed system for collecting, enriching and distributing network telemetry.   . We developed Panoptes to address several issues inherent in legacy polling systems, including overpolling due to multiple point solutions for metrics, a lack of data normalization, consistent data enrichment and integration with infrastructure discovery systems.   . Panoptes is a pluggable, distributed, high-performance data collection system which supports multiple polling formats, including SNMP and vendor-specific APIs. It is also extensible to support emerging streaming telemetry standards including gNMI. .  Architecture  . The following block diagram shows the major components of Panoptes: . Panoptes is written primarily in Python, and leverages multiple open-source technologies to provide the most value for the least development effort. At the center of Panoptes is a metrics bus implemented on Kafka. All data plane transactions flow across this bus; discovery publishes devices to the bus, polling publishes metrics to the bus, and numerous clients read the data off of the bus for additional processing and forwarding. This architecture enables easy data distribution and integration with other systems. For example, in preparing for open-source, we identified a need for a generally available time series datastore. We developed, tested and released a plugin to push metrics into InfluxDB in under a week. This flexibility allows Panoptes to evolve with industry standards. . Check scheduling is accomplished using Celery, a horizontally scalable, open-source scheduler utilizing a Redis data store. Celery’s scalable nature combined with Panoptes’ distributed nature yields excellent scalability. Across Oath, Panoptes currently runs hundreds of thousands of checks per second, and the infrastructure has been tested to more than one million checks per second. . Panoptes ships with a simple, CSV-based discovery system. Integrating Panoptes with a CMDB is as simple as writing an adapter to emit a CSV, and importing that CSV into Panoptes. From there, Panoptes will manage the task of scheduling polling for the desired devices. Users can also develop custom discovery plugins to integrate with their CMDB and other device inventory data sources. . Finally, any metrics gathering system needs a place to send the metrics. Panoptes’ initial release includes an integration with InfluxDB, an industry-standard time series store. Combined with Grafana and the InfluxData ecosystem, this gives teams the ability to quickly set up a fully-featured monitoring environment. .  Deployment at Oath  . At Oath, we anticipate significant benefits from building Panoptes. We will consolidate four siloed polling solutions into one, reducing overpolling and the associated risk of service interruption. As vendors move toward streaming telemetry, Panoptes’ flexible architecture will minimize the effort required to adopt these new protocols. . There is another, less obvious benefit to a system like Panoptes. As is the case with most large enterprises, a massive ecosystem of downstream applications has evolved around our existing polling solutions. Panoptes allows us to continue to populate legacy datastores without continuing to run the polling layers of those systems. This is because Panoptes’ data bus enables multiple metrics consumers, so we can send metrics to both current and legacy datastores. . At Oath, we have deployed Panoptes in a tiered, federated model. We install the software in each of our major data centers and proxy checks out to smaller installations such as edge sites.  All metrics are polled from an instance close to the devices, and metrics are forwarded to a centralized time series datastore. We have also developed numerous custom applications on the platform, including a load balancer monitor, a BGP session monitor, and a topology discovery application. The availability of a flexible, extensible platform has greatly reduced the cost of producing robust network data systems.  .  Easy Setup  . Panoptes’ open-source release is packaged for easy deployment into any Linux-based environment. Deployment is straightforward, so you can have a working system up in hours, not days. . We are excited to share our internal polling solution and welcome engineers to contribute to the codebase, including contributing device adapters, metrics forwarders, discovery plugins, and any other relevant data consumers.   .  Panoptes is available at  https://github.com/yahoo/panoptes , and you can connect with our team at  network-automation@oath.com .   ", "date": "2018-10-04"}, {"website": "Yahoo", "title": "Sharing Vespa (Open Source Big Data Serving Engine) at the SF Big Analytics Meetup", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/179150583591/sharing-vespa-open-source-big-data-serving", "abstract": "  yahoodevelopers : . By Jon Bratseth, Distinguished Architect, Oath  . I had the wonderful opportunity to present  Vespa  at the  SF Big Analytics Meetup  on September 26th, hosted by  Amplitude . Several members of the Vespa team ( Kim ,  Frode  and  Kristian ) also attended. We all enjoyed meeting with members of the Big Analytics community to discuss how Vespa could be helpful for their companies. Thank you to  Chester Chen ,  T.J. Bay , and  Jin Hao Wan  for planning the meetup, and here’s our presentation, in case you missed it (slides are also available  here ): . Largely developed by Yahoo engineers, Vespa is our big data processing and serving engine, available as open source on GitHub. It’s in use by many products, such as Yahoo News, Yahoo Sports, Yahoo Finance and Oath Ads Platforms.  . Vespa use is growing even more rapidly; since it is open source under a permissive Apache license, Vespa can power other external third-party apps as well.  . A great example is Zedge, which uses Vespa for  search and recommender systems to support content discovery for personalization of mobile phones  (Android, iOS, and Web). Zedge uses Vespa in production to serve millions of monthly active users. . Visit  https://vespa.ai/  to learn more and download the code. We encourage code contributions and welcome opportunities to collaborate. ", "date": "2018-10-17"}, {"website": "Yahoo", "title": "Hadoop Contributors Meetup at Oath", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/179901430546/hadoop-contributors-meetup-at-oath", "abstract": "  yahoodevelopers : . By Scott Bush, Director, Hadoop Software Engineering, Oath    . On Tuesday, September 25, we hosted a special day-long  Hadoop Contributors Meetup  at our Sunnyvale, California campus. Much of the early Hadoop development work started at Yahoo, now part of Oath, and has continued over the past decade. Our campus was the perfect setting for this meetup, as we continue to make Hadoop a priority.  . More than 80 Hadoop users, contributors, committers, and PMC members gathered to hear talks on key issues facing the Hadoop user community.  . Speakers from Ampool, Cloudera, Hortonworks, Microsoft, Oath, and Twitter detailed some of the challenges and solutions pertinent to their parts of the Hadoop ecosystem. The talks were followed by a number of parallel, birds of a feather breakout sessions to discuss HDFS, Tez, containers and low latency processing. The day ended with a reception and consensus that the event went well and should be repeated in the near future. . Presentation recordings ( YouTube playlist ) and slides (links included in the video description) are available here: . Thank you to all the presenters and the attendees both in person and remote! . P.S. We’re hiring!  Learn more  about career opportunities at Oath.  ", "date": "2018-11-08"}, {"website": "Yahoo", "title": "Bullet Updates - Windowing, Apache Pulsar PubSub, Configuration-based Data Ingestion, and More", "author": [" rosaliebeevm"], "link": "https://yahooeng.tumblr.com/post/183315480351/bullet-updates-windowing-apache-pulsar-pubsub", "abstract": "  yahoodevelopers : . By  Akshay Sarma , Principal Engineer, Verizon Media &amp;  Brian Xiao , Software Engineer, Verizon Media  . This is the first of an ongoing series of blog posts sharing releases and announcements for  Bullet , an open-sourced lightweight, scalable, pluggable, multi-tenant query system.    . Bullet allows you to query any data flowing through a streaming system without having to store it first through its UI or API. The queries are injected into the running system and have minimal overhead. Running hundreds of queries generally fit into the overhead of just reading the streaming data. Bullet requires running an instance of its backend on your data. This backend runs on common stream processing frameworks (Storm and Spark Streaming currently supported). . The data on which Bullet sits determines what it is used for. For example, our team runs an instance of Bullet on user engagement data (~1M events/sec) to let developers find their own events to validate their code that produces this data. We also use this instance to interactively explore data, throw up quick dashboards to monitor live releases, count unique users, debug issues, and more. . Since  open sourcing Bullet in 2017 , we’ve been hard at work adding many new features! We’ll highlight some of these here and continue sharing update posts for future releases. .  Windowing  . Bullet used to operate in a request-response fashion - you would submit a query and wait for the query to meet its termination conditions (usually duration) before receiving results. For short-lived queries, say, a few seconds, this was fine. But as we started fielding more interactive and iterative queries, waiting even a minute for results became too cumbersome. . Enter windowing! Bullet now supports time and record-based windowing. With time windowing, you can break up your query into chunks of time over its duration and retrieve results for each chunk.  For example, you can calculate the average of a field, and stream back results every second: . In the above example, the aggregation is operating on all the data since the beginning of the query, but you can also do aggregations on just the windows themselves. This is often called a  Tumbling  window:  . With record windowing, you can get the intermediate aggregation for each record that matches your query (a  Sliding  window). Or you can do a  Tumbling  window on records rather than time. For example, you could get results back every three records:  . Overlapping windows in other ways (Hopping windows) or windows that reset based on different criteria (Session windows, Cascading windows) are currently being worked on. Stay tuned!   .  Apache Pulsar support as a native PubSub   . Bullet uses a PubSub (publish-subscribe) message queue to send queries and results between the Web Service and Backend. As with everything else in Bullet, the PubSub is pluggable. You can use your favorite pubsub by implementing a few interfaces if you don’t want to use the ones we provide. Until now, we’ve maintained and supported a REST-based PubSub and an  Apache Kafka  PubSub. Now we are excited to announce supporting  Apache Pulsar  as well!  Bullet Pulsar  will be useful to those users who want to use Pulsar as their underlying messaging service.  . If you aren’t familiar with Pulsar, setting up a local standalone is very simple, and by default, any Pulsar topics written to will automatically be created. Setting up an instance of Bullet with Pulsar instead of REST or Kafka is just as easy. You can refer to  our documentation  for more details. .  Plug your data into Bullet without code  . While Bullet worked on any data source located in any persistence layer, you still had to implement an interface to connect your data source to the Backend and convert it into a record container format that Bullet understands. For instance, your data might be located in Kafka and be in the Avro format. If you were using Bullet on Storm, you would perhaps write a Storm Spout to read from Kafka, deserialize, and convert the Avro data into the Bullet record format. This was the only interface in Bullet that required our customers to write their own code. Not anymore! Bullet DSL is a text/configuration-based format for users to plug in their data to the Bullet Backend without having to write a single line of code. .  Bullet DSL  abstracts away the two major components for plugging data into the Bullet Backend. A Connector piece to read from arbitrary data-sources and a Converter piece to convert that read data into the Bullet record container. We currently support and maintain a few of these - Kafka and Pulsar for Connectors and Avro, Maps and arbitrary Java POJOs for Converters. The Converters understand typed data and can even do a bit of minor ETL (Extract, Transform and Load) if you need to change your data around before feeding it into Bullet. As always, the DSL components are pluggable and you can write your own (and contribute it back!) if you need one that we don’t support. . We appreciate your feedback and contributions! Explore Bullet on  GitHub , use and help contribute to the project, and chat with us on  Google Groups . To get started, try our Quickstarts on  Spark  or  Storm  to set up an instance of Bullet on some fake data and play around with it. ", "date": "2019-03-08"}, {"website": "Yahoo", "title": "Open Sourcing Daytona: A Framework For Automated and Application-agnostic Performance Analysis", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/160987779296/open-sourcing-daytona-a-framework-for-automated", "abstract": " By Sapan Panigrahi and Deepesh Mittal    . Today, we are pleased to offer  Daytona , an open-source framework for automated performance testing and analysis, to the community. Daytona is an application-agnostic framework to conduct integrated performance testing and analysis with repeatable test execution, standardized reporting, and built-in profiling support. . Daytona gives you the capability to build a customized test harness in a single, unified framework to test and analyze the performance of any application. You’ll get easy repeatability, consistent reporting, and the ability to capture trends. Daytona’s UI accepts a performance testing script that can run on a command line. This includes websites, databases, networks, or any workload you need to test and tune for performance. You can submit tests to the scheduler queue from the Daytona UI or from your CI/CD tool. You can deploy Daytona as a hosted service in your on-prem environment or on the public cloud of your choice. In fact, you can even host test harnesses for multiple applications with a single centralized service so that developers, architects, and systems engineers from different parts of your organization can work together on a unified view and manage your performance analysis on a continuous basis.  . Daytona’s differentiation lies in its ability to aggregate and present essential aspects of application, system, and hardware performance metrics with  a simple and unified user interface . This helps you maintain your focus on performance analysis without changing context across various sources and formats of data. The overall goal of performance analysis is to find ways of maximizing application throughput with minimum hardware resource and the best user experience. Metrics and insights from Daytona help achieve this objective. . Prior to Daytona, we created multiple, heterogenous performance tools to meet the specific needs of various applications. This meant that we often stored test results inconsistently, making it harder to analyze performance in a comprehensive manner. We had a difficult time sharing results and analyzing differences in test runs in a standard manner, which could lead to confusion.  . With Daytona, we are now able to integrate all our load testing tools under a single framework and aggregate test results in one common central repository. We are gaining insight into the performance characteristics of many of our applications on a continuous basis. These insights help us optimize our applications which results in better utilization of our hardware resources and helps improve user experience by reducing the latency to serve end-user requests. Ultimately, Daytona helps us reduce capital expenditure on our large-scale infrastructure and makes our applications more robust under load. Sharing performance results in a common format encourages the use of common optimization techniques that we can leverage across many different applications. . Daytona was built knowing that we would want to publish it as open source and share the technology with the community for validation and improvement of the framework. We hope the community can help extend its use cases and make it suitable for an even broader set of applications and workloads.  .  Architecture  . Daytona is comprised of a centralized scheduler, a distributed set of agents running on SUTs (systems under test), a MySQL database to store all metadata for tests, and a PHP-based UI. A test harness can be customized by answering a simple set of questions about the application/workload. A test can be submitted to Daytona’s queue through the UI or through a CLI (Command Line Interface) from the CI/CD system. The scheduler process polls the database for a test to be run and sends all the actions associated with the execution of the test to the agent running on a SUT. An agent process executes the test, collects application and system performance metrics, and sends the metrics back as a package to the scheduler. The scheduler saves the test metadata in the database and test results in the local file system. Tests from multiple harnesses proceed concurrently.  .    .  .  Looking Forward  . Our goal is to integrate Daytona with popular open source CI/CD tools and we welcome contributions from the community to make that happen. It is available under Apache License Version 2.0. To evaluate Daytona, we provide simple instructions to deploy it on your in-house bare metal, VM, or public cloud infrastructure. We also provide instructions so you can quickly have a test and development environment up and running on your laptop with Docker. Please join us on the path of making application performance analysis an enjoyable and insightful experience. Visit the  Daytona Yahoo repo  to get started!  ", "date": "2017-05-23"}, {"website": "Yahoo", "title": "Vespa Product Updates, December 2018 - ONNX Import and Map Attribute Grouping", "author": [" amberwilsonla-blog"], "link": "https://yahooeng.tumblr.com/post/181089393751/vespa-product-updates-december-2018-onnx-import", "abstract": "  yahoodevelopers : . Today we’re kicking off a blog post series of need-to-know updates on Vespa, summarizing the features and fixes detailed in  Github issues .  . We welcome your  contributions  and feedback about any new features or improvements you’d like to see.  . For December, we’re excited to share the following product news: .  Streaming Search Performance Improvement  . Streaming Search is a solution for applications where each query only searches a small, statically determined subset of the corpus. In this case, Vespa searches without building reverse indexes, reducing storage cost and making writes more efficient. With the latest changes, the document type is used to further limit data scanning, resulting in lower latencies and higher throughput. Read more  here . .   .  ONNX Integration  .  ONNX  is an open ecosystem for interchangeable AI models. Vespa now supports importing models in the ONNX format and transforming the models into  Tensors  for use in ranking. This adds to the TensorFlow import included earlier this year and allows Vespa to support many training tools. While Vespa’s strength is real-time model evaluation over large datasets, to get started using single data points, try the  stateless model evaluation API . Explore this integration more in  Ranking with ONNX models . .   .  Precise Transaction Log Pruning  . Vespa is built for large applications running continuous integration and deployment. This means nodes restart often for software upgrades, and node restart time matters. A common pattern is serving while restarting hosts one by one. Vespa has optimized transaction log pruning with prepareRestart, due to flushing as much as possible before stopping, which is quicker than replaying the same data after restarting. This feature is on by default. Learn more in live  upgrade  and  prepareRestart . .   .  Grouping on Maps  . Grouping is used to implement faceting. Vespa has added support to group using map attribute fields, creating a group for values whose keys match the specified key, or field values referenced by the key. This support is useful to create indirections and relations in data and is great for use cases with structured data like e-commerce. Leverage key values instead of field names to simplify the search definition. Read more in  Grouping on Map Attributes . .   .  Questions or suggestions?  Send us a tweet  or an  email .   ", "date": "2018-12-13"}, {"website": "Yahoo", "title": "Open Sourcing Bullet, Yahoo’s Forward-Looking Query Engine for Streaming Data", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/161855616651/open-sourcing-bullet-yahoos-forward-looking", "abstract": " By Michael Natkovich, Akshai Sarma, Nathan Speidel, Marcus Svedman, and Cat Utah . Big Data is no longer just Apache server logs. Nowadays, the data may be user engagement data, performance metrics, IoT (Internet of Things) data, or something else completely atypical. Regardless of the size of the data, or the type of querying patterns on it (exploratory, ad-hoc, periodic, long-term, etc.), everyone wants queries to be as fast as possible and cheap to run in terms of resources. Data can be broadly split into two kinds: the streaming (generally real-time) kind or the batched-up-over-a-time-interval (e.g., hourly or daily) kind. The batch version is typically easier to query since it is stored somewhere like a data warehouse that has nice SQL-like interfaces or an easy to use UI provided by tools such as Tableau, Looker, or Superset. Running  arbitrary  queries on streaming data  quickly  and  cheaply  though, is generally much harder… until now. Today, we are pleased to share our newly open sourced,  forward-looking  general purpose query engine, called  Bullet , with the  community on GitHub .  . With Bullet, you can:  . One of the key differences between how Bullet queries data and the standard querying paradigm is that Bullet does not store any data. In most other systems where you have a persistence layer (including in-memory storage), you are doing a look-back when you query the layer. Instead, Bullet operates on data flowing through the system after the query is started – it’s a look-forward system that doesn’t need persistence. On a real-time data stream, this means that  Bullet is querying data after the query is submitted . This also means that Bullet does not query any data that has already passed through the stream. The fact that Bullet does not rely on a persistence layer is exactly what makes it extremely lightweight and cheap to run.  . To see why this is better for the kinds of use cases Bullet is meant for – such as quickly looking at some metric, checking some assumption, iterating on a query, checking the status of something right now, etc. – consider the following: if you had a 1000 queries in a traditional query system that operated on the same data, these query systems would most likely scan the data 1000 times each. By the very virtue of it being forward looking, 1000 queries in Bullet scan the data only once because the arrival of the query determines and fixes the data that it will see. Essentially, the data is coming to the queries instead of the queries being farmed out to where the data is. When the conditions of the query are satisfied (usually a time window or a number of events), the query terminates and returns you the result.  .  A Brief Architecture Overview  .    . High Level Bullet Architecture . The Bullet architecture is  multi-tenant , can  scale linearly  for more queries and/or more data, and has been tested to handle  700+  simultaneous queries on a data stream that had up to  1.5 million  records per second, or  5-6 GB/s . Bullet is currently implemented on top of Storm and can be extended to support other stream processing engines as well, like Spark Streaming or Flink. Bullet is pluggable, so you can plug in any source of data that can be read in Storm by implementing a simple data container interface to let Bullet work with it.  . The UI, web service, and the backend layers constitute your standard three-tier architecture. The Bullet backend can be split into three main subsystems: . The web service can be deployed on any servlet container, like Jetty. The UI is a Node-based Ember application that runs in the client browser. Our  full documentation  contains all the details on exactly how we perform computationally-intractable queries like Count Distincts on fields with cardinality in the millions, etc. ( DataSketches ).  .  Usage at Yahoo   . An instance of Bullet is currently running at Yahoo in production against a small subset of Yahoo’s user engagement data stream. This data is roughly 100,000 records per second and is about 130 MB/s compressed. Bullet queries this with about 100 CPU Virtual Cores and 120 GB of RAM. This fits on less than 2 of our (64 Virtual Cores, 256 GB RAM each) test Storm cluster machines.  . One of the most popular use cases at Yahoo is to use Bullet to manually validate the instrumentation of an app or web application. Instrumentation produces user engagement data like clicks, views, swipes, etc. Since this data powers everything we do from analytics to personalization to targeting, it is absolutely critical that the data is correct. The usage pattern is generally to:  . In addition, Bullet is also used programmatically in continuous delivery pipelines for functional testing instrumentation on product releases. Product usage is simulated, then data is generated and validated in seconds using Bullet. Bullet is orders of magnitude faster to use for this kind of validation and for general data exploration use cases, as opposed to waiting for the data to be available in Hive or other systems. The Bullet UI supports pivot tables and a multitude of charting options that may speed up analysis further compared to other querying options.  . We also use Bullet to do a bunch of other interesting things, including instances where we dynamically compute cardinalities (using a Count Distinct Bullet query) of fields as a check to protect systems that can’t support extremely high cardinalities for fields like Druid.  . What you do with Bullet is entirely determined by the data you put it on. If you put it on data that is essentially some set of performance metrics (data center statistics for example), you could be running a lot of queries that find the 95th and 99th percentile of a metric. If you put it on user engagement data, you could be validating instrumentation and mostly looking at raw data.  . We hope you will find Bullet interesting and tell us how you use it. If you find something you want to change, improve, or fix, your contributions and ideas are always welcome! You can contact us  here .  .  Helpful Links   ", "date": "2017-06-15"}, {"website": "Yahoo", "title": "Speed and Stability: Yahoo Mail’s Forward-Thinking Continuous Integration and Delivery Pipeline", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/162320459636/speed-and-stability-yahoo-mails-forward-thinking", "abstract": " By Mohit Goenka, Senior Engineering Manager .  Building the technology  powering the best consumer email inbox in the world is no easy task. When you start on such a journey, it is important to consider how to deliver such an experience to the users. After all, any consumer feature we build can only make a difference after it is delivered to everyone via the tech pipeline.  . As we began building out the  new version of Yahoo Mail , we wanted to ensure that our internal developer productivity would not be hindered by how our pipelines work. Keeping this in mind, we identified the following principles as most important while designing the delivery pipeline for the new Yahoo Mail experience:  .  Product updates are pushed at regular intervals   . We ensure that our engineers can push any code changes to all Mail users everyday, with the ability to push multiple times a day, if necessary or desired. This is possible because of the time we spent building a solid testing infrastructure, which continues to evolve as we scale to new users and add new features to the product. Every one of our builds runs 10,000+ unit tests and 5,000+ integration tests on various combinations of operating systems and browsers. It is important to push product updates regularly as it allows all our users to get the best Mail experience possible.  .  Releases are stable   . Every code release starts with the company’s internal audience first, where all our employees get to try out the latest changes before they go out to production. This begins with our  alpha  and  beta  environments that our Mail engineers use by default. Our build then goes out to the  canary  environment, which is a small subset of production users, before making it to all users. This gives us the ability to analyze quality metrics on internal and  canary  servers before rolling the build out to 100% of users in production. Once we go through this process, the code pushed to all our users is thoroughly baked and tested.  .  Builds are not blocked by irrational test failures   . Running tests using web drivers on multiple browsers, as is standard when testing frontend code, comes with the problem of tests irrationally failing. As part the Yahoo Mail continuous delivery pipeline, we employ various novel strategies to recover from such failures. One such strategy is recording the data related to failed tests in the first pass of a build, and then rerunning only the failed tests in the subsequent passes. This is achieved by creating a metadata file that stores all our build-related information. As part of this process, a new bundle is created with a new set of code changes. Once a bundle is created with build metadata information, the same build job can be rerun multiple times such that subsequent reruns would only run the failing tests. This significantly improves rerun times and eliminates the chances of build detentions introduced by irrational test failures. The recorded test information is analyzed independently to understand the pattern of failing tests. This helps us in improving the stability of those intermittently failing tests.  .  Developers are notified of code pushes   . Our build and deployment pipelines collect data related to all the authors contributing to any release through code commits or by merging various pull requests. This enables the build pipeline to send out email notifications to all our Mail developers as their code flows through each environment in our build pipeline ( alpha ,  beta ,  canary , and  production ). With this ability, developers are well aware of where their code is in the pipeline and can test their changes as needed.  .  Hotfixes   . We have also created a pipeline to deploy major code fixes directly to production. This is needed even after the existence of tens of thousands of tests and multitudes of checks. Every now and then, a bug may make its way into production. For such instances, we have  hotfixes  that are very useful. These are code patches that we quickly deploy on top of production code to address critical issues impacting large sets of users.  .  Rollbacks   . If we find any issues in production, we do our best to minimize the impact on users by swiftly utilizing rollbacks, ensuring there is zero to minimal impact time. In order to do rollbacks, we maintain lists of all the versions pushed to production along with their release bundles and change logs. If needed, we pick the stable version that was previously pushed to production and deploy it directly on all the machines running our production instance.  .  Heartbeat pushes  . As part of our continuous delivery efforts, we have also developed a concept we call  heartbeat  pushes. Heartbeat pushes are notifications we send users to refresh their browsers when we issue important builds that they should immediately adopt. These can include bug fixes, product updates, or new features. Heartbeat allows us to dynamically update the latest version of Yahoo Mail when we see that a user’s current version needs to be updated. .    .    .  . In building the new Yahoo Mail experience, we knew that we needed to revamp from the ground up, starting with our continuous integration and delivery pipeline. The guiding principles of our new, forward-thinking infrastructure allow us to deliver new features and code fixes at a very high launch velocity and ensure that our users are always getting the latest and greatest Yahoo Mail experience. ", "date": "2017-06-27"}, {"website": "Yahoo", "title": "Yahoo Mail’s New Tech Stack, Built for Performance and Reliability", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/162320493306/yahoo-mails-new-tech-stack-built-for-performance", "abstract": " By Suhas Sadanandan, Director of Engineering  . When it comes to performance and reliability, there is perhaps no application where this matters more than with email. Today,  we announced  a new Yahoo Mail experience for desktop based on a completely rewritten tech stack that embodies these fundamental considerations and more.  . We built the new Yahoo Mail experience using a best-in-class front-end tech stack with open source technologies including React, Redux, Node.js,  react-intl  (open-sourced by Yahoo), and others. A high-level architectural diagram of our stack is below.  .    .    . In building our new tech stack, we made use of the most modern tools available in the industry to come up with the best experience for our users by optimizing the following fundamentals:  .  Performance  . A key feature of the new Yahoo Mail architecture is blazing-fast initial loading (aka, launch). . We introduced new network routing which sends users to their nearest geo-located email servers ( proximity-based routing ). This has resulted in a significant reduction in time to first byte and should be immediately noticeable to our international users in particular. . We now do  server-side rendering  to allow our users to see their mail sooner. This change will be immediately noticeable to our low-bandwidth users. Our application is  isomorphic , meaning that the same code runs on the server (using Node.js) and the client. Prior versions of Yahoo Mail had programming logic duplicated on the server and the client because we used PHP on the server and JavaScript on the client.    . Using  efficient bundling strategies  (JavaScript code is separated into application, vendor, and lazy loaded bundles) and pushing only the changed bundles during production pushes, we keep the cache hit ratio high. By using react-atomic-css, our homegrown solution for writing modular and scoped CSS in React, we get much  better CSS reuse .   . In prior versions of Yahoo Mail, the need to run various experiments in parallel resulted in additional branching and bloating of our JavaScript and CSS code. While rewriting all of our code, we solved this issue using  Mendel , our homegrown solution for bucket testing isomorphic web apps, which we have  open sourced .   . Rather than using custom libraries, we use  native HTML5 APIs and ES6  heavily and use PolyesterJS, our homegrown polyfill solution, to fill the gaps. These factors have further helped us to keep payload size minimal.  . With all the above optimizations, we have been able to reduce our JavaScript and CSS footprint by approximately 50% compared to the previous desktop version of Yahoo Mail, helping us achieve a blazing-fast launch. . In addition to initial launch improvements, key features like search and message read (when a user opens an email to read it) have also benefited from the above optimizations and are considerably faster in the latest version of Yahoo Mail.  . We also significantly reduced the memory consumed by Yahoo Mail on the browser. This is especially noticeable during a long running session. .  Reliability  . With this new version of Yahoo Mail, we have a 99.99% success rate on core flows: launch, message read, compose, search, and actions that affect messages. Accomplishing this over several billion user actions a day is a significant feat. Client-side errors (JavaScript exceptions) are reduced significantly when compared to prior Yahoo Mail versions. .  Product agility and launch velocity  . We focused on independently deployable components. As part of the re-architecture of Yahoo Mail, we invested in a robust  continuous integration and delivery flow . Our new pipeline allows for daily (or more) pushes to all Mail users, and we push only the bundles that are modified, which keeps the cache hit ratio high. .  Developer effectiveness and satisfaction  . In developing our tech stack for the new Yahoo Mail experience, we heavily leveraged open source technologies, which allowed us to ensure a shorter learning curve for new engineers. We were able to implement a consistent and intuitive onboarding program for 30+ developers and are now using our program for all new hires. During the development process, we emphasise predictable flows and easy debugging. .  Accessibility  . The accessibility of this new version of Yahoo Mail is state of the art and delivers outstanding usability (efficiency) in addition to accessibility. It features six enhanced visual themes that can provide accommodation for people with low vision and has been optimized for use with Assistive Technology including alternate input devices, magnifiers, and popular screen readers such as NVDA and VoiceOver. These features have been rigorously evaluated and incorporate feedback from users with disabilities. It sets a new standard for the accessibility of web-based mail and is our  most-accessible Mail experience  yet. .  Open source   . We have open sourced some key components of our new Mail stack, like  Mendel , our solution for bucket testing isomorphic web applications. We invite the community to use and build upon our code. Going forward, we plan on also open sourcing additional components like react-atomic-css, our solution for writing modular and scoped CSS in React, and lazy-component, our solution for on-demand loading of resources.  . Many of our company’s best technical minds came together to write a brand new tech stack and enable a delightful new Yahoo Mail experience for our users.  . We encourage our users and engineering peers in the industry to test the limits of our application, and to provide feedback by clicking on the Give Feedback call out in the lower left corner of the new version of Yahoo Mail.  ", "date": "2017-06-27"}, {"website": "Yahoo", "title": "Success at Apache: A Newbie’s Narrative", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/170536010891/success-at-apache-a-newbies-narrative", "abstract": "  yahoodevelopers : . By Kuhu Shukla .  This post first appeared  here  on the Apache Software Foundation blog as part of ASF’s “Success at Apache” monthly blog series.   . As I sit at my desk on a rather frosty morning with my coffee, looking up new JIRAs from the previous day in the Apache Tez project, I feel rather pleased. The latest community release vote is complete, the bug fixes that we so badly needed are in and the new release that we tested out internally on our many thousand strong cluster is looking good. Today I am looking at a new stack trace from a different Apache project process and it is hard to miss how much of the exceptional code I get to look at every day comes from people all around the globe. A contributor leaves a JIRA comment before he goes on to pick up his kid from soccer practice while someone else wakes up to find that her effort on a bug fix for the past two months has finally come to fruition through a binding +1.    . Yahoo – which joined AOL, HuffPost, Tumblr, Engadget, and many more brands to form the Verizon subsidiary Oath last year – has been at the frontier of open source adoption and contribution since before I was in high school. So while I have no historical trajectories to share, I do have a story on how I found myself in an epic journey of migrating all of Yahoo jobs from Apache MapReduce to Apache Tez, a then-new DAG based execution engine. . Oath grid infrastructure is through and through driven by Apache technologies be it storage through HDFS, resource management through YARN, job execution frameworks with Tez and user interface engines such as Hive, Hue, Pig, Sqoop, Spark, Storm. Our grid solution is specifically tailored to Oath’s business-critical data pipeline needs using the polymorphic technologies hosted, developed and maintained by the Apache community. . On the third day of my job at Yahoo in 2015, I received a YouTube link on  An Introduction to Apache Tez . I watched it carefully trying to keep up with all the questions I had and recognized a few names from my academic readings of Yarn ACM papers. I continued to ramp up on YARN and HDFS, the foundational Apache technologies Oath heavily contributes to even today. For the first few weeks I spent time picking out my favorite (necessary) mailing lists to subscribe to and getting started on setting up on a pseudo-distributed Hadoop cluster. I continued to find my footing with newbie contributions and being ever more careful with whitespaces in my patches. One thing was clear – Tez was the next big thing for us. By the time I could truly call myself a contributor in the Hadoop community nearly 80-90% of the Yahoo jobs were now running with Tez. But just like hiking up the Grand Canyon, the last 20% is where all the pain was. Being a part of the solution to this challenge was a happy prospect and thankfully contributing to Tez became a goal in my next quarter. . The next sprint planning meeting ended with me getting my first major Tez assignment – progress reporting. The progress reporting in Tez was non-existent – “Just needs an API fix,”  I thought. Like almost all bugs in this ecosystem, it was not easy. How do you define progress? How is it different for different kinds of outputs in a graph? The questions were many. . I, however, did not have to go far to get answers. The Tez community actively came to a newbie’s rescue, finding answers and posing important questions. I started attending the bi-weekly Tez community sync up calls and asking existing contributors and committers for course correction. Suddenly the team was much bigger, the goals much more chiseled. This was new to anyone like me who came from the networking industry, where the most open part of the code are the RFCs and the implementation details are often hidden. These meetings served as a clean room for our coding ideas and experiments. Ideas were shared, to the extent of which data structure we should pick and what a future user of Tez would take from it. In between the usual status updates and extensive knowledge transfers were made. . Oath uses Apache Pig and Apache Hive extensively and most of the urgent requirements and requests came from Pig and Hive developers and users. Each issue led to a community JIRA and as we started running Tez at Oath scale, new feature ideas and bugs around performance and resource utilization materialized. Every year most of the Hadoop team at Oath travels to the Hadoop Summit where we meet our cohorts from the Apache community and we stand for hours discussing the state of the art and what is next for the project. One such discussion set the course for the next year and a half for me. . We needed an innovative way to shuffle data. Frameworks like MapReduce and Tez have a shuffle phase in their processing lifecycle wherein the data from upstream producers is made available to downstream consumers. Even though Apache Tez was designed with a feature set corresponding to optimization requirements in Pig and Hive, the Shuffle Handler Service was retrofitted from MapReduce at the time of the project’s inception. With several thousands of jobs on our clusters leveraging these features in Tez, the Shuffle Handler Service became a clear performance bottleneck. So as we stood talking about our experience with Tez with our friends from the community, we decided to implement a new Shuffle Handler for Tez. All the conversation points were tracked now through an umbrella JIRA TEZ-3334 and the to-do list was long. I picked a few JIRAs and as I started reading through I realized, this is all new code I get to contribute to and review. There might be a better way to put this, but to be honest it was just a lot of fun! All the whiteboards were full, the team took walks post lunch and discussed how to go about defining the API. Countless hours were spent debugging hangs while fetching data and looking at stack traces and Wireshark captures from our test runs. Six months in and we had the feature on our sandbox clusters. There were moments ranging from sheer frustration to absolute exhilaration with high fives as we continued to address review comments and fixing big and small issues with this evolving feature. . As much as owning your code is valued everywhere in the software community, I would never go on to say “I did this!” In fact, “we did!” It is this strong sense of shared ownership and fluid team structure that makes the open source experience at Apache truly rewarding. This is just one example. A lot of the work that was done in Tez was leveraged by the Hive and Pig community and cross Apache product community interaction made the work ever more interesting and challenging. Triaging and fixing issues with the Tez rollout led us to hit a 100% migration score last year and we also rolled the Tez Shuffle Handler Service out to our research clusters. As of last year we have run around 100 million Tez DAGs with a total of 50 billion tasks over almost 38,000 nodes. . In 2018 as I move on to explore Hadoop 3.0 as our future release, I hope that if someone outside the Apache community is reading this, it will inspire and intrigue them to contribute to a project of their choice. As an astronomy aficionado, going from a newbie Apache contributor to a newbie Apache committer was very much like looking through my telescope － it has endless possibilities and challenges you to be your best. .   About the Author:   .  Kuhu Shukla is a software engineer at Oath and did her Masters in Computer Science at North Carolina State University. She works on the Big Data Platforms team on Apache Tez, YARN and HDFS with a lot of talented Apache PMCs and Committers in Champaign, Illinois. A recent Apache Tez Committer herself she continues to contribute to YARN and HDFS and spoke at the 2017 Dataworks Hadoop Summit on “Tez Shuffle Handler: Shuffling At Scale With Apache Hadoop”. Prior to that she worked on Juniper Networks’ router and switch configuration APIs. She likes to participate in open source conferences and women in tech events. In her spare time she loves singing Indian classical and jazz, laughing, whale watching, hiking and peering through her Dobsonian telescope.  ", "date": "2018-02-05"}, {"website": "Yahoo", "title": "Secure Images", "author": [" marcelatoath"], "link": "https://yahooeng.tumblr.com/post/172068649246/secure-images", "abstract": "  oath-postmaster : . By Marcel Becker . The mail team at OATH is busy  integrating  Yahoo and AOL technology to deliver an even better experience across all our consumer mail products. While privacy and security are top priority for us, we also want to improve the experience and remove unnecessary clutter across all of our products.  . Starting this week we will be serving images in mails via our own secure proxy servers. This will not only increase speed and security in our own mail products and reduce the risk of phishing and other scams,  but it will also mean that our users don’t have to fiddle around with those “enable images” settings. Messages and inline images will now just show up as originally intended.  . We are aware that commercial mail senders are relying on images (so-called pixels) to track delivery and open rates. Our proxy solution will continue to support most of these cases and ensure that true mail opens are recorded.  . For senders serving dynamic content based on the recipient’s location   (leveraging standard IP-based browser and app capabilities)   we recommend falling back on other tools and technologies which do not rely on IP-based targeting.  . All of our consumer mail applications (Yahoo and AOL) will benefit from this change. This includes our desktop products as well as our mobile applications across iOS and Android. . If you have any feedback or want to discuss those changes with us personally, just send us a note to  mail-questions@oath.com . . (via  verizon-postmaster ) ", "date": "2018-03-20"}, {"website": "Yahoo", "title": "How to Make Your Web App More Reliable and Performant Using webpack: a Yahoo Mail Case Study", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/168508200981/how-to-make-your-web-app-more-reliable-and", "abstract": "  yahoodevelopers : .  . By Murali Krishna Bachhu, Anurag Damle, and Utkarsh Shrivastava  . As engineers on the Yahoo Mail team at Oath, we pride ourselves on the things that matter most to developers: faster development cycles, more reliability, and better performance. Users don’t necessarily  see  these elements, but they certainly feel the difference they make when significant improvements are made. Recently, we were able to upgrade all three of these areas at scale by adopting  webpack ® as Yahoo Mail’s underlying module bundler, and you can do the same for your web application. .  What is webpack?  . webpack is an open source module bundler for modern JavaScript applications. When webpack processes your application, it recursively builds a dependency graph that includes every module your application needs. Then it packages all of those modules into a small number of bundles, often only one, to be loaded by the browser. . webpack became our choice module bundler not only because it supports on-demand loading, multiple bundle generation, and has a relatively low runtime overhead, but also because it is better suited for web platforms and NodeJS apps and has great community support. .    .  How did we integrate webpack?   . Like any developer does when integrating a new module bundler, we started integrating webpack into Yahoo Mail by looking at its basic  config  file. We explored available  default webpack plugins  as well as  third-party webpack plugins  and then picked the plugins most suitable for our application. If we didn’t find a plugin that suited a specific need, we wrote the  webpack plugin ourselves  (e.g., We wrote a plugin to execute Atomic CSS scripts in the  latest Yahoo Mail experience  in order to decrease our overall CSS payload**). . During the development process for Yahoo Mail, we needed a way to make sure webpack would continuously run in the background. To make this happen, we decided to use the task runner  Grunt . Not only does Grunt keep the connection to webpack alive, but it also gives us the ability to pass different parameters to the webpack config file based on the given environment. Some examples of these parameters are  source map options ,  enabling HMR , and uglification. . Before deployment to production, we wanted to optimize the javascript bundles for size to make the Yahoo Mail experience faster. webpack provides good default support for this with the  UglifyJS  plugin. Although the default options are conservative, they give us the ability to  configure the options . Once we modified the options to our specifications, we saved approximately 10KB.  .    .  Faster development cycles for developers   . While developing a new feature, engineers ideally want to see their code changes reflected on their web app instantaneously. This allows them to maintain their train of thought and eventually results in more productivity. Before we implemented webpack, it took us around 30 seconds to 1 minute for changes to reflect on our Yahoo Mail development environment. webpack helped us reduce the wait time to 5 seconds. .  More reliability  . Consumers love a reliable product, where all the features work seamlessly every time. Before we began using webpack, we were generating javascript bundles on demand or during run-time, which meant the product was more prone to exceptions or failures while fetching the javascript bundles. With webpack, we now generate all the bundles during build time, which means that all the bundles are available whenever consumers access Yahoo Mail. This results in significantly fewer exceptions and failures and a better experience overall. .  Better Performance  . We were able to attain a significant reduction of payload after adopting webpack. . Below are some charts that demonstrate the payload size of Yahoo Mail before and after implementing webpack. .    .    .    .  Conclusion   . Shifting to webpack has resulted in significant improvements. We saw a common build process go from 30 seconds to 5 seconds, large JavaScript bundle size reductions, and a halving in server-side rendering time. In addition to these benefits, our engineers have found the community support for webpack to have been impressive as well. webpack has made the development of Yahoo Mail more efficient and enhanced the product for users. We believe you can use it to achieve similar results for your web application as well. .  **Optimized CSS generation with Atomizer  . Before we implemented webpack into the development of Yahoo Mail, we looked into how we could decrease our CSS payload. To achieve this, we developed an in-house solution for writing modular and scoped CSS in React. Our solution is similar to the  Atomizer  library, and our CSS is written in JavaScript like the example below: .    . Every React component creates its own styles.js file with required style definitions. React-Atomic-CSS converts these files into unique class definitions. Our total CSS payload after implementing our solution equaled all the unique style definitions in our code, or only 83KB (21KB gzipped).  . During our migration to webpack, we created a custom plugin and loader to parse these files and extract the unique style definitions from all of our CSS files. Since this process is tied to bundling, only CSS files that are part of the dependency chain are included in the final CSS. ", "date": "2017-12-13"}, {"website": "Yahoo", "title": "Achieving Major Stability and Performance Improvements in Yahoo Mail with a Novel Redux Architecture", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/173062946866/achieving-major-stability-and-performance", "abstract": "  yahoodevelopers : . By Mohit Goenka, Gnanavel Shanmugam, and Lance Welsh . At Yahoo Mail, we’re constantly striving to upgrade our product experience. We do this not only by adding  new features based on our members’   feedback , but also by providing the best technical solutions to power the most engaging experiences. As such, we’ve recently introduced a number of novel and unique revisions to the way in which we use Redux that have resulted in significant stability and performance improvements. Developers may find our methods useful in achieving similar results in their apps.  .  Improvements to product metrics  . Last year Yahoo Mail implemented a  brand new architecture  using Redux. Since then, we have transformed the overall architecture to reduce latencies in various operations, reduce JavaScript exceptions, and better synchronized states. As a result, the product is much faster and more stable. . Stability improvements: . Performance improvements: . We have also reduced API calls by approximately 20%. .  How we use Redux in Yahoo Mail  . Redux architecture is reliant on one large store that represents the application state. In a Redux cycle, action creators dispatch actions to change the state of the store. React Components then respond to those state changes. We’ve made some modifications on top of this architecture that are atypical in the React-Redux community. . For instance, when fetching data over the network, the traditional methodology is to use Thunk middleware. Yahoo Mail fetches data over the network from our API. Thunks would create an unnecessary and undesirable dependency between the action creators and our API. If and when the API changes, the action creators must then also change. To keep these concerns separate we dispatch the action payload from the action creator to store them in the Redux state for later processing by “ action syncers ”. Action syncers use the payload information from the store to make requests to the API and process responses. In other words, the action syncers form an API layer by interacting with the store. An additional benefit to keeping the concerns separate is that the API layer can change as the backend changes, thereby preventing such changes from bubbling back up into the action creators and components. This also allowed us to optimize the API calls by batching, deduping, and processing the requests only when the network is available. We applied similar strategies for handling other side effects like route handling and instrumentation. Overall, action syncers helped us to reduce our API calls by ~20% and bring down API errors by 20-30%. . Another change to the normal Redux architecture was made to avoid unnecessary props. The React-Redux community has learned to avoid passing unnecessary props from high-level components through multiple layers down to lower-level components (prop drilling) for rendering. We have introduced  action enhancers  middleware to avoid passing additional unnecessary props that are purely used when dispatching actions. Action enhancers add data to the action payload so that data does not have to come from the component when dispatching the action. This avoids the component from having to receive that data through props and has improved frame rendering by ~40%. The use of action enhancers also avoids writing utility functions to add commonly-used data to each action from action creators. .    . In our new architecture, the store reducers accept the dispatched action via action enhancers to update the state. The store then updates the UI, completing the action cycle. Action syncers then initiate the call to the backend APIs to synchronize local changes. .  Conclusion  . Our novel use of Redux in Yahoo Mail has led to significant user-facing benefits through a more performant application. It has also reduced development cycles for new features due to its simplified architecture. We’re excited to share our work with the community and would love to hear from anyone interested in learning more. ", "date": "2018-04-18"}, {"website": "Yahoo", "title": "A Peek Behind the Mail Curtain", "author": [" marcelatoath"], "link": "https://yahooeng.tumblr.com/post/174023151641/a-peek-behind-the-mail-curtain", "abstract": "  USE IMAP TO ACCESS SOME UNIQUE FEATURES  .  By Libby Lin, Principal Product Manager  . Well, we actually won’t show you how we create the magic in our big OATH consumer mail factory. But nevertheless we wanted to share how interested developers could leverage some of our unique features we offer for our Yahoo and AOL Mail customers.  . To drive experiences like our travel and shopping smart views or message threading, we tag qualified mails with something we call  DECOS  and  THREADID . While we will not indulge in explaining how exactly we use them internally, we wanted to share how they can be used and accessed through IMAP.  . So let’s just look at a sample IMAP command chain. We’ll just assume that you are familiar with the IMAP protocol at this point and you know how to properly talk to an IMAP server.  . So here’s how you would retrieve  DECO  and  THREADID s for specific messages:  .   1. CONNECT   .      openssl s_client -crlf -connect imap.mail.yahoo.com:993  .   2. LOGIN    .      a login username password  .     a OK LOGIN completed .   3. LIST FOLDERS   .      a list “” “*”  .     * LIST (\\Junk \\HasNoChildren) “/” “Bulk Mail” .     * LIST (\\Archive \\HasNoChildren) “/” “Archive” .     * LIST (\\Drafts \\HasNoChildren) “/” “Draft” .     * LIST (\\HasNoChildren) “/” “Inbox” .     * LIST (\\HasNoChildren) “/” “Notes” .     * LIST (\\Sent \\HasNoChildren) “/” “Sent” .     * LIST (\\Trash \\HasChildren) “/” “Trash” .     * LIST (\\HasNoChildren) “/” “Trash/l2” .     * LIST (\\HasChildren) “/” “test level 1” .     * LIST (\\HasNoChildren) “/” “test level 1/nestedfolder” .     * LIST (\\HasNoChildren) “/” “test level 1/test level 2” .     * LIST (\\HasNoChildren) “/” “&amp;T2BZfXso-” .     * LIST (\\HasNoChildren) “/” “&amp;gQKAqk7WWr12hA-” .     a OK LIST completed .   4.SELECT FOLDER   .      a select inbox  .     * 94 EXISTS .     * 0 RECENT .     * OK [UIDVALIDITY 1453335194] UIDs valid .     * OK [UIDNEXT 40213] Predicted next UID .     * FLAGS (\\Answered \\Deleted \\Draft \\Flagged \\Seen $Forwarded $Junk $NotJunk) .     * OK [PERMANENTFLAGS (\\Answered \\Deleted \\Draft \\Flagged \\Seen $Forwarded $Junk $NotJunk)] Permanent flags .     * OK [HIGHESTMODSEQ 205] .     a OK [READ-WRITE] SELECT completed; now in selected state .   5. SEARCH FOR UID   .      a uid search 1:*  .     * SEARCH 1 2 3 4 11 12 14 23 24 75 76 77 78 114 120 121 124 128 129 130 132 133 134 135 136 137 138 40139 40140 40141 40142 40143 40144 40145 40146 40147 40148     40149 40150 40151 40152 40153 40154 40155 40156 40157 40158 40159 40160 40161 40162 40163 40164 40165 40166 40167 40168 40172 40173 40174 40175 40176     40177 40178 40179 40182 40183 40184 40185 40186 40187 40188 40190 40191 40192 40193 40194 40195 40196 40197 40198 40199 40200 40201 40202 40203 40204     40205 40206 40207 40208 40209 40211 40212  .     a OK UID SEARCH completed .   6. FETCH DECOS BASED ON UID   .      a uid fetch 40212 (X-MSG-DECOS X-MSG-ID X-MSG-THREADID)  .     * 94 FETCH (UID 40212 X-MSG-THREADID “108” X-MSG-ID “ACfIowseFt7xWtj0og0L2G0T1wM” X-MSG-DECOS (“FTI” “F1” “EML”)) .     a OK UID FETCH completed ", "date": "2018-05-18"}, {"website": "Yahoo", "title": "Innovating on Authentication Standards", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/175238642656/innovating-on-authentication-standards", "abstract": "  yahoodevelopers : . By George Fletcher and Lovlesh Chhabra  . When Yahoo and AOL came together a year ago as a part of the new Verizon subsidiary Oath,  we took on the challenge of unifying their identity platforms based on current identity standards. Identity standards have been a critical part of the Internet ecosystem over the last 20+ years. From single-sign-on and identity federation with SAML; to the newer identity protocols including OpenID Connect, OAuth2, JOSE, and SCIM (to name a few); to the explorations of “self-sovereign identity” based on distributed ledger technologies; standards have played a key role in providing a secure identity layer for the Internet. . As we navigated this journey, we ran across a number of different use cases where there was either no standard or no best practice available for our varied and complicated needs. Instead of creating entirely new standards to solve our problems, we found it more productive to use existing standards in new ways.  . One such use case arose when we realized that we needed to migrate the identity stored in mobile apps from the legacy identity provider to the new Oath identity platform. For most browser (mobile or desktop) use cases, this doesn’t present a huge problem; some DNS magic and HTTP redirects and the user will sign in at the correct endpoint. Also it’s expected for users accessing services via their browser to have to sign in now and then. . However, for mobile applications it’s a completely different story. The normal user pattern for mobile apps is for the user to sign in (via OpenID Connect or OAuth2) and for the app to then be issued long-lived tokens (well, the refresh token is long lived) and the user never has to sign in again on the device (entering a password on the device is NOT a good experience for the user). . So the issue is, how do we allow the mobile app to move from one  identity provider to another without the user having to re-enter their  credentials? The solution came from researching what standards currently  exist that might addres this use case (see figure “Standards Landscape”  below) and finding the OAuth 2.0 Token Exchange draft specification ( https://tools.ietf.org/html/draft-ietf-oauth-token-exchange-13 ). .    . The Token Exchange draft allows for a given token to be exchanged for new tokens in a different domain. This could be used to manage the “audience” of a token that needs to be passed among a set of microservices to accomplish a task on behalf of the user, as an example. For the use case at hand, we created a specific implementation of the Token Exchange specification (a profile) to allow the refresh token from the originating Identity Provider (IDP) to be exchanged for new tokens from the consolidated IDP. By profiling this draft standard we were able to create a much better user experience for our consumers and do so without inventing proprietary mechanisms. . During this identity technical consolidation we also had to address how to support sharing signed-in users across mobile applications written by the same company (technically, signed with the same vendor signing key). Specifically, how can a signed-in user to Yahoo Mail not have to re-sign in when they start using the Yahoo Sports app? The current best practice for this is captured in OAuth 2.0 for Natives Apps ( RFC 8252 ). However, the flow described by this specification requires that the mobile device system browser hold the user’s authenticated sessions. This has some drawbacks such as users clearing their cookies, or using private browsing mode, or even worse, requiring the IDPs to support multiple users signed in at the same time (not something most IDPs support). . While, RFC 8252 provides a mechanism for single-sign-on (SSO) across mobile apps provided by any vendor, we wanted a better solution for apps provided by Oath. So we looked at how could we enable mobile apps signed by the vendor to share the signed-in state in a more “back channel” way. One important fact is that mobile apps cryptographically signed by the same vender can securely share data via the device keychain on iOS and Account Manager on Android. . Using this as a starting point we defined a new OAuth2 scope,  device_sso , whose purpose is to require the Authorization Server (AS) to return a unique “secret” assigned to that specific device. The precedent for using a scope to define specification behaviour is OpenID Connect itself, which defines the “openid” scope as the trigger for the OpenID Provider (an OAuth2 AS) to implement the OpenID Connect specification. The device_secret is returned to a mobile app when the OAuth2 code is exchanged for tokens and then stored by the mobile app in the device keychain and with the id_token identifying the user who signed in. . At this point, a second mobile app signed by the same vendor can look in the keychain and find the id_token, ask the user if they want to use that identity with the new app, and then use a profile of the token exchange spec to obtain tokens for the second mobile app based on the id_token and the device_secret. The full sequence of steps looks like this: .    . As a result of our identity consolidation work over the past year, we derived a set of principles identity architects should find useful for addressing use cases that don’t have a known specification or best practice. Moreover, these are applicable in many contexts outside of identity standards: . As we learned during the consolidation of our Yahoo and AOL identity platforms, and as demonstrated in our examples, there is no need to resort to proprietary solutions for use cases that at first look do not appear to have a standards-based solution. Instead, it’s much better to follow these principles, avoid the NIH (not-invented-here) syndrome, and invest the time to build solutions on standards. ", "date": "2018-06-25"}, {"website": "Yahoo", "title": "Join Us at the 10th Annual Hadoop Summit / DataWorks Summit, San Jose (Jun 13-15)", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/160966148886/join-us-at-the-10th-annual-hadoop-summit", "abstract": "  yahoohadoop : .    . We’re excited to co-host the 10th Annual  Hadoop Summit,  the leading conference for the  Apache Hadoop  community, taking place on June 13 – 15 at the  San Jose Convention Center . In the last few years, the Hadoop Summit has expanded to cover all things data beyond just Apache Hadoop – such as data science, cloud and operations, IoT and applications – and has been aptly renamed the  DataWorks Summit . The three-day program is bursting at the seams! Here are just a few of the reasons why you cannot miss this must-attend event: . Similar to previous years, we look forward to continuing Yahoo’s decade-long tradition of thought leadership at this year’s summit. Join us for an in-depth look at Yahoo’s Hadoop culture and for the latest in technologies such as Apache Tez, HBase, Hive, Data Highway Rainbow, Mail Data Warehouse and Distributed Deep Learning at the  breakout sessions  below. Or, stop by  Yahoo kiosk #700  at the  community showcase . . Also, as a co-host of the event, Yahoo is pleased to offer a  20% discount for the summit with the code MSPO20 . Register  here  for   Hadoop Summit, San Jose, California! .  DAY 1. TUESDAY June 13, 2017   .     .  12:20 - 1:00 P.M. TensorFlowOnSpark - Scalable TensorFlow Learning On Spark Clusters  . Andy Feng - VP Architecture, Big Data and Machine Learning . Lee Yang - Sr. Principal Engineer . In this talk, we will introduce a new framework, TensorFlowOnSpark, for scalable TensorFlow learning, that was open sourced in Q1 2017. This new framework enables easy experimentation for algorithm designs, and supports scalable training &amp; inferencing on Spark clusters. It supports all TensorFlow functionalities including synchronous &amp; asynchronous learning, model &amp; data parallelism, and TensorBoard. It provides architectural flexibility for data ingestion to TensorFlow and network protocols for server-to-server communication. With a few lines of code changes, an existing TensorFlow algorithm can be transformed into a scalable application. .   .  2:10 - 2:50 P.M. Handling Kernel Upgrades at Scale - The Dirty Cow Story  .  Samy Gawande - Sr. Operations Engineer  .  Savitha Ravikrishnan - Site Reliability Engineer  . Apache Hadoop at Yahoo is a massive platform with 36 different clusters spread across YARN, Apache HBase, and Apache Storm deployments, totaling 60,000 servers made up of 100s of different hardware configurations accumulated over generations, presenting unique operational challenges and a variety of unforeseen corner cases. In this talk, we will share methods, tips and tricks to deal with large scale kernel upgrade on heterogeneous platforms within tight timeframes with 100% uptime and no service or data loss through the Dirty COW use case (privilege escalation vulnerability found in the Linux Kernel in late 2016). .   .  5:00 – 5:40 P.M. Data Highway Rainbow -  Petabyte Scale Event Collection, Transport, and Delivery at Yahoo  . Nilam Sharma - Sr. Software Engineer . Huibing Yin - Sr. Software Engineer . This talk presents the architecture and features of Data Highway Rainbow, Yahoo’s hosted multi-tenant infrastructure which offers event collection, transport and aggregated delivery as a service. Data Highway supports collection from multiple data centers &amp; aggregated delivery in primary Yahoo data centers which provide a big data computing cluster. From a delivery perspective, Data Highway supports endpoints/sinks such as HDFS, Storm and Kafka; with Storm &amp; Kafka endpoints tailored towards latency sensitive consumers. .     .  DAY 2. WEDNESDAY June 14, 2017  .     .  9:05 - 9:15 A.M. Yahoo General Session - Shaping Data Platform for Lasting Value  . Sumeet Singh  – Sr. Director, Products . With a long history of open innovation with Hadoop, Yahoo continues to invest in and expand the platform capabilities by pushing the boundaries of what the platform can accomplish for the entire organization. In the last 11 years (yes, it is that old!), the Hadoop platform has shown no signs of giving up or giving in. In this talk, we explore what makes the shared multi-tenant Hadoop platform so special at Yahoo. .   .  12:20 - 1:00 P.M. CaffeOnSpark Update - Recent Enhancements and Use Cases  . Mridul Jain - Sr. Principal Engineer  . Jun Shi - Principal Engineer . By combining salient features from deep learning framework Caffe and big-data frameworks Apache Spark and Apache Hadoop, CaffeOnSpark enables distributed deep learning on a cluster of GPU and CPU servers. We released CaffeOnSpark as an open source project in early 2016, and shared its architecture design and basic usage at Hadoop Summit 2016. In this talk, we will update audiences about the recent development of CaffeOnSpark. We will highlight new features and capabilities: unified data layer which multi-label datasets, distributed LSTM training, interleave testing with training, monitoring/profiling framework, and docker deployment. .   .  12:20 - 1:00 P.M. Tez Shuffle Handler - Shuffling at Scale with Apache Hadoop  . Jon Eagles - Principal Engineer   . Kuhu Shukla - Software Engineer . In this talk we introduce a new Shuffle Handler for Tez, a YARN Auxiliary Service, that addresses the shortcomings and performance bottlenecks of the legacy MapReduce Shuffle Handler, the default shuffle service in Apache Tez. The Apache Tez Shuffle Handler adds composite fetch which has support for multi-partition fetch to mitigate performance slow down and provides deletion APIs to reduce disk usage for long running Tez sessions. As an emerging technology we will outline future roadmap for the Apache Tez Shuffle Handler and provide performance evaluation results from real world jobs at scale. .   .  2:10 - 2:50 P.M. Achieving HBase Multi-Tenancy with RegionServer Groups and Favored Nodes  . Thiruvel Thirumoolan – Principal Engineer  . Francis Liu – Sr. Principal Engineer . At Yahoo! HBase has been running as a hosted multi-tenant service since 2013. In a single HBase cluster we have around 30 tenants running various types of workloads (ie batch, near real-time, ad-hoc, etc). We will walk through multi-tenancy features explaining our motivation, how they work as well as our experiences running these multi-tenant clusters. These features will be available in Apache HBase 2.0. .   .  2:10 - 2:50 P.M. Data Driving Yahoo Mail Growth and Evolution with a 50 PB Hadoop Warehouse  . Nick Huang – Director, Data Engineering, Yahoo Mail   . Saurabh Dixit – Sr. Principal Engineer, Yahoo Mail . Since 2014, the Yahoo Mail Data Engineering team took on the task of revamping the Mail data warehouse and analytics infrastructure in order to drive the continued growth and evolution of Yahoo Mail. Along the way we have built a 50 PB Hadoop warehouse, and surrounding analytics and machine learning programs that have transformed the way data plays in Yahoo Mail. In this session we will share our experience from this 3 year journey, from the system architecture, analytics systems built, to the learnings from development and drive for adoption. .   .  DAY3. THURSDAY June 15, 2017  .     .  2:10 – 2:50 P.M. OracleStore - A Highly Performant RawStore Implementation for Hive Metastore  . Chris Drome - Sr. Principal Engineer   . Jin Sun - Principal Engineer  . Today, Yahoo uses Hive in many different spaces, from ETL pipelines to adhoc user queries. Increasingly, we are investigating the practicality of applying Hive to real-time queries, such as those generated by interactive BI reporting systems. In order for Hive to succeed in this space, it must be performant in all aspects of query execution, from query compilation to job execution. One such component is the interaction with the underlying database at the core of the Metastore. As an alternative to ObjectStore, we created OracleStore as a proof-of-concept. Freed of the restrictions imposed by DataNucleus, we were able to design a more performant database schema that better met our needs. Then, we implemented OracleStore with specific goals built-in from the start, such as ensuring the deduplication of data. In this talk we will discuss the details behind OracleStore and the gains that were realized with this alternative implementation. These include a reduction of 97%+ in the storage footprint of multiple tables, as well as query performance that is 13x faster than ObjectStore with DirectSQL and 46x faster than ObjectStore without DirectSQL. .   .  3:00 P.M. - 3:40 P.M. Bullet - A Real Time Data Query Engine  . Akshai Sarma - Sr. Software Engineer . Michael Natkovich - Director, Engineering . Bullet is an open sourced, lightweight, pluggable querying system for streaming data without a persistence layer implemented on top of Storm. It allows you to filter, project, and aggregate on data in transit. It includes a UI and WS. Instead of running queries on a finite set of data that arrived and was persisted or running a static query defined at the startup of the stream, our queries can be executed against an arbitrary set of data arriving after the query is submitted. In other words, it is a look-forward system. Bullet is a multi-tenant system that scales independently of the data consumed and the number of simultaneous queries. Bullet is pluggable into any streaming data source. It can be configured to read from systems such as Storm, Kafka, Spark, Flume, etc. Bullet leverages Sketches to perform its aggregate operations such as distinct, count distinct, sum, count, min, max, and average. .   .  3:00 P.M. - 3:40 P.M. Yahoo - Moving Beyond Running 100% of Apache Pig Jobs on Apache Tez  . Rohini Palaniswamy - Sr. Principal Engineer . Last year at Yahoo, we spent great effort in scaling, stabilizing and making Pig on Tez production ready and by the end of the year retired running Pig jobs on Mapreduce. This talk will detail the performance and resource utilization improvements Yahoo achieved after migrating all Pig jobs to run on Tez. After successful migration and the improved performance we shifted our focus to addressing some of the bottlenecks we identified and new optimization ideas that we came up with to make it go even faster. We will go over the new features and work done in Tez to make that happen like custom YARN ShuffleHandler, reworking DAG scheduling order, serialization changes, etc. We will also cover exciting new features that were added to Pig for performance such as bloom join and byte code generation.  .   .  4:10 P.M. - 4:50 P.M. Leveraging Docker for Hadoop Build Automation and Big Data Stack Provisioning  . Evans Ye,  Software Engineer . Apache Bigtop as an open source Hadoop distribution, focuses on developing packaging, testing and deployment solutions that help infrastructure engineers to build up their own customized big data platform as easy as possible. However, packages deployed in production require a solid CI testing framework to ensure its quality. Numbers of Hadoop component must be ensured to work perfectly together as well. In this presentation, we’ll talk about how Bigtop deliver its containerized CI framework which can be directly replicated by Bigtop users. The core revolution here are the newly developed Docker Provisioner that leveraged Docker for Hadoop deployment and Docker Sandbox for developer to quickly start a big data stack. The content of this talk includes the containerized CI framework, technical detail of Docker Provisioner and Docker Sandbox, a hierarchy of docker images we designed, and several components we developed such as Bigtop Toolchain to achieve build automation. .  Register here for    Hadoop Summit, San Jose, California  with a  20% discount code MSPO20 .   .    . Questions? Feel free to reach out to us at bigdata@yahoo-inc.com. Hope to see you there! . (via  yahoohadoop ) ", "date": "2017-05-22"}, {"website": "Yahoo", "title": "Refactoring Components for Redux Performance", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/152078809581/refactoring-components-for-redux-performance", "abstract": " By Shahzad Aziz    . Front-end web development is evolving fast; a lot of new tools and libraries are published which challenge best practices everyday. It’s exciting, but also overwhelming. One of those new tools that can make a developer’s life easier is Redux, a popular open source state container. This past year, our team at Yahoo Search has been using Redux to refresh a legacy tool used for data analytics. We paired Redux with another popular library for building front-end components called React. Due to the scale of Yahoo Search, we measure and store thousands of metrics on the data grid every second. Our data analytics tool allows internal users from our Search team to query stored data, analyze traffic, and compare A/B testing results. The biggest goal of creating the new tool was to deliver ease of use and speed. When we started, we knew our application would grow complex and we would have a lot of state living inside it. During the course of development we got hit by unforeseen performance bottlenecks. It took us some digging and refactoring to achieve the performance we expected from our technology stack. We want to share our experience and encourage developers who use Redux, that by reasoning about your React components and your state structure, you will make your application performant and easy to scale.  . Redux wants your application state changes to be more predictable and easier to debug. It achieves this by extending the ideas of the  Flux pattern  by making your application’s state live in a single store. You can use reducers to split the state logically and manage responsibility. Your React components subscribe to changes from the Redux store. For front-end developers this is very appealing and you can imagine the cool debugging tools that can be paired with Redux (see  Time Travel ). . With React it is easier to reason and create components into two different categories, Container vs Presentational. You can read more about it ( here ). The gist is that your Container components will usually subscribe to state and manage flow, while Presentational are concerned with rendering markup using the properties passed to them. Taking guidance from early Redux documentation, we started by adding container components at the top of our component tree. The most critical part of our application is the interactive   ResultsTable  component; for the sake of brevity, this will be the focus for the rest of the post. . To achieve optimal performance from our API, we make a lot of simple calls to the backend and combine and filter data in the reducers. This means we dispatch a lot of async actions to fetch bits and pieces of the data and use  redux-thunk  to manage the control flow. However, with any change in user selections it invalidates most of the things we have fetched in the state, and we need to then re-fetch. Overall this works great, but it also means we are mutating state many times as responses come in.  .  The Problem  . While we were getting great performance from our API, the browser flame graphs started revealing performance bottlenecks on the client. Our  MainPage  component sitting high up in the component tree triggered re-renders against every dispatch. While your component render should not be a costly operation, on a number of occasions we had a huge amount of data rows to be rendered. The delta time for re-render in such cases was in seconds. . So how could we make render more performant? A well-advertized method is the  shouldComponentUpdate  lifecycle method, and that is where we started. This is a common method used to increase performance that should be implemented carefully across your component tree. We were able to filter out access re-renders where our desired props to a component had not changed.  . Despite the  shouldComponentUpdate  improvement, our whole UI still seemed clogged. User actions got delayed responses, our loading indicators would show up after a delay, autocomplete lists would take time to close, and small user interactions with the application were slow and heavy. At this point it was not about React render performance anymore.  . In order to determine bottlenecks, we used two tools:  React Performance Tools  and  React Render Visualizer . The experiment was to study users performing different actions and then creating a table of the count of renders and instances created for the key components. . Below is one such table that we created. We analyzed two frequently-used actions. For this table we are looking at how many renders were triggered for our main table components.  . Experiments revealed that state needed to travel through the tree and recompute a lot of things repeatedly along the way before affecting the relevant component. This was costly and CPU intensive. While switching products from the UI, React spent 1080ms on table and row components. It was important to realize this was more of a problem of shared state which made our thread busy. While React’s virtual DOM is performant, you should still strive to minimize virtual DOM creation. Which means minimizing renders.  .  Performance Refactor  . The idea was to look at container components and try and distribute the state changes more evenly in the tree. We also wanted to put more thought into the state, make it less shared, and more derived. We wanted to store the most essential items in the state while computing state for the components as many times as we wanted. . We executed the refactor in two steps: .  Step 1.  We added more container components subscribing to the state across the component hierarchy. Components consuming exclusive state did not need a container component at the top threading props to it; they can subscribe to the state and become a container component. This dramatically reduces the amount of work React has to do against Redux actions.  .    .   React Component Tree with potential container components  . We identified how the state was being utilized across the tree. The important question was, “How could we make sure there are more container components with exclusive states?” For this we had to split a few key components and wrap them in containers or make them containers themselves.  .  . In the tree above, notice how the  MainPage  container is no longer responsible for any  Table  renders. We extracted another component  ControlsFooter  out of  ResultsTable  which has its own exclusive state. Our focus was to reduce re-renders on all table-related components.  .  Step 2.  Derived state and should component update. . It is critical to make sure your state is well defined and flat in nature. It is easier if you think of Redux state as a database and start to normalize it. We have a lot of entities like products, metrics, results, chart data, user selections, UI state, etc. For most of them we store them as Key/Value pairs and avoid nesting. Each container component queries the state object and de-normalizes data. For example, our navigation menu component needs a list of metrics for the product which we can easily extract from the metrics state by filtering the list of metrics with the product ID.  . The whole process of deriving state and querying it repeatedly can be optimized. Enter  Redux Reselect . Reselect allows you to define selectors to compute derived state. They are memoized for efficiency and can also be cascaded. So a function like the one below can be defined as a selector, and if there is no change in  metrics  or  productId  in state, it will return a memoized copy. .  getMetrics(productId, metrics) {  .    metrics.find(m =&gt; m.productId === productId)  .  }  . We wanted to make sure all our async actions resolved and our state had everything we needed before triggering a re-render. We created selectors to define such behaviours for  shouldComponentUpdate  in our container components (e.g our table will only render if  metrics  and  results  were all loaded). While you can still do all of this directly in  shouldComponentUpdate , we felt that using selectors allowed you to think differently. It makes your containers predict state and wait on it before rendering while the state that it needs to render can be a subset of it. Not to mention, it is more performant. .  The Results  . Once we finished refactoring we ran our experiment again to compare the impact from all our tweaks. We were expecting some improvement and better flow of state mutations across the component tree.  . A quick glance at the table above tells you how the refactor has clearly come into effect. In the new results, observe how changing dates distributes the render count between  comparisonTable (7)  and  controlsHeader (2) . It still adds up to 9. Such logical refactors have allowed us to to speed up our application rendering up to 4 times. It has also allowed us to optimize time wasted by React easily. This has been a significant improvement for us and a step in the right direction.  .  What’s Next   . Redux is a great pattern for front-end applications. It has allowed us to reason about our application state better, and as the app grows more complex, the single store makes it much easier to scale and debug.  . Going forward we want to explore Immutability for Redux State. We want to enforce the discipline for Redux state mutations and also make a case for faster shallow comparison props in  shouldComponentUpdate .  ", "date": "2016-10-20"}, {"website": "Yahoo", "title": "Why Professional Open Source Management is Critical for your Business", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/152340372151/why-professional-open-source-management-is", "abstract": " By Gil Yehuda, Sr. Director of Open Source and Technology Strategy  .  This byline was originally written for and appears in  CIO Review .   . In his   Open Source Landscape  keynote  at LinuxCon Japan earlier this year, Jim Zemlin, Executive Director of the Linux Foundation said that the trend toward corporate-sponsored open source projects is one of the most important developments in the open source ecosystem. The  jobs report released  by the Linux Foundation earlier this year found that open source professionals are in high demand. The report was followed by the announcement that  TODOGroup , a collaboration project for open source professionals who run corporate open source program offices, was joining the Linux Foundation. Open source is no longer exclusively a pursuit of the weekend hobbyist. Professional open source management is a growing field, and it’s critical to the success of your technology strategy.  .  Open Source Potential to Reality Gap  . Open source has indeed proven itself to be a transformative and disruptive part of many companies’ technology strategies. But we know it’s hardly perfect and far from hassle-free. Many developers trust open source projects without carefully reviewing the code or understanding the license terms, thus inviting risk. Developers say they like to contribute to open source, but are not writing as much of it as they wish. By legal default, their code is not open source unless they make it so. Despite being touted as an engineering recruitment tool, developers don’t flock to companies who toss the words “open source” all over their tech blogs. They first check for serious corporate commitment to open source. . Open source offers  potential  to lower costs, keep up with standards, and make your developers more productive. Turning potential into practice requires open source professionals on your team to unlock the open source opportunity. They will steer you out of the legal and administrative challenges that open source brings, and help you create a voice in the open source communities that matter most to you. Real work goes into managing the strategy, policies, and processes that enable you to benefit from the open source promise. Hence the emerging trend of hiring professionals to run open source program offices at tech companies across the industry.  .  Getting the Program off the Ground  .  Program office  sounds big. Actually, many companies staff these with as few as one or two people. Often the rest is a virtual team that includes someone from legal, PR, developer services, an architect, and a few others depending on your company. As a virtual team, each member helps address the areas they know best. Their shared mission is to provide an authoritative and supportive decision about all-things open source at your company. Ideally they are technical, respected, and lead with pragmatism – but what’s most important is that they all collaborate toward the same goal.  . The primary goal of the open source program office is to steer the technology strategy toward success using the right open source projects and processes. But the day-to-day program role is to provide services to engineers. Engineers need to know when they can use other people’s code within the company’s codebase (to ‘inbound’ code), and when they can publish company code to other projects externally (to ‘outbound’ code). Practical answers require an understanding of engineering strategy, attention to legal issues surrounding licenses (copyright, patent, etc.), and familiarity with managing GitHub at scale. . New open source projects and foundations will attract (or distract) your attention. Engineers will ask about the projects they contribute to on their own time, but in areas your company does business. They seek to contribute to projects and publish new projects. Are there risks? Is it worth it? The questions and issues you deal with on a regular basis will help give you a greater appreciation for where open source truly works for you, and where process-neglect can get in the way of achieving your technology mission. .  Will it Work?  . I’ve been running the open source program office at Yahoo for over six years. We’ve been publishing and supporting industry-leading open source projects for AI, Big Data, Cloud, Datacenter, Edge, Front end, Mobile, all the way to Zookeeper. We’ve created foundational open source projects like Apache Hadoop and many of its related technologies. When we find promise in other projects, we support and help accelerate them too, like we did with OpenStack, Apache Storm and Spark. Our engineers support hundreds of our own projects, we contribute to thousands of outside projects, and developers around the world download and use our open source code millions of times per week! We are able to operate at scale and take advantage of the open source promise by providing our engineers with a lightweight process that enables them to succeed in open source. . You can do the same at your company. Open source professionals who run program offices at tech companies share openly – it comes with the territory. I publish answers about  open source on Quora  and I’m a member of TODOGroup, the collaboration project managed by the Linux Foundation for open source program directors. There, I share and learn from my peers who manage the open source programs at  various tech companies .  . Bottom line: If you want to take advantage of the value that open source offers, you’ll need someone on your team who understands open source pragmatics, who’s plugged into engineering initiative, and who’s empowered to make decisions. The good news is you are not alone and there’s help out there in the open source community.  ", "date": "2016-10-26"}, {"website": "Yahoo", "title": "The Apache Traffic Server Project’s Next Chapter", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/152602307661/the-apache-traffic-server-projects-next-chapter", "abstract": " By Bryan Call, Yahoo Distinguished Software Engineer, Apache Traffic Server PMC Chair     .  This post also appears on the ATS blog,  https://blogs.apache.org/trafficserver .   . Last week, the ATS Community held a productive and informative Apache Traffic Server (ATS) Fall Summit, hosted by LinkedIn in Sunnyvale, CA. At a hackathon during the Summit, we fixed bugs, cleaned up code, users were able to spend time with experts on ATS and have their questions answered, and the next release candidate for ATS 7.0.0 was made public. There were talks on operations, new and upcoming features, and supporting products. More than 80 people registered for the event and we had a packed room with remote video conferencing.  . I have been attending the ATS Summits since their inception in 2010 and have had the pleasure of being involved with the  Apache Traffic Server  Project for the last nine years. I was also part of the team at Yahoo that open sourced the code to Apache. Today, I am honored to serve as the new Chair and VP of the ATS Project,  having been elected to the position  by the ATS community a couple weeks ago.  . Traffic Server was originally created by Inktomi and distributed as a commercial product. After Yahoo acquired Inktomi, Yahoo open sourced Traffic Server and submitted it to the Apache Incubator in July 2009.  . Since graduating as Apache Traffic Server (an Apache Top-Level Project as of April 2010), many large and small companies use it for caching and proxying HTTP requests. ATS supports HTTP/2, HTTP/1.1, TLS, and many other standards. The Apache Committers on the project are actively involved with the Internet Engineering Task Force (IETF) – whose mission it is to “make the Internet work better by producing high quality, relevant technical documents that influence the way people design, use, and manage the Internet” – to make sure ATS is able to support the latest standards going forward. . Many companies have greatly benefited from the open sourcing of ATS; numerous industry colleagues and invested individuals have improved the project by fixing bugs and adding features, tests, and documentation. An example is Yahoo, which uses ATS for nearly all of its incoming HTTP/2 and HTTP/1.1 traffic. It is a common layer that all users go through before making a request to the origin server. Having a common layer has made it easier for Yahoo to deploy security fixes and updates extremely quickly. ATS is used as a caching proxy in locations worldwide and is also used to proxy requests for dynamic content from remote locations through already-established persistent connections. This decreases the latency for users when their cacheable content can be served, and connection establishments can be made to a nearby server. . The ATS PMC and I will focus on continuing to increase the ATS user base and having more developers contribute to the project. The ATS community welcomes other companies’ contributions and enhancements to the software through a  well-established process  with Apache. Unlike other commercial products, ATS has no limits or restrictions with accepting open source contributions. . Moving forward, we would also like to focus on three specific areas of ATS as a means of increasing the user base, while maintaining the performance advantage of the server: ease of use, features, and stability. . I support the further simplification of the configuration of ATS to make it so that end users can quickly get a server up with little effort. Common requirements should be easy to configure, while continuing to allow users to write custom plugins for more advanced requirements. . Adding new features to ATS is important and there are a lot of new drafts and standards currently being worked on in IETF with HTTP, TLS, and QUIC that will improve user experience. ATS will need to continue to support the latest standards that allow deployments of ATS to decrease the latency for the users. Having our developers attend the IETF meetings and participate in the decision-making is key to our ability to keep on top of these latest developments. . Stability is a fundamental requirement for a proxy server. Since all the incoming HTTP/2 and HTTP/1.1 traffic is handled by the server, it must be stable and resilient. We are continually working on improving our continuous integration and testing. We are making it easier for developers to write testing and run tests before making contributions to the code. . The ATS community is a welcoming group of people that encourages contributions and input from users, and I am excited to help lead the Project into its next chapter. Please feel free to join the mailing lists, attend one of our events such as the recent ATS Summit, or jump on IRC to talk to the users and developers of this project. We invite you to learn more about ATS at  http://trafficserver.apache.org .    ", "date": "2016-11-01"}, {"website": "Yahoo", "title": "Open Sourcing Screwdriver, Yahoo’s Continuous Delivery Build System for Dynamic Infrastructure", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/155765242061/open-sourcing-screwdriver-yahoos-continuous", "abstract": "  By James Collins, Sr. Director, Developer Platforms and Services, and St. John Johnson, Principal Engineer   . Continuous Delivery enables software development teams to move faster and adapt to users’ needs quicker by reducing the inherent friction associated with releasing software changes. Yahoo’s engineering has modernized as it has embraced Continuous Delivery as a strategy for improving product quality and engineering agility. All our active products deliver from commit to production with full automation and this has greatly improved Yahoo’s ability to deliver products.    . Part of what enabled Yahoo to make Continuous Delivery at scale a reality was our improved build and release tooling. Now, we are open sourcing an adaptation of our code as  Screwdriver.cd , a new streamlined build system designed to enable Continuous Delivery to production at scale for dynamic infrastructure.  . Some of the key design features of Screwdriver have helped Yahoo achieve Continuous Delivery at scale. At a high level these are: .  Easy deployment pipelines:  Deployment pipelines that continuously test, integrate, and deploy code to production greatly reduce the risk of errors and reduce the time to get feedback to developers. The challenge for many groups had been that pipelines were cumbersome to setup and maintain. We designed a solution that made pipelines easy to configure and completely self-service for any developer. By managing the pipeline configuration in the code repository Screwdriver allows developers to configure pipelines in a manner familiar to them, and as a bonus, to easily code review pipeline changes too. .  Trunk development:  Internally, we encourage workflows where the trunk is always shippable. Our teams use a modified  GitHub flow  for their workflows. Pull Requests (PRs) are the entry point for running tests and ensuring code that entered the repository has been sufficiently tested. Insisting on formal PRs also improves the quality of our code reviews.  . To ensure trunks are shippable, we enable functional testing of code in the PRs. Internally, this is a configuration baked into pipelines that dynamically allocates compute resources, deploys the code, and runs tests. These tests include web testing using tools like Selenium. These dynamically-allocated resources are also available for a period after the PR build, allowing engineers to interact with the system and review visual aspects of their changes. .  Easy rollbacks:  To allow for easy code rollbacks, we allow phases of the pipeline to be re-run at a previously-saved state. We leverage features in our PaaS to handle the deployment, but we store and pass metadata to enable us to re-run from a specific git SHA with the same deployment data. This allows us to roll back to a previous state in production. This design makes rolling back as easy as selecting a version from a dropdown menu and clicking “deploy.” Anyone with write access to the project can make this change. This helped us move teams to a DevOps model where developers were responsible for the production state. . The successful growth of Screwdriver over the past 5 years at Yahoo has today led to Screwdriver being synonymous with Continuous Delivery within the company.  Screwdriver handles over 25,000+ builds per day and 12,000+ daily git commits as a single shared entrypoint for Yahoo. It supports multiple languages and handles both virtual machine and container-based builds and deployment.   .    .    . Screwdriver.cd’s architecture is comprised of four main components: a frontend for serving content to the user, a stateless API that orchestrates between user interactions and build operations, the execution engines (Docker Swarm, Kubernetes, etc.) that checkout source code and execute in containers, and the launcher that executes and monitors commands inside the container. . The diagram below shows this architecture overlaid with a typical developer flow.  .    .    . To give some context around our execution engines, internal Screwdriver started as an abstraction layer on top of Jenkins and used Docker to provide isolation, common build containers, etc. We used features provided by Jenkins plugins to leverage existing work around coverage and test reports. However, as Screwdriver usage continued to climb, it outgrew a single Jenkins cluster. So in order to grow to our needs, we added capabilities in Screwdriver that allowed us to scale horizontally while also adding capabilities to schedule pipelines across a number of Jenkins clusters. As we scaled Screwdriver, we used less from Jenkins and built more supporting services utilizing our cloud infrastructure. The open-source version is focused on Kubernetes and Docker Swarm as our primary supported execution engines.    . In the coming months we will expand our offering to match many of the features we have internally, including: . Please join us on the path to making Continuous Delivery easy. Visit  http://screwdriver.cd  to get started.  ", "date": "2017-01-12"}, {"website": "Yahoo", "title": "Presenting an Open Source Toolkit for Lightweight Multilingual Entity Linking", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/154168092396/presenting-an-open-source-toolkit-for-lightweight", "abstract": "  yahooresearch : . By  Aasish Pappu , Roi Blanco, and Amanda Stent . What’s the first thing you want to know about any kind of text document (like a Yahoo News or Yahoo Sports article)? What it’s about, of course! That means you want to know something about the people, organizations, and locations that are mentioned in the document. Systems that automatically surface this information are called  named entity recognition and linking  systems. These are one of the most useful components in text analytics as they are required for a wide variety of applications including search, recommender systems, question answering, and sentiment analysis. . Named entity recognition and linking systems use statistical models trained over large amounts of labeled text data. A major challenge is to be able to accurately detect entities, in new languages, at scale, with limited labeled data available, and while consuming a limited amount of resources (memory and processing power). . After researching and implementing solutions to enhance our own personalization technology, we are pleased to offer the open source community  Fast Entity Linker , our unsupervised, accurate, and extensible multilingual named entity recognition and linking system, along with   datapacks  for English, Spanish, and Chinese. . For broad usability, our system links text entity mentions to Wikipedia. For example, in the sentence  Yahoo is a company headquartered in Sunnyvale, CA with Marissa Mayer as CEO , our system would identify the following entities: . On the algorithmic side, we use entity embeddings, click-log data, and efficient clustering methods to achieve high precision. The system achieves a low memory footprint and fast execution times by using compressed data structures and aggressive hashing functions. .  Entity embeddings  are vector-based representations that capture how entities are referred to in context. We train entity embeddings using Wikipedia articles, and use hyperlinked terms in the articles to create canonical entities. The context of an entity and the context of a token are modeled using the neural network architecture in the figure below, where entity vectors are trained to predict not only their surrounding entities but also the global context of word sequences contained within them. In this way, one layer models entity context, and the other layer models token context. We connect these two layers using the same technique that   (Quoc and Mikolov ‘14)   used to train paragraph vectors. .  .  Search click-log data  gives very useful signals to disambiguate partial or ambiguous entity mentions. For example, if searchers for “Fox” tend to click on “Fox News” rather than “20th Century Fox,” we can use this data in order to identify “Fox” in a document. To disambiguate entity mentions and ensure a document has a consistent set of entities, our system supports three entity disambiguation algorithms: .  *Currently, only the Forward Backward Algorithm is available in our open source release–the other two will be made available soon!  . These algorithms are particularly helpful in accurately linking entities when a popular candidate is NOT the correct candidate for an entity mention. In the example below, these algorithms leverage the surrounding context to accurately link  Manchester City, Swansea City, Liverpool, Chelsea, and Arsenal  to their respective football clubs. .  .  The technical contributions of this system are described in two scientific papers: . There are numerous possible applications of the open-source toolkit. One of them is attributing sentiment to entities detected in the text, as opposed to the entire text itself. For example, consider the following actual review of the movie “Inferno” from a user on  MetaCritic  (revised for clarity):  “While the great performance of  Tom Hanks (wiki_Tom_Hanks)  and company make for a mysterious and vivid movie, the plot is difficult to comprehend. Although the movie was a clever and fun ride, I expected more from  Columbia (wiki_Columbia_Pictures) .”   Though the review on balance is neutral, it conveys a positive sentiment about wiki_Tom_Hanks and a negative sentiment about wiki_Columbia_Pictures. . Many existing sentiment analysis tools collate the sentiment value associated with the text as a whole, which makes it difficult to track sentiment around any individual entity. With our toolkit, one could automatically extract “positive” and “negative” aspects within a given text, giving a clearer understanding of the sentiment surrounding its individual components. . Feel free to use the code, contribute to it, and come up with addtional applications; our system and models are available at  https://github.com/yahoo/FEL . . Great work from our Yahoo Research team! ", "date": "2016-12-07"}, {"website": "Yahoo", "title": "Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/157196488076/open-sourcing-tensorflowonspark-distributed-deep", "abstract": "  yahoohadoop : . By Lee Yang, Jun Shi, Bobbie Chern, and Andy Feng (@afeng76), Yahoo Big ML team .  Introduction  . Today, we are pleased to offer  TensorFlowOnSpark  to the community, our latest open source framework for distributed deep learning on big-data clusters. . Deep learning (DL) has evolved significantly in recent years. At Yahoo, we’ve found that in order to gain insight from massive amounts of data, we need to deploy  distributed  deep learning. Existing DL frameworks often require us to set up separate clusters for deep learning, forcing us to create multiple programs for a machine learning pipeline (see Figure 1 below). Having separate clusters requires us to transfer large datasets between them, introducing unwanted system complexity and end-to-end learning latency. .    .    .    . Last year we addressed scaleout issues by developing and publishing  CaffeOnSpark , our open source framework that allows distributed deep learning and big-data processing on identical Spark and Hadoop clusters. We use CaffeOnSpark at Yahoo to improve our  NSFW image detection , to automatically identify eSports game highlights from live-streamed videos, and more. With the community’s valuable feedback and contributions, CaffeOnSpark has been upgraded with LSTM support, a new data layer, training and test interleaving, a Python API, and deployment on docker containers. This has been great for our Caffe users, but what about those who use the deep learning framework  TensorFlow ? We’re taking a page from our own playbook and doing for TensorFlow for what we did for Caffe.   . After TensorFlow’s initial publication, Google released an enhanced TensorFlow with distributed deep learning capabilities in April 2016. In October 2016, TensorFlow introduced HDFS support. Outside of the Google cloud, however, users still needed a dedicated cluster for TensorFlow applications. TensorFlow programs could not be deployed on existing big-data clusters, thus increasing the cost and latency for those who wanted to take advantage of this technology at scale. . To address this limitation, several community projects wired TensorFlow onto Spark clusters.  SparkNet  added the ability to launch TensorFlow networks in Spark executors. DataBricks proposed  TensorFrame  to manipulate Apache Spark’s DataFrames with TensorFlow programs. While these approaches are a step in the right direction, after examining their code, we learned we would be unable to get the TensorFlow processes to communicate with each other directly, we would not be able to implement asynchronous distributed learning, and we would have to expend significant effort to migrate existing TensorFlow programs. .  TensorFlowOnSpark  .    .    .    . Our new framework, TensorFlowOnSpark (TFoS), enables distributed TensorFlow execution on Spark and Hadoop clusters. As illustrated in Figure 2 above, TensorFlowOnSpark is designed to work along with SparkSQL, MLlib, and other Spark libraries in a single pipeline or program (e.g. Python notebook). . TensorFlowOnSpark supports all types of TensorFlow programs, enabling both asynchronous and synchronous training and inferencing. It supports model parallelism and data parallelism, as well as TensorFlow tools such as TensorBoard on Spark clusters. . Any TensorFlow program can be easily modified to work with TensorFlowOnSpark. Typically, changing fewer than 10 lines of Python code are needed. Many developers at Yahoo who use TensorFlow have easily migrated TensorFlow programs for execution with TensorFlowOnSpark. . TensorFlowOnSpark supports direct tensor communication among TensorFlow processes (workers and parameter servers). Process-to-process direct communication enables TensorFlowOnSpark programs to scale easily by adding machines. As illustrated in Figure 3, TensorFlowOnSpark doesn’t involve Spark drivers in tensor communication, and thus achieves similar scalability as stand-alone TensorFlow clusters. .    .    .    . TensorFlowOnSpark provides two different modes to ingest data for training and inference: . Figure 4 illustrates how the synchronous distributed training of  Inception image classification  network scales in TFoS using QueueRunners with a simple setting: 1 GPU, 1 reader, and batch size 32 for each worker. Four TFoS jobs were launched to train 100,000 steps. When these jobs completed after 2+ days, the top-5 accuracy of these jobs were 0.730, 0.814, 0.854, and 0.879. Reaching top-5 accuracy of 0.730 takes 46 hours for a 1-worker job, 22.5 hours for a 2-worker job, 13 hours for a 4-worker job, and 7.5 hours for an 8-worker job. TFoS thus achieves near linear scalability for Inception model training. This is very encouraging, although TFoS scalability will vary for different models and hyperparameters. .    .    .    .  RDMA for Distributed TensorFlow  . In Yahoo’s Hadoop clusters, GPU nodes are connected by both Ethernet and Infiniband. Infiniband provides faster connectivity and supports direct access to other servers’ memories over RDMA. Current TensorFlow releases, however, only support distributed learning using gRPC over Ethernet. To speed up distributed learning, we have enhanced the TensorFlow C++ layer to enable RDMA over Infiniband. . In conjunction with our TFoS release, we are introducing a new protocol for TensorFlow servers in addition to the default  “grpc”  protocol. Any distributed TensorFlow program can leverage our enhancement via specifying  protocol=“grpc_rdma”  in  tf.train.ServerDef()  or  tf.train.Server() . . With this new protocol, a RDMA rendezvous manager is created to ensure tensors are written directly into the memory of remote servers. We minimize the tensor buffer creation: Tensor buffers are allocated once at the beginning, and then reused across all training steps of a TensorFlow job. From our early experimentation with large models like the  VGG-19 network , our RDMA implementation has demonstrated a significant speedup on training time compared with the existing gRPC implementation. . Since RDMA support is a highly requested capability (see TensorFlow issue  #2916 ), we decided to make our current implementation available as an alpha release to the TensorFlow community. In the coming weeks, we will polish our RDMA implementation further, and share detailed benchmark results. .  Simple CLI and API  . TFoS programs are launched by the standard Apache Spark command,  spark-submit . As illustrated below, users can specify the number of Spark executors, the number of GPUs per executor, and the number of parameter servers in the CLI. A user can also state whether they want to use TensorBoard (–tensorboard) and/or RDMA (–rdma). .       spark-submit –master ${MASTER} \\        ${TFoS_HOME}/examples/slim/train_image_classifier.py \\        –model_name inception_v3 \\       –train_dir hdfs://default/slim_train \\        –dataset_dir hdfs://default/data/imagenet \\       –dataset_name imagenet \\       –dataset_split_name train \\       –cluster_size ${NUM_EXEC} \\       –num_gpus ${NUM_GPU} \\       –num_ps_tasks ${NUM_PS} \\       –sync_replicas \\       –replicas_to_aggregate ${NUM_WORKERS} \\       –tensorboard \\       –rdma   . TFoS provides a high-level Python API (illustrated in our  sample Python notebook ): .  Open Source  . Yahoo is happy to release TensorFlowOnSpark at  github.com/yahoo/TensorFlowOnSpark  and a RDMA enhancement of TensorFlow at  github.com/yahoo/tensorflow/tree/yahoo . Multiple  example programs  (including mnist, cifar10, inception, and VGG) are provided to illustrate the simple conversion process of TensorFlow programs to TensorFlowOnSpark, and leverage RDMA. An Amazon Machine Image is also  available  for applying TensorFlowOnSpark on AWS EC2. . Going forward, we will advance TensorFlowOnSpark as we continue to do with CaffeOnSpark. We welcome the community’s continued feedback and contributions to CaffeOnSpark, and are interested in thoughts on ways TensorFlowOnSpark can be enhanced. . (via  yahoohadoop ) ", "date": "2017-02-13"}, {"website": "Yahoo", "title": "Operating OpenStack at Scale", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/159795571841/operating-openstack-at-scale", "abstract": " By James Penick, Cloud Architect &amp; Gurpreet Kaur, Product Manager .  A version of this byline was originally written for and appears in  CIO Review .  .  A successful private cloud presents a consistent and reliable facade over the complexities of hyperscale infrastructure. It must simultaneously handle constant organic traffic growth, unanticipated spikes, a multitude of hardware vendors, and discordant customer demands. The depth of this complexity only increases with the age of the business, leaving a private cloud operator saddled with legacy hardware, old network infrastructure, customers dependent on legacy operating systems, and the list goes on. These are the foundations of the horror stories told by grizzled operators around the campfire.  . Providing a plethora of services globally for over a billion active users requires a hyperscale infrastructure. Yahoo’s on-premises infrastructure is comprised of datacenters housing hundreds of thousands of physical and virtual compute resources globally, connected via a multi-terabit network backbone. As one of the very first hyperscale internet companies in the world, Yahoo’s infrastructure had grown organically – things were built, and rebuilt, as the company learned and grew. The resulting web of modern and legacy infrastructure became progressively more difficult to manage. Initial attempts to manage this via IaaS (Infrastructure-as-a-Service) taught some hard lessons. However, those lessons served us well when OpenStack was selected to manage Yahoo’s datacenters, some of which are shared below. .  Centralized team offering Infrastructure-as-a-Service  . Chief amongst the lessons learned prior to OpenStack was that IaaS must be presented as a core service to the whole organization by a dedicated team. An a-la-carte-IaaS, where each user is expected to manage their own control plane and inventory, just isn’t sustainable at scale. Multiple teams tackling the same challenges involved in the curation of software, deployment, upkeep, and security within an organization is not just a duplication of effort; it removes the opportunity for improved synergy with all levels of the business. The first OpenStack cluster, with a centralized dedicated developer and service engineering team, went live in June 2012.  This model has served us well and has been a crucial piece of making OpenStack succeed at Yahoo. One of the biggest advantages to a centralized, core team is the ability to collaborate with the foundational teams upon which any business is built: Supply chain, Datacenter Site-Operations, Finance, and finally our customers, the engineering teams. Building a close relationship with these vital parts of the business provides the ability to streamline the process of scaling inventory and presenting on-demand infrastructure to the company. .  Developers love instant access to compute resources  . Our developer productivity clusters, named “OpenHouse,” were a huge hit. Ideation and experimentation are core to developers’ DNA at Yahoo. It empowers our engineers to innovate, prototype, develop, and quickly iterate on ideas. No longer is a developer reliant on a static and costly development machine under their desk. OpenHouse enables developer agility and cost savings by obviating the desktop. .  Dynamic infrastructure empowers agile products  . From a humble beginning of a single, small OpenStack cluster, Yahoo’s OpenStack footprint is growing beyond 100,000 VM instances globally, with our single largest virtual machine cluster running over a thousand compute nodes, without using Nova Cells.  . Until this point, Yahoo’s production footprint was nearly 100% focused on baremetal – a part of the business that one cannot simply ignore. In 2013, Yahoo OpenStack Baremetal began to manage all new compute deployments. Interestingly, after moving to a common API to provision baremetal and virtual machines, there was a marked increase in demand for virtual machines.  . Developers across all major business units ranging from Yahoo Mail, Video, News, Finance, Sports and many more, were thrilled with getting instant access to compute resources to hit the ground running on their projects. Today, the OpenStack team is continuing to fully migrate the business to OpenStack-managed. Our baremetal footprint is well beyond that of our VMs, with over 100,000 baremetal instances provisioned by OpenStack Nova via Ironic. .  How did Yahoo hit this scale?    . Scaling OpenStack begins with understanding how its various components work and how they communicate with one another. This topic can be very deep and for the sake of brevity, we’ll hit the high points. . 1.  Start at the bottom and think about the underlying hardware  . Do not overlook the unique resource constraints for the services which power your cloud, nor the fashion in which those services are to be used. Leverage that understanding to drive hardware selection. For example, when one examines the role of the database server in an OpenStack cluster, and considers the multitudinous calls to the database: compute node heartbeats, instance state changes, normal user operations, and so on; they would conclude this core component is extremely busy in even a modest-sized Nova cluster, and in need of adequate computational resources to perform. Yet many deployers skimp on the hardware. The performance of the whole cluster is bottlenecked by the DB I/O. By thinking ahead you can save yourself a lot of heartburn later on. . 2.  Think about how things communicate  . Our cluster databases are configured to be multi-master single-writer with automated failover. Control plane services have been modified to split DB reads directly to the read slaves and only write to the write-master. This distributes load across the database servers. . 3.  Scale wide  . OpenStack has many small horizontally-scalable components which can peacefully cohabitate on the same machines: the Nova, Keystone, and Glance APIs, for example. Stripe these across several small or modest hardware. Some services, such as the Nova scheduler, run the risk of race conditions when running multi-active. If the risk of race conditions is unacceptable, use ZooKeeper to manage leader election. . 4.  Remove dependencies  . In a Yahoo datacenter, DHCP is only used to provision baremetal servers. By statically declaring IPs in our instances via cloud-init, our infrastructure is less prone to outage from a failure in the DHCP infrastructure. . 5.  Don’t be afraid to replace things  . Neutron used Dnsmasq to provide DHCP services, however it was not designed to address the complexity or scale of a dynamic environment. For example, Dnsmasq must be restarted for any config change, such as when a new host is being provisioned.  In the Yahoo OpenStack clusters this has been replaced by ISC-DHCPD, which scales far better than Dnsmasq and allows dynamic configuration updates via an API. . 6.  Or split them apart  . Some of the core imaging services provided by Ironic, such as DHCP, TFTP, and HTTPS communicate with a host during the provisioning process. These services are normally  part of the Ironic Conductor (IC) service. In our environment we split these services into a new and physically-distinct service called the Ironic Transport Service (ITS). This brings value by: .  Be prepared for faulty hardware!  . Running IaaS reliably at hyperscale is more than just scaling the control plane. One must take a holistic look at the system and consider everything. In fact, when examining provisioning failures, our engineers determined the majority root cause was faulty hardware. For example, there are a number of machines from varying vendors whose IPMI firmware fails from time to time, leaving the host inaccessible to remote power management. Some fail within minutes or weeks of installation. These failures occur on many different models, across many generations, and across many hardware vendors. Exposing these failures to users would create a very negative experience, and the cloud must be built to tolerate this complexity. .  Focus on the end state  . Yahoo’s experience shows that one can run OpenStack at hyperscale, leveraging it to wrap infrastructure and remove perceived complexity. Correctly leveraged, OpenStack presents an easy, consistent, and error-free interface. Delivering this interface is core to our design philosophy as Yahoo continues to double down on our OpenStack investment. The Yahoo OpenStack team looks forward to continue collaborating with the OpenStack community to share feedback and code.  ", "date": "2017-04-20"}, {"website": "Yahoo", "title": "Introducing Tripod: Flickr’s Backend, Refactored", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/157200523046/introducing-tripod-flickrs-backend-refactored", "abstract": " By Peter Welch, Senior Principal Engineer  . Today, Yahoo Mail  introduced  a feature that allows you to automatically sync your mobile photos to Yahoo Mail so that they’re readily available when you’re composing an email from your computer. A key technology behind this feature is a new photo and video platform called “Tripod,” which was born out of the innovations and capabilities of  Flickr .  . For 13 years, Flickr has served as one of the world’s largest photo-sharing communities and as a platform for millions of people who have collectively uploaded more than 13 billion photos globally. Tripod provides a great opportunity to bring some of the most-loved and useful Flickr features to the Yahoo network of products, including Yahoo Mail, Yahoo Messenger, and  Yahoo Answers Now . .  Tripod and its Three Services  . As the name suggests, Tripod offers three services:  . The combination of these three services makes Tripod an end-to-end platform for smart image services. There is also an administrative console for configuring the integration of an application with Tripod, and an identity service for authentication and authorization.  .    .  .  The Pixel Service  . Flickr has achieved a highly-scalable photo upload and resizing pipeline. Particularly in the case of large-scale ingestion of thousands of photos and videos, Flickr’s mobile and API teams tuned techniques, like resumable upload and deduplication, to create a high-quality photo-sync experience. On serving, Flickr tackled the challenge of  optimizing storage  without impacting photo quality, and added  dynamic resizing  to support more diverse client photo layouts. . Over many years at Flickr, we’ve demonstrated sustained uploads of more than 500 photos per second. The full pipeline includes the PHP Upload API endpoint, backend Java services (Image Daemon, Storage Master), hot-hot uploads across the US West and East Coasts, and five worldwide photo caches, plus a massive CDN. . In Tripod’s Pixel Service, we leverage all of this core technology infrastructure as-is, except for the API endpoint, which is now written in Java and implements a new bucket-based data model.  .  The Enrichment Service  . In 2013, Flickr made an exciting leap. Yahoo acquired two Computer Vision technology companies,  IQ Engines  and  LookFlow , and rolled these incredible teams into Flickr. Using their image recognition algorithms, we enhanced Flickr Search and introduced  Magic View  to the Flickr Camera Roll.  . In Tripod, the Enrichment Service applies the image recognition technology to each photograph, resulting in rich metadata that can be used to enhance filtering, indexing, and searching. The Enrichment Service can identify places, themes, landmarks, objects, colors, text, media similarity, NSFW content, and best thumbnail. It also performs OCR text recognition and applies an aesthetic score to indicate the overall quality of the photograph.  .  The Aggregation Service  . The Aggregation Service lets an application, such as Yahoo Mail, find media based on any criteria. For example, it can return all the photos belonging to a particular person within a particular application, all public photos, or all photos belonging to a particular person taken in a specific location during a specific time period (e.g. San Francisco between March 1, 2015 and May 31, 2015.)  . Vespa, Yahoo’s internal search engine, indexes all metadata for each media item. If the Enrichment Service has been run on the media, the metadata is indexed in Vespa and is available to the Aggregation API. The result set from a call to the Aggregation Service depends on authentication and the read permissions defined by an API key.  .  APIs and SDKs  . Each service is expressed as a set of APIs. We upgraded our API technology stack, switching from PHP to Spring MVC on a Java Jetty servlet container, and made use of the latest Spring features such as Spring Data, Spring Boot, and Spring Security with OAuth 2.0. Tripod’s API is defined and documented using  Swagger . Each service is developed and deployed semi-autonomously from a separate Git repository with a separate build lifecycle to an independent micro-service container.  .    .  .  Swagger Editor  makes it easy to auto-generate SDKs in many languages, depending on the needs of Yahoo product developers. The mobile SDKs for iOS and Android are most commonly used, as is the JS SDK for Yahoo’s web developers. The SDKs make integration with Tripod by a web or mobile application easy. For example, in the case of the Yahoo Mail photo upload feature, the Yahoo Mail mobile app includes the embedded Tripod SDK to manage the photo upload process.  .  Buckets and API Keys  . The Tripod data model differs in some important ways from the Flickr data model. Tripod applications, buckets, and API keys introduce the notion of multi-tenancy, with a strong access control boundary. An  application  is simply the name of the application that is using Tripod (e.g. Yahoo Mail).  Buckets  are logical containers for the application’s media, and media in an application is further affected by bucket settings such as compression rate, capacity, media time-to-live, and the selection of enrichments to compute. .    .  . Beyond Tripod’s generic attributes, a bucket may also have custom  organizing attributes  that are defined by an application’s developers.  API keys  control read/write permissions on buckets and are used to generate OAuth tokens for anonymous or user-authenticated access to a bucket. .    .  . App developers at Yahoo use the  Tripod Console  to:  . Another departure from the Flickr API is that Tripod can handle media that is not user-generated content (UGC). This is critical for storing curated content, as is required by many Yahoo applications.  .  Architecture and Implementation   . Going from a monolithic architecture to a microservices architecture has had its challenges. In particular, we’ve had to find the right internal communication process between the services. At the core of this is our  Pulsar  Event Bus, over which we send  Avro  messages backed by a  strong schema registry . This lets each Tripod team move fast, without introducing incompatible changes that would break another Tripod service.  . For data persistence, we’ve moved most of our high-scale multi-colo data to  Yahoo’s distributed noSQL database . We’ve been experimenting with using  Redis Cluster  as our caching tier, and we use Vespa to drive the Aggregation service. For Enrichment, we make extensive use of  Storm  and  HBase  for real-time processing of Tripod’s Computer Vision algorithms. Finally, we run large backfills using  PIG ,  Oozie , and  Hive  on  Yahoo’s massive grid infrastructure . . In 2017, we expect Tripod will be at 50% of Flickr’s scale, with Tripod supporting the photo and video needs across many Yahoo applications that serve Yahoo’s 1B users across mobile and desktop. .  After reading about Tripod, you might have a few questions  .  Did Tripod replace Flickr?!   . No! Flickr is still here, better than ever. In fact,  Flickr celebrated its 13th birthday  last week! Over the past several years, the Flickr team has implemented significant innovations on core photo management features (such as an  optimized storage  footprint,  dynamic resizing ,  Camera Roll ,  Magic View , and  Search ). We wanted to make these technology advancements available to other teams at Yahoo! .  But, what about the Flickr API? Why not just use that?  .  Flickr APIs  are being used by hundreds of thousands of third-party developers around the world. Flickr’s API was designed for interacting with Flickr Accounts, Photos, and Groups, generally on lower scale than the Flickr site itself; it was not designed for independent, highly configurable, multi-tenant core photo management at large scale.  .  How can I join the team?  . We’re hiring and we’d love to talk to you about our open opportunities! Just email  tripodjobs@yahoo-inc.com  to start the conversation.  ", "date": "2017-02-13"}, {"website": "Yahoo", "title": "Open Sourcing Athenz: Fine-Grained, Role-Based Access Control", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/160481899076/open-sourcing-athenz-fine-grained-role-based", "abstract": "  yahoocs : . By Lee Boynton, Henry Avetisyan, Ken Fox, Itsik Figenblat, Mujib Wahab, Gurpreet Kaur, Usha Parsa, and Preeti Somal .   . Today, we are pleased to offer  Athenz , an open-source platform for fine-grained access control, to the community. Athenz is a role-based access control (RBAC) solution, providing trusted relationships between applications and services deployed within an organization requiring authorized access.    . If you need to grant access to a set of resources that your applications or services manage, Athenz provides both a centralized  and a decentralized  authorization model to do so. Whether you are using container or VM technology independently or on bare metal, you may need a dynamic and scalable authorization solution. Athenz supports moving workloads from one node to another and gives new compute resources authorization to connect to other services within minutes, as opposed to relying on IP and network ACL solutions that take time to propagate within a large system. Moreover, in very high-scale situations, you may run out of the limited number of network ACL rules that your hardware can support.  . Prior to creating Athenz, we had multiple ways of managing permissions and access control across all services within Yahoo. To simplify, we built a fine-grained, role-based authorization solution that would satisfy the feature and performance requirements our products demand. Athenz was built with open source in mind so as to share it with the community and further its development.  . At Yahoo, Athenz authorizes the dynamic creation of compute instances and containerized workloads, secures builds and deployment of their artifacts to our Docker registry, and among other uses, manages the data access from our centralized key management system to an authorized application or service.  . Athenz provides a REST-based set of APIs modeled in  Resource Description Language (RDL)  to manage all aspects of the authorization system, and includes Java and Go client libraries to quickly and easily integrate your application with Athenz. It allows product administrators to manage what roles are allowed or denied to their applications or services in a centralized management system through a self-serve UI.  .   Access Control Models   . Athenz provides two authorization access control models based on your applications’ or services’ performance needs. More commonly used, the centralized access control model is ideal for provisioning and configuration needs. In instances where performance is absolutely critical for your applications or services, we provide a unique decentralized access control model that provides on-box enforcement of authorization.   . Athenz’s authorization system utilizes two types of tokens: principal tokens (N-Tokens) and role tokens (Z-Tokens). The principal token is an identity token that identifies either a user or a service. A service generates its principal token using that service’s private key. Role tokens authorize a given principal to assume some number of roles in a domain for a limited period of time. Like principal tokens, they are signed to prevent tampering. The name “Athenz” is derived from “Auth” and the ‘N’ and ‘Z’ tokens. .  Centralized Access Control:  The centralized access control model requires any Athenz-enabled application to contact the Athenz Management Service directly to determine if a specific authenticated principal (user and/or service) has been authorized to carry out the given action on the requested resource. At Yahoo, our internal continuous delivery solution uses this model. A service receives a simple Boolean answer whether or not the request should be processed or rejected. In this model, the Athenz Management Service is the only component that needs to be deployed and managed within your environment. Therefore, it is suitable for provisioning and configuration use cases where the number of requests processed by the server is small and the latency for authorization checks is not important.  The diagram below shows a typical control plane-provisioning request handled by an Athenz-protected service.  .    .  Decentralized Access Control:  This approach is ideal where the application is required to handle large number of requests per second and latency is a concern. It’s far more efficient to check authorization on the host itself and avoid the synchronous network call to a centralized Athenz Management Service. Athenz provides a way to do this with its decentralized service using a local policy engine library on the local box. At Yahoo, this is an approach we use for our centralized key management system. The authorization policies defining which roles have been authorized to carry out specific actions on resources, are asynchronously updated on application hosts and used by the Athenz local policy engine to evaluate the authorization check. In this model, a principal needs to contact the Athenz Token Service first to retrieve an authorization role token for the request and submit that token as part of its request to the Athenz protected service. The same role token can then be re-used for its lifetime.    . The diagram below shows a typical decentralized authorization request handled by an Athenz-protected service.  .    . With the power of an RBAC system in which you can choose a model to deploy according your performance latency needs, and the flexibility to choose either or both of the models in a complex environment of hosting platforms or products, it gives you the ability to run your business with agility and scale.    .   Looking to the Future   . We are actively engaged in pushing the scale and reliability boundaries of Athenz. As we enhance Athenz, we look forward to working with the community on the following features: . Our goal is to integrate Athenz with other open source projects that require authorization support and we welcome contributions from the community to make that happen. It is available under Apache License Version 2.0. To evaluate Athenz, we provide both  AWS AMI  and  Docker  images so that you can quickly have a test development environment up and running with ZMS (Athenz Management Service), ZTS (Athenz Token Service), and UI services. Please join us on the path to making application authorization easy. Visit  http://www.athenz.io  to get started!  ", "date": "2017-05-09"}, {"website": "Yahoo", "title": "Simple workflow for building web service APIs", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/142418165386/simple-workflow-for-building-web-service-apis", "abstract": "  Norbert Potocki, Software Engineer @ Yahoo Inc.  . APIs are at the core of  server-client communications, and well-defined API contracts are essential to the overall experience of client developer communities. At  Yahoo, we have explored the best methods to develop APIs - both external (like  apiary ,  apigee ,  API Gateway ) and internal. In our examination,  our main focus was to devise a methodology that provides a simple way to build new server endpoints, while guaranteeing a stable,  streamlined integration for client developers. The workflow itself can be used with one of many  domain-specific languages  (DSL) for API modeling (e.g.  Swagger ,  RAML ,  ardielle ). The  main driver for this project was a need to build a new generation of Flickr  APIs. Flickr has had a long tradition of exposing rich capabilities via our API and innovating. One of Flickr’s contributions to this  domain was  inventing an early version of OAuth   protocol. In this post, we will share a simple workflow that demonstrates the new approach to building APIs. For the purpose of this  article we will focus on the Swagger, although the workflow can easily be adapted to use on another DSL.   . Our goals for developing the workflow: . Let’s take a look at two popular approaches to building APIs. . The first is an implementation-centric approach. Backend developers implement the  service thus defining the API in the code. If you’re lucky there will be some code-level documentation - like javadoc - attached. Other  teams (e.g., frontend, mobile, 3rd party engineers) have to read the javadoc and/or the code to understand the API contract nuances. They  need to understand the programming language used, and the implementation has to be publicly available. Versioning may be tricky and the  situation can get worse when multiple development teams work on different services that represent a single umbrella-project. There’s a  chance that their APIs will fall out-of-sync, be it by implementing rate-limiting headers or by using different versioning models. .   The other approach is to use in-house developed DSL and tools. There are already a slew of mature open source DSLs and tools available on  the market, and opting for this route may be more efficient. Swagger is a perfect example. Many engineers know it and you can share your  early API design with the open source community and get feedback. Also, there’s no extra learning curve involved so the chances that  somebody will contribute are higher. . Let’s start by discussing the API elements we work with and what roles they play in the workflow: . An API specification is one of the most important parts of a web service. You may want to consider  keeping the contract definition separate from the implementation because it will allow you to: . There are  a few popular domain-specific languages that can be used for describing the contract of your service. At Flickr, we use Swagger, and keep  all Swagger files in a GitHub repository called “ api-spec ”. It contains multiple yaml files that describe different parts of the API -  both reusable elements and service-specific endpoint and resource definitions. . To give you a taste of Swagger here’s how a combined yaml file could look like: . One nice aspect of Swagger is the  Swagger editor . It’s a browser-based IDE that shows you a  live preview of your API documentation (generated from the spec) and also provides a console for querying mock backend implementation.  Here’s how it looks like: . Once the changes are approved and merged to master, a number of CD  (continuous delivery) pipelines kick in: one per each SDK that we host and another pipeline for generating documentation. There is also an  option of triggering a CD pipeline generating stubs for backend implementation but the decision is left up to the service owner. . Documentation is the most important yet most unappreciated part of software engineering. At Yahoo, we devote  lots of attention to documenting APIs, and in this workflow we keep the documentation in a separate  GitHub  repository. GitHub offers a great feature called  GitHub    Pages  that allows us to host documentation on their servers and avoid building a custom CD pipeline for documentation. It also gives  you the ability to edit files directly in the browser. GitHub pages are powered by  Jekyll , which  serves HTML pages directly from the repository. You use  Markdown  files to provide  content, select a web template to use and push it to the “ gh-pages ” branch: . The  repo contains both a hand-crafted user’s guide and an automatically generated API reference. The API reference is generated from the API  specification and put in a directory called “ api-reference . . The process of  generating the API reference is executed by a simplistic CD pipeline. Every time you merge changes to the master branch of the API  specification repository, it will assemble the yaml files into a single Swagger json file and submit it as a pull-request towards the  documentation repository. Here’s the simple node.js script that does the transformation: . And a snippet from CD pipeline steps that creates the pull-request: . The “api-reference” directory also contains the  Swagger UI code ,  which is  responsible for rendering the Swagger json file in the browser. It also provides a  console that allows you to send requests against a test backend instance, and comes in very handy when a customer wants to quickly explore  our APIs. Here’s how the final result looks: . Calling an API is  fun. Dealing with failures, re-tries and HTTP connection issues - not so much. That’s where services which have a dedicated SDK really  shine. An SDK can either be a thin wrapper around an HTTP client that deals with marshalling of requests and responses, or a fat client  that has extra business logic in it. Since this extra business logic is handcrafted most of the time, we will exclude it from the  discussion and focus on a thin client instead. . Thin API clients can usually be auto-generated from API specifications. We have a CD  pipeline (similar to the documentation CD pipeline) that is responsible for this process. Each SDK is kept in a separate GitHub  repository. For each API specification change, all SDKs are regenerated and pushed (as pull-requests) to appropriate repositories. Take a  look at the  swagger-codegen  project to learn more about SDK generation. . It’s worth mentioning that the thin layer could also be generated in runtime based on the Swagger json file itself. . The major question that pops out when implementing an API is: should we automatically generate the stub code? From  our experience - it may be worth it, but most often it’s not. API stub scaffolding saves you some initial work when you add a new API.  However, different service owners prefer to structure their code in various manners (packages, class names, how code is divided between  REST controllers, etc.) and thus it’s expensive to develop a one-size-fits-all generator. . The last topic we want to cover is  validating implementation against API specification. Validation happens via tests (written in Cucumber) that are executed with every  change to the implementation. We validate API responses schema, different failure scenarios (for valid HTTP status code usage), returned  headers, rate-limiting mechanism, pagination and others. To maximize code-reuse and simplify test code, we use one of the thin SDKs for  API calls within tests.  . In this article, we provided a simple, yet comprehensive, overview for working with APIs that  we use at Flickr, and examined the key features, including clear separation of different system components (specification, implementation,  documentation, sdks, tests), developer-friendliness and automation options. We also presented the workflow that binds all the components  together in an easy to use, streamlined way. ", "date": "2016-04-07"}, {"website": "Yahoo", "title": "Not All Bugs Are Created Equal", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/146077281316/not-all-bugs-are-created-equal", "abstract": "  yahoo-security : .  Doug DePerry, Senior Security Engineer, Paranoids   . In our inaugural post to  The Paranoid , we discussed the human element behind online attacks– the human adversary . We sought to give some perspectives as to who is behind online threats in order to better understand how to defend against them. Yahoo’s bug bounty program applies that insight in our ongoing efforts to provide a safe environment for our users. By thinking about the economics of security, we’ve found that we can tilt the advantage in our favor by partnering with industry-leading security researchers. . We often get questions from both security researchers, and people just interested in learning about how programs like these work. We thought we’d use this opportunity to take a quick look under the hood. . First, some background. Bug bounty programs essentially crowd-source security. They allow companies to improve coverage so they are able to add additional eyes where they need them. Bug bounty researchers also bring depth of expertise and different skill sets that can uncover hard to find bugs.   . For the past two years, Yahoo has developed one of the largest and most successful bug bounty programs in the industry. We’ve paid out over $1.7 million dollars in bounties, resolved more than 2,000 security bugs and maintain a “hackership” of more than 2,000 researchers, some of whom make careers out of it.  . Security researchers often ask us how we decide the payout associated with a given bug report. At first it might seem logical that we pay based on the type or classification of a security bug. Some bug types tend to be bad, so you might think that they would be paid the same. However, in the vast majority of cases, that’s not the complete story. So if the bug type alone is not what we use to determine the payout, what is? The missing input to the calculation is the impact of the vulnerability. We take into account what data might have been exposed, the sensitivity of that data, the role that data plays, network location and the permissions of the server involved. Those factors are of great importance. . Given the importance of the impact of a bug, the Yahoo bug bounty program does not reward researchers solely based on bug type. The type of bug a security researcher finds is mostly irrelevant. It’s what the bug allows them to do and where that are most important. What can an attacker actually do with this specific bug to potentially affect the security of Yahoo or our users? Furthermore, Yahoo’s application landscape is not necessarily uniform; certain properties or applications are more equal than others. . Here’s an example to show how these factors work in practice. SQL injection bugs are often a devastating bug class because they can provide full access to a database. Odds are, if a company has a presence on the web, they are storing sensitive information in databases. But just because an attacker can access the database does not mean it’s game over. The real reason that the SQL injection bug class can be so devastating is the data stored in the database may be accessed or changed by unauthorized parties. The typical impact of a SQL injection bug is high because the data exposed is typically sensitive, except when it’s not. What if the database doesn’t contain any sensitive data?  . Part of the process in determining impact can seem opaque to the researcher, and we understand that. That obscurity is an unfortunate but necessary fact of life in a bug bounty program. As an external party, it is just not possible to have all the information. The sort of testing available to participants in a public bug bounty program is inherently “black box”–no documentation, no source code, what you see is what you get. . So we encourage bug reporters to include in their reports what they believe the impact of the vulnerability to be (example report  here ). Submitting a report that contains a thorough and detailed explanation of a legitimate security issue is much more highly valued and rewarded. . We also work closely with the developers to ensure the bug is fixed in a timely manner, and to obtain their expert opinion on impact if necessary. If the developers that created the application tell us that no sensitive data is stored in a particular database, we take that into consideration when awarding your bug. More detailed guidelines for our bug bounty program are available at hackerone.com/yahoo. . To paraphrase a little-known quote, “bug bounty programs don’t reward you for being clever.” Users and researchers should know that we place far more weight on how impactful bugs are to our platforms. . For the hackers our there interested in how  @yahoo ’s Bug Bounty program works.  . (via  yahoo-security ) ", "date": "2016-06-17"}, {"website": "Yahoo", "title": "Combining Druid and DataSketches for Real-time, Robust Behavioral Analytics", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/147711922956/combining-druid-and-datasketches-for-real-time", "abstract": " By Himanshu Gupta    . Millions of users around the world interact with Yahoo through their web browsers and mobile devices, generating billions of events every day (e.g. clicking on ads, clicking on various pages of interest, and logging in). As Yahoo’s data grows larger and more complex, we are investing in new ways to better manage and make sense of it. Behavioral analytics is one important branch of analytics in which we are making significant advancements, and is helping us accomplish these tasks. . Beyond simply measuring  how many times  a user has performed a certain action, we also try to understand  patterns  in their actions. We do this in order to help us decide which of our features are impactful and might grow our user base, and to understand responses to ads that might help us improve users’ future experiences. . One example of behavioral analytics is measuring user retention rates for Yahoo properties such as Mail, News, and Finance, and breaking down these rates by different user demographics. Another example is to determine which ads perform well for various types of users (as measured by various signals), and to serve ads appropriately based on that implicit or explicit feedback. . The challenges we face in answering these questions mainly concern storing and interactively querying our user-generated events at massive scale. We heavily make use of distributed systems, and Druid is at the forefront of powering most of our real-time analytics at scale. . One of the features that makes Druid very useful is the ability to summarize data at storage time. This leads to greatly-reduced storage requirements, and hence, faster queries. For example, consider the dataset below:  . This data represents ad clicks for different website domains. We can see that there are many repeated attributes, which we call “dimensions,” in our data across different timestamps. Now, most of the time we don’t care that a certain ad was clicked at a precise millisecond in time. What is a lot more interesting to us, is how many times an ad was clicked over the period of an hour. Thus, we can truncate the raw event timestamps and group all events with the same set of dimensions. When we group the dimensions, we also aggregate the raw event values for the “clicked” column.  . This method is known as summarization, and in practice, we see summarization significantly reduce the amount of raw data we have to store. We’ve chosen to lose some information about the time an event occurred, but there is no loss of fidelity for the “clicked” metric that we really care about.  . Let’s consider the same dataset again, but now with information about which user performed the click. When we go to summarize our data, the highly cardinal and unique “user-id” column prevents our data from compacting very well.  . The number of unique user-ids could be very high due to the number of users visiting Yahoo everyday. So, in our “user-id” column, we end up effectively storing our raw data. Given that we are mostly interested in  how many  unique users performed certain actions, and we don’t really care about  precisely which  users did those actions, it would be nice if we could somehow lose some information about the individual users so that our data could still be summarized.  . One approach to solving this problem is to create a “sketch” of the user-id dimension. Instead of storing every single unique user-id, we instead maintain a hash-based data structure – also known as a sketch – which has smaller storage requirements and gives estimates of user-id dimension cardinality with predictable accuracy. . Leveraging sketches, our summarized data for the user dimension looks something like this:  . Sketch algorithms are highly desirable because they are very scalable, use predictable storage, work with real-time streams of data, and provide predictable estimates. There are many different algorithms to construct different type of sketches, and a lot of fancy mathematics goes into detail about how sketch algorithms work and why we can get very good estimations of results.  . At Yahoo, we  recently  developed an open source library called  DataSketches . DataSketches provides implementations of various approximate sketch-based algorithms that enable faster, cheaper analytics on large datasets. By combining DataSketches with an extremely low-latency data store, such as Druid, you bring sketches into practical use in a big data store. Embedding sketch algorithms in a data store and persisting the actual sketches is relatively novel in the industry, and is the future structure of big data analytics systems. . Druid’s flexible plugin architecture allows us to integrate it with DataSketches; as such, we’ve developed and open sourced an  extension to Druid  that allows DataSketches to be used as a Druid aggregation function. Druid applies the aggregation function on selected columns and stores aggregated values instead of raw data. . By leveraging the fast, approximate calculations of DataSketches, complex analytic queries such as cardinality estimation and retention analysis can be completed in less than one second in Druid. This allows developers to visualize the results in real-time, and to be able to slice and dice results across a variety of different filters. For example, we can quickly determine how many users visited our core products, including Yahoo News, Sports, and Finance, as well as see how many of those users returned some time later. We can also break down our results in real-time based on user demographics such as age and location. . If you have similar use cases to ours, we invite you to try out DataSketches and Druid for behavioral analytics. For more information about DataSketches, please visit the DataSketches  website . For more information about Druid, please visit the  project webpage . And finally, documents for the DataSketches and Druid integration can be found in the  Druid docs .  ", "date": "2016-07-20"}, {"website": "Yahoo", "title": "Open-sourcing Pulsar, Pub-sub Messaging at Scale", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/150078336821/open-sourcing-pulsar-pub-sub-messaging-at-scale", "abstract": " By Joe Francis and Matteo Merli, Yahoo Platforms    . Pub-sub messaging is a very common design pattern that is increasingly found in distributed systems powering Internet applications. These applications provide real-time services, and need publish-latencies of 5ms on average and no more than 15ms at the 99th percentile. At Internet scale, these applications require a messaging system with ordering, strong durability, and delivery guarantees. In order to handle the “five 9’s” durability requirements of a production environment, the messages have to be committed on multiple disks or nodes. . At the time we started, we could not find any existing open-source messaging solution that could provide the scale, performance, and features Yahoo required to provide messaging as a hosted service, supporting a million topics. So we set out to build Pulsar as a general messaging solution, that also addresses these specific requirements. .  Pulsar  is a highly scalable, low latency pub-sub messaging system running on commodity hardware. It provides simple pub-sub messaging semantics over  topics , guaranteed at-least-once delivery of messages, automatic cursor management for subscribers, and cross-datacenter replication. . Using Pulsar, one can set up a centrally-managed cluster to provide pub-sub messaging as a service; applications can be onboarded as tenants. Pulsar is horizontally scalable; the number of topics, messages processed, throughput, and storage capacity can be expanded by adding servers to the pool. . Pulsar has a robust set of APIs to manage the service, namely, account management activities like  provisioning users, allocating capacity, accounting usage, and monitoring the service. Tenants can administer, manage, and monitor their own domains via APIs. Pulsar also provides security via a pluggable authentication scheme, and access control features that let tenants manage access to their data. . Application development using Pulsar is easy due to the simple messaging model and API. Pulsar includes a client library that encapsulates the messaging protocol; complex functions like service discovery, as well as connection establishment and recovery, are handled internally by the library.  .  Architecture  . At a high level, a Pulsar instance is composed of multiple clusters, typically residing in different geographical regions. A Pulsar cluster is composed of a set of Brokers and BookKeepers (bookies), plus ZooKeeper ensembles for coordination and configuration management. .    . A Pulsar  broker  serves topics. Each topic is assigned to a broker, and a broker serves thousands of topics. The broker accepts messages from writers, commits them to a durable store, and dispatches them to readers. The broker also serves admin requests. It has no durable state. The broker has built-in optimizations; for example, it caches the data in order to avoid additional disk reads when dispatching messages to clients as well as replication clusters. Pulsar brokers also manage the replicators, which asynchronously push messages published in the local cluster to remote clusters. .  Apache  BookKeeper   is the building block for Pulsar’s durable storage. BookKeeper is a distributed write-ahead log system, a top-level Apache project that was originally developed at and open-sourced by Yahoo in 2011. BookKeeper has an active developer community with contributors across the  industry . Using the BookKeeper built-in semantics, Pulsar creates multiple independent logs, called ledgers, and uses them for durable message storage. Bookkeeper hosts, called bookies, are designed to handle thousands of ledgers with concurrent reads and writes. BookKeeper is horizontally scalable in capacity and throughput; from an operational perspective we can elastically add more bookies to a Pulsar cluster to increase capacity.  . By using separate physical disks (one for journal and another for general storage), bookies are able to isolate the effects of read operations from impacting the latency of ongoing write operations, and vice-versa. Since read and write paths are decoupled, spikes in reads – which commonly occur when readers drain backlog to catch up – do not impact publish latencies in Pulsar. This sets Pulsar apart from other commonly-used messaging systems. .  Managed Ledger  represents the storage layer for a single topic. It is the abstraction of a stream of messages, with a single writer, and multiple readers, each with its own associated cursor position, the offset of the reader in the message stream. A single managed ledger uses multiple BookKeeper ledgers to store the data. Cursor positions are maintained in per-cursor ledgers. . A Pulsar cluster runs a   ZooKeeper   (another top-level Apache project open-sourced by Yahoo in 2008) ensemble used for coordinating assignment of topics among brokers, and storing BookKeeper metadata. In addition, Pulsar runs a Global ZooKeeper ensemble to store the provisioning and configuration data. At Yahoo, we have presence in multiple regions and our users create global topics that are replicated between these regions. The Global Zookeeper ensemble keeps provisioning and configuration data consistent globally. We can tolerate higher write latencies on these writes (e.g.: ~150ms latency for configuration writes).  . The  load balancer  is a distributed service that runs on the brokers, to make sure the traffic is equally spread across all available brokers. Since Pulsar brokers have no durable state, topics can be redistributed within seconds. .  Messaging Model  . The Pulsar topic is the core of the system; applications and components communicate by publishing to and consuming from the same topic. Topics are created dynamically as needed when a producer (writer) starts publishing on it; and topics are removed when not in use. .    . Subscriptions are created automatically when a consumer (reader) subscribes to the topic. A subscription persists until it is deleted, and receives all messages published during its lifetime. Common messaging semantics (like JMS Topic or Queue) are available as subscription modes; an exclusive subscription is equivalent to a “topic,” and a shared subscription is equivalent to a “queue.” .  Performance  . Pulsar is designed for low-publish latencies at scale. Our typical publish latencies on average are well below 5ms. With SSD as the bookie journal device, Pulsar can achieve 99 percentile latencies of 5ms with two guaranteed copies and total ordering. .    . The latency remains within the acceptable range until the throughput reaches the limit of the disk IO capacity. . Pulsar supports partitioned topics, which can further increase the per-topic throughput. .  Pulsar at Yahoo  . Pulsar backs major Yahoo applications like Mail, Finance, Sports, Gemini Ads, and Sherpa, Yahoo’s distributed key-value service. . We deployed our first Pulsar instance in Q2 2015. Pulsar use has rapidly grown since then, and as of today, Yahoo runs Pulsar at scale. . As Pulsar use grows at Yahoo, we have been scaling the service horizontally. Most of the challenges we faced were with JVM GC impacting publish latencies, and reducing failover times when the number of topics on a broker went up to tens of 1000s (now 40,000). This led to significant changes to the Pulsar broker and to BookKeeper. .  Looking to the Future  . We are actively engaged in pushing the scale and reliability boundaries of Pulsar further. Current improvements being worked on include:  .  Conclusion  . Pulsar is a highly scalable pub-sub messaging system, production-ready and battled tested at Yahoo. We are glad to make Pulsar available as open source under Apache License Version 2.0. Detailed instructions and documentation are available at Yahoo’s  Github repository . Our goal is to make Pulsar widely used and well integrated with other large-scale open source software, and we welcome contributions from the community to make that happen. .  Additional Links  ", "date": "2016-09-07"}, {"website": "Yahoo", "title": "Omid’s First Step in the Apache Community", "author": [" davglass"], "link": "https://yahooeng.tumblr.com/post/151015726181/omids-first-step-in-the-apache-community", "abstract": "  yahoohadoop : .  By Francisco Perez-Sorrosal, Ohad Shacham, Kostas Tsioutsiouliklis, and Edward Bortnikov  .  We are proud to announce that Omid (“Hope” in Persian), Yahoo’s transaction manager for HBase [1][2], has been accepted as an  Apache Incubator project . Yahoo has been a long-time contributor to the Apache community in the Hadoop ecosystem, including HBase, YARN, Storm, and Pig. Our acceptance as an Apache Incubator project is another step forward following the success of ZooKeeper [3] and BookKeeper [4], which were born at Yahoo and graduated to top-level Apache projects.  . These days, most NoSQL databases, including HBase, do not provide the OLTP support available in traditional relational databases, forcing the applications running on top of them to trade transactional support for greater agility and scalability. However, transactions are essential in many applications using NoSQL datastores as the main source of data, for example, in incremental content processing systems. Omid enables these applications to benefit from the best of both worlds: the scalability provided by NoSQL datastores, such as HBase, and the concurrency and atomicity provided by transaction processing systems. . Omid provides a high-performant ACID transactional framework with Snapshot Isolation guarantees on top of HBase [5], being able to scale to thousands of clients triggering transactions on application data. It’s one of the few open-source transactional frameworks that can scale beyond 100K transactions per second on mid-range hardware while incurring minimal impact on the latency accessing the datastore.  .    . At its core, Omid utilizes a lock-free approach to support multiple concurrent clients. Its design relies on a centralized conflict detection component called Transaction Status Oracle (TSO), which efficiently resolves write-set collisions among concurrent transactions [6]. Another important benefit is that Omid does not require any modification of the underlying key-value datastore – HBase in this case. Moreover, the recently-added high-availability algorithm eliminates the single point of failure represented by the TSO in those deployments that require a higher degree of dependability [7]. Last but not least, the API is very simple – mimicking the transaction manager APIs in the relational world: begin, commit, rollback – and the client and server configuration processes have been simplified to help both application developers and system administrators. . Efforts toward growing the community have already been underway in the last few months. Apache Hive [8] contributors from Hortonworks expressed interest in storing Hive metadata in HBase using Omid, and this led to a fruitful collaboration that resulted in Omid now supporting HBase 1.x versions. Omid could also be used as the transaction manager in other SQL abstraction layers on top of HBase such as Apache Phoenix [9], or as the transaction coordinator in distributed systems, such as the Apache DistributedLog project [10] and  Pulsar , a distributed pub-sub messaging platform recently open sourced by Yahoo. . Since its inception in 2011 at Yahoo Research, Omid has matured to operate at Web scale in a production environment. For example, since 2014 Omid has been used at Yahoo – along with other Hadoop technologies – to power our incremental content ingestion platform for search and personalization products. In this role, Omid is serving millions of transactions per day over HBase data. . We have decided to move the Omid project to “ the Apache Way ” because we think it is the next logical step after having battle-tested the project in production at Yahoo and having open-sourced the code in Yahoo’s public Github in 2012 (The Omid Github repository currently has 269 stars and 101 forks, and we were asked by our colleagues in the Open Source community to release it as an Apache Incubator project.). As we aim to form a larger Omid community outside Yahoo, we think that the Apache Software Foundation is the perfect umbrella to achieve this. We invite the Apache community to contribute by providing patches, reviewing code, proposing new features or improvements, and giving talks at conferences such as Hadoop Summit, HBaseCon, ApacheCon, etc. under the Apache rules. .    . We see Omid being recognized as an Apache Incubator Project as the first step in growing a vibrant community around this technology. We are confident that contributors in the Apache community will add more features to Omid and further enhance the current performance and latency. Stay tuned to  @ApacheOmid  on Twitter! .   .  References   .    . [1] Apache Omid Gihthub repo:  https://github.com/apache/incubator-omid  . [2] Apache Omid documentation:  http://omid.incubator.apache.org/  . [3] Apache ZooKeeper project:  http://zookeeper.apache.org/  . [4] Apache BookKeeper project:  http://bookkeeper.apache.org/  . [5] Blog Entry introducing Omid:  http://yahoohadoop.tumblr.com/post/129089878751/introducing-omid-transaction-processing-for  . [6] Blog Entry on Omid’s Architecture and Protocol:   http://yahoohadoop.tumblr.com/post/132695603476/omid-architecture-and-protocol  . [7] Blog Entry on Omid’s High Availability:  http://yahoohadoop.tumblr.com/post/138682361161/high-availability-in-omid  . [8] Apache Hive project:  https://hive.apache.org/  . [9] Apache Phoenix project:  https://phoenix.apache.org/  . [10] Apache DistributedLog project:  http://distributedlog.incubator.apache.org/  . (via  yahoohadoop ) ", "date": "2016-09-27"}, {"website": "Yahoo", "title": "Open Sourcing a Deep Learning Solution for Detecting NSFW Images", "author": [" davglass"], "link": "https://yahooeng.tumblr.com/post/151148689421/open-sourcing-a-deep-learning-solution-for", "abstract": " By Jay Mahadeokar and Gerry Pesavento . Automatically identifying that an image is not suitable/safe for work (NSFW), including offensive and adult images, is an important problem which researchers have been trying to tackle for decades. Since images and user-generated content dominate the Internet today, filtering NSFW images becomes an essential component of Web and mobile applications. With the evolution of computer vision, improved training data, and deep learning algorithms, computers are now able to automatically classify NSFW image content with greater precision. . Defining NSFW material is subjective and the task of identifying these images is non-trivial. Moreover, what may be objectionable in one context can be suitable in another. For this reason, the model we describe below focuses only on one type of NSFW content: pornographic images. The identification of NSFW sketches, cartoons, text, images of graphic violence, or other types of unsuitable content is not addressed with this model. . To the best of our knowledge, there is no open source model or algorithm for identifying NSFW images. In the spirit of collaboration and with the hope of advancing this endeavor, we are releasing our deep learning model that will allow developers to experiment with a classifier for NSFW detection, and provide feedback to us on ways to improve the classifier. .  Our general purpose Caffe deep neural network model (Github code)  takes an image as input and outputs a probability (i.e a score between 0-1) which can be used to detect and filter NSFW images. Developers can use this score to filter images below a certain suitable threshold based on a  ROC  curve for specific use-cases, or use this signal to rank images in search results. .  Convolutional Neural Network (CNN) architectures and tradeoffs  . In recent years, CNNs have become very successful in image classification problems [1] [5] [6]. Since 2012, new CNN architectures have continuously improved the accuracy of the standard  ImageNet  classification challenge. Some of the major breakthroughs include AlexNet (2012) [6], GoogLeNet [5], VGG (2013) [2] and Residual Networks (2015) [1]. These networks have different tradeoffs in terms of runtime, memory requirements, and accuracy. The main indicators for runtime and memory requirements are: . Ideally we want a network with minimum flops and minimum parameters, which would achieve maximum accuracy. .  Training a deep neural network for NSFW classification  . We train the models using a dataset of positive (i.e. NSFW) images and negative (i.e. SFW – suitable/safe for work) images. We are not releasing the training images or other details due to the nature of the data, but instead we open source the output model which can be used for classification by a developer. . We use the  Caffe  deep learning library and  CaffeOnSpark ; the latter is a powerful open source framework for distributed learning that brings Caffe deep learning to Hadoop and Spark clusters for training models (Big shout out to Yahoo’s CaffeOnSpark team!). . While training, the images were resized to 256x256 pixels, horizontally flipped for data augmentation, and randomly cropped to 224x224 pixels, and were then fed to the network. For training residual networks, we used scale augmentation as described in the ResNet paper [1], to avoid overfitting. We evaluated various architectures to experiment with tradeoffs of runtime vs accuracy. .  Tradeoffs of different architectures: accuracy vs number of flops vs number of params in network.  . The deep models were first pre-trained on the  ImageNet  1000 class dataset. For each network, we replace the last layer (FC1000) with a 2-node fully-connected layer.  Then we fine-tune the weights on the NSFW dataset. Note that we keep the learning rate multiplier for the last FC layer 5 times the multiplier of other layers, which are being fine-tuned. We also tune the hyper parameters (step size, base learning rate) to optimize the performance. . We observe that the performance of the models on NSFW classification tasks is related to the performance of the pre-trained model on ImageNet classification tasks, so if we have a better pretrained model, it helps in fine-tuned classification tasks. The graph below shows the relative performance on our held-out NSFW evaluation set. Please note that the false positive rate (FPR) at a fixed false negative rate (FNR) shown in the graph is specific to our evaluation dataset, and is shown here for illustrative purposes. To use the models for NSFW filtering, we suggest that you plot the ROC curve using your dataset and pick a suitable threshold. .  Comparison of performance of models on Imagenet and their counterparts fine-tuned on NSFW dataset.  . We are releasing the thin ResNet 50 model, since it provides good tradeoff in terms of accuracy, and the model is lightweight in terms of runtime (takes &lt; 0.5 sec on CPU) and memory (~23 MB). Please refer our  git repository  for instructions and usage of our model. We encourage developers to try the model for their NSFW filtering use cases. For any questions or feedback about performance of model, we encourage  creating a issue  and we will respond ASAP. . Results can be improved by  fine-tuning  the model for your dataset or use case. If you achieve improved performance or you have trained a NSFW model with different architecture, we encourage contributing to the model or sharing the link on our  description  page. . Disclaimer: The definition of NSFW is subjective and contextual. This model is a general purpose reference model, which can be used for the preliminary filtering of pornographic images. We do not provide guarantees of accuracy of output, rather we make this available for developers to explore and enhance as an open source project. . We would like to thank  Sachin Farfade ,  Amar Ramesh Kamat ,  Armin Kappeler , and Shraddha Advani for their contributions in this work. .  References : . [1] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. “Deep residual learning for image recognition” arXiv preprint arXiv:1512.03385 (2015). . [2] Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.”; arXiv preprint arXiv:1409.1556(2014). . [3] Iandola, Forrest N., Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. “SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 1MB model size.”; arXiv preprint arXiv:1602.07360 (2016). . [4] He, Kaiming, and Jian Sun. “Convolutional neural networks at constrained time cost.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5353-5360. 2015. . [5] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet,Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. “Going deeper with convolutions”  In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9. 2015. . [6] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks” In Advances in neural information processing systems, pp. 1097-1105. 2012. ", "date": "2016-09-30"}, {"website": "Yahoo", "title": "Managing Your Yahoo Account Access is Easier than Ever", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/151708893366/managing-your-yahoo-account-access-is-easier-than", "abstract": "  yahoo : .  By Dylan Casey, Vice President of Product Management  . We’re making it easier than ever to see and manage all of the devices connected to your Yahoo account. Today, you might notice some new improvements to help you keep track of the account activity and devices associated with your Yahoo account. This information is available to all users under “Account Info” here:  https://login.yahoo.com/account/activity . Before we get too technical, let’s explain how this works in a real-world scenario. . Imagine that your phone falls out of your pocket in a taxi and later that day you realize that you’ve lost it. From a computer, tablet or alternate device, just sign in to your Yahoo account and head over to “Account Info.” There you’ll find a tab that says “Recent Activity.” Find the apps on your phone that are shown to have access to your account and remove them. This will invalidate the  OAuth token  so that no one else can use those apps to access your account on your lost phone. The same can be done for any other devices you might own that are authorized to use your Yahoo account, including a laptop, desktop computer, tablet or cell phone.  . Users already had the ability to invalidate OAuth tokens through the Member Center, but this feature makes it easier to see and control which devices and apps are validated to access their Yahoo account, offering greater convenience and peace of mind. ", "date": "2016-10-12"}, {"website": "Yahoo", "title": "Bringing the Viewer In: The Video Opportunity in Virtual Reality", "author": [" mikesefanov"], "link": "https://yahooeng.tumblr.com/post/151940036881/bringing-the-viewer-in-the-video-opportunity-in", "abstract": " By Satender Saroha, Video Engineering    . Virtual reality (VR) 360° videos are the next frontier of how we engage with and consume content. Unlike a traditional scenario in which a person views a screen in front of them, VR places the user inside an immersive experience. A viewer is “in” the story, and not on the sidelines as an observer.  . Ivan Sutherland, widely regarded as the father of computer graphics, laid out the vision for virtual reality in his famous speech, “Ultimate Display” in 1965 [1]. In that he said, “You shouldn’t think of a computer screen as a way to display information, but rather as a window into a virtual world that could eventually look real, sound real, move real, interact real, and feel real.” . Over the years, significant advancements have been made to bring reality closer to that vision. With the advent of headgear capable of rendering 3D spatial audio and video, realistic sound and visuals can be virtually reproduced, delivering immersive experiences to consumers.  . When it comes to entertainment and sports, streaming in VR has become the new 4K HEVC/UHD of 2016. This has been accelerated by the release of new camera capture hardware like GoPro and streaming capabilities such as 360° video streaming from Facebook and YouTube. Yahoo streams lots of engaging sports, finance, news, and entertainment video content to tens of millions of users. The opportunity to produce and stream such content in 360° VR opens a unique opportunity to Yahoo to offer new types of engagement, and bring the users a sense of depth and visceral presence. . While this is not an experience that is live in product, it is an area we are actively exploring. In this blog post, we take a look at what’s involved in building an end-to-end VR streaming workflow for both Live and Video on Demand (VOD). Our experiments and research goes from camera rig setup, to video stitching, to encoding, to the eventual rendering of videos on video players on desktop and VR headsets. We also discuss challenges yet to be solved and the opportunities they present in streaming VR.  .  1. The Workflow   . Yahoo’s video platform has a workflow that is used internally to enable streaming to an audience of tens of millions with the click of a few buttons. During experimentation, we enhanced this same proven platform and set of APIs to build a complete 360°/VR experience. The diagram below shows the end-to-end workflow for streaming 360°/VR that we built on Yahoo’s video platform.  .  1.1. Capturing 360° video   . In order to capture a virtual reality video, you need access to a 360°-capable video camera. Such a camera uses either fish-eye lenses or has an array of wide-angle lenses to collectively cover a 360 (θ) by 180 (ϕ) sphere as shown below.  . Though it sounds simple, there is a real challenge in capturing a scene in 3D 360° as most of the 360° video cameras offer only 2D 360° video capture.  . In initial experiments, we tried capturing 3D video using two cameras side-by-side, for left and right eyes and arranging them in a spherical shape. However this required too many cameras – instead we use view interpolation in the stitching step to create virtual cameras.  . Another important consideration with 360° video is the number of axes the camera is capturing video with. In traditional 360° video that is captured using only a single-axis (what we refer as horizontal video), a user can turn their head from left to right. But this setup of cameras does not support a user tilting their head at 90°.  . To achieve true 3D in our setup, we went with 6-12 GoPro cameras having 120° field of view (FOV) arranged in a ring, and an additional camera each on top and bottom, with each one outputting 2.7K at 30 FPS.  .  1.2. Stitching 360° video   .  Projection Layouts  . Because a 360° view is a spherical video, the surface of this sphere needs to be projected onto a planar surface in 2D so that video encoders can process it. There are two popular layouts:    .  Equirectangular layout:  This is the most widely-used format in computer graphics to represent spherical surfaces in a rectangular form with an aspect ratio of 2:1. This format has redundant information at the poles which means some pixels are over-represented, introducing distortions at the poles compared to the equator (as can be seen in the equirectangular mapping of the sphere below). .  CubeMap layout:  CubeMap layout is a format that has also been used in computer graphics. It contains six individual 2D textures that map to six sides of a cube. The figure below is a typical cubemap representation. In a cubemap layout, the sphere is projected onto six faces and the images are folded out into a 2D image, so pieces of a video frame map to different parts of a cube, which leads to extremely efficient compact packing. Cubemap layouts require about 25% fewer pixels compared to equirectangular layouts. .  Stitching Videos   . In our setup, we experimented with a couple of stitching softwares. One was from Vahana VR [4], and the other was a modified version of the open-source Surround360 technology that works with a GoPro rig [5]. Both softwares output equirectangular panoramas for the left and the right eye. Here are the steps involved in stitching together a 360° image: .  Raw frame image processing:  Converts uncompressed raw video data to RGB, which involves several steps starting from black-level adjustment, to applying Demosaic algorithms in order to figure out RGB color parts for each pixel based on the surrounding pixels. This also involves gamma correction, color correction, and anti vignetting (undoing the reduction in brightness on the image periphery). Finally, this stage applies sharpening and noise-reduction algorithms to enhance the image and suppress the noise. .  Calibration:  During the calibration step, stitching software takes steps to avoid vertical parallax while stitching overlapping portions in adjacent cameras in the rig. The purpose is to align everything in the scene, so that both eyes see every point at the same vertical coordinate. This step essentially matches the key points in images among adjacent camera pairs. It uses computer vision algorithms for feature detection like Binary Robust Invariant Scalable Keypoints (BRISK) [6] and AKAZE [7]. .  Optical Flow:  During stitching, to cover the gaps between adjacent real cameras and provide interpolated view, optical flow is used to create virtual cameras. The optical flow algorithm finds the pattern of apparent motion of image objects between two consecutive frames caused by the movement of the object or camera. It uses OpenCV algorithms to find the optical flow [8]. . Below are the frames produced by the GoPro camera rig:  .  .    . To get the full depth in stereo, the rig is set-up so that i = r * sin(FOV/2 - 360/n). where:    . Given IPD is normally 6.4 cms, i should be greater than 3.2 cm. This implies that with a 12-camera setup, the radius of the the rig comes to 14 cm(s). Usually, if there are more cameras it is easier to avoid black stripes.  .  Reducing Bandwidth – FOV-based adaptive transcoding   For a truly immersive experience, users expect 4K (3840 x 2160) quality resolution at 60 frames per second (FPS) or higher. Given typical HMDs have a FOV of 120 degrees, a full 360° video needs a resolution of at least 12K (11520 x 6480). 4K streaming needs a bandwidth of 25 Mbps [9]. So for 12K resolution, this effectively translates to &gt; 75 Mbps and even more for higher framerates. However, average wifi in US has bandwidth of 15 Mbps [10].  . One way to address the bandwidth issue is by reducing the resolution of areas that are out of the field of view. Spatial sub-sampling is used during transcoding to produce multiple viewport-specific streams. Each viewport-specific stream has high resolution in a given viewport and low resolution in the rest of the sphere. . On the player side, we can modify traditional adaptive streaming logic to take into account field of view. Depending on the video, if the user moves his head around a lot, it could result in multiple buffer fetches and could result in rebuffering. Ideally, this will work best in videos where the excessive motion happens in one field of view at a time and does not span across multiple fields of view at the same time. This work is still in an experimental stage. . The default output format from stitching software of both Surround360 and Vahana VR is equirectangular format. In order to reduce the size further, we pass it through a cubemap filter transform integrated into ffmpeg to get an additional pixel reduction of ~25%  [11] [12]. . At the end of above steps, the stitching pipeline produces high-resolution stereo 3D panoramas which are then ingested into the existing Yahoo Video transcoding pipeline to produce multiple bit-rates HLS streams.  .  1.3. Adding a stitching step to the encoding pipeline  .  Live  – In order to prepare for multi-bitrate streaming over the Internet, a live 360° video-stitched stream in RTMP is ingested into Yahoo’s video platform. A live Elemental encoder was used to re-encode and package the live input into multiple bit-rates for adaptive streaming on any device (iOS, Android, Browser, Windows, Mac, etc.) .  Video on Demand  – The existing Yahoo video transcoding pipeline was used to package multiple bit-rates HLS streams from raw equirectangular mp4 source videos.  .   1.4. Rendering 360° video into the player   . The spherical video stream is delivered to the Yahoo player in multiple bit rates. As a user changes their viewing angle, different portion of the frame are shown, presenting a 360° immersive experience. There are two types of VR players currently supported at Yahoo: .  WebVR based Javascript Player  – The Web community has been very active in enabling VR experiences natively without plugins from within browsers. The W3C has a Javascript proposal [13], which describes support for accessing virtual reality (VR) devices, including sensors and head-mounted displays on the Web. VR Display is the main starting point for all the device APIs supported. Some of the key interfaces and attributes exposed are: . We implemented a subset of webvr spec in the Yahoo player (not in production yet) that lets you watch monoscopic and stereoscopic 3D video on supported web browsers (Chrome, Firefox, Samsung), including Oculus Gear VR-enabled phones. The Yahoo player takes the equirectangular video and maps its individual frames on the Canvas javascript element. It uses the webGL and Three.JS libraries to do computations for detecting the orientation and extracting the corresponding frames to display.  . For web devices which support only  monoscopic  rendering like desktop browsers without HMD, it creates a single Perspective Camera object specifying the FOV and aspect ratio. As the device’s  requestAnimationFrame  is called it renders the new frames. As part of rendering the frame, it first calculates the projection matrix for FOV and sets the X (user’s right), Y (Up), Z (behind the user) coordinates of the camera position. . For devices that support  stereoscopic  rendering like mobile phones from Samsung Gear, the webvr player creates two PerspectiveCamera objects, one for the left eye and one for the right eye. Each Perspective camera queries the VR device capabilities to get the eye parameters like FOV,  renderWidth  and  render Height  every time a frame needs to be rendered at the native refresh rate of HMD. The key difference between stereoscopic and monoscopic is the perceived sense of depth that the user experiences, as the video frames separated by an offset are rendered by separate canvas elements to each individual eye. .  Cardboard VR  – Google provides a VR sdk for both iOS and Android [14]. This simplifies common VR tasks like-lens distortion correction, spatial audio, head tracking, and stereoscopic side-by-side rendering. For iOS, we integrated Cardboard VR functionality into our Yahoo Video SDK, so that users can watch stereoscopic 3D videos on iOS using Google Cardboard.  .  2. Results   . With all the pieces in place, and experimentation done, we were able to successfully do a 360° live streaming of an internal company-wide event.  . In addition to demonstrating our live streaming capabilities, we are also experimenting with showing 360° VOD videos produced with a GoPro-based camera rig. Here is a screenshot of one of the 360° videos being played in the Yahoo player.  .  3. Challenges and Opportunities  .  3.1.  Enormous amounts of data   . As we alluded to in the video processing section of this post, delivering 4K resolution videos for each eye for each FOV at a high frame-rate remains a challenge. While FOV-adaptive streaming does reduce the size by providing high resolution streams separately for each FOV, providing an impeccable 60 FPS or more viewing experience still requires a lot more data than the current internet pipes can handle. Some of the other possible options which we are closely paying attention to are: .  Compression efficiency with HEVC and VP9  – New codecs like HEVC and VP9 have the potential to provide significant compression gains. HEVC open source codecs like x265 have shown a 40% compression performance gain compared to the currently ubiquitous H.264/AVC codec. LIkewise, a VP9 codec from Google has shown similar 40% compression performance gains. The key challenge is the hardware decoding support and the browser support. But with Apple and Microsoft very much behind HEVC and Firefox and Chrome already supporting VP9, we believe most browsers would support HEVC or VP9 within a year. .  Using 10 bit color depth vs 8 bit color depth  – Traditional monitors support 8 bpc (bits per channel) for displaying images. Given each pixel has 3 channels (RGB), 8 bpc maps to 256x256x256 color/luminosity combinations to represent 16 million colors. With 10 bit color depth, you have the potential to represent even more colors. But the biggest stated advantage of using 10 bit color depth is with respect to compression during encoding even if the source only uses 8 bits per channel. Both x264 and x265 codecs support 10 bit color depth, with ffmpeg already supporting encoding at 10 bit color depth. .  3.2.  Six degrees of freedom   . With current camera rig workflows, users viewing the streams through HMD are able to achieve three degrees of Freedom (DoF) i.e., the ability to move up/down, clockwise/anti-clockwise, and swivel. But you still can’t get a different perspective when you move inside it i.e., move forward/backward. Until now, this true six DoF immersive VR experience has only been possible in CG VR games. In video streaming, LightField technology-based video cameras produced by Lytro are the first ones to capture light field volume data from all directions [15]. But Lightfield-based videos require an order of magnitude more data than traditional fixed FOV, fixed IPD, fixed lense camera rigs like GoPro. As bandwidth problems get resolved via better compressions and better networks, achieving true immersion should be possible.  .  4. Conclusion  . VR streaming is an emerging medium and with the addition of 360° VR playback capability, Yahoo’s video platform provides us a great starting point to explore the opportunities in video with regard to virtual reality. As we continue to work to delight our users by showing immersive video content, we remain focused on optimizing the rendering of high-quality 4K content in our players. We’re looking at building FOV-based adaptive streaming capabilities and better compression during delivery. These capabilities, and the enhancement of our webvr player to play on more HMDs like HTC Vive and Oculus Rift, will set us on track to offer streaming capabilities across the entire spectrum. At the same time, we are keeping a close watch on advancements in supporting spatial audio experiences, as well as advancements in the ability to stream volumetric lightfield videos to achieve true six degrees of freedom, with the aim of realizing the true potential of VR.  .  Glossary – VR concepts:   .  VR  – Virtual reality, commonly referred to as VR, is an immersive computer-simulated reality experience that places viewers inside an experience. It “transports” viewers from their physical reality into a closed virtual reality. VR usually requires a headset device that takes care of sights and sounds, while the most-involved experiences can include external motion tracking, and sensory inputs like touch and smell. For example, when you put on VR headgear you suddenly start feeling immersed in the sounds and sights of another universe, like the deck of the Star Trek Enterprise. Though you remain physically at your place, VR technology is designed to manipulate your senses in a manner that makes you truly feel as if you are on that ship, moving through the virtual environment and interacting with the crew.    .  360 degree video  – A 360° video is created with a camera system that simultaneously records all 360 degrees of a scene. It is a flat equirectangular video projection that is morphed into a sphere for playback on a VR headset. A standard world map is an example of equirectangular projection, which maps the surface of the world (sphere) onto orthogonal coordinates. .  Spatial Audio  – Spatial audio gives the creator the ability to place sound around the user. Unlike traditional mono/stereo/surround audio, it responds to head rotation in sync with video. While listening to spatial audio content, the user receives a real-time binaural rendering of an audio stream [17]. .  FOV  – A human can naturally see 170 degrees of viewable area (field of view). Most consumer grade head mounted displays HMD(s) like Oculus Rift and HTC Vive now display 90 degrees to 120 degrees.  .  Monoscopic video  – A monoscopic video means that both eyes see a single flat image, or video file. A common camera setup involves six cameras filming six different fields of view. Stitching software is used to form a single equirectangular video. Max output resolution on 2D scopic videos on Gear VR is 3480×1920 at 30 frames per second. .  Presence  – Presence is a kind of immersion where the low-level systems of the brain are tricked to such an extent that they react just as they would to non-virtual stimuli. .  Latency  – It’s the time between when you move your head, and when you see physical updates on the screen. An acceptable latency is anywhere from 11 ms (for games) to 20 ms (for watching 360 vr videos). .  Head Tracking  - There are two forms: .  References:  . [1] Ultimate Display Speech as reminisced by Fred Brooks:  http://www.roadtovr.com/fred-brooks-ivan-sutherlands-1965-ultimate-display-speech/  . [2] Equirectangular Layout Image:  https://www.flickr.com/photos/54144402@N03/10111691364/  . [3] CubeMap Layout:  http://learnopengl.com/img/advanced/cubemaps_skybox.png  . [4] Vahana VR:  http://www.video-stitch.com/   . [5] Surround360 Stitching software:  https://github.com/facebook/Surround360   . [6] Computer Vision Algorithm BRISK:  https://www.robots.ox.ac.uk/~vgg/rg/papers/brisk.pdf  . [7] Computer Vision Algorithm AKAZE:  http://docs.opencv.org/3.0-beta/doc/tutorials/features2d/akaze_matching/akaze_matching.html  . [8] Optical Flow:  http://docs.opencv.org/trunk/d7/d8b/tutorial_py_lucas_kanade.html  . [9] 4K connection speeds:  https://help.netflix.com/en/node/306   . [10] Average connection speeds in US:  https://www.akamai.com/us/en/about/news/press/2016-press/akamai-releases-fourth-quarter-2015-state-of-the-internet-report.jsp   . [11] CubeMap transform filter for ffmpeg:  https://github.com/facebook/transform   . [12] FFMPEG software:  https://ffmpeg.org/   . [13] WebVR Spec:  https://w3c.github.io/webvr/  . [14] Google Daydream SDK:  https://vr.google.com/cardboard/developers/   . [15] Lytro LightField Volume for six DoF:  https://www.lytro.com/press/releases/lytro-immerge-the-worlds-first-professional-light-field-solution-for-cinematic-vr   . [16] 10 bit color depth:  https://gist.github.com/l4n9th4n9/4459997   ", "date": "2016-10-17"}, {"website": "Yahoo", "title": "Yahoo Champaign - Scoble-ized", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/138159851891/yahoo-champaign-scoble-ized", "abstract": " Tech evangelist, Robert Scoble, got a rare glimpse into Yahoo’s research center in Champaign, IL where he spoke with engineers about about the innovations in big data that the teams were working on.  . Cathy Singer, senior director of engineering, served as Robert’s gracious tour guide. . Tour of Yahoo research in Illinois. Internet-based computer science. Cathy Singer senior director of engineering showing me around. ", "date": "2016-01-27"}, {"website": "Yahoo", "title": "Hadoop Turns 10", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/138742476996/hadoop-turns-10", "abstract": "  yahoohadoop : .    It is hard to believe that 10 years have already passed since Hadoop was started at Yahoo. We initially applied it to web search, but since then, Hadoop has become central to everything we do at the company. Today, Hadoop is the de facto platform for processing and storing big data for thousands of companies around the world, including most of the Fortune 500. It has also given birth to a thriving industry around it, comprised of a number of companies who have built their businesses on the platform and continue to invest and innovate to expand its capabilities.    . At Yahoo, Hadoop remains a cornerstone technology on which virtually every part of our business relies on to power our world-class products, and deliver user experiences that delight more than a billion users worldwide. Whether it is content personalization for increasing engagement, ad targeting and optimization for serving the right ad to the right consumer, new revenue streams from native ads and mobile search monetization, data processing pipelines, mail anti-spam or search assist and analytics – Hadoop touches them all. . When it comes to scale, Yahoo still boasts one of the largest Hadoop deployments in the world. From a footprint standpoint, we maintain over 35,000 Hadoop servers as a central hosted platform running across 16 clusters with a combined 600 petabytes in storage capacity (HDFS), allowing us to execute 34 million monthly compute jobs on the platform. . But we aren’t stopping there, and actively collaborate with the Hadoop community to further push the scalability boundaries and advance technological innovation. We have used MapReduce historically to power batch-oriented processing, but continue to invest in and adopt low latency data processing stacks on top of Hadoop, such as Storm for stream processing, and Tez and Spark for faster batch processing. . What’s more, the applications of these innovations have spanned the gamut – from cool and fun features, like  Flickr’s Magic View  to one of our most exciting recent  projects  that involves combining Apache Spark and  Caffe . The project allows us to leverage GPUs to power deep learning on Hadoop clusters. This custom deployment bridges the gap between HPC (High Performance Computing) and big data, and is helping position Yahoo as a frontrunner in the next generation of computing and machine learning. . We’re delighted by the impact the platform has made to the big data movement, and can’t wait to see what the next 10 years has in store. . Cheers! . (via  yahoohadoop ) ", "date": "2016-02-05"}, {"website": "Yahoo", "title": "Introducing 1080p Video Experience on Yahoo", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/139421974901/introducing-1080p-video-experience-on-yahoo", "abstract": "  By P.P.S. Narayan, VP of Engineering  . Today, we are excited to announce the debut of a Full HD experience on Yahoo’s video programming.  Going forward, we will be delivering, both live and video-on-demand content, at 1080p – a high definition standard, characterised by a resolution of 1920 pixels wide by 1080 pixels in height, with progressive scan – to our users, based on devices, platforms and network capabilities. In addition to this, users will also be able to enjoy the content at up to 60 frames-per-second (fps). . Back in October 2015,  we delivered  the first, global live stream of a regular season NFL game  at a maximum of 720p/60fps. And this past week, we streamed the  AT&amp;T Pebble Beach Pro-Am  from PGA TOUR LIVE on Yahoo, at formats up to 1080p/60fps, with TV-like quality. Fans were able to enjoy panoramic views of Pebble Beach, while watching players hit shots off the tight fairways, follow balls as they sailed across the horizon, and see the professional and celebrity golfers nail putts – all thanks to this high resolution and smooth motion video.    . We’re also  excited to announce  that on April 30, you’ll be able to experience this enhanced video quality as Yahoo Finance hosts the first-ever, global livestream of the Berkshire Hathaway annual shareholders meeting, from Omaha, NE. The “Woodstock of Capitalism” will be broadcast across all devices - with exclusive video-on-demand (VOD) of the event available on Yahoo Finance for 30 days following the meeting. . We have also upgraded our studios to support 1080p/60fps. Starting with Yahoo’s cameras, the whole video pipeline, including video signal acquisition, video mixing, encoding, transcoding, and all the packetization, will be 1080p capable, when we produce the content. . These days, while most of our television displays support Full HD, most television HDTV broadcasts are delivered at 1080i/60  fields-per-second , a lower resolution than 1080p/60  frames-per-second . And, over-the-top (OTT) streaming experiences do not widely support a top quality resolution with purity in signal from start to finish. We are thrilled at Yahoo to be able to deliver this top-notch quality. . We recently delivered  our Yahoo Sports’ analysts preview of Super Bowl 50  from our new HD-supported studio. The lineup included: Shaun King, former NFL Quarterback and current Yahoo Sports’ NFL and College Football analyst, Frank Schwab, Editor of Yahoo Sports’ Shutdown Corner blog, and Tank Williams, former NFL safety and Yahoo Sports NFL/Fantasy contributor. . This is an exciting milestone for our team and for Yahoo. We look forward to delivering the best possible viewing experience to users as we continue to iterate on and improve the quality of our stream. Sit back, relax and enjoy the next round.  ", "date": "2016-02-16"}, {"website": "Yahoo", "title": "Yahoo Hosts The Streaming Video Alliance’s Quarterly Member Meeting", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/139612658811/yahoo-hosts-the-streaming-video-alliances", "abstract": "    Yesterday, Yahoo hosted the Streaming Video Alliance’s quarterly member meeting where over 70 executives from across the streaming video landscape convened to advance discussions on a broad range of streaming video topics and reach agreements on best practices, policy and proposed standards. Ron Jacoby, VP of Engineering, Yahoo Video &amp; TV Applications, and P.P.S. Narayan, VP of Engineering, Yahoo Video were the morning’s featured keynotes.    .         . Ron kicked off discussing the challenges and complexities behind building a strong streaming video experience. The root of which stemmed from the rapidly changing consumption patterns of today’s audiences. . Millennials, which now represent over 30% of the US population, consume 283% more media via the internet than non-millennial age groups – a vast change from how their parents watched TV. This reflects a dramatic shift in how TV is being consumed, and is accelerating in key demographics. . Additionally, the 18-24 year old demographic saw a 37% decline in traditional TV viewing. Ron attributed the shift to the pervasiveness of online video content to premium video services and social media across portable media devices, including laptops, tablets, smartphones, etc. . “In order for the industry to succeed in the face of these trends, it needs to look at content and delivery differently,” said Jacoby. “investments in live streaming and innovation in video protocols and delivery is necessary.” . P.P.S. Narayan followed up with a presentation about the technical opportunities and challenges in video streaming. To echo Ron’s statements, PPSN said “folks are moving away from TV..and watching video across different social and OTT platforms. Gone are the days of sitting in the same room with everyone watching the same show.” . He added, “the shift in consumer behavior and consumption patterns is leading to the disaggregation of content – providers are taking content from TV and cable, and making it accessible on multiple platforms, such as phones, tablets, and connected devices. Services, like Hulu, HBOGo, and MLB.TV have invested heavily in this which is a clear indication that they, and the rest of the industry, are serious about embracing this consumer shift.” . This move is indicative of bigger technology shifts, which begs the question “Can the quality of the video be as good as what we see on TV?” Almost. PPSN explained that when Yahoo hosted the first ever NFL live stream, the technological considerations he and his team had to account for, included resolution, bandwidth, encoding, ads, and latency. . He also talked about the next-generation immersive experiences, including time-based immersion, made possible by cloud DVRs and live scrubbing; space-based immersion with VR and 360 degree videos; and people-based immersion, evidenced by the sharing of content on social media. Additionally, he covered how the disaggregation of content without having a “TV Guide” is leading to gaps in content discovery and personalization. The Yahoo Video Guide, is one such example of addressing the growing needs for users to discover and consume relevant and contextual content. . PPSN concluded by expressing the importance of the groups like the SVA, as they are critical to working together as an industry and help move the ball forward in streaming video. ", "date": "2016-02-19"}, {"website": "Yahoo", "title": "Yahoo Account Key Now Available With New Features On Even More Apps", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/140219851556/yahoo-account-key-now-available-with-new-features", "abstract": "  yahoo : .  By Lovlesh Chhabra, Product Manager  . Passwords suck and we’re on a mission to kill them. That’s why we  introduced  Yahoo Account Key in October 2015, which is now available on over 50M devices. This product lets you access your Yahoo account with the simple tap on an Account Key push notification sent to your mobile device. It is a major step towards a password-free future, and one where we can say “Goodbye complicated passwords!” . Since then, we’ve made improvements to the product and wanted to share the steps we’ve taken to make signing-in easier: .   The password is antiquated and needs to be put to rest. We’ve been investing in better and more simplified ways to sign in and can’t wait to show you what we have in store. Stay tuned for updates in our mission to kill passwords!      ", "date": "2016-02-29"}, {"website": "Yahoo", "title": "CaffeOnSpark Open Sourced for Distributed Deep Learning on Big Data Clusters", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/139916828451/caffeonspark-open-sourced-for-distributed-deep", "abstract": "  yahoohadoop : .    By Andy Feng(@afeng76), Jun Shi and Mridul Jain (@mridul_jain), Yahoo Big ML Team   .  Introduction   . Deep learning (DL) is a critical capability required by Yahoo product teams (ex.  Flickr , Image Search) to gain intelligence from massive amounts of online data. Many existing DL frameworks require a separated cluster for deep learning, and multiple programs have to be created for a typical machine learning pipeline (see Figure 1). The separated clusters require large datasets to be transferred among them, and introduce unwanted system complexity and latency for end-to-end learning. .    .  Figure 1:  ML Pipeline with multiple programs on separated clusters  .   . As discussed in our earlier  Tumblr post , we believe that deep learning should be conducted in the same cluster along with existing data processing pipelines to support feature engineering and traditional (non-deep) machine learning. We created CaffeOnSpark to allow deep learning training and testing to be embedded into Spark applications (see Figure 2).  .    .  Figure 2:  ML Pipeline with single program on one cluster .   .  CaffeOnSpark: API &amp; Configuration and CLI   .    . CaffeOnSpark is designed to be a Spark deep learning package.  Spark MLlib  supported a variety of non-deep learning algorithms for classification, regression, clustering, recommendation, and so on. Deep learning is a key capacity that Spark MLlib lacks currently, and CaffeOnSpark is designed to fill that gap.  CaffeOnSpark API  supports  dataframes  so that you can easily interface with a training dataset that was prepared using a Spark application, and extract the predictions from the model or features from intermediate layers for results and data analysis using MLLib or SQL. .  Figure 3:  CaffeOnSpark as a Spark Deep Learning package  .   . 1:   def main(args: Array[String]): Unit = { .  2:   val ctx = new SparkContext(new SparkConf()) .  3:   val cos = new  CaffeOnSpark (ctx) .  4:   val conf = new  Config (ctx, args).init() .   5:   val dl_train_source =  DataSource.getSource (conf, true) .   6:   cos. train (dl_train_source) .   7:   val lr_raw_source =  DataSource.getSource (conf, false) .   8:   val extracted_df = cos. features (lr_raw_source) .   9:   val lr_input_df = extracted_df.withColumn(“Label”, cos.floatarray2doubleUDF(extracted_df(conf.label))) . 10:     .withColumn(“Feature”, cos.floatarray2doublevectorUDF(extracted_df(conf.features(0)))) . 11:  val lr = new LogisticRegression().setLabelCol(“Label”).setFeaturesCol(“Feature”) . 12:  val lr_model = lr.fit(lr_input_df) .  13:  lr_model.write.overwrite().save(conf.outputPath) .  14: } .    .  Figure 4:   Scala application  using CaffeOnSpark both MLlib  .   . Scala program in Figure 4 illustrates how CaffeOnSpark and MLlib work together: .    . As illustrated in Figure 4, CaffeOnSpark enables deep learning steps to be seamlessly embedded in Spark applications. It eliminates unwanted data movement in traditional solutions (as illustrated in Figure 1), and enables deep learning to be conducted on big-data clusters directly. Direct access to big-data and massive computation power are critical for DL to find meaningful insights in a timely manner. . CaffeOnSpark uses the configuration files for solvers and neural network as in standard Caffe uses. As illustrated in our  example , the neural network will have a MemoryData layer with 2 extra parameters: . The initial CaffeOnSpark release has several built-in data source classes (including  com.yahoo.ml.caffe.LMDB  for  LMDB  databases and  com.yahoo.ml.caffe.SeqImageDataSource  for  Hadoop sequence files ). Users could easily introduce customized data source classes to interact with the existing data formats.  .    . CaffeOnSpark applications will be launched by standard Spark commands, such as spark-submit. Here are 2 examples of spark-submit commands. The first command uses CaffeOnSpark to train a DNN model saved onto HDFS. The second command is a custom Spark application that embedded CaffeOnSpark along with MLlib. . First command:  .  spark-submit \\    –files caffenet_train_solver.prototxt,caffenet_train_net.prototxt \\     –num-executors 2  \\     –class com.yahoo.ml.caffe.CaffeOnSpark  \\       caffe-grid-0.1-SNAPSHOT-jar-with-dependencies.jar \\       -train -persistent \\       -conf caffenet_train_solver.prototxt \\       -model hdfs:///sample_images.model \\       -devices 2    . Second command:  .    .  spark-submit \\     –files caffenet_train_solver.prototxt,caffenet_train_net.prototxt \\     –num-executors 2  \\     –class com.yahoo.ml.caffe.examples.MyMLPipeline \\                                            caffe-grid-0.1-SNAPSHOT-jar-with-dependencies.jar \\   .          -features fc8 \\         -label label \\         -conf caffenet_train_solver.prototxt \\         -model hdfs:///sample_images.model  \\         -output hdfs:///image_classifier_model \\         -devices 2  .     .  System Architecture   .  Figure 5:  System Architecture  .   . Figure 5 describes the system architecture of CaffeOnSpark. We launch Caffe engines on GPU devices or CPU devices within the Spark executor, via invoking a JNI layer with fine-grain memory management. Unlike traditional Spark applications, CaffeOnSpark executors communicate to each other via MPI allreduce style interface via TCP/Ethernet or RDMA/Infiniband. This Spark+MPI architecture enables CaffeOnSpark to achieve similar performance as dedicated deep learning clusters. . Many deep learning jobs are long running, and it is important to handle potential system failures. CaffeOnSpark enables training state being snapshotted periodically, and thus we could resume from previous state after a failure of a CaffeOnSpark job.  .   Open Source   . In the last several quarters, Yahoo has applied CaffeOnSpark on several projects, and we have received much positive feedback from our internal users. Flickr teams, for example, made significant improvements on image recognition accuracy with CaffeOnSpark by training with millions of photos from the Yahoo Webscope  Flickr Creative Commons 100M dataset  on Hadoop clusters.  . CaffeOnSpark is beneficial to deep learning community and the Spark community. In order to advance the fields of deep learning and artificial intelligence, Yahoo is happy to release CaffeOnSpark at  github.com/yahoo/CaffeOnSpark  under Apache 2.0 license.  . CaffeOnSpark can be tested on an   AWS EC2  cloud or on  your own Spark clusters . Please find the detailed instructions at Yahoo  github  repository, and share your feedback at  bigdata@yahoo-inc.com . Our goal is to make CaffeOnSpark widely available to deep learning scientists and researchers, and we welcome contributions from the community to make that happen. .  . (via  yahoohadoop ) ", "date": "2016-02-24"}, {"website": "Yahoo", "title": "“Turing Test” for OTT Video Streaming: \nCan a viewer distinguish between Streaming and Broadcast Video in 2016?", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/140395457866/turing-test-for-ott-video-streaming-can-a", "abstract": "  P.P.S. Narayan, VP of Engineering   . About sixty-five years ago, Alan Turing, considered to be the father of Computer Science and modern computing machines, put forth a deep and philosophical question: can machines think? Asked differently, “Can a machine exhibit (intelligent) behavior indistinguishable from a human [1]?” In order to confirm the conclusion, he devised a test, coined the imitation game.  While there have been various interpretations (and extensive debate) of the test, our objective is to look at its application to video streaming by understanding the premise and appreciating the concept. .  . Turing says: . I believe that in about fifty years’ time it will be possible to program computers, with a storage capacity of about 109, to make them play the imitation game so well that an average interrogator will not have more than 70 percent chance of making the right identification after five minutes of questioning. … I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted. . Over the years, the Turing test has become an important concept in artificial intelligence and the evolution of computing in modern times. In fact, some modifications to the Turing test have been proposed and have been adopted widely. CAPTCHA is a form of reverse Turing test, where we have participant C replaced by a machine, and the task of C, as a gatekeeper, is to distinguish the humans (in A or B or …) from machines [1]. . Television and video broadcasting has been around for decades. Starting with the first television broadcasts in the 1930s to modern broadcasts in 3D and 4K UHD, the evolution has been phenomenal and eye-popping. Over the years, TV broadcast has moved from terrestrial over-the-air radio waves to satellite or fiber delivery to the home. Analog signals have modernized to digital; black-and-white to color. Televisions have evolved from mechanical to electronic to digital, and traditional CRT displays have changed to ultra thin LED displays. . In the mid-1990s, with the explosion of the consumer Internet, a new wave of video consumption started on devices other than the traditional television sets. Video streaming was soon available on desktops and then laptops, and with the WiFi and mobile revolution, we began streaming video on our phones and tablets. . Nearly a century of evolution of standards, technology, infrastructure, and innovation helped the US TV industry grow to more than a $100B [6]. We are in a new generation where internet streaming is a revolution and growing rapidly. “Over-the-Top” video streaming, or OTT as it is commonly referred to, is delivered through the open unmanaged internet, with the “last-mile” companies (e.g., Comcast) acting only as the internet service provider [3]. Netflix, an OTT video streaming service, already accounts for more than 35% of peak US internet traffic. In fact, all of OTT video streaming accounts for more than 50% of both internet and mobile traffic in the US [4]. . This new form of video consumption is pushing the infrastructure and technology built for the Internet beyond its original design parameters and capabilities. While content and user interfaces have advanced quite drastically with OTT, the overall quality of experience is still years behind. . Several months ago, I attended the Streaming Media West conference. And, during one of the panels, the question was asked about the “success metrics” for OTT streaming. Rather than invent what we will call as “success”, I believe we need to use existing TV quality as a baseline benchmark.  . Say, we modify the original Turing test, to replace A and B with simple LED TV-like monitors. The video inputs for A and B will have the similar videos (e.g. say any NFL game) from either over-the-air HD signals or from an OTT stream. The participant C, or the interrogator, has to interact with the two monitors to determine which of A or B is displaying video that is OTT streamed. We coin this as the “OTT Turing test”. .  .  . And, to make it even more challenging, C could be either a human or a machine. Therefore, any variance in human perception will be eliminated via more systematic and objective evaluation of the inputs.   . If we did this test today, would OTT pass the Turing test? My answer is no. The reason is because there are a number of challenges that have yet to be completely solved. Let’s examine some of these challenges, why they exist, and how far away the technology is for us to achieve this goal. .  Startup Time  . When we want to start watching television today, we switch “on” the TV set and/or our connected device (e.g. cable box). Typically, these startup in at most 2-3 seconds, and you can see the video on your screen in that time. At times, the audio even starts up sooner. In traditional television, the latency for startup can be broken down into two parts: the time taken for the first few frames of the video to reach the device which is determined by the speed of light, and then to the additional time taken for decoding the signal and constructing the frames to be rendered on the display. This holds true for the over-the-air broadcasts as well as for cable or satellite transmissions. .  .  .  Video Quality  . These days, most displays support HD and resolution of 720p or 1080i/p – table stakes in video broadcasting. In fact, most television HDTV broadcasts are shot at 1080i/60 fields-per-sec. Modern television sets have inbuilt sophisticated upscaling algorithms and technologies that can take lower resolution incoming signals and upscale them to 1080p or even 4K. However, these algorithms vary from device to device, manufacturer to manufacturer, and upscaling can have artifacts introduced which can appear on certain types of content (e.g., high motion sports like NFL or video games). .  .  .  The Spinner  . Anyone who has watched video on the internet knows (or should know) what the “Spinner” is. It is the non-technical term given to the manifestation of not being able to deliver content reliably, over a network which is not dedicated. .   .    .  . Buffering and re-buffering (after the video has started) have become a pervasive pain point for viewers of OTT streaming. And there are many different causes of rebuffering. The reasons vary from end viewer device capabilities (and software), to network congestion inside the viewer’s home/premise, to ISP or local area bandwidth issues, to the core getting clogged and so on.   . Rebuffering ratio is a typical industry measure to determine the perceived user quality. It is a ratio, measured for a time window, as the total time spent rebuffering across all users to the total time spent viewing the stream. Rebuffering ratios of 2-4% are considered acceptable across the industry today. For example, if you are watching a video online for 2 minutes, it is normal for you (and all users) to see the spinner for 2-3 seconds. This typically gets worse during live streams, so if you watch an NFL game for more than 2 hours, you are more than likely to experience many interruptions in your viewing totaling up to 2-3 minutes!! Imagine that experience as compared to what you get on TV or cable today. . At Yahoo, our goal for re-buffering is ZERO. Plus, we’ve focused on some new metrics. For example, the percentage of rebuffering impacted views is more important and relevant. Having 100% of your viewers impacted for 1 minute is far worse than 1% of your viewers impacted for 100 minutes. Basically, this would mean that 99% of your viewers had a FLAWLESS experience. That number needs to grow to five 9s like traditional television. .  Visual Artifacts   . Video encoding is a complex process from signal acquisition to mixing to adding audio tracks and then shipping the packets to the device. The receiving device capabilities can vary quite drastically. And, this is handled when encoding video digitally for transmission, by picking a few different resolutions. This allows for rendering the video well on various form factors (e.g., a 4K display vs a VGA device), and also allows for optimizing the number of bits transmitted depending on the viewing device. The optimization comes from the fact that lower resolutions would require lower number of bits to encode. . Usual OTT streams are encoded in a few different resolutions or “bitrates”. The availability of various bitrates has become a mechanism for handling poor network conditions. Video players on devices “adapt” and deliver a continuous experience (i.e. avoid rebuffering). The tradeoff has become straightforward – reduce resolution rather than stop the video. .    .  .  .  Ad Transition  . Advertising on television is native and seamless. What do I mean by native? A viewer sees very little or no difference between content and advertising. In other words, when there is an imperceptible break of under 500ms between the content and ads and from the viewer perspective, it is considered “seamless”.  Over time, the television ad ecosystem has evolved and local stations have the ability to insert local video ads into global or national content. .  .  .  Synchronization  . Consider watching a live (American) football, cricket or soccer game on television. Maybe with a bunch of friends or strangers in a bar, with multiple big screen televisions. We quite frequently do this, to enjoy a social viewing experience. With the current television technology, typically all the television sets in the bar are within a few milliseconds of each other – not much lag or lack of synchronization.    . However, with OTT, multiple viewers of the same “live” event cannot be guaranteed a synchronized viewing experience. Even if they all started streaming at the same exact moment! This common problem is due to the inherent nature of various streaming protocols and buffer management. In OTT streaming, especially for live events, it is likely that a viewer may watch or experience a goal at a significantly different moment in time as compared to his neighbors in the same apartment complex, which creates a bad user experience. . In summary, as an industry, we have a lot of challenges to deliver a TV-like experience for OTT streaming. Some of them are easy, while the others are quite difficult to overcome.    . It is likely that very soon, startup time, video quality and ad transitions will improve significantly and be indistinguishable from the current television experience. The tougher technical challenges will be to get to ZERO rebuffering and to enable synchronized watching experience for OTT. These are fundamental challenges in the today’s technology, that will require significant innovation and even some revolutionary new concepts such as P2P or new protocols to up the game for OTT. . However, I am very confident that we will overcome these challenges, and win the OTT Turing Test – much before the original Turing Test is solved!! . [1]  https://en.wikipedia.org/wiki/Turing_test   . [2]  http://plato.stanford.edu/entries/turing-test/#Tur195ImiGam   . [3]  https://en.wikipedia.org/wiki/Internet_television   . [4]  http://www.statista.com/chart/1620/top-10-traffic-hogs/   . [5]  https://en.wikipedia.org/wiki/Display_resolution#/media/File:Vector_Video_Standards8.svg   . [6]  http://www.statista.com/topics/977/television/   . [7]  http://www.bestshareware.net/howto/img1/how-to-remove-pixellation-from-video-1.jpg   . [8]  http://blogs.tigarus.com/patcoola/wp-content/uploads/sites/patcoola/2014/buffering-2014.png   ", "date": "2016-03-03"}, {"website": "Yahoo", "title": "Using NSURLProtocol for Testing", "author": [" staticpulse"], "link": "https://yahooeng.tumblr.com/post/141143817861/unit-testing-networking-code-can-be-problematic", "abstract": "  By Roberto Osorio-Goenaga, iOS Developer  . Unit testing networking code can be problematic, due mostly to its asynchronous nature. Using a staging server introduces lag and external factors that can cause your tests to run slowly, or not at all. Frameworks like  OCMock  exist to specify how an object responds to a specific query to address this behavior, but a mock object must still be set up for each type of behavior being mocked. .       . Using Apple’s  NSURLProtocol , we can create a test suite that eschews these problems by mocking the response to our network calls centrally, essentially letting your test focus only on business logic. This protocol can be used not only with the built-in  NSURLSession  class, but can also be used to test classes and structs written with modern third party networking libraries, such as the popular  Alamofire . In this article, we will look at mocking network responses in Swift for requests made using Alamofire. The sample project can be found on  github . . NSURLProtocol’s main purpose is to extend URL loading to incorporate custom schemes or enhance existing ones. A secondary, yet extremely powerful, use of NSURLProtocol is to mock a server by sending canned responses back to callbacks and delegates. Say we have a very simple struct that uses Alamofire to make an  HTTP GET  request. .  Fig 1 - A simple struct that serves as a REST client  . The sample in Figure 1 creates a struct with an  NSURL  as an init parameter, and a sole method,  getAvailableItems() , taking in a completion block as an argument, making a rest call to the NSURL and populating an array of  MyItem  in the block sent into it. From a testing perspective, we’d like to have a JSON response that matches the expected response, containing an object called  items  whose value pair is an array of strings. In order to make our tests as thorough and robust as possible, we’d also include at least two other mock responses: a JSON response that does not match this expectation, to test the else clause, and a garbage or erroneous response to check our error handling. .    Fig 2 - A valid response  .    Fig 3 - A non-valid response  .    Fig 4 - A throw-away garbage response    . Figures 2, 3 and 4 show a valid response for our purposes, a non-valid yet correct JSON response, and a throw-away string that isn’t even valid JSON, respectively. Without having to make a full-blown staging server, let’s see how we could go about testing these using NSURLProtocol. . To understand where NSURLProtocol fits into this problem, it’s important to look at a bit of the architecture Alamofire employs. Alamofire works as a  singleton , as one can see from the above example. There is no instantiation required. Just feed a URL in, and make a request. Under the hood, the entity making the request is called the  Manager . Manager is the entity that actually stores the URL and parameters, and is responsible for firing off an NSURLSession request abstracted from the caller class. . The manager for Alamofire can be initialized with a custom configuration of type  NSURLSessionConfiguration , which has a property called protocolClasses, an array of NSURLProtocol members. By creating a new protocol that defines what happens when NSURLSession tries to reach a certain type of endpoint, loading it into the protocol array of a new configuration at index 0 (the default configuration), and initializing a new Manager object with this configuration, we can inject Alamofire with a simple, local mock server that will return whatever we want, given any request. Let’s start setting up a test class for our REST client by extending NSURLProtocol to respond to GET requests, and creating an Alamofire.Manager object with a custom NSURLSessionConfiguration that employs our protocol. .  Fig 5 - Setting up a testing class for our client  . Great, now we have an NSURLProtocol class that takes a GET request, checks the URL, and returns either a valid JSON response, or a simple “GARBAGE” response. This should allow us to test how our client responds. We still haven’t written any cases. We have a MyRESTClient property, as well as a Manager property. We also have a setup initial method that instantiates and loads our custom protocol into the manager instance. We now need a way to inject this manager instance into our Alamofire singleton. Let’s extend our client to the following. .  Fig 6 - The REST client with an injectable “manager” parameter  . We’ve added an initializer to our struct that allows us to send either a custom manager or nil into Alamofire. When the parameter is nil, the manager will load with its standard configuration. We also edited the request execution to be called via the manager we selected instead of directly through Alamofire. We can now add the following test case to our test class. .  Fig 7 - Our first test case  . In this test case, we create a new client, and give it our custom manager through the new initializer. We set a  testing expectation , since the result comes back on a closure, and, after loading our itemsArray inside, fulfill the expectation. We tell the test case to wait for said expectation to be fulfilled, and, once it is, we make sure the itemsArray contains three items. If so, our test is successful, and our business logic is tested for getAvailableItems. Notice that we have used a bogus URL of “ http://notNil ”, which we have defined in the protocol to be selected in the conditional for populating the response correctly. To test the “garbage” case, we could write a test like the following. . In the third and final test case, our protocol class will return a “concepts” array instead of an “items” array, so the end result should still be a nil array in the closure. . As you can see, using NSURLProtocol we have created what amounts to a tiny server that responds to our requests and replies as specified, perfect for testing our asynchronous net calls. Now, go forth and test! ", "date": "2016-03-16"}, {"website": "Yahoo", "title": "Elements of API design: Delivering a flawless NFL experience", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/141211499516/elements-of-api-design-delivering-a-flawless-nfl", "abstract": " Sid Reddy, Director of Engineering, VDEO  . On October 25, 2015, Yahoo  live streamed the first-ever regular season NFL game over the Internet   t o   a   g l o b a l   a u d i e n c e. This was a phenomenal effort, as several teams at Yahoo came together to deliver a flawless experience to viewers across devices and geographies. To give you a sense of the scale involved, the NFL live stream on Yahoo attracted over 15 million unique viewers, over 33 million views, and had a peak of over 3 million concurrent viewers. 33% of these views came from international viewers from 185 countries, who watched over 460 million minutes of the game.  . Our innovative API systems were one of the key drivers for the broadcast’s success. During the 4 hours of the NFL game, the API platform fielded over 215 million calls, with a median latency of 11ms, and a 95th percentile latency of 16ms. The APIs showed six 9s of availability during this time period, despite failure of dependencies during spikes in the game. . In this post, we will discuss the importance of APIs to the NFL video experience, examine the Key Performance Indicators (KPIs) to pay attention to, and highlight several essential elements that comprise a robust API platform. .  Why are the APIs important?  . The APIs are critical to powering the player and the video experiences. As an example, the APIs provide the player with several pieces of information about the video, including the title, description, as well as URLs to fetch “streams” of video (each stream represents a particular combination of resolution, bitrate, and encoding profile); the APIs also filter and provide only those streams to the player that are supported by the given device. .  The Lifecycle of an API request  . The lifecycle starts with a client doing a DNS lookup for the API, then issuing an HTTPS request that is terminated at the Edge; the Edge then forwards this request to the API origin which calls several dependency services, processes the data received, and builds the final response (see the figure below). . In the remainder of this post, we will discuss our KPIs, the Yahoo ecosystem that we leveraged, and several lessons from having examined our API stack end-to-end. The sections below are organized around components as they are encountered in the lifecycle of an API request.  .  Know your KPIs   . Always start by defining your KPIs and measure your performance against them. This will help ensure that changes you make to the system lead to improved performance. For our system, we had 4 KPIs .  Yahoo ecosystem  . Yahoo has a long history of operating services at the highest scale, and as such, several infrastructure components are readily available for use. For example, to avoid a single point of failure, API systems at Yahoo are present by default in several data centers around the world (serving the Americas, Europe, and APAC regions). Paradigms exist to replicate data across these data centers, while ensuring consistency. Also, code is deployed to these data centers with a CI/CD (Continuous Integration/Continuous Delivery) pipeline. Given this baseline Yahoo ecosystem, we will identify some of the most interesting enhancements below. .  DNS   . This is the first stop for any client talking to APIs. Should a data center become unavailable, clients should get routed appropriately for high availability. Yahoo operates such a DNS-based GSLB (Global Server Load Balancing) system. There are typically two components to such a system: periodic health checks for the different data centers, and setting the TTL (Time To Live) for DNS records. We decreased these times by 5x to improve availability, with the tradeoff being an increase in health check traffic to our origin and DNS servers. .  Use the Edge   . While our APIs are already present in several data centers, adding an edge layer (aka ADN: Application Delivery Network) helps in lowering latencies significantly, by up to 25% in our case, because the HTTPS termination happens closer to the user. SSL costs 2 additional round trips (1 additional round trip if session is resumed), and optimizing this latency helps significantly. .  Abuse protection   . We have a system in place to detect abuse, and are able to dynamically accept/reject/degrade the request. This system helps protect the APIs against potential DDoS attacks, preventing a meltdown of either the APIs or the dependencies. Our systems detect abuse from IP addresses as well as “users”, and enables whitelisting/blacklisting specific entities. This system runs on the edge layer, and relays only “good” requests to the API system. .  Caching     . We have a caching layer on each of the API nodes. This technique not only improves latency (by up to 20%), but enhances availability when dependency services go down. In addition, the load on dependency systems reduces significantly (by up to 70% in our case). For maximum flexibility, we place the cache close to the dependency calls, and use Guava in our implementation. .  Dependency protection     . Occasionally, we notice patterns where a dependency service would degrade, causing threads in the API system to stall. This dramatically reduces throughput, and can bring the system down, due to thread exhaustion. To avoid this, we implemented aggressive timeouts on dependencies, and an automated mechanism to eliminate calls to dependency services when they are down. This technique improves scalability significantly as threads can now proceed instead of waiting for a timeout.  .  Speculative retry  . In our analysis, some dependencies tend to have a disproportionately large p99 (aka 99th-percentile) latency, compared to say the p95 latency. More generally, we see patterns where there is a steep jump in latency. At the point where the latency jump occurs, we introduced a parallel retry, and consume the first response received. The tradeoff is an increase in traffic to our dependency systems, in exchange for lower latency and error rate. In our case, this approach reduced latency by up to 22%, and error rate by up to 81%. .  Throughput benchmarking  . We  invested a significant amount of time in optimizing our API performance. We have high scalability targets, and squeezing performance from hardware is critical to keeping costs under control. We improved our performance 2.3X, with a further 3X improvement planned for the future (by optimizing our core Java platform). An important lesson from this exercise is to benchmark a dummy API as well as the APIs of interest, and measure where the throughput drops are happening; then systematically target the causes for the biggest drops. Another lesson is to pre-materialize the data in an off-stage process, and keep the serving APIs simple; this technique, along with an in-memory data store, improves throughput significantly. .  Monitoring/Alerting  . When you measure, you can act. We developed extensive monitoring capabilities for the entire API system, including dependencies, on a variety of metrics to track the KPIs. We also have proactive alerting, whenever the SLAs are not met. This proved critical during the NFL live stream, as we detected some dependency services that were briefly unavailable, and were able to repair the services involved. Given the robust design and fallbacks, such intermittent issues did not significantly degrade the end-user experience. .  Summary   . Building a robust and highly scalable API platform was key to delivering a flawless experience for the first NFL game that was live streamed over the Internet. In this post, we examined several techniques and components that enabled such a robust API platform. Here’s a table summarizing the techniques, and the KPIs they promote. .   Future work   . We have several interesting projects on our roadmap to make our APIs more resilient, including incorporating a destructive testing framework into our API platform to ensure continued robustness, as well as an automated framework to take bad servers/data centers out of rotation. . If you are excited about this sort of work, reach out to sidreddy at yahoo-inc dot com. ", "date": "2016-03-17"}, {"website": "Yahoo", "title": "Configuration management for distributed systems (using GitHub and cfg4j)", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/141920508211/configuration-management-for-distributed-systems", "abstract": " When working with large-scale software systems, configuration management becomes crucial - supporting non-uniform environments gets  greatly simplified, if you decouple code from configuration. While building complex software/products such as  Flickr , we had to come up with a simple yet  powerful way to  manage configuration. Popular approaches to solving this problem include using configuration files or having a dedicated configuration  service. Our new solution combines both the extremely popular  GitHub  and  cfg4j   library, giving you a very flexible approach that will work with applications of any size.  . Let’s see what configuration-specific components we’ll be working with today:  . All these players work as a team to provide an end-to-end configuration management solution.  . The first thing you might expect from the configuration repository and editor is ease of use. Let’s enumerate what that means: . So what options are out there that may satisfy those requirements? The three very popular formats for storing configuration are  YAML , Java Property files, and XML files. We use YAML because it is widely supported by multiple programming languages and  IDEs and it’s very readable and easy to understand, even for the non-engineer.   . We could use a dedicated configuration store; however, the great thing about files is that they can be easily versioned by  version control tools like Git, which we decided to use as it’s widely known and proven.  . Git provides us with a history of changes and an easy way to branch off configuration. It also has great support in the form of GitHub which we  use both as an editor (built-in support for YAML files) and collaboration tool (pull requests, forks, review tool). Both are nicely glued  together by following the  Git flow branching model.  Here’s an example of  configuration file that we use: . One of the goals was  to make managing multiple configuration sets (execution environments) a breeze. We needed the ability to add and remove environments quickly. If  you look at the screenshot below, you’ll notice a “prod-us-east” directory in the path. For every environment, we stored a separate directory  with config files in Git. All of them have the exact same structure and only differ in YAML file contents.  . This solution makes working with environments simple and comes in very handy during local development or new production fleet rollout (see  use cases at the end of this article). Here’s a sample config repo for a project that has only one “feature”: . Some of the  products that we work with at Yahoo have a very granular architecture - hundreds of micro-services working together. For scenarios like  this, it’s convenient to store configurations for all services in a single repository, which greatly reduces the overhead of maintaining multiple  repositories. We support this use case by having multiple top-level directories each holding configurations for one service only.  . The main role  of push cache is to decrease load put on the GitHub server and improve configuration fetch time. Since speed is the only  concern here, we decided to keep the push cache simple - it’s just a key-value store.  Consul  was our  choice: the nice thing is that it’s fully distributed.   . You can install Consul clients on the edge nodes and they will  keep being synchronized across the fleet. This greatly improves both reliability and performance of the system. If performance is not a  concern, any key-value store will do. You can skip using push cache altogether and connect directly to Github, which comes in handy during  development (see use cases below to learn more about this).  . When the configuration  repository is updated, a CD pipeline kicks in. This fetches configuration, converts it into a more optimized format and pushes it to the cache.  Additionally, the CD pipeline validates the configuration (once at the pull-request stage and again after being merged to master) and  controls multi-phase deployment by deploying config change to only 20% of production hosts at one time.  . Before we can connect to the push cache to fetch configuration we need to know where it is.  That’s where bootstrap configuration comes into play - it’s very simple. The config contains the hostname, port to connect to, and the name of  the environment to use. You need to put this config with your code or as part of the CD pipeline. This simple yaml file binding Spring  profiles to different Consul hosts suffices for our needs:  . The configuration library takes care of fetching the  configuration from push cache and exposing it to your business logic. We use the library called  cfg4j   (“configuration for java”). This library re-loads configurations from the push cache every few seconds and injects them into configuration  objects that our code uses. It also takes care of local caching, merging properties from different repositories, and falling back to  user-provided defaults when necessary (read more at  http://www.cfg4j.org/ ). . Briefly summarizing  how we use cfg4j’s features: . If you want to play with this library yourself, there’s plenty of examples both in  its  documentation  and  cfg4j-sample-apps Github repository . . The most important piece is business logic. To best make use of a configuration service,  the business logic has to be able to re-configure itself in runtime. Here are a few rules of thumb and code samples:    . Direct configuration injection (won’t reload as config changes) . Configuration injection via “interface binding” (will reload as config changes): . When you develop a feature, a main concern is the ability to evolve your code quickly. A full  configuration management pipeline is not conducive to this ability. We use the following approaches when doing local  development:    . When you work with multiple environments, some of them may share a common configuration. That’s when using configuration defaults may be  convenient. You can do this by creating a “default” environment and using cfg4j’s MergeConfigurationSource  for reading config first from the original environment and then (as a fallback) from the “default” environment. . Configuration repository, push cache and configuration CD pipeline can experience outages. To minimize impact of such  events, it’s good practice to cache the configuration locally (in-memory) after each fetch. cfg4j does that automatically. . Tests can’t always detect all problems. Bugs leak to  production environment and at times it’s important to make a config change as fast as possible to stop the fire. If you’re using push  cache, the fastest way to modify config values is to make changes directly within the cache. Consul offers a rich REST API and web UI for  updating configuration in the key-value store.  . Verifying that code and  configuration are kept in sync happens at the configuration CD pipeline level. One part of the continuous deployment process deploys the  code into a temporary execution environment, and pointing it to the branch that contains the configuration changes. Once the service is up,  we execute a batch of functional tests to verify configuration correctness. . The  presented solution is the result of work that we put into building huge-scale photo-serving services. We needed a simple yet flexible configuration management system, and combining  Git ,  Github ,  Consul  and  cfg4j  provided a  very satisfactory solution that we encourage you to try. .  I want to thank the following people for reviewing this article:  Bhautik Joshi, Elanna Belanger, Archie Russell .  ", "date": "2016-03-29"}, {"website": "Yahoo", "title": "Building Flickr’s Magic View with HBase and the Lambda Architecture", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/128273446181/building-flickrs-magic-view-with-hbase-and-the", "abstract": "  Introduction  . Flickr’s Magic View takes the hassle out of organizing your own photos by applying our cutting-edge, computer-vision technology to automatically categorize photos in your photostream and present them in a seamless view based on the content in the photos. This all happens in real-time - as soon as a photo is uploaded, it is categorized and placed into the Magic View. . This post provides an overview of the how we’ve able to do this. For a deeper dive, take a look at our Flickr Code blog post:  http://code.flickr.net/2015/09/03/powering-flickrs-magic-view/   .  The Challenge  . Our computational footprint made it easy to create per-user Magic View categories for over 12 billion images on Flickr; however, we also needed to combine this with updating the categories with the the tens of millions of tags generated from photos as they are uploaded in real-time. Ideally, the system has to allow us to efficiently but separately manage the bulk and real-time data that only computes the final state when requested. We turned to  Yahoo’s Hadoop stack  to find a way to build this at the massive scale we needed. .  Our Solution  . Powered by  Apache HBase , we developed a new scheme to fuse results from bulk and real-time aggregations. Using a single table in HBase, we are able to independently update and manage the bulk and real-time data in the system while always being able to provide a consistent, correct result.  . Pig and Oozie are used to quickly compute and load the results of large-scale, offline bulk computation. These robust tools are great for quick initialization of the system and periodically backfilling any missing data. Storm is used to power the real-time pump of data into the system and is mediated by a Java layer that fans out writes to HBase. When a user requests to see their data, a final Java process is responsible for combining the bulk and real-time data into its final state. . We believe that this solution is a novel simplification of what is  sometimes known as Lambda Architecture. We improve it by simplifying  some of its more complex components making maintenance and development  easier.  .  Lambda Architecture  .    Existing approach    . We’ll start with Nathan Marz’s book, ‘ Big Data ’, which proposes the database concept of  ‘Lambda Architecture’. In his analysis, he states that a database query can be represented as a function - Query - which operates on all the data: .    .    . This equation is shown graphically in the figure above. The real-time and bulk compute subsystem write to independent databases, which could be totally different systems. When dealing with a high volume of realtime data, the operational advantage here can be significant - there’s no need to have the expense of combining it with bulk data every time an event comes in. . Concerns around this approach center on the complicated nature of the Combiner function -  there is the developer and systems cost from the need to maintain two separate databases, the differing latencies of querying both sides and then the mechanics of merging the result.   .   Our Approach    . We addressed the complications of the Combiner by instead using a single database to store the real-time and bulk data. A Combiner is still required to compute a final result: .    .    How was this achieved? We implement our simplified Lambda architecture in HBase by giving each row two sets of columns - real-time and bulk - which are managed independently by the real-time subsystem (Storm and Java) and the bulk compute subsystem (Pig Latin and Oozie). It’s worth noting that  FiloDb  takes a similar approach - but since we only require the latest version of the data, our implementation is simpler. . The combiner stage is abstracted into a single Java process running  on its own hardware which computes on the data in HBase and pushes the  photostream tag aggregations to a cache for serving. .   The Combiner and Cleaner   . When reading a single row of data from HBase, we need to combine the data from the real-time and the bulk columns. If only the bulk or real-time data exists, then selecting the data is obvious. If both bulk and realtime data exists, we always pick real-time. This seems reasonable, but causes a subtle problem. . Let’s say a photos computer vision tags are added via real-time compute - there is no bulk data. Later on, we recompute all available photos using a new version of the computer vision tagging, and load this data (including this photo) via a bulk load. Even though the newer data exists in the bulk column, we can’t get to it because the combiner will only read the real-time column. We solve this by running the Cleaner process on all the data in HBase after we do a bulk load. . The Cleaner simply visits each row and sees if the HBase  timestamp for the real-time data is older than the bulk load. If it is,  then we delete the real-time data for that row since it’s already  captured in the bulk columns. This way the results of the bulk compute  aren’t ‘published’ until the cleaner has run. .  Acknowledgements  . Thanks to the entire Flickr Magic View and team for helping out and to Nathan Marz for kindly reviewing this work. .  Try it for yourself!     .    https://flickr.com/cameraroll   .  https://github.com/yahoo/simplified-lambda  ", "date": "2015-09-03"}, {"website": "Yahoo", "title": "Yahoo Daily Fantasy: Everyone’s Invited—and We Mean “Everyone”", "author": [" davglass"], "link": "https://yahooeng.tumblr.com/post/129855575131/yahoo-daily-fantasy-everyones-invitedand-we", "abstract": "  imbrianj : .  .    When we’re building products at Yahoo we get really excited about our work. No surprise. We envision that millions of people are going to love our products and be absolutely delighted when using them.  .    With our new  Yahoo Sports Daily Fantasy  game, we wanted to include everyone.  .    We support all major modern browsers on desktop and mobile as well as native apps. However, that, in and of itself, won’t ensure that the billion individuals around the world who use assistive technology will be able to manage and play our fantasy games. One billion. That’s a lot of everyone.  .    Daily Fantasy baked in accessibility. Baked in. Important point. In order to ensure that everyone is able to compete in our games at the same level, accessibility can’t be an add-on.  .    Check out our pages. Title and ARIA attributes. Structured headers. Brilliant labels. TabIndex and other attributes that are convenience features for many of us and a necessity for a great experience for others—especially our assistive technology users. There are a lot of them and if we work to make our pages and apps accessible, well, we figure, there can be a lot more of them using Daily Fantasy.  .    Think about it: whether you’re a sighted user and just need to hover over an icon to get the full description of what it indicates—or a totally blind user who would otherwise miss that valuable content—it makes sense to work on making our game as enjoyable and as easy to use as possible for everyone.  .    So, the technical bits. What specific things did we do to ensure good accessibility on Daily Fantasy?  .    A properly accessible site starts on a foundation of good, semantic markup. We work to ensure that content is presented in the markup in the order that makes the most sense, then worry about how to style it to look as we desire. The markup we choose is also important: while  &lt;div&gt;  and  &lt;span&gt;  are handy wrappers, we try to make sure the context is appropriate. Should this player info be a  &lt;dl&gt; ? Should this alert be a  &lt;p&gt; ?  .    One of the biggest impacts to screen readers is the appropriate use of header tags and well-written labels. With these a user can quickly navigate to the appropriate part of the page based on the headers presented—allowing them to skip some of the navigation stuff that sighted users take for granted—and know exactly what they can do when, for example, they need to subtract or add a player to their roster. When content changes, we make use of ARIA attributes. With a single-page web app (that does not do a page refresh as you navigate) we make use of ARIA’s role=“alert” to give a cue to users what change has occurred. Similarly, we’ve tried to ensure some components, such as our tab selectors and sliders, are compatible and present information that is as helpful as possible. With our scrolling table headers, we had to use ARIA to “ignore” them, as it’d be redundant for screen readers as the natural  &lt;th&gt;  elements were intentionally left in place but visibly hidden.  .    Although we have done some testing with OSX and VoiceOver, our primary testing platform is NVDA on Windows using Chrome. NVDA’s support has been good - and, it’s free and open source. Even if you’re on OSX, you can install a free Windows VM for testing thanks to a program Microsoft has set up (thank you!). These free tools make it so anyone is able to ensure a great experience for all users:  .    Accessibility should not be considered a competitive advantage. It’s something everyone should strive for and something we should all be supporting. If you’re interested in participating in the conversation, give us a tweet, reblog, join in the forum conversations or drop us a line! We share your love of Daily Fantasy games and want to make sure everyone’s invited.  .    If you have a suggestion on what could improve our product, please let us know! For Daily Fantasy we personally lurk in some of the more popular forums and have gotten some really great feedback from their users. It’s not uncommon to read a comment and have a fix in to address it within hours.  .    Did I mention that we are excited about our work and delighting users—everyone?  . - Gary, Darren and Brian  ", "date": "2015-09-25"}, {"website": "Yahoo", "title": "Perceptual Image Compression at Flickr", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/130574301641/perceptual-image-compression-at-flickr", "abstract": "  Archie Russell, Peter Norby, Saeideh Bakhshi  . At Flickr our users really care about image quality. They also care a lot about how responsive our apps are. Addressing both of these concerns simultaneously is challenging;  higher quality images have larger file sizes and are slower to transfer. Slow transfers are especially noticeable on mobile devices. Flickr had historically targeted the high image quality, but in late 2014 we implemented a method to both maintain image quality and decrease file size. As image appearance is very important to our users,  we performed an extensive user test before rolling this change out. Here’s how we did it. . JPEG compression has several tunable knobs. The q-value is the best known of these; it adjusts the level of spatial detail stored for fine details: a higher q-value typically keeps more detail. However, as q-value gets very close to 100, file size increases dramatically, usually without improving image appearance. . If file size and app performance isn’t an issue, dialing up q-value is an easy way to get really nice-looking images; this is what Flickr has done in the past. And if appearance isn’t very important, dialing down q-value is a viable option. But if you want both,  you’re kind of stuck. Additionally, q-value is not one size fits all,  some images look great at q-value 80 while others don’t. . Another commonly adjusted setting is chroma-subsampling,  which alters the amount of color information stored in a JPEG file. With a setting of 4:4:4, the two chroma (color) channels in a JPG have as much information as the luminance channel. In an image with a setting of 4:2:0, each chroma channel has only a quarter as much information as in an a 4:4:4 image. .  .  . Table 1:  JPEG stored at different quality and chroma levels. The top image is saved at high quality and chroma level – notice the color and detail in the folds of the red flag. The bottom image has the lowest quality – notice artifacts along the right edges of the red flag.  . Ideally, we’d have an algorithm which automatically tuned all JPEG parameters to make a file smaller, but which would limit perceptible changes to the image. Technology exists that attempts to do this and can decrease image file size by over 30%. This compression ratio is highly dependent on image content and dimensions. . Fig 2. Compressed (l) and uncompressed ® images. Compressed image is 36% smaller. . We were pleased with perceptually compressed images in cursory examinations, compressed images were smaller and nearly indistinguishable from their sources. But we wanted to really quantify how well it worked before rolling it out. The standard computational tools for evaluating compression, such as SSIM, are fairly simplistic and don’t do a great job at modeling how a user sees things. To really evaluate this technology had to use a better measure of perceptibility:  human minds.   . To test whether our image compression would impact user perception of image quality, we put together a “taste test”. The taste test was constructed as a game with multiple rounds where users looked at both compressed and uncompressed images. Users accumulated points the longer they played, and got more points for doing well at the game.  We maintained a leaderboard to encourage participation and used only internal testers.    . The game’s test images came from a diverse collection of 250 images contributed by Flickr staff. The images came from a variety of cameras and included a number of subjects from photographers with varying skill levels. . In each round, our test code randomly selected a test image, and presented two variants of this image side by side. 50% of the time we presented the user two identical images; the rest of the time we presented one compressed image and one uncompressed image. We asked the tester if the two images looked the same or different. We expected a user choosing randomly OR a user unable to distinguish the two cases would answer correctly about half the time.  We randomly swapped the location of the compressed images to compensate for user bias to the left or the right.  If testers chose correctly, they were presented with a second question: \"Which image did you prefer, and why?” . Fig 4. Screenshot of taste test. . Our test displayed images simultaneously to prevent testers noticing a longer load time for the larger, non-compressed image. The images were presented with either 320, 640, or 1600 pixels on their longest side. The 320 &amp; 640px images were shown for 12 seconds before being dimmed out. The intent behind this detail was to represent how real users interacted with our images. The 1600px images stayed on screen for 20 seconds, as we expected larger images to be viewed for longer periods of time by real users.  . We ran our taste test for two weeks and analyzed our results. Although we let users play as long as they liked, we skipped the first result per user as a “warm-up” and considered only the subsequent ten results, which limited the potential for users training themselves to spot compression artifacts. We disregarded users that had fewer than eleven results. . Table 2. Taste test results. Testers selected “identical” at nearly the same rate, whether the input was identical or not. . When our testers were presented with two  identical  images, they thought the images were identical only 68.8% of the time(!), and when presented with a compressed image next to a non-compressed image, our testers thought the images were identical slightly less often: 67.6% of the time. This difference was small enough for us, and our statisticians told us it was statistically insignificant. Our image pairs were so similar that multiple testers thought all images were identical and reported that the test system was buggy. We inspected the images most often labeled different, and found no significant artifacts in the compressed versions.  . So even in this side-by-side test, perceptual image compression was just barely noticeable when images were presented side-by-side.  As the Flickr website wouldn’t ever show compressed and uncompressed images at the same time, and the use of compression had large benefits in storage footprint and site performance, we elected to go forward. . At the beginning of 2014 we silently rolled out perceptual-based compression on our image thumbnails (we don’t alter the “original” images uploaded by our users). The slight changes to image appearance went unnoticed by users, but user interactions with Flickr became much faster, especially for users with slow connections, while our storage footprint became much smaller. This was a best-case scenario for us. . Evaluating perceptual compression was a considerable task, but it gave the confidence we needed to apply this compression in production to our users.   This marked the first time Flickr had adjusted image settings in years, and, it was fun. . Fig 5.  Taste test high score list . After eighteen months of perceptual compression at Flickr,  we adjusted our settings slightly to shrink images an additional 15%. For our users on mobile devices, 15% fewer bytes per image makes for a much more responsive experience. We had run a taste test on this newer setting and users were were able to spot our compression slightly more often than with our original settings. When presented a pair of identical images, our testers declared these images identical 65.2% of the time, when presented with different images, our testers declared the images identical 62% of the time. It wasn’t as imperceptible as our original approach, but, we decided it was close enough to roll out. . Boy were we wrong! A few very vocal users spotted the compression and didn’t like it at all. The Flickr Help Forum had a very lively thread which  Petapixel picked up . We  beat our heads against the wall  considered our options and came up with a middle path between our initial and follow-on approaches, giving us smaller, faster-to-load files while still maintaining the appearance our users expect. . Through our use of perceptual compression, combined with our use of  on-the-fly resize  and  COS , we were able to decrease our storage footprint dramatically, while simultaneously improving user experience. It’s a win all around but we’re not done yet — we still have a few tricks up our sleeves. ", "date": "2015-10-05"}, {"website": "Yahoo", "title": "Yahoo Account Key – Signing in Has Never Been Easier", "author": [" davglass"], "link": "https://yahooeng.tumblr.com/post/131218006711/yahoo-account-key-signing-in-has-never-been", "abstract": "  yahoo : .  By Dylan Casey, VP of Product Management  . Earlier this year, we launched  on-demand passwords  so you can sign into your Yahoo account using an SMS code, instead of memorizing a complicated password. It was the first step toward a password-free future.  . Today, we’re excited to take user convenience a step further by introducing Yahoo Account Key, which uses push notifications to provide a quick and simple way for you to access a Yahoo account using your mobile device. . Passwords are usually simple to hack and easy to forget. Account Key streamlines the sign-in process with a secure, elegant and easy-to-use interface that makes access as easy as tapping a button. It’s also more secure than a traditional password because once you activate Account Key – even if someone gets access to your account info – they can’t sign in.   . Account Key is now available globally for the new Yahoo Mail app and will be rolling out to other Yahoo apps this year. We’re thrilled about this next step towards a password-free future! ", "date": "2015-10-15"}, {"website": "Yahoo", "title": "Under the Hood: Delivering the First Free Global Live Stream of an NFL Game on Yahoo", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/132155634066/under-the-hood-delivering-the-first-free-global", "abstract": " P.P.S. Narayan, VP of Engineering  . On Sunday, October 25, Yahoo delivered the first-ever, global live stream of a regular season NFL game to football fans around the world, for free, across devices. Our goal was to distribute the game over the Internet and provide a broadcast-quality experience. Leveraging our focus on consumer products, we worked to identify features and experiences that would be unique for users enjoying a live stream for the first time. In other words, we wanted to make you feel like you were watching on TV, but make the experience even better.  . For us, success was twofold: provide the best quality viewing experience and deliver that quality at global scale. In this blog, we will talk about some key technology innovations that helped us achieve this for over 15M unique viewers in 185 countries across the world.  . On the technical side, the HD video signal was shipped from London to our encoders in Dallas and Sunnyvale, where it was converted into Internet video. The streams were transcoded (compression that enables efficient network transmission) into 9 bitrates ranging from 6Mbps to 300kbps. We also provided a framerate of 60 frames per second (fps), in addition to 30fps, thus allowing for smooth video playback suited for a sport like NFL football. Having a max bitrate of 6Mbps with 60fps gave a “wow” factor to the viewing experience, and was a first for NFL and sports audiences. . One special Yahoo addition to the programming was an overlaid audio commentary from our Yahoo Studio in Sunnyvale. It was as if you were watching the game alongside our Yahoo Sports experts on your couch. This unique Yahoo take gave NFL viewers a whole new way to experience the game. .    . Our goal was to deliver a premium streaming quality that would bring users a best-in-class viewing experience, similar to TV–one that was extremely smooth and uninterrupted. This meant partnering with multiple CDNs to get the video bits as close to the viewer as possible, optimizing bandwidth usage, and making the video player resilient to problems on the Internet or the user’s network. .     .  Multiple CDNs   . In addition to Yahoo’s own Content Delivery Network (CDN) and network infrastructure, which are capable of delivering video around the world, we partnered with six CDNs and Internet Service Providers (ISPs).  . The NFL game streams were available across all seven CDNs; however, we wanted to route the viewer to the most suitable CDN server based on multiple factors – device, resolution, user-agent, geography, app or site, cable/DSL network, and so on. We built sophisticated capabilities in our platform to be able to define routing and quality policy decisions. The policy engine served more than 80M requests during the game.  . Policy rules to routes were adjusted based on CDN performance and geographies. For example, we were able to reduce the international traffic to one underperforming CDN during the game and the changes were propagated in under six seconds across viewers. Such capabilities delivered a near flawless  viewing experience.  . During the game, we served on average about 5Tbps across the CDNs, and at peak we were serving 7Tbps of video to viewers globally.  .     .  Bitrates and Adaptation  . Viewers of video streaming on the Internet are all too familiar with the visual aspects of poor quality: the infamous “spinner,” technically termed re-buffering; the blockiness of the video that represents low bitrates; jerkiness of the video, which could be due to frame drops; and plain old errors that show up on the screen. . Since we had nine bitrates available, our Yahoo player could use sophisticated techniques to measure the bandwidth (or network capacity) on a user’s home or service provider network, and pick the best bitrate to minimize buffering or errors. Such adaptive bitrate (ABR) algorithms make the viewing experience smooth. Since we supported 60fps streams, the algorithm also monitored frame drops to decide if the device was capable of supporting the high frame rate. It then reacted appropriately by switching to the 30fps stream if necessary. .    .     .  Testing and simulation   . Manually testing adaptive video playback is very difficult, subjective and time consuming. So we built a network and device simulation framework called “Adaptive Playground” that brings automation, integration and a more scientific approach to testing and measuring video playback performance. .    .   . Another tool we developed is a “Stream Monitor” that was used to constantly monitor all the streams across CDNs, check the validity or correctness of the streams, and ultimately identify ingestion or delivery problems. During the game, the tool detected issues, helped to identify the exact problem and take action.    . Yahoo broadcasts live events, news segments and concerts regularly. So these types of tools are continuously used on these events to measure, test and analyze our infrastructure and partner systems.  .     .  Player Recovery  . The video playback must be smooth even if the connection to the streaming server is lost or if there are Internet connectivity issues. So, we introduced seamless recovery in the Yahoo video player. Under problematic conditions, the recovery mechanism is automatically activated, and the player reconnects to our backend API servers to fetch from another CDN. In essence, this replaces a user reloading the page or clicking the player when problems occur–an otherwise manual action that is incredibly frustrating. . During the game on Sunday, thanks to the seamless recovery of our player, many viewers automatically switched CDNs when their current CDN or ISP had issues. This resulted in a smooth watching experience. In one severe case, we had up to 100K viewers automatically switching CDNs in less than 30 seconds, as seen in the graph below. .    .   . We wanted to make sure that our global audience could watch this stream anywhere in the world, on any device so we delivered it on laptops and desktops, on phones and tablets; and finally, we wanted to reach the ardent fans on the big screen TVs, game consoles, and other connected devices.  . Our  destination page , which provided a full screen experience of the game on web and mobile web, was built on node.js and React, and extensively optimized for page load and startup latency. In addition, we decided to launch the NFL experience on our key mobile apps: Yahoo, Tumblr, Yahoo Sports and Yahoo Sports Fantasy.  .   .  Pure HTML5 on Safari   . We brought users a pure HTML5 video delivery on the Safari web browser. There is currently an industry-wide move away from Flash, and Yahoo is no exception. As the first step toward achieving this goal, we deployed a “pure” HTML5 player on Safari for the NFL live stream. Making this leap had a positive impact to millions of viewers during the game. .   .  Connected Devices &amp; TV Experience   . Our objective was to create a connected TV video experience better than cable/satellite TV. In just a few months, we were able to develop and deploy on nine different connected device platforms and on 60+ different TV models. . We wanted a large percentage of our big screen viewers to experience the 60fps streams. However, we soon realized that even on modern devices this was not easily feasible due to memory, CPU and firmware limitations. So we conducted hundreds of hours of testing to come up with the right stream configuration for each device. We developed automation tools to quickly validate stream configurations from various CDNs, as well as created a computer vision (camera automation) test tool to monitor and verify video quality and stability across devices.     .  Chromecast   . Because NFL games are traditionally viewed on television, we wanted to provide viewers an easy way to watch the NFL/Yahoo Live Stream on their big screens. In addition to connected TV apps, we built Chromecast support into apps for iOS and Android, allowing viewers to cast the game on big screen TVs from their mobile devices. . To ensure a high-quality, uninterrupted cast, we also built a custom Chromecast receiver app with the same improved resiliency through robust recovery algorithms. Judging by the engagement on our Chromecast streams, we consistently matched or surpassed the viewing times on other experiences.   . Yahoo operates multiple data centers across the US and the world for service reliability and capacity. We also have dozens of smaller point-of-presence (POPs) located close to all major population centers to provide a low latency connection to Yahoo’s infrastructure. Our data centers and POPs are connected together via a high redundancy private backbone network. For the NFL game, we upgraded our network and POPs to handle the extra load. We also worked with the CDN vendors to setup new peering points to efficiently route traffic to their networks. . As part of running “Internet” scale applications, we always build our software to take advantage of Yahoo’s multiple data centers. Every system has a backup, and in most cases, each backup has another backup. Our architecture and contingency plans account for multiple simultaneous failures.  . During an NFL game, which typically lasts just under four hours, there is a very small margin of error for detecting and fixing streaming issues. Real-time metrics as well as detailed data from our backend systems provide a high fidelity understanding of the stream quality that viewers are experiencing.  . Yahoo is a world leader in data, analytics and real-time data processing. So, we extensively used our data infrastructure, including Hadoop, to provide industry leading operational metrics during the game.  .     .  Player Instrumentation   . The Yahoo video player has extensive instrumentation to track everything happening during video playback. And, this data is regularly beaconed back to our data centers. The data includes service quality metrics like first video frame latency, bitrate, bandwidth observed, buffering and frame drops.   . The beacons are processed in real-time, and we have dashboards showing KPIs like the number of concurrent viewers, total video starts, re-buffering ratio by numerous dimensions like CDN, device, OS and geo. These real-time dashboards enabled our operations team to make decisions about routing policies and switching CDN(s) in real-time based on quality metrics. . In terms of scale, our beacon servers peaked at more than 225K events/sec, handling about two billion events in total, which equaled about 4TB of data during the game.    Backend APIs  . Prior to the NFL streaming event, we had designed the backend APIs to deliver at scale, with low latency and high availability. During the game, we served 216 million API calls, with a median latency of 11ms, and a 95th percentile latency of 16ms. The APIs showed six 9s of availability during this time period. . Our systems are instrumented exhaustively to obtain real-time feedback on performance. These metrics were available for monitoring through dashboards, and were also used for alerting when performance breached acceptable thresholds.   .   . Pioneering the delivery of a smooth 60fps live video experience to millions of users around the world was a significant undertaking. Huge thanks to the team for executing against our vision - it was a coordinated effort across Yahoo.  . While much of our technology and infrastructure was already set up to handle the scale and load–one of the reasons the NFL chose us–in preparation for the main event, we designed a new destination page and enhanced our mobile applications. We also enhanced the control and recovery mechanisms, as well as expanded our infrastructure to handle the huge traffic of the game. We worked hard to ensure that the experience was available on every Internet connected device. We tuned our video players to deliver the optimal video stream, taking into account device, connectivity, location and ISP. Behind everything was our massive analytical system that would measure and aggregate all aspects of quality and engagement. We conducted comprehensive tests with our partners so that game day would be successful. In the end, the game exceeded our high expectations, setting a bar for quality and scale for live Internet broadcasts to come. We’re thrilled and proud of the experience we delivered, and further, the reception and accolades from our community of users has been gratifying.   . Looking to the future, we expect live sporting events to be routinely streamed over the Internet to massive global audiences. People will expect these broadcasts to be flawless, with better than HD quality. October 25th 2015 was a significant step towards this vision. Yahoo, as a leading technology company and a top destination for sports, is proud of our role in setting a new standard for sports programming. We look forward to making other global scale broadcasts like the NFL game happen in the future. . Want to help? Email me at  ppsn@yahoo-inc.com  and we can talk about opportunities on our team. ", "date": "2015-10-29"}, {"website": "Yahoo", "title": "Controlling audio output on iOS with AVAudioSession", "author": [" staticpulse"], "link": "https://yahooeng.tumblr.com/post/133423436921/controlling-audio-output-on-ios-with", "abstract": "  Nano Anderson, Engineering Mgr @ Yahoo Video NYC  . If your app plays sound of any kind, you have probably asked yourself “why is/isn’t sound playing now?”. You toggle the ringer switch, which sometimes solves the problem. But sometimes it doesn’t. You copy and paste some code from Stack Overflow which overrides the ringer switch permanently, but then sound starts playing in the background while the phone’s in your pocket. Why? . If you end up in Apple’s iOS documentation and sample code, you’ll probably run into a wall. In most of Apple’s examples, you’re not told that your  AVPlayer  or  MPMoviePlayerViewController  or whatever you use simply will not play audio if the ringer is muted. Meanwhile, in Messages.app, Videos.app, and Apple’s other built-in apps, you will commonly see that audio and video, when initiated by the user by tapping a play button or similar action item, immediately play back with sound, regardless of whether the ringer is muted. Apple’s defaults for developers do not address this use case, but  AVAudioSession  is hardly mentioned in general audiovisual  playback  documentation  1  . . The answer lies deep within the bowels of   AVAudioSession  . This  AVFoundation  class is a singleton for your app which controls every aspect of audio input and output. . The trickiness with  AVAudioSession  is that you that you don’t  have  to use  AVAudioSession  in order to play video or audio. Under most circumstances, your app will play a media file through headphones (regardless of ringer switch setting) or speakers (if the ringer is not set to  Silent ). When you eventually have the question “How do I play audio through the speaker when the ringer is muted?”, you’re most likely off to the internet to find an answer. . And if you find yourself in some other corner of the internet, this is the code you’ll likely be told to just shove in your UIApplicationDelegate: . This works, until you realize you then have to deal with rightfully upset customers who had their favorite podcast muted the second they opened up your news app, which happens to maybe occasionally play videos, among other side effects you weren’t ready for. . If you’re confused, you are not alone. Let’s figure this out together. . First, let’s look at that method that the internet told us to use to override the ringer switch,  -[AVAudioSession setCategory:error:] . Here’s a handy table that explains what each combination of  AVAudioSessionCategory  (which you control) and “Ring/Silent switch” setting (controlled by the user) will result in. . Most apps only need to deal with  SoloAmbient  as their default category and one other category, such as  Playback , for playing back multimedia files. With that in mind, a simple  AVAudioSession  manager class can help toggle back and forth between categories. Here is an example of how to one could call  -[AVAudioSession setCategory:error:]  (or  -[AVAudioSession setCategory:withOptions:error:] ): . That’s the gist of it. But don’t worry, there are a few things you still need to worry about. . Does your app need to continue playback in the background? Perhaps you have a podcast app, or an app which wants to take advantage of Airplay or Picture-in-Picture? Well you’re in luck, because now you get to muck with  UIBackgroundModes  in your  Info.plist ! . Now, this isn’t really a big deal in terms of code. In your  Info.plist , within the array for  UIBackgroundModes  (add it if it doesn’t already exist), add an entry with the string  audio . . If you’re only playing audio files, you’re basically done  3  . If you’re playing video files, there’s more fun involved. Why would you want to continue playing videos in the background, you ask? Two words: Airplay &amp; Picture-in-Picture. For now, I’m only going to cover Airplay, because PiP is less prone to side-effects in my opinion. . There’s one main item you have to worry about with backgrounded apps and video. Video-playing apps need to disconnect the currently-playing  AVPlayerLayer  from its  AVPlayer  by setting its  player  property to  nil    before   your application goes to the background. If you don’t do this, your  AVPlayer  will automatically pause. That’s a good thing most of the time, but not if you want your Airplay-ing video to keep playing! Just make sure you’re only doing this for videos that are Airplaying, otherwise you’ll get some ridiculous audio coming out of your phone’s speakers after you lock the screen. Here is some slightly modified code from Apple’s  Technical Q&amp;A QA1668 – Playing media while in the background using AV Foundation on iOS : . Thanks for tuning in to this week’s episode of   Controlling audio output on iOS with AVAudioSession  . Tune in next week, when we’ll be talking about literally anything else. . Meanwhile, talk to an iOS audiovisual  recording  &amp;  mixing  engineer and you’ll probably find an  AVAudioSession  expert.  ↩  . See   AVAudioSessionCategoryOptionMixWithOthers    ↩  . If you want to play well with other apps’ audio, you’ll want to look at the advanced features of  AVAudioSession  in Apple’s  Audio Session Programming Guide   ↩  ", "date": "2015-11-17"}, {"website": "Yahoo", "title": "Elide : Simplify Your CRUD", "author": [" aklish"], "link": "https://yahooeng.tumblr.com/post/134800436351/elide-simplify-your-crud", "abstract": "  By Aaron Klish &amp; Jon Kilroy  . Mobile and client-side web applications have been reshaping the design principles of service layer APIs. Business logic traditionally implemented on the server and exposed as functions (think  RPC ) has morphed into exposing data and shifting that logic to the application.   Object hierarchies and graphs are one of the most common and natural forms for data representation and often pair well with simple  CRUD  (create, read, update, &amp; delete) operations.  The main challenges with building these services  is that powerful &amp; flexible APIs that expose data are often expensive to build. There is usually only time to build the minimal set of interfaces that are needed by a single application. . Elide is a new Yahoo technology designed to significantly reduce the development cost to expose a hierarchical data model as a production quality REST API.  Elide adopts the best standards to date for API definition targeting the concerns of front end developers -   JSON-API  . .  JSON-API  is a standard for representing and manipulating an entity relationship graph through a REST API. . Oversimplifying things, the path segment of URLs in JSON-API are constructed by: . Entity representations consist a type, an ID, a set of attributes, and a set of relationships to other entities: . JSON-API needs a data model to expose.  Rather than starting from scratch with a new  DSL , we adopted the most mature, feature rich, and industry proven data modeling framework for the backend -  JPA (Java Persistence API) .  JPA allows developers to define complex models but also to leverage existing providers for a wide variety of persistence architectures - or to build their own custom solutions.   We’ve done both at Yahoo.   Elide exposes any JPA annotated data model as a complete, JSON-API web service. . Having a powerful API and a powerful modeling language are not enough to build a production quality service.  There are four other components that are required to expose data as a service: . The marriage of JSON-API, JPA, and a custom set of Elide annotations provide other benefits: . Elide is working on the release of other features including: . For now, check out our documentation at  elide.io .  We hope to have more posts soon with some tutorials for simple projects. ", "date": "2015-12-08"}, {"website": "Yahoo", "title": "MySQL Partition Manager is Open Source", "author": [" gilyehuda"], "link": "https://yahooeng.tumblr.com/post/135332825141/mysql-partition-manager-is-open-source", "abstract": "  mysqlatyahoo : . At Yahoo, we manage a massive number of MySQL databases spread across multiple data centers. . We have thousands of databases and each database has many partitioned tables. In order to efficiently create and maintain partitions we developed a partition manager which automatically manages these for you with minimal pre configuration. . Today, we’re releasing MySQL Partition Manager. You can check out the code on  GitHub . . We’re looking forward to interacting with the MySQL community and continue developing new features. . - MySQL Database Engineering Team, Yahoo ", "date": "2015-12-16"}, {"website": "Yahoo", "title": "Benchmarking Streaming Computation Engines at Yahoo!", "author": [" revans2"], "link": "https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at", "abstract": "  (Yahoo Storm Team in alphabetical order)  Sanket Chintapalli ,  Derek Dagit ,  Bobby Evans ,  Reza Farivar ,  Tom Graves , Mark Holderbaugh,  Zhuo Liu ,  Kyle Nusbaum ,  Kishorkumar Patil ,  Boyang Jerry Peng  and  Paul Poulosky.    .    DISCLAIMER:   Dec 17th 2015 data-artisans has pointed out to us that we accidentally left on some  debugging  in the flink benchmark. So the flink numbers should not be directly compared to the storm and spark numbers.  We will rerun and repost the numbers when we have fixed this.  .  UPDATE:  Dec 18, 2015 there was a miscommunication and the code that was checked in was not the exact code we ran with for flink.  The real code had the debugging removed.  Data-Artisans has looked at the code and confirmed it and the current numbers are good.  We will still rerun at some point soon.  .   Executive Summary -  Due to a lack of real-world streaming benchmarks, we developed  one  to compare Apache Flink, Apache Storm and Apache Spark Streaming. Storm 0.10.0, 0.11.0-SNAPSHOT and Flink 0.10.1 show sub- second latencies at relatively high throughputs with Storm having the lowest 99th percentile latency. Spark streaming 1.5.1 supports high throughputs, but at a relatively higher latency.     . At Yahoo, we have invested heavily in a number of open source big data platforms that we use daily to support our business. For streaming workloads, our platform of choice has been Apache Storm, which replaced our internally developed S4 platform. We have been using Storm extensively, and the number of nodes running Storm at Yahoo has now reached about 2,300 (and is still growing). . Since our initial decision to use Storm in 2012, the streaming landscape has changed drastically. There are now several other noteworthy competitors including Apache Flink, Apache Spark (Spark Streaming), Apache Samza, Apache Apex and Google Cloud Dataflow. There is increasing confusion over which package offers the best set of features and which one performs better under which conditions (for instance see    here ,  here ,    here , and  here ). . To provide the best streaming tools to our internal customers, we wanted to know what Storm is good at and where it needs to be improved compared to other systems. To do this we started to look for stream processing benchmarks that we could use to do this evaluation, but all of them were lacking in several fundamental areas. Primarily, they did not test with anything close to a real world use case. So we decided to write one and released it as open source  https://github.com/yahoo/streaming-benchmarks .  In our initial evaluation we decided to limit our test to three of the most popular and promising platforms (Storm, Flink and Spark), but welcome contributions for other systems, and to expand the scope of the benchmark. . The benchmark is a simple advertisement application. There are a number of advertising campaigns, and a number of advertisements for each campaign. The job of the benchmark is to read various JSON events from Kafka, identify the relevant events, and store a windowed count of relevant events per campaign into Redis. These steps attempt to probe some common operations performed on data streams. . The flow of operations is as follows (and shown in the following figure): . The input data has the following schema: . Producers create events with timestamps marking creation time. Truncating this timestamp to a particular digit gives the  begin-time  of the time window the event belongs in. In Storm and Flink, updates to Redis are written periodically, but frequently enough to meet a chosen SLA. Our SLA was 1 second, so once per second we wrote updated windows to Redis. Spark operated slightly differently due to great differences in its design. There’s more details on that in the Spark section. Along with the data, we record the time at which each window in Redis was last updated. . After each run, a utility reads windows from Redis and compares the windows’ times to their  last_updated_at  times, yielding a latency data point. Because the last event for a window cannot have been emitted after the window closed but will be very shortly before, the difference between a window’s time and its last_updated_at time minus its duration represents the time it took for the final tuple in a window to go from Kafka to Redis through the application. .  window.final_event_latency = (window.last_updated_at – window.timestamp) – window.duration  . This is a bit rough, but this benchmark was not purposed to get fine-grained numbers on these engines, but to provide a more high-level view of their behavior. . Benchmark setup . Since the Redis node in our architecture only performs in-memory lookups using a well-optimized hashing scheme, it did not become a bottleneck. The nodes are homogeneously configured, each with two Intel E5530 processors running at 2.4GHz, with a total of 16 cores (8 physical, 16 hyperthreading) per node. Each node has 24GiB of memory, and the machines are all located within the same rack, connected through a gigabit Ethernet switch. The cluster has a total of 40 nodes available. . We ran multiple instances of the Kafka producers to create the required load since individual producers begin to fall behind at around 17,000 events per second. In total, we use anywhere between 20 to 25 nodes in this benchmark. . The use of 10 workers for a topology is near the average number we see being used by topologies internal to Yahoo. Of course, our Storm clusters are larger in size, but they are multi-tenant and run many topologies. . To begin the benchmarks Kafka is cleared, Redis is populated with initial data ( ad_id  to  campaign_id  mapping), the streaming job is started, and then after a bit of time to let the job finish launching, the producers are started with instructions to produce events at a particular rate, giving the desired aggregate throughput. The system was left to run for 30 minutes before the producers were shut down. A few seconds were allowed for all events to be processed before the streaming job itself was stopped. The benchmark utility was then run to generate a file containing a list of  window.last_updated_at – window.timestamp  numbers. These files were saved for each throughput we tested and then were used to generate the charts in this document. . The benchmark for Flink was implemented in Java by using Flink’s DataStream API.  The Flink DataStream API has many similarities to Storm’s streaming API.  For both Flink and Storm, the dataflow can be represented as a directed graph. Each vertex is a user defined operator and each directed edge represents a flow of data.  Storm’s API uses spouts and bolts as its operators while Flink uses map, flatMap, as well as many pre-built operators such as filter, project, and reduce. Flink uses a mechanism called checkpointing to guarantee processing which offers similar guarantees to Storm’s acking. Flink has checkpointing off by default and that is how we ran this benchmark. Notable configs we used in Flink is listed below: . The Flink version of the benchmark uses the FlinkKafkaConsumer to read data in from Kafka.  The data read in from Kafka—which is in a JSON formatted string— is then deserialized and parsed by a custom defined flatMap operator. Once deserialized, the data is filtered via a custom defined filter operator. Afterwards, the filtered data is projected by using the project operator. From there, the data is joined with data in Redis by a custom defined flapMap function. Lastly, the final results are calculated from the data and written to redis. . The rate at which Kafka emitted data events into the Flink benchmark is varied from 50,000 events/sec to 170,000 events/sec. For each Kafka emit rate, the percentile latency for a tuple to be completely processed in the Flink benchmark is illustrated in the graph below.  . The percentile latency for all Kafka emit rates are relatively the same. The percentile latency rises linearly until around the 99th percentile, where the latency appears to increase exponentially.    . For the Spark benchmark, the code was written in Scala. Since the micro-batching methodology of Spark is different than the pure streaming nature of Storm, we needed to rethink parts of the benchmark. Storm and Flink benchmarks would update the Redis database once a second to try and meet our SLA, keeping the intermediate update values in a local cache. As a result, the batch duration in the Spark streaming version was set to 1 second, at least for smaller amounts of traffic. We had to increase the batch duration for larger throughputs. . The benchmark is written in a typical Spark style using DStreams. DStreams are the streaming equivalent of regular RDDs, and create a separate RDD for every micro batch. Note that in the subsequent discussion, we use the term “RDD” instead of “DStream” to refer to the RDD representation of the DStream in the currently active microbatch. Processing begins with the direct Kafka consumer included with Spark 1.5. Since the Kafka input data in our benchmark is stored in 5 partitions, this Kafka consumer creates a DStream with 5 partitions as well. After that, a number of transformations are applied on the DStreams, including maps and filters. The transformation involving joining data with Redis is a special case. Since we do not want to create a separate connection to Redis for each record, we use a mapPartitions operation that can give control of a whole RDD partition to our code.  This way, we create one connection to Redis and use this single connection to query information from Redis for all the events in that RDD partition. The same approach is used later when we update the final results in Redis. . It should be noted that our writes to Redis were implemented as a side-effect of the execution of the RDD transformation in order to keep the benchmark simple, so this would not be compatible with exactly-once semantics. . We found that with high enough throughput, Spark was not able to keep up.  At 100,000 messages per second the latency greatly increased. We considered adjustments along two control dimensions to help Spark cope with increasing throughput. . The first is the microbatch duration. This is a control dimension that is not present in a pure streaming system like Storm. Increasing the duration increases latency while reducing overhead and therefore increasing maximum throughput. The challenge is that the choice of the optimal batch duration that minimizes latency while allowing spark to handle the throughput is a time consuming process. Essentially, we have to set a batch duration, run the benchmark for 30 minutes, check the results and decrease/increase the duration. . The second dimension is parallelism. However, increasing parallelism is simpler said than done in the case of Spark. For a true streaming system like Storm, one bolt instance can send its results to any number of subsequent bolt instances by using a random shuffle. To scale, one can increase the parallelism of the second bolt. In the case of a micro batch system like Spark, we need to perform a reshuffle operation similar to how intermediate data in a Hadoop MapReduce program are shuffled and merged across the cluster. But the reshuffling itself introduces considerable overhead. Initially, we thought our operations were CPU-bound, and so the benefits of reshuffling to a higher number of partitions would outweigh the cost of reshuffling.  Instead, we found the bottleneck to be scheduling, and so reshuffling only added overhead. We suspect that at higher throughput rates or with operations that are CPU-bound, the reverse would be true. . The final results are interesting. There are essentially three behaviors for a Spark workload depending on the window duration. First, if the batch duration is set sufficiently large, the majority of the events will be handled within the current micro batch. The following figure shows the resulting percentile processing graph for this case (100K events, 10 seconds batch duration).  . But whenever 90% of events are processed in the first batch, there is possibility of improving latency. By reducing the batch duration sufficiently, we get into a region where the incoming events are processed within 3 or 4 subsequent batches. This is the second behavior, in which the batch duration puts the system on the verge of falling behind, but is still manageable, and results in better latency. This situation is shown in the following figure for a sample throughput rate (100K events, 3 seconds batch duration).  . Finally, the third behavior is when Spark streaming falls behind. In this case, the benchmark takes a few minutes after the input data finishes to process all of the events. This situation is shown in the following figure. Under this undesirable operating region, Spark spills lots of data onto disks, and in extreme cases we could end up running out of disk space. . One final note is that we tried the new back pressure feature introduced in Spark 1.5. If the system is in the first operating region, enabling back pressure does nothing. In the second operating region, enabling back pressure results in longer latencies. The third operating region is where back pressure shows the most negative impact.  It changes the batch length, but Spark still cannot cope with the throughput and falls behind. This is shown in the next figures. Our experiments showed that the current back pressure implementation did not help our benchmark, and as a result we disabled it.  .   Performance without back pressure (top), and with back pressure enabled (bottom). The latencies with the back pressure enabled are worse (70 seconds vs 120 seconds). Note that both of these results are unacceptable for a streaming system as both fall behind the incoming data. Batch duration was set to 2 seconds for each run, with 130,000 throughput.   . Storm’s benchmark was written using the Java API. We tested both Apache Storm 0.10.0 release and a 0.11.0 snapshot. The snapshot’s commit hash was a8d253a. One worker process per host was used, and each worker was given 16 tasks to run in 16 executors - one for each core. . Storm 0.10.0: . Storm 0.11.0: . Storm compared favorably to both Flink and Spark Streaming. Storm 0.11.0 beat Storm 0.10.0, showing the optimizations that have gone in since the 0.10.0 release. However, at high-throughput both versions of Storm struggled. Storm 0.10.0 was not able to handle throughputs above 135,000 events per second. . Storm 0.11.0 performed similarly until we disabled acking. In the benchmarking topology, acking was used for flow control but not for processing guarantees. In 0.11.0, Storm added a simple back pressure controller, allowing us to avoid the overhead of acking. With acking enabled, 0.11.0 performed terribly at 150,000/s—slightly better than 0.10.0, but still far worse than anything else. With acking disabled, Storm even beat Flink for latency at high throughput. However, with acking disabled, the ability to report and handle tuple failures is disabled also. . It is interesting to compare the behavior of these three systems. Looking at the following figure, we can see that Storm and Flink both respond quite linearly. This is because these two systems try to process an incoming event as it becomes available. On the other hand, the Spark Streaming system behaves in a stepwise function, a direct result from its micro-batching design.  . The throughput vs latency graph for the various systems is maybe the most revealing, as it summarizes our findings with this benchmark. Flink and Storm have very similar performance, and Spark Streaming, while it has much higher latency, is expected to be able to handle much higher throughput.  . We did not include the results for Storm 0.10.0 and 0.11.0 with acking enabled beyond 135,000 events per second, because they could not keep up with the throughput. The resulting graph had the final point for Storm 0.10.0 in the 45,000 ms range, dwarfing every other line on the graph. The longer the topology ran, the higher the latencies got, indicating that it was losing ground. . All of these benchmarks except where otherwise noted were performed using default settings for Storm, Spark, and Flink, and we focused on writing correct, easy to understand programs without optimizing each to its full potential. Because of this each of the six steps were a separate bolt or spout. Flink and Spark both do operator combining automatically, but Storm (without Trident) does not. What this means for Storm is that events go through many more steps and have a higher overhead compared to the other systems. . In addition to further optimizations to Storm, we would like to expand the benchmark in terms of functionality, and to include other stream processing systems like Samza and Apex. We would also like to take into account fault tolerance, processing guarantees, and resource utilization. . The bottom line for us is Storm did great. Writing topologies is simple, and it’s easy to get low latency comparable to or better than Flink up to fairly high throughputs. Without acking, Storm even beat Flink at very high throughput, and we expect that with further optimizations like combining bolts, more intelligent routing of tuples, and improved acking, Storm with acking enabled would compete with Flink at very high throughput too. . The competition between near real time streaming systems is heating up, and there is no clear winner at this point. Each of the platforms studied here have their advantages and disadvantages. Performance is but one factor among others, such as security or integration with tools and libraries. Active communities for these and other big data processing projects continue to innovate and benefit from each other’s advancements. We look forward to expanding this benchmark and testing newer releases of these systems as they come out. ", "date": "2015-12-16"}, {"website": "Yahoo", "title": "Data Sketches", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/135390948446/data-sketches", "abstract": "  .   Abstract   . In the analysis of big data there are often problem queries that don’t scale because they require huge compute resources to generate exact results, or don’t parallelize well. Examples include count distinct, quantiles, most frequent items, joins, matrix computations, and graph analysis. Algorithms that can produce “good enough” approximate answers for these problem queries are a required toolkit for modern analysis systems that need to process massive amounts of data quickly. For interactive queries there may not be other viable alternatives, and in the case of real-time streams, these specialized algorithms, appropriately called streaming algorithms, or sketches, are the only known solution. This methodology has helped Yahoo successfully reduce event processing times from days to hours or minutes on a number of its internal platforms. This article provides a short introduction to sketching and to  DataSketches , an open source library of a core set of these algorithms designed for large analysis systems. .   The Distinct Count Computational Challenges   .  Removing Duplicates . Suppose we have a small stream of visits to our new bookstore: {Alice, Ben, Dorothy, Alice, Ben, Dorothy, Alice, Ben}. The total count of visits is 8 and the distinct count of visitors is 3. In order to compute the distinct count exactly, the system must retain a reference to each distinct identifier it has seen so far in order to ignore the duplicates. This means that the system must reserve O(n) space, where n is the anticipated maximum number of distinct identifiers. This is straightforward if we know that the number of distinct identifiers is small. . Now extend the scale of this distinct counting challenge to streams that contain billions of identifiers with many duplicates. This is not an unrealistic scenario: Yahoo sees over a billion distinct users in a  month.  It can be even larger as our input streams often include multiple identifiers that we want to count, such as cookies, login-IDs, device-IDs, session-IDs, etc. The space required is now O(n1 + n2, …), where ni is the number of distinct identifiers of type i. .  Partitioning and Non-Additivity . The challenge becomes exacerbated when we have to partition the data by anything other than the identifier itself. Partitionings that are often dimensions of interest to the business include time, product, location or other parameters. . For example, suppose we had decided to partition the visits to our bookstore by product area and day: . We have 4 partitions each with a distinct count of 2, but simple addition of these count values across any combination of more than one of these partitions will result in the wrong distinct count due to set overlap.  The distinct count values are non-additive. This is a more sinister form of duplication because not only has the total storage requirement increased, but the duplicates across partitions cannot be removed!  It is generally not possible to partition the data by business dimensions and guarantee that the identifier sets do not overlap. . This non-additivity property eliminates the possibility of answering queries across multiple partitions by only referring to the distinct count values for each partition. Any query across multiple partitions requires an entirely new distinct count operation that would have to read from the raw data or a copy of it. This non-additive property also has an impact on the system architecture in that we cannot create the nice aggregate hypercubes of a data mart and then query only the rows that qualify some predicate and sum up the results. . In other words, exact distinct count operations do not scale well. This non-additivity property of distinct counts is generally well understood by systems engineers. However, what is less well known is that now there are advanced algorithms that help us address the scalability challenge of distinct counting. .   Sketching Algorithms   . The name “sketch”, with its allusion to an artist’s sketch, has become the popular term to describe algorithms that return “good enough” approximate answers to queries, as these algorithms typically create small summary data structures that approximately resemble the much larger stream that it processed. . Sketching is a relatively recent  development  and has evolved from a synergistic blend of theoretical mathematics, statistics and computer science. Sketching refers to a broad range of algorithms and has experienced a great deal of interest and growth since the mid-1990’s coinciding with the growth of the Internet and the need to process and analyze massive data. .  There are several common characteristics of sketches:  .   Distinct Count Sketch, High-Level View   . The first stage of a   distinct count sketching process is a transformation that gives the input data stream the property of  white noise , or equivalently, a  uniform distribution  of values. This is commonly achieved by hashing of the input distinct keys and then normalizing the result to be a uniform random value between zero and one. . The second stage of the sketch is a data structure that follows a set of  rules  for retaining a small bounded set of the hash values it receives from the transform stage. This fixed upper bound on sketch size enables straightforward memory management. . The final element of the sketch process is a set of estimator algorithms that, upon a request, examines the sketch data structure and returns a result value. This result value will be approximate but will have well established and mathematically proven error distribution bounds. . As an example of accuracy, a sketch configured to retain 16,000 values will have a relative error of less than 1.6% with a confidence of 95%. The error distribution is approximately Gaussian or bell shaped (as shown in the figure), and is independent of the size of the input stream, which can be in the many billions of distinct items. .   The DataSketch Library   .  DataSketches  is a Java software library of streaming algorithms specifically designed for the analysis of massive data. The library includes multiple high performing sketching algorithms and numerous other supporting algorithms targeted to the practical application of these advanced algorithms in real systems. . Sketch Adaptors are provided for  Hadoop Pig ,  Hadoop Hive , and  Druid . In both Hive and Druid, the adaptors are being integrated as built-in functions by the respective teams. .   Our Experience at Yahoo   . Our experience at Yahoo in using this library has had a profound impact on a number of our internal platforms that must deal with massive data. Processing times for distinct count operations have been reduced by orders-of-magnitude. The mergeability and additivity property of sketches has enabled the simplification of complex reporting systems that had many thousands of process steps down to a few dozen. And recently, Yahoo made available real-time user count metrics for  Flurry  that enabled mobile app developers to view the number of distinct users visiting their application within 15 seconds of real-time. All of this has been made possible with the DataSketches Library. ", "date": "2015-12-17"}, {"website": "Yahoo", "title": "Automated testing in Yahoo Mail", "author": [" ankitdshah-blog"], "link": "https://yahooeng.tumblr.com/post/115664629471/automated-testing-in-yahoo-mail", "abstract": "  Background:  . We blogged about the  evolution of Yahoo Mail to React + Flux and Node.js . It is important to focus on building a strong foundation when you are building a new platform and having a robust test infrastructure is a big part of the foundation. Yahoo Mail today relies on automated testing on our Continuous Integration pipeline before we deploy changes to production. We run  Cucumber  and  Waitr-Webdriver  based functional tests run across IE, Chrome and Firefox using selenium to certify our builds. Building this infrastructure gave us a lot of insight into the challenges of doing automated testing at the scale of Yahoo Mail. . Our requirements for a robust automated test infrastructure are as follows . All engineers are accountable for quality of the product and maintaining the infrastructure. So the infrastructure should be easy to understand. We want to make sure that the tests are comprehensive enough so that it gives us the confidence to push the code everyday without human intervention and we should have the ability to run the tests multiple times a day. . There are different levels and capabilities of tests we could have invested in. Based on the above requirements, we chose to focus on the following types of tests .  Unit Tests:  . We have used multiple unit testing infrastructures in Mail in the past. So going by our experience, we arrived fairly quickly at our decision to use  Mocha  as our test framework,  Karma  as our test runner and  Chai  for assertion.  We also decided to use  Sinon  to stub out external methods. .  Functional Tests:  . Now comes the interesting part. We knew that we needed to test UI thoroughly since there are chances of things breaking when code components start interacting with each other. No one can just rely on unit tests to determine whether the code will work as expected. . On the other hand, we had to be careful about what we actually end up testing as part of the functional test suite. In an application like mail, executing our tests on actual mail data could mean that the functional tests are actually executing as integration tests. It was important for us to call out this difference that functional tests should just test the functionality of the code written agnostic to the actual data. Working with actual data brings in dependencies to setup the account to a given initial state and also go over network to all our sub-systems every time. This can be time consuming and can potentially trigger false alarms. . We divided our functional tests into two categories . Component level tests focused on functionally testing a component in isolation. We would pass different props and see if the component behaved as expected. React TestUtils with Karma and PhantomJS seemed like the way to go as it was fast and easy to write. We earlier blogged about our experience with using  React TestUtils  for this. . App level tests focused on launching the entire app in the browser and testing real user scenarios. For example, Marking a mail as read. The Mail backend and APIs have their own robust automated tests, so our focus was to test the functionality of the client. So we decided to stub our data request layer using Sinon and return responses that can execute all the code paths for the given functionality. This means our test runs are very fast, the tests are reliable and predictable. . Now for the choice of framework, we narrowed down to two options. First option was to use the already familiar  Waitr Webdriver  with Cucumber. We loved this because we can write true BDD styles tests with Cucumber. We had well integrated tooling around this like support for screen capture for failing tests and running tests in parallel. On the down side, not everyone was comfortable with the relaxed syntax of  Ruby , there were plenty of hacks done to make sure the tests run consistently on Chrome, Firefox and IE. . The second option was to use  Protractor . The biggest advantage Protractor brought to the table was that we would be doing development and tests in the same language, Javascript. This would eliminate learning curve for writing and debugging tests. Protractor also speeds up tests by avoiding the need for a lot of “sleeps” and “waits” in tests. We used  chai-as-promised  for asserting promises returned by Protractor. Even though Protractor was built for testing AngularJS applications, in reality it can be used for testing any web application. It had everything that you would expect from UI testing framework. . Based on the benefits we saw and the fact that other teams in Yahoo were also going with Protractor, we chose the second option. Since we had good experience with BDD, it was easy to  set this up with Protractor  in the future if needed. .  Organizing Protractor tests:  . A typical protractor test starts to get messy very soon with promise chains. We wanted our code to be readable and maintainable. So we started creating Page Objects for the different components of our app that eliminated duplication of code and made our test files to read more like business-like expressions. With page objects and support for adding stubs for API, our protractor test started looking like this .  Smoke and Integration Tests:  . We chose to use the Protractor setup for writing smoke and integration tests as well. The only difference is that with Smoke and Integration tests we interact with actual mail data instead of using stubs. Smoke tests comprise of a collection of tests just to make sure application is able to launch and the core flows work. A core flow could be something as simple as whether clicking on compose and sending a message works. . Integration tests are the meanest, most intensive tests we want to run on actual data before the code goes to production. Using protractor for functional, smoke and integration tests meant reusing the same infrastructure and same code for running all the tests (e.g. page objects and specs) .  Running tests in the pipeline  . Having a clear separation between the various tests meant we can now configure the various stages in the automation pipeline to run the test suites. The unit tests are the lowest level tests we want to run to make sure the code units are all working as expected. We want these to be completed within 5 minutes. The functional tests make sure that the components work well together. Smoke is a quick sanity check that nothing major is broken and the Integration tests are the true gate keepers for end to end quality. . On every pull request, we would run all unit tests, functional tests and smoke tests. That gave us high confidence whenever any code is merged to master, we are keeping the quality bar high and at the same time having them finished really quickly. . Every 3 hours, we build a production candidate from master. We put this build through all the tests including integration tests to make sure that the package is completely certified end to end. . The setup also allows for deploying individual components separately because we can now pick and choose the test suite we want to run, either the entire functional test suite or just the functional tests for the given component. We also have strict code coverage thresholds, application metric checks, payload size checks, code style ( lint ) checks etc on every build before its deployed to production. . We are overall excited to see things fall into place. We have many more challenges we need to overcome. We will do a separate blog post for a deep dive into our protractor setup where we will talk about page objects, writing synchronous code, stubbing and failing the test on js exceptions . Ankit Shah (@ankitshah2787) - Yahoo Mail Engineer ", "date": "2015-04-06"}, {"website": "Yahoo", "title": "Yahoo Cloud Object Store - Object Storage at Exabyte Scale", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/116391291701/yahoo-cloud-object-store-object-storage-at", "abstract": " Yahoo stores more than 250 Billion objects and half an exabyte of perpetually durable user content such as photos, videos, email, and blog posts. Object storage at Yahoo is growing at 20-25% annually. The growth is primarily driven by mobile, images, video, and user growth. Yahoo is betting on software defined storage to scale storage cost effectively along with the durability and latency guarantees. .  Object Storage Landscape at Yahoo  . What is “object storage”? Images and photos in Flickr, Videos, and documents, spreadsheets, and presentations exchanged as Mail attachments are classic examples of “objects.” The typical quality of this class of data is “write-once-read-many.” Traditionally, Yahoo has used storage appliances for object storage. As Yahoo is increasingly becoming the guide for digital information to our users, object storage need in Yahoo is growing rapidly. Additionally, application characteristics differ in access patterns, durability and latency needs, and cost targets. To support growth cost effectively and meet the varying application needs, object storage in Yahoo requires different tradeoffs. We need the flexibility offered by software defined storage to deliver these tradeoffs. .  Why Software Defined Storage?  . Key benefits of software defined storage are: . Cloud Object Store (COS) is Yahoo’s commodity hardware based software defined storage solution. In partnership with Flickr we have completed a multi-petabyte initial deployment of COS. And, in 2015, we plan to offer COS as a multi-tenant hosted service, and grow COS by ten-fold to support Flickr, Yahoo Mail and Tumblr. That is 100s of petabytes of storage to be supported on COS. .  Under the Hood  . COS is deployed using Ceph storage technology. We evaluated open-source solutions such as Swift and Ceph, as well as commercial solutions.  We chose Ceph because it enables consolidation of storage tiers for Object, Block, and File with inherent architectural support. Also, being an open-source product, Ceph provides the flexibility needed to customize for Yahoo needs.   .  Deployment Architecture  . COS deployment consists of modular Ceph clusters with each Ceph cluster treated as a pod. Multiple such Ceph clusters deployed simultaneously form a COS “supercluster” as shown in Fig 1. Objects are uniformly distributed across all the clusters in a supercluster. We use a proprietary hashing mechanism to distribute objects. The hashing algorithm is implemented in a client library embedded in the applications.   . After several trial runs and software tweaks, current deployment of each Ceph cluster is approximately 3 petabytes of raw capacity to provide predictable latency both during normal operations and recovery from hardware failures. Since each cluster consists of tens of commodity servers and hundreds of disks, it is highly likely that components will fail frequently. High disk and network activity occurs during recovery due to rebalancing of objects, which in turn increases object read latency during this phase. Capping the size of each cluster allows us to limit the resource usage during recovery phases in order to adhere to latency SLAs. . Yahoo users expect their images, videos and mail attachments to be perpetually stored, and made available instantaneously from anywhere around the world. This requires high data “durability” guarantees. Durability is typically achieved in storage systems either via redundancy or encoding. Redundancy can be provided through extra copies of data or replicas. On the other hand, encoding can be provided via traditional mechanisms like simple parity, or more sophisticated mechanisms like erasure coding. Erasure coding breaks down an object into fragments and stores them across multiple disks with a few redundant pieces to tolerate multiple failures.   . The usable capacity of each cluster depends on the durability technique used. We currently employ erasure coding with each object broken down into eight data and three coding fragments. This mechanism, called 8/3 erasure coding, can tolerate up to three simultaneous server and/or disk failures with about 30% storage overhead for durability. This is much lower than the 200% overhead in case of replication.    . The two durability techniques offer different price points and latency characteristics. Replication offers lower latency but a higher cost, whereas erasure coding reduces cost (sometimes by up to 50%)  at a slightly higher latency. We can also deploy different storage media such as SSD, HDD and Shingled Magnetic Recording (SMR) drives to enable different service levels depending on the application.  . Technically, it is possible to scale a COS supercluster by adding storage needs to increase the capacity of the component clusters. However, this will lead to rebalancing of data within the component clusters, thereby creating prolonged disk and network activity and impact latency SLA. To scale COS, our preferred approach is to add COS superclusters as needed similar to adding storage farms. This approach is consistent with our current appliance-based storage solution that applications are already familiar with. .  Latency Optimizations  . COS is in the serving path for many Yahoo applications and has to guarantee latency SLAs to ensure consistent high quality of user experience. We have implemented over 40 optimizations in Ceph to realize 50% improvement on average, and 70% improvement in 99.99% latency. Fig 2 depicts the latency chart before and after the optimizations under normal operations. The latencies in this chart are measured across objects of different sizes in the Flickr workload. . Some of the major optimizations are: .  Future Development  . So far, we have tuned COS to a large Yahoo use-case, namely Flickr. However, other Yahoo use cases require object storage with different workload patterns and different tradeoffs. To make COS a widely used platform at Yahoo, we are addressing several enhancements in near to mid term. . By Narayan P.P.S, Sambit Samal, Satheesh Nanniyur  ", "date": "2015-04-14"}, {"website": "Yahoo", "title": "SquiDB, a SQLite database layer for Android", "author": [" sbosley88"], "link": "https://yahooeng.tumblr.com/post/116923604061/squidb-a-sqlite-database-layer-for-android", "abstract": " We are pleased to announce  SquiDB , a SQLite database layer for Android. It is designed to make it as easy as possible to work with SQLite databases  while still enabling the power and flexibility of raw SQL. SquiDB combines features of an ORM with object-oriented SQL statement builders to simplify  reading and writing your data without messy and complicated SQL strings in your Java code. It also includes built in tools and hooks to help you write  database migrations as well as implement ContentProviders. . Like most ORMs, SquiDB represents rows in your SQLite tables as objects. Unlike some other ORMs, SquiDB uses compile time code generation to let you  define your models/table schemas as minimally as possible – the actual code you will work with is generated at compile time. A DatabaseDao object  mediates reading and writing these objects from the database. Setting up all these components is quick and easy. For example: . In addition to defining getters and setters for all the columns, the generated model class also defines constant fields you can reference for constructing queries: . The example is simple, but SquiDB’s query object and associated classes support almost the entire SQL grammar. It is much cleaner and easier to maintain, particularly for complex queries: . The above example with strings uses the  ?  character as placeholders for arguments to the statement. The values of these arguments are  placed in a separate  String[] . Users of Android’s SQLiteDatabase will recognize this as the pattern used by many of its methods,  including query methods. This is good practice, but it makes the code harder to read and necessitates that extra string array for the arguments. SquiDB inserts those placeholders for you when compiling the  Query  object and binds the arguments automatically at  query time. The raw SQL version is also prone to errors when updating the SQL adds, removes, or changes the contents of  sqlArgs .  You must always count the number of  ? s to find the appropriate argument in the array; for large and complex queries, this can  be difficult. SquiDB’s  Query  object makes it a non-issue and also prevents several classes of typos – you won’t ever mistype a  keyword or forget a space character somewhere. . Furthermore, it becomes easier to build/compose queries or SQL clauses as objects: .  DatabaseDao  can return either single rows of data represented by model objects, or a  SquidCursor  parametrized by a model type: . Model objects are designed to be efficiently reusable, so iterating through the cursor and inflating model objects to work with is cheap if you don’t need the row data to live outside of the loop: .  SquidCursor  is an instance of Android’s  CursorWrapper , so you can use one anywhere a standard Android Cursor is expected. It also provides users a typesafe  get()  method that can work directly with table columns if you don’t want or need to inflate a full model object: . These are simple examples that only use a single table, but it’s still easy to work with model objects even if you need to join across multiple tables. . We’ve shown several simple examples here, but there’s a lot that SquiDB can do to make more complicated use cases easy too – it can help you work with SQL  views using model objects, write database migrations, implement flexible  ContentProviders  backed by your SQLite database, and more. For a more in-depth look  at all you can do with SquiDB, check out our  wiki , or to clone and start working with SquiDB go to our  GitHub repo . SquiDB is being actively maintained and developed, so we welcome  feedback and contributions! . By Sam Bosley and Jonathan Koren ", "date": "2015-04-20"}, {"website": "Yahoo", "title": "Web Security: Introducing CSPTESTER.IO - A quick way to test and learn CSP on modern browsers", "author": [" prbinu"], "link": "https://yahooeng.tumblr.com/post/117515291106/web-security-introducing-csptester-io-a-quick", "abstract": " Content Security Policy (CSP) is an additional layer of security protection that can significantly reduce the risk and impact of web injection attacks like XSS on modern browsers. At Yahoo we are serious with enabling CSP on all major properties and have made significant progress towards that goal. Setting the CSP policy and fine tuning it is a challenge because of feature and implementation disparities between versions or browsers.  csptester.io  is a tool to test policy behavior across multiple browsers, learn CSP and understand disparities. .  What is CSPTESTER.IO?  .  csptester.io  is a Node.js-based web app that can frame a user’s HTML content and allow them to test CSP policies in a browser of their choice to see what fails/works. You may optionally even try XSS attacks against your code. .  Features  .  What’s more?  .  csp-validator.js  is a phantomjs based command-line script to validate CSP policy for the given URL. You may find this script useful during the web application build/CICD (integration testing) phase to validate CSP policy to make sure your web page complies with the defined policy, before it gets deployed to production. . We’re pleased to share  csptester  with the community, and collaborate with others on this project. If you’d like to start contributing, don’t hesitate to fork our repo or open an issue at  https://github.com/yahoo/csptester  . Binu Ramakrishnan (@securitysauce) - Security Engineer, Yahoo Mail ", "date": "2015-04-27"}, {"website": "Yahoo", "title": "Registry In A Box", "author": [" bengl"], "link": "https://yahooeng.tumblr.com/post/118110129916/registry-in-a-box", "abstract": "  NodeConf  2014 was fantastic! I love the format, and the location. It was my first NodeConf, and I enjoyed every bit of it. . Except the Internet connection. . While the WiFi itself was fine, with hundreds of us trying to download packages from the npm registry at the same time, the tiny Internet connection at Walker Creek Ranch left something to be desired. Some folks set up their own registry mirrors on the local network, which was great, but that meant finding their URLs on whiteboards here and there. .   reginabox   is a solution to this, providing  discoverable  npm registry mirrors for situations like NodeConf, where the network is great, but the Internet bandwidth is limited. . If you’re going to NodeConf 2015 at Walker Creek Ranch, you should install  reginabox  on the machine that you’ll be bringing: .  $ [sudo] npm i -g reginabox  . Then, once you’re at the event, you can run this to discover local npm registry mirrors: .  $ reginabox discover  . This will list available mirrors and how up-to-date they are. Then, once you’ve chosen a suitable one, you can set it as your registry, and you’re all set: .  $ npm config set registry &lt;url&gt;  . That’s all there is to it! We’ll be bringing at least one dedicated mirror machine, but if you’d like to provide your own, keep reading. The process is pretty simple.  . You’ll want to get this started a few days  before  the day you leave for NodeConf, as the mirroring process can take  days . Get yourself a big hard drive (2TB should be sufficient to handle the registry for the foreseeable future), and a machine to host it, with WiFi. You’ll need a case-sensitive filesystem on the drive. Once your OS installed and the hard drive is mounted, install  node.js  and  reginabox  as above. . Then, in a directory that’s on the hard drive, do this: .  $ reginabox mirror  . This will kick off the mirroring process. It’s running   registry-static   behind the scenes to download the entire registry to disk and keep up to date with changes as long as it’s running. It also runs an HTTP server that serves up all the tarballs and metadata, and acts as a registry server, which is exposed on the network via mDNS, so that it’s discoverable by  reginabox discover . . Make sure you run this right up until the time you leave for the event, and then start running it again as soon as you get there, so that you minimize the amount of data that needs to be transferred using the event’s Internet connection. It’s probably best to wrap it up in some kind of process monitoring like  upstart  or  monit , as you’ll pretty much always want this to be running. . I hope this helps us use the Internet at NodeConf more efficiently this year. Everything is on  github  so feel free to open issues or pull-requests. See you all at Walker Creek Ranch! ", "date": "2015-05-04"}, {"website": "Yahoo", "title": "Webbing, a Template for Firefox OS Apps", "author": [" greyson-p"], "link": "https://yahooeng.tumblr.com/post/120453453401/webbing-a-template-for-firefox-os-apps", "abstract": "  Today we are releasing  Webbing , a template for creating Firefox OS apps from existing web applications. Firefox OS is built upon standard web technologies, which makes it easy for developers to rapidly create apps for the platform. Many companies, like Yahoo, already have existing web applications that would work well on the platform with little modification. We created Webbing as a way to make the transition from a mobile web app to Firefox OS as painless as possible.  .  Ideally, when creating an app for Firefox OS, you would build it from the ground up specifically for the platform. However, this isn’t always possible with limited resources. A more expedient way to port a web app would be to create a Firefox OS app that simply redirects to your web site. This would have a lot of benefits, such as a short development time and complete code reuse with your mobile site. However, you would likely face several issues:  .  Creating a Firefox OS-specific version of your site that had extra navigation controls would fix the first problem, but not the others. Plus, these changes would take time and could complicate site maintenance. When creating the Yahoo Firefox OS apps, we wanted to avoid those issues by restricting changes to the client whenever possible. Through investigating client-side solutions, we developed Webbing. .  Webbing serves as a wrapper for your mobile site. It embeds your site in an iframe, allowing it to add extra features from the outside. Most importantly, this allows Webbing to display custom back navigation over the app.   .    . Normally there is no back navigation on the login screen. Webbing added the back button on the bottom. .  While the back navigation could be shown all the time, that likely isn’t what you want. That’s why Webbing offers the ability to specify URL patterns of pages where you want to show or hide the navigation. For the Yahoo! app, that included the login screen (where no back navigation is present) and external sites.  .  If you’re familiar with iframes, you know that normally there is no way to get the URL of the actively-displayed page. To achieve this, we took advantage of Firefox OS’s Browser API. The  Browser API  effectively adds additional features to an iframe: namely, you can retrieve the currently displayed URL and listen to a long list of events, such as URL changes and page load completion.   .  In order to make your mobile site feel more like a native app, Webbing also adds a configurable loading screen and spinner. This will help keep the user’s attention better than a white screen while your page loads. Also, Webbing will show a custom notification if there happens to be no network connection, as opposed to the stock notification that the user would see if they were in the web browser.  .  Finally, Webbing sets up L20n, Mozilla’s localization framework. By default, Webbing includes translations in both English and Spanish.  .  Webbing still has areas for improvement. Overall, we’d like to make customization easier, whether that be having more options for navigation overlays or new built-in styles of loading screens. Still, we feel that Webbing will greatly ease your transition from taking a mobile website and launching it on Firefox OS.   .  If you’d like to use Webbing, visit the  Github page . If you’d like to get started with general Firefox OS development, check out Mozilla’s  Quick Start  documentation.  ", "date": "2015-06-01"}, {"website": "Yahoo", "title": "Sherpa Scales New Heights", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/120730204806/sherpa-scales-new-heights", "abstract": "  Overview  . Sherpa is Yahoo’s distributed NoSQL key-value store offered as a hosted service. It is the cornerstone of Yahoo properties and hosts over 2,000 tables, 1 trillion records, and serves over 1 million queries a second across a wide spectrum of applications. . Though core features described in prior publications (see References section) have solidly served applications over the years, we have constantly strived to evolve Sherpa to meet new requirements. Today, web properties have to deliver sub-second responsiveness with engaging experiences, application needs have diverged across durability and latency requirements, and scale has increased multifold due to user growth and mobile pervasiveness. . To address these needs, cost effectively, we have introduced differentiated service tiers to meet the varying durability and latency requirements. We have also adopted  Log Structured Merge (LSM)  storage based on  RocksDB  and devised a new serving stack compatible with existing applications using  Nginx  HTTP server for REST API support and  ZeroMQ  for parallel access to storage. .  Key Drivers  . The following key drivers influenced the evolution of Sherpa in the last 2-3 years: . Other requirements such as 100% uptime, horizontal scalability, ease of on-boarding and application development, geo-replication and modular architecture are assumed for Sherpa.  .  Differentiated Services  . Traditionally, Sherpa offered a single service tier optimized for durability and latency. As application needs bifurcated across durability and latency requirements, it was more effective to introduce differentiated service tiers. To enable this, we introduced a separate service tier optimized for latency while retaining the traditional service tier for higher durability. The characteristics of these services are summarized below: . Figure 1 shows the relative read/write latencies of the two Sherpa  services including network round trip. As observed in this chart, Sherpa  LL provides significantly lower latency bounds at 99% for latency  sensitive applications.  . Figure 2 shows the TCO benchmarking of  Sherpa LL with a popular public cloud service. We have chosen two record  sizes, 1.5KB and 4KB, for this benchmark. As the record size increases,  Sherpa cost advantage over public cloud services is even more  pronounced. .  Architecture Update  . Sherpa architecture has been discussed in prior publications (see References section). Our solution to reduce latency while lowering cost was based on optimizing the storage node - the component of Sherpa that consists of distributed database implementation and the underlying embedded storage engine. . The architecture can be logically divided into a serving layer that front-ends REST API requests, request handlers which embed Sherpa distributed database software including  query processing, sharding, consistency, failure handling, conflict resolution, etc., and the storage engine responsible for organizing data on disk.  . In the legacy architecture, the serving layer used yApache, Yahoo’s customized  Apache HTTP server  with embedded request handlers. The HTTP server and the request handlers are now decoupled. We retained an Nginx-based HTTP server for backward compatibility with REST API. The request handlers are based on ZeroMQ. We used the ZeroMQ router-dealer pattern for increased parallelism. In addition, we adopted LSM storage based on RocksDB. Each database shard, called a tablet, is stored in a column family in RocksDB. LSM storage engine reduced read latency by 50% due to use of bloom filters and Level 0 in-memory cache. However, several optimizations were required to provide stable latency SLA, especially during compaction cycles.  . Further, we introduced a proxy client in the application, which caches the mapping of a tablet to storage node and helps the application send simple Store/Retrieve queries directly to the storage unit avoiding a network hop to the Router. . For cost effectiveness, we retained storage based on magnetic media for Sherpa HD, but introduced PCIe Flash SSD for Sherpa LL to deliver lower latency. We piloted the Sherpa LL service as an in-memory database service, but with a rigorous focus on latency, including software architecture updates detailed above, we met our latency goal with PCIe Flash storage. PCIe Flash is approximately 5x cheaper than memory based on raw storage cost. Further, with PCIe Flash we can currently store 1TB of data per storage node which further reduces cost by amortizing common components over larger data density.  . With all of these optimizations, we have increased the throughput per storage node by more than 20x and reduced end to end latency to 1-2ms  to meet our application needs.   .  Future Development  . Storage needs at Yahoo are growing at a rapid pace because of our initiatives across mobile, video, native advertising and social, also known as Mavens. To support this growth, we envision several improvements to Sherpa: .  References:  . By Satheesh Nanniyur, Sherpa Product Manager ", "date": "2015-06-04"}, {"website": "Yahoo", "title": "Easier instrumentation with react-i13n", "author": [" kaesonho-blog"], "link": "https://yahooeng.tumblr.com/post/122162710056/easier-instrumentation-with-react-i13n", "abstract": "  react-i13n  is provided for all React.js users to have a performant and scalable approach to instrumentation. . Typically, you have to manually add instrumentation code throughout your application, e.g., hooking up  onClick  handlers to the links you want to track.  react-i13n  provides a simplified approach by letting you define the data model you want to track and handling the beaconing for you. .  react-i13n  is made to be pluggable, it does this by building its own  eventing system  so you can define your own  plugin  for your preferred instrumentation libraries. . Moreover, by using  react-i13n , you can define your instrumentation data with an inherit architecture, it does this by building an instrumentation tree that mirrors your applications React component hierarchy. With the  instrumentation tree  built with  react life cycle events , we can get all the information we need in memory instead of DOM manipulation, which is efficient and fast. .     .  react-i13n  is simple, flexible and performant. Please try it, add your own   plugins  and provide us feedback at  github . . By Kaeson Ho, Seth Bertalotto, Rafael Martins, Irae Carvalho ", "date": "2015-06-22"}, {"website": "Yahoo", "title": "Complementing Hadoop at Yahoo: Interactive Analytics with Druid", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/125287346011/complementing-hadoop-at-yahoo-interactive", "abstract": " Over the last decade, Yahoo has been a pioneer in the data infrastructure space and staunch supporter of the open source developer community. It has been incredible for us to witness the growth of the “big data” space and the technologies that have evolved in the ecosystem. We are especially proud of the growth of Hadoop, a project that was first developed and open sourced at Yahoo. To this day, we still run some of the world’s largest Hadoop clusters, and use it for everything from clickstream analysis to image processing and business intelligence analytics. Additionally, our developers continue to act as good open source citizens, and contribute all our Hadoop developments back to the community. While Hadoop still solves many critical problems in our business, as our needs have grown, we’ve come to realize that Hadoop is not the end all, be all solution to the entirety of our data problems. . Yahoo initially built Hadoop as an answer to a very acute pain around efficiently storing and processing large volumes of data. Ever since Yahoo open sourced Hadoop, it has become widely adopted in the technology world. However, time has taught us that when a system becomes extremely popular for solving one class of problems, its limitations in solving other problems become more apparent.  . While MapReduce is a great general solution for almost every distributed computing problem, it is not particularly optimized for certain things. Specifically, MapReduce style queries are very slow. As our data volumes grew, we faced increasing demand to make our data more accessible, both to internal users and our customers. Not all of our end users were back end analysts, and many had no prior experience with traditional analytic tools, so we wanted to build simple, interactive data applications that anyone could use to derive insights from their data.  . Initially, we attempted to power the data applications we wanted to build using both traditional and contemporary infrastructure choices, including Hadoop/Hive, relational databases, key/value stores, Spark/Shark, Impala, and many others. The solutions each have their strengths, but none of them seemed to support the full set of requirements that we had, including: . It was only after some time that we stumbled across a new, then relatively unknown project called  Druid . . Druid is a column-oriented, distributed, streaming analytics database designed for OLAP queries. The architecture blends traditional search infrastructure with database technologies and has parallels to other closed-source systems like Google’s Dremel, Powerdrill and Mesa.  Druid excels at finding exactly what it needs to scan for a query, and was built for fast aggregations over arbitrary slice-and-diced data. Combined with its high availability characteristics, and support for multi-tenant query workloads, Druid is ideal for powering interactive, user-facing, analytic applications. . Another key property we really like about Druid is its lock-free, streaming ingestion capabilities, which are useful when ingesting tens of billions of events a day. Moreover, Druid’s extensions allow it to ingest data directly not only from open source systems like  Kafka and Storm but also internal, proprietary systems, which means the technology fits nicely into our stack. Events can be explored milliseconds after they occur while providing a single consolidated view of both real-time events and historical events that occurred years in the past. . Lastly, Druid enables us to natively integrate sketches and other algorithms that we have developed. This integration allows us to maximally leverage the distributed, shared-nothing architecture that Druid provides for handling large amounts of data. . This feature set has allowed Druid to find a home in a number of areas in and around Yahoo, from executive-level dashboards to customer-facing analytics and even some analytically-powered products.  Historically, when Yahoo has found value in an open source project, we have chosen to invest resources back into the project and are currently working with the community to help push Druid’s feature development forward. . We invite you to read about some of the  great conversations  we’ve had around the developments to our Hadoop infrastructure.  If you want to learn more about Druid specifically, check out  www.druid.io  ", "date": "2015-07-28"}, {"website": "Yahoo", "title": "Simplified FE testing with Selenium", "author": [" marcelerz-blog"], "link": "https://yahooeng.tumblr.com/post/103124603756/announcing-preceptor-simplified-fe-testing", "abstract": "  . The Programmatic Innovations team is proud to announce the release of  Preceptor ,  Hodman , and  Kobold , a collection of Node modules that will simplify front-end testing by doing all the laborious and hard work that is needed to create a complete Selenium testing framework. .  Run tests - collect results and coverage-reports in one place  . Preceptor is a test-runner that knows how to run and communicate with multiple testing frameworks, including popular frameworks such as Mocha and Cucumber.Js. Preceptor is able to coordinate when, how, and what should be run, facilitating the composition of independent test-suites across testing frameworks that can be run in parallel, sequentially, or a combination thereof, each running in its own process.  . The configuration of Preceptor is easy - simply list the tasks that should be run: . Test results from each testing framework are collected, organized, and combined in one place, even when tests were run in parallel. Through an extensive list of customizable reporters, Preceptor will create a consistent output across frameworks. In addition to test results, Preceptor also collects coverage reports by automatically instrumenting your JavaScript code, aggregating the reports into one final and complete coverage report. . Since Preceptor knows how to talk to the frameworks, it is able to react to events within the testing-lifecycle, making it possible to inject code for setup, teardown, or any other code that is needed to simplify your tests. .  Selenium/WebDriver plugin  . Preceptor comes with a WebDriver  plugin  that will acquire browser sessions through commonly used Selenium services such as SauceLabs, BrowserStack, or by using binaries such as ChromeDriver, PhantomJs, or the Selenium stand-alone server, connecting sessions to a WebDriver client that can be used by the tests without having to write a line of code. The plugin is also capable of collecting browser-side coverage reports that will be mapped and combined with the server-side collected coverage data. .  Best practices for Selenium tests  . Adhering to best practices for test creation can reduce the time that is needed to maintain these tests. However, creating the infrastructure can easily use-up valuable project time that could have been spent more productively. Hodman was created to mitigate these problems by providing commonly used objects that includes page-objects to support modern single-page applications. In addition, Hodman supplies generic service-objects for accessing RESTful APIs, and storage-objects to lazily create and delete test-data when they are needed. .  Visual regression testing made easy  . Visual testing has never been easy but rather error prone and difficult. To alleviate this, we created Kobold that can be used together with Hodman to capture webpages or smaller sub-sections of a website, automatically blacking-out elements to reduce false-positives. Kobold is then able to  compare  the images to previously approved ones without having to write a line of code, producing test-results that should be expected from a testing framework. . Each of the above mentioned modules can be used independently and do not have to be used in conjunction, however, together they create a truly unique testing experience. . All-in-all,  Preceptor ,  Hodman , and  Kobold  were created to take the pain away from front-end testing. Give it a try, and see how enjoyable front-end testing can be. . Interested? . Come and  join us  when we host the  South Bay Selenium Meetup  on December 9th at our main campus in Sunnyvale. Learn how these tools came to be and how we use them to test our UI. . The Programmatic Innovations Team ", "date": "2014-11-20"}, {"website": "Yahoo", "title": "MDBM - High-speed database", "author": [" timmahnator"], "link": "https://yahooeng.tumblr.com/post/104861108931/mdbm-high-speed-database", "abstract": "    Introduction  . Back in 1979, AT&amp;T released a lightweight database engine written by Ken Thompson, called DBM ( http://en.wikipedia.org/wiki/Dbm ). In 1987 Ozan Yigit created a work-alike version, SDBM, that he released to the public domain. . The DBM family of databases has been quietly powering lots of things “under the hood” on various versions of unix. I first encountered it rebuilding sendmail rulesets on an early version of linux. . A group of programmers at SGI, including Larry McVoy, wrote a version based on SDBM, called MDBM, with the twist that it memory-mapped the data. . This is how MDBM came to Yahoo, over a decade ago, where it has also been quietly powering lots of things “under the hood”. We’ve been tinkering with it since that time, improving performance for some of our particular use cases, and adding  lots  of features (some might say too many). We’ve also added extensive documentation and tests. . And I’m proud to say that Yahoo has released our version back into the wild. . These days, all the cool kids are saying “NoSQL”, and “Zero Copy”, for high performance, but MDBM has been living it for well over a decade. Lets talk about what they mean, how they are achieved, and why you should care. . The exact definition of “NoSQL” has gotten a bit muddy these days, now including “not-only-SQL”. But at it’s core, it means optimizing the structure and interface to your DB to maximize performance for your particular application. . There are a number of things that SQL databases can do that MDBM can not. MDBM is a simple key-value store. You can search for a key, and it will return references to the associated value(s). You can store, overwrite, or append a value to a given key. The interface is minimal. You can iterate over the database, but there are no “joins”, “views” or “select” clauses, nor any relationship between tables or entities unless you explicitly create them. . So, if MDBM doesn’t have any of these features, why would you want to use it? . The API has a lot of features, but using the main functionality is very simple. Here’s a quick example in C: . Additionally, fully functional databases can be less than 1k in size. They can also be many terabytes in size (though that’s not very practical yet on current hardware). However, we do have DBs that are 10s of Gigabytes in common use, in production. . On hardware that was current several years ago, MDBM performed 15 million QPS for read/write locking, and almost 10 million QPS for partitioned locking. Both with latencies well under 5 microseconds. . Performance: (based on LevelDB benchmarks) Machine: 8 Core Intel® Xeon® CPU L5420 @ 2.50GHz . [Performance Comparison] . NOTES: These are single-process, single-thread timings. LevelDB does not support multi-process usage, and many features must be lock-protected externally. MDBM iteration (sequential read) is un-ordered. Minimal tuning was performed on all of the candidates. . How does MDBM achieve this performance? There are two important components. . Behind the scenes, Linux (and many other operating systems) keep often used parts of files in-memory via the virtual-memory subsystem. As different disk pages are needed, memory pages will be written out to disk (if they’ve changed) and discarded. Then the needed pages are read in to memory. MDBM leverages this system by explicitly telling the VM system to load (memory-map) the database file. As pages are modified, they are written out to disk, but writes can be delayed and bunched up until some threshold is reached, or the pages are needed for something else. . This means less wear-and-tear on your spinning-rust or solid-state disks, but it also makes a huge difference in performance. Disks are perhaps an order-of-magnitude (10x) slower than memory for sequential access (reading from beginning to end, or always appending to the end of a file). However, for random access (what most DBs need), disks can be 5 orders-of-magnitude (100,000 times) slower than memory. Solid state disks fare a bit better, but there’s still a huge gap. . If there is a lot of memory pressure, you can “pin” the MDBM pages so that the VM system will keep them in memory. Or, you could let the VM page parts in and out, with some performance hit. But what if your dataset is bigger than your available memory? Out of the box, MDBM can run with two (or more) levels, so you can have a “cache” MDBM that keeps frequently used items together in memory, and lets less used entries stay on-disk. You can also use “windowed-mode” where MDBM explicitly manages mapping portions in and out of memory itself (with some performance penalty). . Lets look at what used to be involved in sending a message out over the network: . Each one of these copies (and transitions) has a very noticeable performance cost. The linux kernel team spent a good amount of time and effort reducing this to: . If you’re connecting to a remote SQL DB over the network, you’re incurring these costs for the request and the response on both sides. If you’re connecting to a local service, then you can replace the driver section with a copy to userspace for the DB server.  (This completely ignores network/loopback latency, and any disk writes for the server)  . For something like LevelDB, you still have to wait to copy data for the kernel, and DMA it to the disk.  (LevelDB appends new entries to a “log” file, and squashes the various log files together as another pass over the data)  . For an MDBM in steady state, you can do a normal store with the cost of one memory copy. To avoid that extra copy, you can reserve space for a value, and update it in-place. The data will be written out to disk eventually by the VM system, but you don’t have to wait for it.  NOTE: you can explicitly flush the data to disk, but for highest performance, you should let the VM batch up changes and flush them when when there is spare I/O and cycles available.  . Because the data is explicitly mapped into memory, once you know the location of a bit of data, you can treat it like any other bit of memory on the stack or the heap. i.e. you can do something like: . MDBM allows you to use various hashing functions on a file-by-file basis, including FNV, Jenkins, and MD5. So, you can usually find a decent page distribution for your data. However, it’s hard to escape statistics, so you will end up with pages that have higher and lower occupancy than other pages. Also, if your values are not uniformly-sized, then you may have some individual DB entries that vary wildly from the average. These factors can all conspire to reduce the space efficiency of your DB. . MDBM has several ways to cope with this: . This all sounds great, but there are some costs of which you should be aware. . On clean shutdown of the machine, all of the MDBM data will be flushed to disk. However, in cases like power-failure and hardware problems, it’s possible for data to be lost, and the resulting DB to be corrupted. MDBM includes a tool to check DB consistency. However, you should always have contingencies. One way or another this is some form of redundancy… . At Yahoo, MDBM use typically falls into a few categories: . There is one other cost. Because MDBM gives you raw pointers into the DB’s data, you have to be very careful about making sure you don’t have array over-runs, invalid pointer access, or the like. Unit tests and tools like valgrind are a great help in preventing issues. (You do have unit tests, right?) . If you do run into a problem, MDBM does provide “protected mode”, where pages of the DB individually become writable only as needed. However, this comes at a noticeable performance cost, so it isn’t used in normal production. . You shouldn’t let the preceding costs scare you away, just be aware that some care is required. Redundancy is always your friend. . Yahoo has been using MDBM in production for over a decade, for things both small (a few KB) and large (10s of GB). One recent project has DBs ranging from 5MB to 10GB spread across 1500 DBs (not counting replicas) for a total dataset size of 4 Terabytes. . When I first encountered MDBM, we had scaled out what was one of the largest Oracle instances (at the time) in about every direction it could be scaled. Unfortunately, the serving side was having trouble expanding enough to meet latency requirements. The solution was a tier of partitioned (aka “sharded”), replicated, distributed copies of the data in MDBMs. . If it looks like it might be a fit for your application, take it out for a spin, and let us know how it works for you. ", "date": "2014-12-10"}, {"website": "Yahoo", "title": "Cronshot Release!", "author": [" westchase-blog"], "link": "https://yahooeng.tumblr.com/post/105960960701/cronshot-release", "abstract": "  From the minds that brought you  gifshot !  . Today we’re proud to open source  cronshot , a node module that allows you to schedule, take, alter, and store web page screenshots. . Cronshot provides an intuitive API for scheduling time-based and/or individual screenshot tasks. This makes it extremely easy to create a customized and automated screenshot solution. . Here’s a simple example that takes a screenshot of  sports.yahoo.com  and saves the screenshot to the local file system . With this release, we provide two utility libraries (middleware) that allow you to save your screenshots to the local file system and/or update your screenshots using the image manipulation library, ImageMagick: . If you would like to create your own small middlware libraries, please read our  writing your own middleware  documentation. . If you’re awesome and want to work with us on other fun projects like this, email us now at  sports-jobs@yahoo-inc.com ! . Happy Holidays! ", "date": "2014-12-23"}, {"website": "Yahoo", "title": "BookKeeper - Yahoo’s Distributed Log Storage - is an Apache Top Level Project", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/109908973316/bookkeeper-yahoos-distributed-log-storage-is", "abstract": " Apache Software Foundation recently announced that BookKeeper has been  promoted as an Apache Top Level Project. BookKeeper is near and dear to  Yahoo: it was actually created here in 2009 to provide horizontally  scalable, reliable, replicated log storage on commodity hardware. We  have been actively developing it ever since, and it was opened up for  community development in 2011. . In  this post, we’ll be sharing an inside look at BookKeeper from the  developers who’ve worked on it, and what we’ll be looking to achieve now  that it is an Apache Top Level Project.  .  Overview  . BookKeeper  is a highly scalable, reliable, replicated log storage based on  commodity hardware. BookKeeper abstracts out certain complexities, such  as replication, failure recovery, and consistency, while building web  scale applications by providing simple constructs to store and retrieve  sequential log entries.  . Logging  is a common pattern in many applications such as databases and file  systems in the form of journals, and in messaging applications in the  form of persistent queues. At scale, the logging service has to support  thousands of logs with millions of transactions per second without  compromising throughput and latency. BookKeeper was initially developed  to serve as the Write Ahead Log for the Hadoop File System (HDFS). It  was later used in our messaging system called Hedwig, for message  queues, and was eventually adopted into the Yahoo Cloud Messaging  Service discussed later in this blog.  . BookKeeper provides capabilities important for Yahoo applications  .  Architecture  . BookKeeper  abstracts out replication of sequential logs to multiple storage  servers called Bookies.  Application use a construct called “Ledger”  that supports simple operations such as create, open, add entry, and  read entry. Ledger entries are replicated by BookKeeper on a minimum  number of Bookies using a quorum protocol. The entries are striped  across Bookies to provide high read/write throughput. BookKeeper also  transparently manages high availability during Bookie failures,  connectivity loss etc.  Finally, BookKeeper’s metadata about bookies and  ledgers is maintained in ZooKeeper. . The following diagram depicts a schematic representation of how the sequential logs are maintained on a Bookie: . Ledger entries are first written to the journal and then to the BookKeeper Entry Log data structure. Writes to the journal are synchronously committed to disk for durability of entries. Journal writes are sequential and supported by high write throughput medium such as SSD.  Writes to the Entry Log data structure is cached in the File System before committing to the storage medium. In case of failures, Entry Logs are recovered from the journal.  . For fast lookup in the read path, BookKeeper maintains an index of entries. The index maps entries of the ledger in the Entry Log data structure, thus enabling fast entry lookup for high read throughput. .  BookKeeper in Yahoo Hosted Messaging Service  . Let’s take a look at an important application of BookKeeper in Yahoo. BookKeeper provides the persistent storage required for Yahoo’s multi-tenant distributed messaging service called Cloud Messaging Service (CMS). CMS is used by over 60 applications in Yahoo including mobile notifications, Weather feeds, the Gemini Ad platform, personalization platform, Homepage, storage systems such as Sherpa  etc.  . CMS provides both Best-Effort and Guaranteed message delivery. Guaranteed delivery has to be highly resilient in case of network, disk, and machine failures. CMS uses BookKeeper to store messages as a reliable queue. Additionally, BookKeeper maintains the position of each receiver in the queue to ensure at-least-once delivery. This offloads applications from maintaining queue positions, thus simplifying application development. . Motivation for CMS to use Bookkeeper: .  Looking to the Future  . BookKeeper has few challenges for the scale and reliability that we anticipate. Here are the types of improvements we think can be done: ", "date": "2015-02-02"}, {"website": "Yahoo", "title": "Kafka @ Yahoo", "author": [" hiral-p"], "link": "https://yahooeng.tumblr.com/post/109994930921/kafka-yahoo", "abstract": "  Kafka  is used by many teams across Yahoo.  The Media Analytics team uses Kafka in our real-time analytics pipeline.  Our Kafka cluster handles a peak bandwidth of more than 20Gbps (of compressed data). . In order to make it simple for our Developers and Service Engineers to maintain our Kafka clusters, we built a web-based tool that we call Kafka Manager.  This interface makes it easier to identify topics which are unevenly distributed across the cluster or have partition leaders unevenly distributed across the cluster. It supports management of multiple clusters, preferred replica election, replica re-assignment, and topic creation.  It is also great for getting a quick bird’s eye view of the cluster. . In the spirit of Kafka, we built Kafka Manager with Scala.  The web console is based on the Play Framework which interacts with an actor based in-memory model built with Akka and Apache Curator.  We’ve ported some of the utils from Apache Kafka to work with the Apache Curator framework as well.  We use Curator to inspect the state of the cluster from Zookeeper.  We also store cluster information and generated assignments in Zookeeper since we don’t expect this information to be large.  This avoided introducing another data store. . Today we open sourced  Kafka Manager .  We hope you find it as useful as we have.  As always, feedback/contributions are appreciated and bug fixes/feature requests can be made using github issues. . If you’re interested in working with us on data analytics using Storm/Kafka/Druid/Hive/Pig email us at  hiral@yahoo-inc.com  ", "date": "2015-02-03"}, {"website": "Yahoo", "title": "Announcing ember-intl: Format.JS Support for Ember Apps", "author": [" jlecomte-yahoo"], "link": "https://yahooeng.tumblr.com/post/110812216531/announcing-ember-intl-format-js-support-for-ember", "abstract": " Internationalizing your ember applications just got a whole lot easier!  The Yahoo Presentation Technologies team is excited to release ember-intl, a toolkit that allows you to seamlessly integrate  Format.JS  into your ember projects. Check it out at  formatjs.io/ember  for more information. . The ember-intl toolkit is built as an  ember-cli  addon, with custom blueprints that allow you to quickly generate your locale and formatting configurations. We’ve also made it easier to work with Format.JS in your templates files with several built-in format helpers that work with both  htmlbars  and  handlebars . . At Yahoo, we use ember extensively in over two dozen production applications (and counting). Internationalization is a big part of our strategy to provide the best possible user experience, worldwide. We’re pleased that we can share our learnings with the community, and hopefully collaborate with others on this project. If you’d like to start contributing, don’t hesitate to fork our repo or open an issue at  github.com/yahoo/ember-intl . . - Jason Mitchell ( @jsonmitchell ) and the Yahoo Presentation Technologies team - ", "date": "2015-02-12"}, {"website": "Yahoo", "title": "Cloud Bouncer - Distributed Rate Limiting at Yahoo", "author": [" yahoo"], "link": "https://yahooeng.tumblr.com/post/111288877956/cloud-bouncer-distributed-rate-limiting-at-yahoo", "abstract": " Most platforms at Yahoo serve dozens of Yahoo Properties and Apps at any given time (think Mail, Finance, Flickr etc.) and are operated as multi-tenant services on shared infrastructure for cost efficiency and on demand elasticity. As such, platforms need to protect against applications exceeding quotas, erroneously sending a flood of requests  and causing systemic outages. .   Enter Cloud Bouncer.    . Developed  and integrated into various Yahoo platforms in 2014, Cloud Bouncer now  stands as a pillar, protecting against abusive usage patterns, enforcing  quotas and bolstering the stability of our platforms. .  Overview  . Cloud  Bouncer is a distributed rate limiter, packaged as an efficient,  light-weight software suite which effortlessly plugs into any host  serving requests. Based on user-defined global policies, Cloud Bouncer  makes the decision whether an incoming request on a given node should be  served or denied. These policies can be based on any attributes, such  as reads, writes or bytes uploaded, which are counted through the  software. Despite policies being enforced at a global level, the  decision of whether to serve the request is made locally without  external communication during the request. Upon receiving the decision  from Cloud Bouncer, the application can take the relevant action.  . Here are some of the key principles which Cloud Bouncer adheres to: .  Architecture  . Cloud Bouncer is operated  through a set of user-defined policies which specify the limits  permitted on any attribute of an application. . Every  node in the cluster stores its traffic information locally and this  information is shared between nodes using gossip protocol over UDP,  which involves little communication over the wire. To keep the gossip  packet size to a minimum, the packets are optimized to send traffic  information for just the last second. This also enables Cloud Bouncer to  react quickly to traffic spikes. . Consensus  between nodes on the global traffic usage is near real-time. The above  graph illustrates a policy at 8K, where the time lag between the traffic  spike to 12K and denials commencing is negligible. More specifically  however, the system achieves consensus with a time complexity of O(log  n), as is the nature of the gossip protocol. . Cloud Bouncer’s rate limiting solution is based on a  token bucket algorithm . A token bucket is defined by: . On  receiving the application request, the library checks its local token  bucket and if there are enough tokens available to satisfy the request  the user application is returned a success. At the end of the configured  time period (generally a second), new tokens are generated and added to  the bucket as per the rate configured in the Cloud Bouncer policy.  Depending on whether the policy has been exceeded locally or globally,  Cloud Bouncer optimizes the gossip communication for efficient network  bandwidth consumption. . Some of the other features available in Cloud Bouncer include: .  Cloud Bouncer in Sherpa (Yahoo’s NoSQL Data Service)  . Its  versatility has enabled Cloud Bouncer to be adopted in a variety of  contexts, both as components within systems for generic overload  protection and in multi-tenant platforms for quota enforcement. Let’s  examine the latter use case where Cloud Bouncer has been deployed at  significant scale. .  Sherpa  is Yahoo’s NoSQL data service,  used by nearly every Yahoo Property for metadata storage. For those not  versed in its architecture, the relevant components of this multi-tenant  platform are the  Storage Units  and  Routers.   . Sherpa is  present in over 10 geographic regions around the world, hosts thousands  of user tables and serves over 40 billion requests per day with hundreds  of Routers and Storage Units per region. Each Sherpa region acts as a  separate Cloud Bouncer cluster. Every user table in Sherpa is  provisioned for a specified request throughput per region the table is  present in. Based on the provisioned rates and availability planning for  traffic failover, Sherpa computes appropriate throughput thresholds.  These values are used as the tokens generated per time period to create  two Cloud Bouncer policies per user table per region, the first at 90%  of their threshold and second at the threshold itself. When the first  policy is exceeded in Cloud Bouncer, Sherpa begins redirecting requests  to other regions to throttle the application and apply back pressure,  and when the final threshold is exceeded Sherpa denies requests. . An  iconic success story from recent weeks is the case of Yahoo Movies. The  Movies pipeline uses a table to process feeds of showtimes and other  flavors of TV data, which is ingested from Hadoop to Sherpa. A faulty  version of the software found itself without the necessary filters  working and began writing into Sherpa at 10 times their provisioned  limits. Such a spike in write throughput would have led to excessive I/O  on the Storage Units and the messaging layers, which would have  impacted other properties and applications in that region. However, in  our case since we have Cloud Bouncer, the policies implemented for the  table kicked in, and the requests for the table were rate limited across  the various routers (metrics as illustrated above).  .  Looking to the Future  . Some of the challenges we face in Cloud Bouncer surround scale and efficiency, which we look forward to solving.  . By Varad Kishore, Software Systems Development Engineer, and Aravind Sethuraman, Senior Software Apps Development Engineer  ", "date": "2015-02-17"}, {"website": "Yahoo", "title": "Announcing redislite: Python support for redis without a separate redis server.", "author": [" dwighthubbard-blog"], "link": "https://yahooeng.tumblr.com/post/114042809966/using-redis-with-your-python-applications-just-got", "abstract": " Using  redis  with your Python applications just got a whole lot easier!  The Yahoo  Python Platform team is excited to release redislite, a Python module that allows  using redis without the need to install and configure a separate redis server. . The redislite module is easy to integrate into existing Python applications that  use the existing redis module. Check it out at  https://pypi.python.org/pypi/redislite   for more information. . At Yahoo, our Python developers frequently need to create new development  environments on multiple platforms. The redislite module simplifies this process.  This speeds up our new developer on-boarding and makes it easy for developers  to have a working software stack to test on locally. . We’re pleased that we can share our learnings with the community, and collaborate  with others on this project. If you’d like to start contributing, don’t hesitate  to fork our repo or open an issue at  https://github.com/yahoo/redislite  . Dwight Hubbard (@dwighthubbard) and the Yahoo Python Platform and Openstack teams ", "date": "2015-03-19"}, {"website": "Yahoo", "title": "Various NodeJS Middleware", "author": [" vinit-sacheti-blog"], "link": "https://yahooeng.tumblr.com/post/111880429321/various-ynodejs-middleware", "abstract": " The NodeJS Platform Team has released several express/connect middleware packages which provide various useful features for NodeJS based Applications. . Here is a brief summary for each of them. . If your app is using node dns api  require('dns')  a lot and you want to cache your dns calls, this is the module. It comes with a built-in in-memory cache with various configuration params, as well as the ability to plug-in an external cache like redis, mdbm etc. . If you app need to limit certain functionality like upload-size, set timeouts from incoming &amp; outoing connections etc, this module does all that. Complete feature list includes . This module restricts application from running  process_wrap ,  child_process ,  process.kill  methods unintentionally which could harm the functioning of applications or framework. It can be configured to run certain executables only and restrict everything else. . This module restricts the file-system access for the node fs api  require('fs')  as well as restricts the  require  call to configured directoris only. This module is quite useful if your app uses third-party libraries and you want to restrict their file-system access. . This module extends the functionality of console.log and console.err by adding a process-id, url and an identification counter to the logged string. This is particulary useful where there are multiple processes and everybody writing to the same log. . This module provides a process monitoring tool which reports various process statistics. It is used inside Yahoo to make the node application Highly Available. Detailed description of this module is found at :   nodejs-high-availability . . This module watches all the nodejs processes and kills a unresponsive process. This module is used in conjunction with the monitr and is used for the  nodejs-high-availability  solution. . This module serves a status page which provides various useful information about the nodejs hosts like Total Number of Requests served, RPS, Total Data Transferred, Total Memory, Free Memory, etc. for each worker processes. . This module provides a simple apache like `/status.html’ page. ", "date": "2015-02-23"}, {"website": "Yahoo", "title": "Important Announcement Regarding YUI", "author": [" jlecomte-yahoo"], "link": "https://yahooeng.tumblr.com/post/96098168666/important-announcement-regarding-yui", "abstract": " The  Yahoo User Interface library  (YUI) has been in use at Yahoo since 2005, and was first  announced  to the public on February 13, 2006. Although it has evolved tremendously since that time, YUI has always served the same overarching purpose of providing a comprehensive toolkit to make it easier for developers to create rich web applications. As such, YUI is an important part of Yahoo’s history: millions of lines of code relying on YUI have been written and are still in use at Yahoo today. However, it has become clear to us that the industry is now headed in a new direction. As most of you know, the web platform has been undergoing a drastic transformation over the past few years. JavaScript is now more ubiquitous than ever. The emergence of Node.JS has allowed JavaScript to be used on the server side, opening the door to creating isomorphic single page applications. New package managers (npm, bower) have spurred the rise of an ecosystem of 3rd party, open source, single-purpose tools that complement each other, embracing the UNIX philosophy and enabling very complex development use cases. New build tools (Grunt and its ecosystem of plugins, Broccoli, Gulp) have made it easier to assemble those tiny modules into large, cohesive applications. New application frameworks (Backbone, React, Ember, Polymer, Angular, etc.) have helped architect web applications in a more scalable and maintainable way. New testing tools (Mocha, Casper, Karma, etc.) have lowered the barrier of entry to building a solid continuous delivery pipeline. Standard bodies (W3C, Ecma) are standardizing what the large JavaScript frameworks have brought to the table over the years, making them available natively to a larger number of devices. Finally, browser vendors are now committed to making continuous improvements to their web browsers while aligning more closely with standards. With so called “evergreen web browsers”, which are making it easier for users to run the latest stable version of a web browser, we can expect a significant reduction in the amount of variance across user agents.  The consequence of this evolution in web technologies is that large JavaScript libraries, such as YUI, have been receiving less attention from the community. Many developers today look at large JavaScript libraries as walled gardens they don’t want to be locked into. As a result, the number of YUI issues and pull requests we’ve received in the past couple of years has slowly reduced to a trickle. Most core YUI modules do not have active maintainers, relying instead on a slow stream of occasional patches from external contributors. Few reviewers still have the time to ensure that the patches submitted are reviewed quickly and thoroughly.  Therefore,  we have made the difficult decision to immediately stop all new development on YUI  in order to focus our efforts on this new technology landscape. This means that, going forward, new YUI releases will likely be few and far between, and will only contain targeted fixes that are absolutely critical to Yahoo properties.  The mission of the YUI team at Yahoo continues to be to deliver the best next-generation presentation technologies with an initial focus on internal developers. We remain optimistic about the future of web presentation technologies and are eager to continue working with the external frontend community to share and learn together.  Julien Lecomte, Director of Engineering, Yahoo Presentation Technologies ", "date": "2014-08-29"}, {"website": "Yahoo", "title": "An Animated GIF is Worth a Thousand Words", "author": [" westchase-blog"], "link": "https://yahooeng.tumblr.com/post/98817655136/an-animated-gif-is-worth-a-thousand-words", "abstract": "     . Today we are happy to open source  gifshot , a client-side JavaScript library that can create animated GIFs from media streams (e.g. webcam), videos (e.g. mp4), or images (e.g. png). Gifshot leverages cutting edge browser APIs (sorry IE9 and below) such as WebRTC, FileSystem, Video, Canvas, Web Workers, Typed Arrays, and Base 64 Encoding, to automate the GIF creation process using only client-side JavaScript. The client-side nature of the library makes it extremely portable and easy to integrate into almost any website (sort of like animated GIFs themselves). .     . Piggybacking on the idea of simplicity, we also created an easy to use API, so that you can start creating GIFs right away. Let’s take a look at an example: . For more details and examples, check out the full API documentation and our extensive list of options. . Gifshot was originally created during a Yahoo Sports team hackathon. The hackathon project allowed Yahoo Fantasy users to talk “smack” to other league members, by creating and publishing funny animated GIFs of themselves. After testing this feature internally, we soon learned what the internet has known for over 25 years; animated GIFs are fun. . The Yahoo Sports team recently released this feature to all Yahoo Fantasy private leagues (currently bucket testing), so try going to your team matchup page and clicking on the red record icon at the bottom right corner of your page to start creating your GIFs! If you aren’t in a Yahoo Fantasy league yet, make sure to not miss out on the fun and  create an account  now. . Happy GIF'ing! ", "date": "2014-09-30"}, {"website": "Yahoo", "title": "Announcing Format.JS: Internationalize your web apps on the client & server", "author": [" jlecomte-yahoo"], "link": "https://yahooeng.tumblr.com/post/100006468771/announcing-formatjs-internationalize-your-web", "abstract": "  . Today, the Yahoo Presentation Technologies team is proud to announce the launch of a new project named  Format.JS  — a modular collection of JavaScript libraries for internationalization that are focused on formatting numbers, dates, and strings. . Format.JS includes a set of core libraries that build on the JavaScript Intl built-ins and industry-wide i18n standards, a set of integrations for  Handlebars ,  React , and  Dust , plus a  guide  to internationalizing web apps.  Head to  http://formatjs.io/ , the project’s website, and start by reading the guide which covers the basics of internationalization and has details on integrating one of Format.JS’ libraries into your web app. . Traditionally at Yahoo, user interface rendering would be done on the server, giving developers access to plethora of established internationalization tools and libraries written for server-side programming languages. With the rise of single page apps (SPAs), user interfaces need to be rendered in the browser using JavaScript. This posed many challenges since JavaScript, as of ECMAScript 5.1, only has  basic internationalization features  that are not available in all commonly-used browsers.  Our team has been working closely with product teams at Yahoo to solve the common internationalization challenges when rendering the apps in JavaScript both in the browser and in Node.js. It became clear that problems were not unique to Yahoo and were primarily about formatting data and strings.  Building on the  built-in Intl APIs  in JavaScript ( ECMA-402 ), the  Unicode CLDR , and  ICU Message syntax , Format.JS has the following features with  support for over 150 languages : . We knew we wanted a solution that was at the same level in the tech stack where developers need to output data formatted in the user’s locale — it dawned on us that this happens in the apps’ view/template layers.  Format.JS has a set of high-level integrations with  Handlebars ,  React , and  Dust  .  This provides a declarative way for developers to internationalize their web apps; e.g., formatting a post’s date in Handlebars:  {{formatRelative post.date}}  which would output something like: “3 hours ago”. Instead of formatting data before you render the template, the raw data can be passed to the template and formatted on-demand when needed. This delegates the responsibility of formatting to the specialized libraries in Format.JS, instead of needing to re-implement the internationalization logic in the business logic. . Format.JS is used in production today in several different Yahoo products. Now any developer, including you, can start using it in their web apps!  Our goal is to make internationalization in JavaScript better. To start making progress on this goal, we’ve  open sourced all aspects of Format.JS  so that any developer can use it, and contribute back. In the future, we plan to propose additions to the built-in JavaScript Intl APIs to add features such as: string message and relative time formatting. . We’d like to give special thanks to  Norbert Lindenberg  who authored the ECMAScript Internationalization API spec., and  Andy Earnshaw  who wrote the  Intl.js Polyfill  for it. . - The Yahoo Presentation Technologies team - ", "date": "2014-10-14"}, {"website": "Yahoo", "title": "To TestUtil or not to TestUtil", "author": [" scsper"], "link": "https://yahooeng.tumblr.com/post/102274727496/to-testutil-or-not-to-testutil", "abstract": " In unit and functional tests for React, the Yahoo Mail team uses the  React test utilities . The test utilities have some very helpful methods to simulate events, find rendered components, and isolate tests from each other. . I refactored the List tests to fully use these utilities. One of the lines I changed looked like this: . which I changed to use the React test utilities: . Generally, when testing React components, we use ReactTestUtils.renderIntoDocument() because it protects against test contamination by creating a  detached div  to put our components into. Since the div is detached from the body, we’re never in danger of one test getting another test’s markup. . However, for List, the test failed when I used renderIntoDocument. Our List has a dependency on the InfiniteScroll component. Inside the InfiniteScroll component, we use  offsetHeight  to determine the number of conversations present on the page. The InfiniteScroll component is never attached to the DOM because  renderIntoDocument  creates a detached DOM node. Because the node is detached,  offsetHeight  is not computed. As a result, the InfiniteScroll component thinks there is nothing to render, when in reality, there is. So, our test fails. . Since we can’t use renderIntoDocument in our List tests, we need another way to prevent test contamination. If we render directly into  document.body , we create a mess that must be cleaned up after every test case. If we don’t clean up, we compromise the integrity of the next test case by leaving old elements in the DOM. . To avoid the risk, we render the List in an iframe. . We still clean up the iframe. But, if we ever neglected to, our test integrity is still intact. . Scott Sperling . Yahoo Mail Engineer ", "date": "2014-11-10"}, {"website": "Yahoo", "title": "Evolving Yahoo Mail", "author": [" subramanyan-murali-blog-blog"], "link": "https://yahooeng.tumblr.com/post/101682875656/evolving-the-yahoo-mail", "abstract": "  . Yahoo Mail has been around since 1999. Over the years, the code base has evolved from a completely server rendered Web 1.0 app all the way to one of the largest YUI based rich internet web application today. . Last month Yahoo hosted the  React JS meetup in the Sunnyvale Campus . We had over 120 people attending and it was great to share knowledge and exchange ideas about Javascript, React, Flux etc. I gave a quick update about  Yahoo Mail’s evolution over the years  and the rationale for choosing ReactJS + Flux as the platform of choice for building the next generation Mail product. .  . This is also where the code started becoming hard to debug. Dispatched events were causing cascading reactions and fixing any issues required a lot of background about the mail code base. For a new developer, coming in and grasping the code was never easy.  . For the next generation Yahoo Mail platform, we wanted . When we were reviewing the technology to use, we evaluated  Ember JS  and  Angular JS . Both of these frameworks force you into their own suggested way of doing things. Relying on past experiences and noticing the trends in the community we also recognized that the age of large frameworks or platform libraries is over. So we evaluated libraries like  KnockOut ,  Durandal  and  Rivets  that can provide a foundation for our custom framework along with a few micro-libraries. In the end we decided to go with  React JS  and  Flux  because of the following reasons  .  . The “one way data flow” appealed to all developers in the team as a neat way to think about a UI application interactions. This also felt like a good solution to make debugging and understand data flows much easier and predictable. With React JS, we have the luxury of just one language in our code base across the client and the server. The Virtual DOM implementation on React JS makes it easier to render the UI on the server using Node.js.  . Though we are aware that not everything can be thought of as a one way data flow and we are cautious not to over engineer the interaction between Action-Creators and Stores. We will post more on that later. . The slides for the talk I presented at the meet up are below  .     . Link:  Yahoo Mail moving to React  . You can meet the Yahoo Mail team at the  next React JS meetup   . Subramanyan Murali, Engineering Manager, Yahoo Mail ", "date": "2014-11-03"}, {"website": "Yahoo", "title": "Using CSS For More Than Styling", "author": [" imbrianj"], "link": "https://yahooeng.tumblr.com/post/67579656567/using-css-for-more-than-styling", "abstract": " Remember when we used to all have pretty W3C badges to prove to our users that we adhered to code standards? We proudly proclaimed “My site is valid HTML 4.01 Strict!”  . Great - we’ve moved our trophy case from the footers of all our pages to the more nerdy areas. But is this all that’s changed? We have tests that make sure you don’t have syntax errors, you’re not polluting the global namespace and all kinds of assertions specific to our applications. But I’ve found the use of any markup validators to be far less common now. We’re testing every bit of code except the ones that we tested exclusively in the past. Maybe we’re just so confident that we’ve got it right after all these years. . I doubt it. . I’m still very interested in good quality (X)HTML and have been working to make getting a quick smoke test for pages as painless as possible. Thankfully, we have an incredibly powerful tool that can be used to check for common errors. The portability of CSS3 and it’s great additions for selectors makes it an excellent tool for testing. With help from more people than I can list, I’ve gotten together a list of CSS selectors that check for things that are clearly out of spec, things that look a little fishy and a few things that might be ok - but you should double check that you’re using them right. .  . Did you know you can have a class name with a “.” in it? It’s not valid, it’ll cause all kinds of issues, but it’ll work in most browsers. Did you know that if you left your image’s “src” attribute empty, some browsers will try and grab the page again, causing performance issues? Did you know that you can’t have a number as a leading character in a class or ID? DebugCSS will look for these coding errors, but also look for issues such as over zealous use of the &lt;br&gt; tag (and offer the suggestion that you should use a paragraph tag instead) - or politely ask if that &lt;table&gt; with a single &lt;tr&gt; is being used for layout. .  . Wait, what? Everybody knows that Javascript runs in browsers - and with the advent of node.js, also on the server. With it, we’re also able to run CSS on the server. The use case for this is to run assertions to say “if this selector exists, do something.” That “something” can be logging a warning to your console or asserting an error to trigger a build failure. .  . Please do! They are both great validators and the more testing done for valid markup, the happier I’ll be. They also will fill in areas that debugCSS will miss. However, debugCSS also points out things that aren’t necessarily “wrong”, but are bad form or have repercussions in regard to performance or accessibility. It just has the added convenience of being able to be run from a bookmarklet, works on mobile - and adds super helpful visual cues to the problems in-page. .  . Well, I’m glad you asked. DebugCSS and Arrow (the framework it’s built into for running on the server) are entirely open source. Fork away on GitHub and send me a pull request for any neat changes you make. If you’re interested in doing this kind of thing and getting paid for it, I should let you know that  Yahoo! is hiring  - just send an email to any of the friendly purveyors of this blog if interested. ", "date": "2013-11-20"}, {"website": "Yahoo", "title": "NodeJS High Availability", "author": [" rohiniwork"], "link": "https://yahooeng.tumblr.com/post/68823943185/nodejs-high-availability", "abstract": " Do you have a NodeJS application serving production traffic? Ever wonder how Yahoo might handle this? If yes, at some point or other you must have thought it would be nice to have a clean and simple solution for high availability. . Read below for a detailed overview of how you can achieve this just like we did here at Yahoo. . Use  monitr  to start monitoring when your application gets loaded with this simple statement  monitor.start()  and stop monitoring with  monitor.stop()  when your application shuts down. . The  monitr  module creates a thread and starts monitoring the application. It looks for process status from various system files and reports the status on a socket. . The  process-watcher  module provides a listener on this socket. This listener is also responsible for sending a  SIGHUP  and  SIGKILL  if the process did not honor the performance boundaries given to the watcher. . How do we add it to an existing/new node app? . The mod_statuspage module provides a simple middleware to view each process status on the specified server. .  Example application and listener  . Run your application and watcher on separate terminals .  node ./application.js   node ./listener.js  . Once you have your application and the watcher running, you can view your application status at http://localhost:8000/status .  . Test URL to trigger watcher to kill the worker process .  http://localhost:8000/late-response  . You can see from the application that the URL ‘late-response’ deliberately engages CPU for 5 seconds. The watcher is configured to kill the worker processes that are idle more than 3 seconds. . Hence hitting this URL above will cause the watcher to send a SIGHUP to the worker process that took the request. . Make sure you check out all of our other NodeJS modules on our  Github page , they are all used in production here at Yahoo. ", "date": "2013-12-02"}, {"website": "Yahoo", "title": "Come join our team at Yahoo Media", "author": [" markpercival"], "link": "https://yahooeng.tumblr.com/post/69110311667/come-join-our-team-at-yahoo-media", "abstract": " Yahoo Media delivers delightful, inspiring and entertaining daily-habit experiences to over half a billion people worldwide. As the number one destination in news, sports, finance and entertainment, we handle more traffic than any other site in the U.S. . Delivering compelling, informative, and relevant content to so many people quickly and beautifully at scale is a huge challenge. We use everything from big data (Hadoop and other grid/cloud technologies) to cutting edge HTML5/CSS/JS to build experiences at a level of complexity few companies do. .  If you eat, sleep and breathe web technologies, if you learn new technologies like Node.js, HTML5 or CSS3 because you can’t help yourself, then we need you to come help us build the next generation of Yahoo sites.  .   The Role: Front End Engineer (Technical Yahoo)   As a Front End Software Engineer at Yahoo, you will specialize in building high performance, scalable, cross-browser compatible, accessible and elegant web applications with HTML5, CSS3, JS and other technologies.  You will work hand in hand with world-class teams of other software engineers and architects as well as backend engineers, cloud data experts and computer scientists to build amazing and emotional experiences that will delight our users. . Get in touch with Tony ( tonytam@yahoo-inc.com ), Alex ( alexnj@yahoo-inc.com ) or Mark ( mdp@yahoo-inc.com ) to get started. . Want more details? Read on! .   Responsibilities:    . Build next generation applications for desktop and mobile browsers with an emphasis on emotional experiences and constant attention to scalability, performance, accessibility and cross-browser compatibility.  . Partner with Product Management and User Experience Design to redesign and implement user interfaces. .     .  Minimum:  . You have at least two years of experience in web development and frontend engineering.  . HTML semantics and accessibility guidelines for the web are natural to you. You take pride in the markup you write and love talking about the benefits of semantic markup all around. . You are a strong follower of progressive enhancement. Stuff that you make works regardless of JavaScript being enabled, lack of modern browsers, during all seasons and weather conditions. . You have a strong command over CSS. You know how to employ the minimal amount of CSS rules to bring just about any design to life. . You are aware of browser nuances, incompatibilities and most things that can go wrong when it comes to CSS. You know the hacks, the costs and the tradeoffs to make layouts stick to your command across all browsers. . You know how to use JavaScript at times to get things done. You at least know when to use it and when not to. . You are comfortable with at least one server side technology - PHP, Node.js, for example.  You know your way around HTTP, how the web works and how that browser is able to fetch and display pages. . You love tuning performance and making things faster. . You have experience in building rich client side interactions using a combination of HTML, CSS and JavaScript. You know JavaScript, not just jQuery or YUI. . You can find your way home if left alone in Unix/Linux. . Not to mention, you are customer-focused, react well to changes, work with teams and able to multi-task on multiple products and projects. .   .  Good to have:  . Experience with server side JavaScript. . Experience with Scrum, Agile and all processes that ship software faster. . B.S. in Computer Science or equivalent (e.g. 2+ years work experience or IED/HID degree). . Still sounds good? Get in touch with Tony ( tonytam@yahoo-inc.com ), Alex ( alexnj@yahoo-inc.com ) or Mark ( mdp@yahoo-inc.com ) to get started. ", "date": "2013-12-05"}, {"website": "Yahoo", "title": "BINGO!!!", "author": [" adrianocastro"], "link": "https://yahooeng.tumblr.com/post/70390803757/bingo", "abstract": " Earlier in December, Yahoo sponsored the  Node Summit , a two-day conference in San Francisco which focused on NodeJS’ transformative role in the future of computing whilst also showcasing real-world solutions from a select group of companies working with NodeJS today. . During the event, we invited everyone to come play  Yahoo Node Bingo . . This was a challenge that consisted of writing a client that connected to a Socket.IO bingo server and listened in on numbers. Once it got bingo it would then call it back to the server. .       . Everyone who wrote a successful client and got bingo was entered into our awesome prize draw in which we gave away an Xbox One, an Apple iPad Mini and an Apple TV to the first, second and third winners, respectively. . Today we’d like to announce the lucky winners: . Congratulations to our winners and well done to everyone who played. We hope you had as much fun playing the  Yahoo Node Bingo  as we did. ", "date": "2013-12-18"}, {"website": "Yahoo", "title": "Bringing Links to Life with Link Preview", "author": [], "link": "http://yahoomail.tumblr.com/post/79395405926/bringing-links-to-life-with-link-preview", "abstract": "  By Mayukh Bhaowal, Senior Product Manager, Yahoo Mail   .  Plain blue links aren’t too exciting on their own, but they often lead to the most interesting content in an email. Now, Yahoo Mail offers a preview of that content right in your inbox.  Today, we are excited to announce that plain blue links in Mail are a thing of the past as we introduce a visual, informative link preview in Yahoo Mail for the web. .     .  . Now, when you share a link in Yahoo Mail, you will see a title, image and short snippet from that web page. For example, if you share a Flickr link, you will see a beautiful preview image, if you share a Wikipedia article, you will see a title and description of the article, and if you share an Airbnb listing, you’ll see a preview of the listing. Link preview works for articles, images and videos. .  .  .  To try it out, simply paste the link to the article, photo or video you wish to share in the email you are composing and you will see a link preview appear. The recipient can see the link preview on either webmail or on a mobile device. Recipients can even play videos right within the email message.  .     . When you paste or type in a link, it will be replaced with a descriptive title. You can easily go back by clicking on the link and deleting the display text in the Link Options. .  The link preview card is available in two sizes. By default, the larger preview card is presented, but you can quickly toggle to the smaller card by clicking on the arrow icon in the right corner of the card. You can also delete the preview card by clicking the X icon in the top right corner of the card.  .     .  . If you don’t want to use the link preview feature at all, you can turn it off in the Settings menu. . We currently support link preview for thousands of publishers and websites including Facebook, The New York Times, The Wall Street Journal, Wikipedia, and, of course, Flickr, Tumblr and all Yahoo sites. We’re adding more publishers and websites all the time, so check back often. Or, better yet, let us know which publishers you’d like us to add  here . . We hope this makes your email more fun and productive. We’re gradually rolling it out to U.S. webmail users and are looking forward to your feedback on it. ", "date": "2014-03-12", "auhtor": "Unknown"}, {"website": "Yahoo", "title": "Yahoo at Hadoop Summit, San Jose 2014", "author": [], "link": "http://yahoodevelopers.tumblr.com/post/86526442223/yahoo-at-hadoop-summit-san-jose-2014", "abstract": "  By Sumeet Singh, Sr. Director, Product Management, Hadoop   .  .  Yahoo  and  Hortonworks  are pleased to host the 7th Annual  Hadoop Summit  - the leading conference for the  Apache Hadoop  community - on June 3-5, 2014 in San Jose, California. .  . Yahoo is a major open source contributor to and one of the largest users of Apache Hadoop.  The Hadoop project is at the heart of many of Yahoo’s important business processes and we continue to make the Hadoop ecosystem stronger by working closely with key collaborators in the community to drive more users and projects to the Hadoop ecosystem. . Join us at one of the following sessions or stop by  Kiosk P9  at the Hadoop Summit to get an in-depth look at Yahoo’s Hadoop culture. .  Keynote  .  Hadoop Intelligence – Scalable Machine Learning  .  Amotz Maimon (@AmotzM) – Chief Architect  .  “This talk will cover how Yahoo is leveraging Hadoop to solve complex computational problems with a large, cross-product feature set that needs to be computed in a fast manner.  We will share challenges we face, the approaches that we’re taking to address them, and how Hadoop can be used to support these types of operations at massive scale.”  .  Track: Hadoop Driven Business  .  Day 1 (12.05 PM). Data Discovery on Hadoop – Realizing the Full Potential of Your Data  .  Thiruvel Thirumoolan (@thiruvel) – Principal Engineer  .  Sumeet Singh (@sumeetksingh) – Sr. Director of Product Management  .  “The talk describes an approach to manage data (location, schema knowledge and evolution, sharing and adhoc access with business rules based access control, and audit and compliance requirements) with an Apache Hive based solution (Hive, HCatalog, and HiveServer2).”  .  Day 1 (4.35 PM). Video Transcoding on Hadoop  .  Shital Mehta (@smcal75) – Architect, Video Platform  .  Kishore Angani (@kishore_angani) – Principal Engineer, Video Platform  .  “The talk describes the motivation, design and the challenges faced while building a cloud based transcoding service (that processes all the videos before they go online) and how a batch processing infrastructure has been used in innovative ways to build a transactional system requiring predictable response times.”    .  Track: Committer  .  Day 1 (2.35 PM). Multi-tenant Storm Service on Hadoop Grid  .  Bobby Evans – Principal Engineer, Apache Hadoop PMC, Storm PPMC, Spark Committer  .  Andy Feng (@afeng76) – Distinguished Architect, Apache Storm PPMC  .  “Multi-tenancy and security are foundational to building scalable-hosted platforms, and we have done exactly that with Apache Storm.  The talk describes our enhancements to Storm that has allowed us to build one of the largest installations of Storm in the world to offer low-latency big data platform services to entire Yahoo on the common storm clusters while sharing infrastructure components with our Hadoop platform.”  .  Day 2 (1.45 PM). Pig on Tez – Low Latency ETL with Big Data  .  Daniel Dai (@daijy)– Member of Technical Staff, Hortonworks, Apache Pig PMC  .  Rohini Palaniswamy (@rohini_aditya) – Principal Engineer, Apache Pig PMC and Oozie Committer  .  “Pig on Tez aims to make ETL faster by using Tez as the execution as it is a more natural fit for the query plan produced by Pig.  With optimized and shorter query plan graphs, Pig on Tez delivers huge performance improvements by executing the entire script within one YARN application as a single DAG and avoiding intermediate storage in HDFS. It also employs a lot of other optimizations made feasible by the Tez framework.”  .  Track: Deployment and Operations  .  Day 1 (3:25 PM). Collection of Small Tips on Further Stabilizing your Hadoop Cluster  .  Koji Noguchi (@kojinoguchi) – Apache Hadoop and Pig Committer  .  “For the first time, the maestro shares his pearls of wisdom in a public forum. Call Koji and he will tell you if you have a slow node, misconfigured node, CPU-eating jobs, or HDFS-wasting users even in the middle of the night when he pretends he is sleeping.”  .  Day 2 (12:05 PM). Hive on Apache Tez: Benchmarked at Yahoo! Scale  .  Mithun Radhakrishnan (@mithunrk), Apache HCatalog Committer  .  “At Yahoo, we’d like our low-latency use-cases to be handled within the same framework as our larger queries, if viable.  We’ve spent several months benchmarking various versions of Hive (including 0.13 on Tez), file-formats, and compression and query techniques, at scale.  Here, we present our tests, results and conclusions, alongside suggestions for real-world performance tuning.”  .  Track: Future of Hadoop  .  Day 1 (4:35 PM). Pig on Storm  .  Kapil Gupta – Principal Engineer, Cloud Platforms  .  Mridul Jain (@mridul_jain) – Senior Principal Engineer, Cloud Platforms  .  “In this talk, we propose PIG as the primary language for expressing real-time stream processing logic and provide a working prototype on Storm.  We also illustrate how legacy code written for MR in PIG, can run with minimal to no changes, on Storm.  We also propose a “Hybrid Mode” where a single PIG script can express logic for both real-time streaming and batch jobs.”  .  .  Day 2 (11:15 AM). Hadoop Rolling Upgrades - Taking Availability to the Next Level  .  Suresh Srinivas (@suresh_m_s) – Co-founder and Architect, Hortonworks, Apache Hadoop PMC  .  Jason Lowe – Senior Principal Engineer, Apache Hadoop PMC  .  “No more maintenance downtimes, coordinating with users, catch-up processing etc. for Hadoop upgrades.  The talk will describe the challenges with getting to transparent rolling upgrades, and discuss how these challenges are being addressed in both YARN and HDFS.”  .  .  Day 3 (11:50 AM). Spark-on-YARN - Empower Spark Applications on Hadoop Cluster  .  Thomas Graves – Principal Engineer, Apache Hadoop PMC and Apache Spark Committer  .  Andy Feng (@afeng76) – Distinguished Architect, Apache Storm PPMC  .  “In this talk, we will cover an effort to empower Spark applications via Spark-on-YARN. Spark-on-YARN enables Spark clusters and applications to be deployed onto your existing Hadoop hardware (without creating a separate cluster). Spark applications can then directly access Hadoop datasets on HDFS.”  .  Track: Data Science  .  Day 2 (11:15 AM) – Interactive Analytics in Human Time - Lighting Fast Analytics using a Combination of Hadoop and In-memory Computation Engines at Yahoo  .  Supreeth Rao (@supreeth_) – Technical Yahoo, Ads and Data Team  .  Sunil Gupta (@_skgupta) – Technical Yahoo, Ads and Data Team  .  “Providing interactive analytics over all of Yahoo’s advertising data across the numerable dimensions and metrics that span advertising has been a huge challenge. From getting results in a concurrent system back in under a second, to computing non-additive cardinality estimations to audience segmentation analytics, the problem space is computationally expensive and has resulted in large systems in the past. We have attempted to solve this problem in many different ways in the past, with systems built using traditional RDBMS to no-sql stores to commercial licensed distributed stores. With our current implementation, we look into how we have evolved a data tech stack that includes Hadoop and in-memory technologies.”  .  Track: Hadoop for Business Apps  .  Day 3 (11:00 AM) – Costing Your Big Data Operations  .  Sumeet Singh (@sumeetksingh) – Sr. Director of Product Management  .  Amrit Lal (@Amritasshwar) – Product Manager, Hadoop and Big Data  .  “As organizations begin to make use of large data sets, approaches to understand and manage true costs of big data will become an important facet with increasing scale of operations. Our approach explains how to calculate the total cost of ownership (TCO), develop a deeper understanding of compute and storage resources, and run the big data operations with its own P&amp;L, full transparency in costs, and with metering and billing provisions. We will illustrate the methodology with three primary deployments in the Apache Hadoop ecosystem, namely MapReduce and HDFS, HBase, and Storm due to the significance of capital investments with increasing scale in data nodes, region servers, and supervisor nodes respectively.”  .  . For public inquiries or to learn more about the opportunities with the Hadoop team at Yahoo, reach out to us at  bigdata AT yahoo-inc DOT com . .  ", "date": "2014-05-22", "auhtor": "Unknown"}, {"website": "Yahoo", "title": "The YQL Unified Console Stack", "author": [" sudocoder"], "link": "https://yahooeng.tumblr.com/post/70487852701/the-yahoo-query-language-yql-is-a-web-service", "abstract": " The   Yahoo! Query Language   (YQL) is a web service that allows developers to call different APIs using SQL-like statements. Since its launch in  October 2008 , YQL has steadily grown in usage and adoption by many Yahoo properties and even external sites. Eventually, more features and products were built to cater to the needs of developers building on top of YQL. Some features include: a form for building YQL queries, a   community repo for external YQL tables  , and a   text editor for creating YQL tables  . .  ￼￼ . Following the release of the YQL Table Editor in  October 2011 , the YQL team decided to provide a more consistent user experience. So in  August 2013 , we re-launched our product offerings as the Unified Console: . ￼   YQL Console   .   ￼￼  . ￼   YQL Table Editor   .    ￼￼ .  The new YQL Console and Editor leverage   YUI Apps  , a solid MVC framework that allows us to build modern web apps that gives us several advantages such as:     Clear separation between data (models) and presentation (views):  .  Ability to write self-contained code with the notion of containers:  .   Less hassle with testing since DOM references are restricted to each module:  .   Improved performance by only loading the modules that the user needs:  .  When a user clicks a Query Builder tab (40s), code for the QueryBuilder is loaded dynamically:  .  ￼ .  For code quality, we’re using   Arrow  , a Node.js tool to automate the execution of our unit and functional tests. Arrow sifts through our codebase looking for tests based on a regex; these tests get hosted on Arrow’s server. Arrow will then contact a Selenium box to run a browser of choice against the tests and record any errors as well as code coverage.   Istanbul   then automatically takes the data generated by Arrow and gives us a dashboard that shows how much of our code has been covered. . For performance, we’re using   YUI Compressor   to minify our assets before deploying them to Yahoo’s CDN,   MObStor  . To keep track of our assets between development and production, we use the   Yahoo Configuration Bundle   (YCB) library; YCB allows us to abstract config values between different environments. This is useful for tracking assets, because after minification and finger-printing, our assets will go from something like foo.css to foo-min-d1238f.css. . Thanks to the   built-in loader   from the YUI library, we can easily load all our optimized modules as well as their dependencies from the CDN. . For more information about YQL, please visit   http://developer.yahoo.com/yql/  . .  Links  YUI:  http://yuilibrary.com   YUI App Framework:  http://yuilibrary.com/yui/docs/app    Arrow for unit testing:  https://github.com/yahoo/arrow   Istanbul for code coverage:  https://github.com/gotwarlost/istanbul   YUI Compressor:  http://yui.github.io/yuicompressor   YCB for configuration:  https://github.com/yahoo/ycb   Mobstor CDN:  http://en.wikipedia.org/wiki/MObStor  ", "date": "2013-12-19"}, {"website": "Yahoo", "title": "The Spaghetti Problem of Low Coverage Features in Industrial Web Applications", "author": [" jarnoux"], "link": "https://yahooeng.tumblr.com/post/73209757588/the-spaghetti-problem-of-low-coverage-features-in", "abstract": " Large applications deployed globally face an engineering complexity problem that directly stems from their size. As they become older, bigger, and go through more iterations, they tend to accumulate legacy code that eventually leads to paralysis. The initial owners left, 7000 people worked on it, and nobody has its overall architecture in mind anymore. . Lately, we at Yahoo Search decided to experiment ( bucket test ) at all levels of the app, much more rapidly and nimbly than before (think every day with a team of 6 developers). Each bit/feature of the page may be served in a different flavor for a small subset of the users, which lets us see how users react to a change in our app and retain only the winning changes. Since each user sees many of those page bits/features, each user is served with a  combination  of experiments. Where then, should we write those flavors so they can be combined dynamically, yet have no dependance to one another, be easily versionable, testable, maintainable and reusable? Depending on the architecture, it can be easy to just stitch-up a feature (think if-statement) on top of the big bowl of spaghetti rather than thinking of a modular, reusable, scalable way of writing it. Here at Yahoo, we use  Mojito  and the answer lies in how a mojito app is structured. .  Mojito  is an node.js app framework that helps you structure, route, configure you webapps. A  Mojito  app is roughly a node.js package with a config file (in json or yaml) plus a set of directories, each corresponding to an independant widget - called “mojit”. Each mojit has resources (files) for each: model, view, controller, client-side assets. . That’s the base application, that’s the forest. Now, say you want to try changing the view of the search box to add a button for some users to see how they react, but you also want to keep the mainline version to be served to most of the users, so you can compare both sets of users simultaneously. You will want to change  search-apps-news/mojits/SearchBox/views/index.html . Right? . This is a recipe for a spaghetti code disaster when you have 40 experiments on that search box. Besides, the logic that decides what user should get what view should be reusable and in the app framework (not your app). If you believe that, then  mojito-dimensions-base  just became your best friend. By including that package in the package containing your experiments, you can then create “mask packages” that mimic the structure of your app  only for those files that you want to override for that experiment . . So to come back to experimenting on that extra button for some users, make a node package for your searchbox experiments that looks like this: . Done. As you can see, the  extrabutton/  directory structure mirrors your base app but only replaces one file: mojits/SearchBox/views/index.html . An experiment can involve many file substitutions, but in this case, we only need to change a single file.  application.yaml  is the config that tells your app when to trigger that “mask” (= who are the “some users”). . Et voila! If you now define  mojito-dimensions-experiment_searchbox  as a dependency of you app, the file loaded when your request matches the  extrabutton  configuration will be the one from the  extrabutton  package,  not  the baseline! You can now easily develop experiment packages that you can test and maintain outside your baseline app, activate and deactivate at will, and merge easily when you determine you have a winner. .  Jacques Arnoux (arnoux [at] yahoo-inc [dot] com) &amp; David Gomez (dgomez [at] yahoo-inc [dot] com) for Yahoo! Search Front-End Platform  ", "date": "2014-01-13"}, {"website": "Yahoo", "title": "Scheduled Rendering and Pipelining in Latency Sensitive Web Applications", "author": [" jarnoux"], "link": "https://yahooeng.tumblr.com/post/74193313158/scheduled-rendering-and-pipelining-in-latency-sensitive", "abstract": " Rendering views for a web app that has less than trivial backends can quickly become problematic as the app may end up holding onto resources while waiting for the full response from all backends. This can become a strategic pain point that monopolizes memory and blocks progress, resulting in seconds of nothing else than a blank page and an idle connection. At Yahoo Search, our dependence on several backends has forced us to become creative to step up end-to-end performance. To decrease perceived latency, even-out bandwidth usage, and free up frontend memory faster, we’ve adopted an approach similar to what Facebook detailed in  this post . . Our goal is to be able to render sections of the page as soon as the corresponding data becomes available and concurrently flush rendered sections to the client, so the total time between transmitting the first to last byte of the response is significantly shorter. The process can be roughly decomposed as follows: .  Step 1.  When a request arrives, the page is divided into small, coherent sections (called   mojits  ) and data is requested from the backends if necessary. .  Step 2.  At the same time, the frontend sends the client a ‘skeleton’ of the page containing empty placeholders to be filled by sections as they get flushed. Something like this: . Notice how the  &lt;body&gt;  tag is not closed. .  Step 3.  The backends start responding with the requested data. Once the data that a section needs arrives, the section is rendered and serialized within a  &lt;script&gt;  block, which is flushed to the client as soon as possible. Something like: .  Step 4.  The client receives the script block containing the serialized section, and executes the script, which inserts the section into its corresponding placeholder on the page. Below is the simplified pipeline.push function that is called by the executed script. .  Step 5.  Once the frontend is done rendering all the sections and there are no more  &lt;script&gt;  blocks to send, it sends the closing tags to the client and closes the connection: . This approach has several advantages. . You’re in luck: we made this stuff open-source and free (as in beer and as in speech) for our favorite Node.js app-framework at Search:  mojito . Mojito makes it easy to divide your page into sections or “mojits” (which is also useful for reusability, maintainability, and overall spiritual wellness). We made it a package called   mojito-pipeline  that you can get on github and npm, and you will be able to see how to get ninja powers with complex scheduling, dependencies between sections, conditional rules and more! .  Jacques Arnoux (arnoux [at] yahoo-inc [dot] com) &amp; Albert Jimenez (jimenez [at] yahoo-inc [dot] com) for Yahoo! Search Front-End Platform  ", "date": "2014-01-22"}, {"website": "Yahoo", "title": "Code coverage for executable Node.js scripts", "author": [" reidburke"], "link": "https://yahooeng.tumblr.com/post/75054690857/code-coverage-for-executable-node-js-scripts", "abstract": " The  YUI  team at Yahoo is serious about automated testing. YUI is a foundational part of Yahoo websites, so it’s very important that we keep quality high. Our test automation system has ran over 31 million tests in the last 6 months across over a dozen  challenging browser environments , with an average of over 188,000 tests ran every day. . We build programs with  Node.js  that help build and test YUI, such as the open-source  Yeti , our  unique test result viewer , and various small command-line utilities that assist every step of our automated testing. Of course, these test automation programs themselves have their own tests with high  code coverage  to ensure quality. . Yahoo’s  Istanbul  makes it very easy for your Node.js project to benefit from code coverage statistics—often as easy as adding  istanbul test  to your  npm test script . . If you use  Mocha , your  package.json  might look like this with Istanbul: . Using  npm test  simply runs  _mocha , but  npm test --coverage  will output handy coverage information with on-the-fly instrumentation. . You also get nice HTML reports ( example ) that let you know exactly what code you’re testing. . It’s great. You should really use Istanbul. . Normally you’d run  mocha  to run Mocha tests, but astute observers may have noticed that my  package.json  uses  _mocha  instead. That’s because  mocha  is merely a  small wrapper script  that starts the  real test script ,  _mocha . Since Istanbul works by hooking into Node’s module loader, it has no influence on the  _mocha  subprocess. So, we call  _mocha  directly, which works fine for the purposes of  npm-test(1) . . The problem of crossing process boundaries occurs when attempting to test and collect code coverage for Node scripts that are  executable scripts —the command-line interface to the rest of the program. . These should be tested like everything else, but testing them can be a challenge. The obvious way to test these scripts would be to use  child_process . . While these tests come very close to testing what an actual user would do, using child_process means that Istanbul cannot instrument the code used by the executable script. . My solution for this problem is to make the executable script as small as possible. It usually looks like this, adapted from  Yeti’s cli.js : . This file will not have code coverage reporting. But since we moved everything to the  lib/cli  module, we can now test the majority of the CLI by passing in a mock stdin, stdout, and stderr. . Instead of using  console.log  and related methods, we switch to using  stdout.write . . Here’s a simple example of the new test using mock streams: . Mocking text streams is something I do often, so I published  mock-utf8-stream  to make this easier. It’s the same code that’s been used by  Yeti ‘s own tests and now I’m using it for other projects to increase code coverage.  View it on GitHub.  Happy testing! ", "date": "2014-01-30"}, {"website": "Yahoo", "title": "How Math Helped Me Write Less Code and Made Our App Faster", "author": [" devon-rifkin"], "link": "https://yahooeng.tumblr.com/post/76324905222/how-math-helped-me-write-less-code-and-made-our-app", "abstract": " When we set out to design the first two digital magazines at Yahoo,  Yahoo Food  and  Yahoo Tech , we knew we wanted an immersive experience with the content front and center. We saw these traits in the design for  Flickr , where large images are shown in an expansive grid view: .   . We decided to build the base of our magazines around this grid, but our designers wanted to take it one step further. Just as the content in our magazines is curated, we also wanted the layouts to feel hand chosen. Our designers devised a series of options and finally we came to a new row type where there would be one large tile that seamlessly punches through two normal rows (our team has become accostomed to calling these sorts of tiles “double talls”). Here’s an example of this new row type as seen in Yahoo Tech: .   . In order to maintain the feeling of a magazine-inspired layout, we knew we couldn’t take a shortcut and programmatically crop images to fit in our layout. Instead, we had to come up with a method to perfectly size and position tiles to fit in our grid. . I knew what my goal was, but I wasn’t quite sure how I was going to end up there. So I sat down and started drawing (I actually picked up a pen!) the double tall layout on paper. Our base grid was pretty easy to reason about and come up with a simple iterative algorithm: add tiles to the row until the row is too wide and then resize the tiles to fit perfectly. But this layout was different — there were too many interdependencies between the sizes of each tile. For example, increasing the size of the big one causes the rest of the tiles to change size, but by different amounts depending on which row they’re in and their particular aspect ratios. This new layout seemed complicated enough that I wouldn’t be able to stare at it and come up with a layout algorithm in my head. . I wanted to feel like I was making progress, so I made my next goal to write as many facts about the layout as I could. I looked at my diagram and started writing down equations, until I realized I had written down enough information to obtain a closed-form solution for the layout. A few hours later, I typed out these 5 lines of code: . Don’t worry about the specific variables on the right side of the equations; the details are in the linked paper below. The important point is that the five equations represent the height of the double tall tile, the height of the bottom row, the height of the top row, the width of the double tall tile and the width of both of the subrows. With these equations solved, I could now implement the new layout using the same logic as in our existing Flickr-style layout code. . So how exactly did I get to these equations? I realized when I was sketching and writing down constraints, that I had enough constraints to solve the system of equations for all of the variables. I’ve written up this approach with a full explanation of the variables and constraints in a paper that you can find here (warning: it contains a small amount of linear algebra):  Breaking Row Boundaries in a Fully Justified Tile Layout  . Instead of manually coming up with a closed form solution, I could have avoided solving these equations altogether and expressed the constraints directly in code using a constraint-based layout system. To make this easier to explain, here’s the diagram of our new layout from the paper: .   . For example, to enforce that all tiles of a row are a given height, we would say that  a1.height  must equal  a2.height , and  a2.height  must equal  a3.height , and so on. We can even introduce other quantities like padding into the equations:  a1.left  should equal  c.right  plus  padding . Once you’ve declared all of these relationships, the constraint solver will tell you the numerical values of things like  a2.height  and  a3.left . . This is a very powerful idea, and is particularly useful when there are many subtle layout variations and edge cases. In fact, Apple added a new constraint-based layout system in iOS 6 that’s based on several papers from the  Cassowary constraint solving toolkit . . In the end, I decided that using a full constraint-based layout system would be overkill for this project. First of all, our default Flickr-like grid isn’t expressed well with constraints; we dynamically choose how many tiles to put in a row while we’re calculating the layout. A constraint-based approach would have us “guess” at different row compositions and then choose the best one. Second, we have extremely tight time budgets to calculate layout: we need our app to feel extremely responsive across a wide variety of devices and we need to lay out new rows while the user is scrolling. . As an experiment, I compared my closed-form layout solution to a constraint-based solution that used  a Javascript implementation of Cassowary . On my local iMac, the constraint-based solution was able to lay out 171 rows per second, while my closed-form solution was able to lay out 735,000 rows per second. Results like these aren’t surprising; a constraint solver uses numerical methods to solve for arbitrary constraints, while our double tall rows have one very specific set of constraints. While 171 rows per second seems like plenty, we currently have plans to dynamically perform different candidate layouts and then choose the ideal layout. By ensuring that we use a very performant layout mechanism, we’ll be able to achieve more visually pleasing results in the future. Also, a constraint system is not a small dependency: minified, the Javascript version of Cassowary is 47 kb and adds substantial complexity to our codebase. . We’re very happy with how this new type of layout looks in our apps. Go check out  Yahoo Food  and  Yahoo Tech  to see double talls in action! And if you’re interested in working on problems like these, we’re  looking for more front-end developers at Yahoo Media . ", "date": "2014-02-11"}, {"website": "Yahoo", "title": "Join Us for the 2nd Annual Hadoop Summit, Europe in Amsterdam", "author": [], "link": "http://yahoodevelopers.tumblr.com/post/78583247851/join-us-for-the-2nd-annual-hadoop-summit-europe-in", "abstract": "    .                                                              April 2-3 Amsterdam, Netherlands     .  Yahoo  and  Hortonworks  are pleased to host the  2nd Annual Hadoop Summit, Europe , the leading conference for the  Apache Hadoop  community to be held on April 2-3, 2014 in Amsterdam, Netherlands.  . The two-day event will feature many of the Apache Hadoop thought leaders who will showcase successful Hadoop use cases, share development and administration tips and tricks, and educate organizations about how best to leverage  Apache Hadoop  as a key component in their enterprise data architecture. This event will also be an excellent networking event for developers, architects, administrators, data analysts and data scientists interested in advancing and extending  Apache Hadoop . . Popular sessions include:  .  The Future of Data  . Real time stream classification with Storm . Apache Hadoop YARN : Present and Future . Making Hive Suitable for Analytics Workloads . HBase 0.96 - A report on the current status . Hive + Tez: A performance deep dive . In-memory caching in HDFS: Lower latency, same great taste . Come and listen to Yahoos present the following sessions at the Hadoop Summit this year: . Capacity Planning in Multi-tenant Hadoop Deployments - Amritashwar Lal, Sumeet Singh . Standalone Block Management Service for HDFS - Edward Bortnikov . Real-time streaming classification with Storm - Norman Huang, Jason Lin  . You can checkout the  full schedule  at the  Hadoop Summit  website. . As a host of this event, Yahoo is pleased to offer you a 20% discount off the registration fee. Enter promo code 14Disc20SponWx to receive your discount. .  Register here today!  ", "date": "2014-03-04", "auhtor": "Unknown"}, {"website": "Yahoo", "title": "Yahoo Betting on Apache Hive, Tez, and YARN", "author": [], "link": "http://yahoodevelopers.tumblr.com/post/85930551108/yahoo-betting-on-apache-hive-tez-and-yarn", "abstract": "  .  by The Hadoop Platforms Team   . Low-latency SQL queries, Business Intelligence (BI), and Data Discovery on Big Data are some of the hottest topics these days in the industry with a range of solutions coming to life lately to address them as either proprietary or open-source implementations on top of Hadoop.  Some of the popular ones talked about in the Big Data communities are  Hive ,  Presto ,  Impala ,  Shark , and  Drill . .  .  Hive’s Adoption at Yahoo  . Yahoo has traditionally used  Apache Pig , a technology developed at Yahoo in 2007, as the de facto platform for processing Big Data, accounting for well over half of all Hadoop jobs till date.  One of the primary reasons for Pig’s success at Yahoo has been its ability to express complex processing needs well through feature rich constructs and operators ideal for large-scale ETL pipelines.  Something that is not easy to express in SQL.  Researchers and engineers working on data systems built on Hadoop at the time found it an order of magnitude better than working with Java MapReduce APIs directly.  Apache Pig settled in and quickly made a place for itself among developers. . Over time and with increased adoption of the Hadoop platform across Yahoo, a SQL or SQL-like solution over Hadoop started to become necessary for adhoc analytics that Pig was not well suited for.  SQL is the most widely used language for data analysis and manipulation, and Hadoop had also started to reach beyond the data scientists and engineers to downstream analysts and reporting teams.   Apache Hive , originally developed at Facebook in 2007-2008, was a popular and scalable SQL-like solution available over Hadoop at the time that ran in batch mode on Hadoop’s MapReduce engine.  While Yahoo adopted Hive in 2010, its use remained limited. . On the other hand, MapReduce, Pig and Hive, all running on top of Hadoop, raised concerns around sharing of data among applications written using these different approaches.  Pig and MapReduce’s tight coupling with underlying data storage was also an issue in terms of managing schema and format changes.   Apache HCatalog , a table and storage management layer was conceived at Yahoo as a result in 2010 to provide a shared schema and data model for MapReduce, Pig, and Hive by providing wrappers around Hive’s metastore.  HCatalog eventually merged with the Hive project in 2013, but remained central to our effort to register all data on the platform in a common metastore, and make them discoverable and sharable with controlled access. .  .  The Need for Interactive SQL on Hadoop  . By mid 2012, the need to make SQL over Hadoop more interactive became material as specific use cases and requirements emerged.  At the same time, Yahoo had also undertaken a large effort to stabilize  Hadoop 0.23  (pre Hadoop 2.x branch) and  YARN  to roll it out at scale on all our production clusters.  YARNs value propositions were absolutely clear.  To address the interactive SQL use cases, we started exploring our options in parallel, and around the same time,  Project Stinger  got announced as a community driven project from  Hortonworks  to make Hive capable of handling a broad spectrum of SQL queries (from interactive to batch) along with extending its analytics functions and standard SQL support.  Early version of  HiveServer2  also became available to address the concurrency and security issues in connecting Hive over standard ODBC and JDBC that BI and reporting tools like  MicroStrategy  and  Tableau  needed.  We decided to stick with Hive and participate in its development and phased (Phases  I ,  II ,  III ) delivery.  At this point, Hive also happens to be one of the fastest growing products in our platform technology stack (Fig 1) confirming the fact that SQL on Hadoop is a hot topic for good reasons. .  .  .  Why Hive?  . So, why did we stick with Hive or as one may say, bet on Hive?  We did an evaluation of available solutions, and stayed the course we were on with Hive as the best solution for our users for several key reasons: .  Query Performance on Hive 0.13  . Since performance was one of users biggest concerns with Hive 0.10, the version Yahoo was running, we conducted Hive’s performance benchmarks, not to say that the significant facelift in features with later versions of Hive wasn’t important. . In one of the recent performance benchmarks Yahoo’s Hive team conducted on the Jan version of Hive 0.13, we found the query execution times dramatically better than Hive 0.10 on a 300 node cluster.  To give you an idea of the magnitude of performance difference we observed, Fig 2 shows TCP-H benchmark results with 100 GB dataset on Hive 0.10 with RCFile (Row Columnar) format on Hadoop 0.23 (MapReduce on YARN) vs. Hive 0.13 with  ORC File  (Optimized Row Columnar),  Apache Tez  on YARN,  Vectorization , and Hadoop 2.3).  Security was turned off in both cases.  With Hive 0.13, 18 out of 21 queries finished under 60 seconds with the longest still under 80 seconds.  Also, Hive 0.13 execution times were comparable or better than Shark on a 100 node cluster. .  .  . On the other hand, Hive 0.13 query execution times were not only significantly better at higher volumes of data (Fig 3 and 4) but also executed successfully without failing.  In our comparisons and observations with Shark, we saw most queries fail with the larger (10TB) dataset.  These same queries ran successfully and much faster on Hive 0.13, allowing for better scale.  This was extremely critical for us, as we needed a single query and BI solution on the Hadoop grid regardless of dataset size.  The Hive solution resonates with our users, as they do not have to worry about learning multiple technologies and discerning which solution to use when.  A common solution also results in cost and operational efficiencies from having to build, deploy, and maintain a single solution. .  .  .    Fig 4.   Hive 0.10 vs. Hive 0.13 on 10 TB of data   .  . The performance of Hive 0.13 is certainly impressive over its predecessors, but one must realize how these performance improvements came by. Several systems rely on caching data in memory to lower latency.  While this works well for some use cases, the approach fails when either the data is too large to fit in the memory or on a shared multi-tenant environment where memory resources have to be shared among tenants or users.  Hive 0.13, on the other hand, achieves comparable performance through ORC, Tez, and Vectorization (vectorized query processing) that does not suffer from the issues noted above.  On the flip side, building solutions in this manner certainly requires heavy engineering investment (100s of man month in case of Hive 0.13 since the start of 2013) for robustness and flexibility. .  .  Looking Ahead  . We are excited about the work going on in the Hive community to take Hive 0.13 to the next level in subsequent releases in terms of both features and performance, in particular the  Cost-based Query Optimizations  and the ability to perform inserts, updates, and deletes with  full ACID support . .  .  .  ", "date": "2014-05-16", "auhtor": "Unknown"}]