[
{"website": "Ebay-Engineering", "title": "Terapeak Research 2.0 - Making the Data Processing Pipeline Robust", "author": ["Kashif Usmani", "Yury Elizarov", "Khachatur  Kocharyan"], "link": "https://tech.ebayinc.com/engineering/terapeak-research-2-0-making-the-data-processing-pipeline-robust/", "abstract": "How the data processing pipeline functions in Terapeak and what factors make it fault tolerant, robust and highly available. Earlier this year in our Seller Hub , we added a new feature called Terapeak Product Research to help eBay sellers determine how to best price item listings, optimize their shipping policy and view sell-through rate and trends. Keyword search is one of the most important elements powering the Terapeak Product Research tab. With Terapeak’s search functionality, sellers can locate items of interest among buyers; gain insight into the keywords that top-selling listings feature in their titles; see keyword suggestions that may surface their listings in the search results; and help place them in the buyer’s line of sight. To provide these capabilities, a data storage must be selected, along with an indexing engine that provides full text search, performs well, has aggregation capabilities and scales well. It’s also important to make sure the delay is small between an event happening and the corresponding records of this event stored in the eBay database, making the system \"near real-time.\" Using Elasticsearch for data storage, the next step is to implement a reliable and fault tolerant, data streaming ETL (Extract Transform Load) pipeline to catch and record the marketplace events. The data pipeline has two major components: realtime and backfill data processing. The realtime pipeline processes real-time data from transaction events (when someone buys an item on eBay) and closed unsold listings events (listings that did not sell and got closed). The realtime pipeline processes real-time data from transaction events (when someone buys an item on eBay) and closed unsold listings events (listings that did not sell and got closed). Backfill pipeline is for the purposes of backfilling historical data (in case of bug fixes or retroactively adding new data points to historical data). Backfill pipeline is for the purposes of backfilling historical data (in case of bug fixes or retroactively adding new data points to historical data). The realtime pipeline processes 6-8 million transaction records and 10-15+ million closed unsold listings records per day. As the events occur in step 1, the BES Consumer (step 2) receives notifications and enriches the payloads before pushing them to Kafka (step 3). Kafka Events Processor (step 4) then loads these events into Elasticsearch (step 5). Step 3 and 4 are required so that documents can be loaded into Elasticsearch at a controlled rate so that its search performance is not impacted. The system must satisfy the following constraints: High availability: The system should be able to continue filling data in Elasticsearch despite disconnection between the components. High availability: The system should be able to continue filling data in Elasticsearch despite disconnection between the components. Failure detection: The system should be able to detect if any component stops functioning. Failure detection: The system should be able to detect if any component stops functioning. Idempotency: The system must always have the most recent version of the document. Replaying the log of events should not violate this rule. For instance, if the messages in Kafka (step 3) are reprocessed via (step 4), Elasticsearch should contain the most recent version. Idempotency: The system must always have the most recent version of the document. Replaying the log of events should not violate this rule. For instance, if the messages in Kafka (step 3) are reprocessed via (step 4), Elasticsearch should contain the most recent version. This is achieved by use of two parallel zones in different regions: Salt Lake City and Las Vegas. The BES Consumer writes the events to two Kafka clusters instead of just one, after which they end up within their respective Elasticsearch Cluster. The user traffic is directed to either of these clusters based on their geo-location. Modified Architecture: Stage 1 (Fig 1.2) Architecture makes sure eBay receives data in both clusters without addressing redundancy or fault tolerance. Modified Architecture: Stage 2 (Fig 1.3) The Kafka Events Processor (step 4) follows a primary-secondary configuration where two copies of the processor are run in each region. The first copy is called primary and writes to the Elasticsearch of the same region; the second copy is called secondary and writes to the Elasticsearch of the other region. Therefore, if one part of the pipeline in a given region is broken for some time, Elasticsearch in that region will not see a data delay. BES Consumer (step 2) and Kafka Events Processor (step 4) are both deployed on multiple machines and hence highly available by design. If the data from both primary and secondary processors is indexed in Elasticsearch, it can create unnecessary load on Elasticsearch. Therefore, data from secondary processors does not get indexed unless those documents (by id) do not exist yet (which means the primary component is falling behind) or existing documents have older versions (i.e. primary is writing stale documents). Failure Detection in Kafka Events Processor As mentioned in Fig 1.3, Kafka Events Processor runs in step 4 source-destination configurations. Each configuration runs on six hosts as a continuous batch job via an internal eBay tool called Altus. Altus is an Application Lifecycle Management system which provides the ability to create, manage and deploy applications in production. It also provides alerting and monitoring capabilities via another internal tool called SherlockIO (which uses Prometheus). The following failure scenarios are possible: A. Single Host level failure: If the host goes down, Altus will send alerts. If the host is under heavy workload (high RAM, CPU), Sherlock will send alerts. B. Record level failure: If Kafka Events Processor gets a corrupted payload that cannot be converted, it creates a new failure record that contains the original payload along with the error message and stack trace and writes it to a separate Elasticsearch index called failure index. This failure index is separately monitored. C. Kafka events processor failure: The processor itself could fail due to a variety of reasons. Introducing a mechanism in the processor where, apart from writing the main payload to Elasticsearch, metadata is also written about the entire batch of messages and mitigates failure. Example metadata document: This metadata document is written to a separate monitored index. An alert is generated when the monitor does not see any new data with a recent timestamp (calculated by “end_time” in above example) in the metadata index. The system must also act gracefully (e.g. not produce duplicate records) when the same record is sent multiple times by the events stream or one batch job fails part way, and another batch job processes the same message again. This is achieved by using listingId and transactionId as \"id\" and external versioning mechanism in Elasticsearch [1]. If a document with a given “id” is already stored, it will be accepted from primary (based on “external_gte” versioning setting) and not accepted from secondary, which uses “external_gt” versioning logic. Backfill Pipeline A backfill component is added to the pipeline to address the following cases: When BES fails to provide an event, it is missed completely. When BES fails to provide an event, it is missed completely. When there is a bug in data processing and the data in Elasticsearch needs to be corrected. When there is a bug in data processing and the data in Elasticsearch needs to be corrected. When a new data field needs to be added that will be available in new incoming data, but needs to be appended to old data in Elasticsearch retroactively. When a new data field needs to be added that will be available in new incoming data, but needs to be appended to old data in Elasticsearch retroactively. The backfill component reads the data (using Spark SQL) from Hive tables in Data Warehouse (which is a different data source than Realtime Events generator) and loads data into Swift cloud storage (Openstack), from where it is finally loaded to Elasticsearch. This allows the system to address nearly any use case, which is not addressed directly by the realtime pipeline and retroactively backfill data in Elasticsearch. Troubleshooting An important part of the system is being able to troubleshoot. This requires the system user to be able to distinguish between various data sources (primary, secondary, backfill) and data from different code versions (backfill cycles, real time code changes and milestone). Using the “origin” field as part of the Elasticsearch document addresses this need. Metrics There are two Elasticsearch clusters. Each individual cluster: is processing 20 million documents per day (roughly 250 messages per second); is processing 20 million documents per day (roughly 250 messages per second); has around 900 indices, 146 Nodes (4.3 TB of RAM, 29 TB of disk space); and has around 900 indices, 146 Nodes (4.3 TB of RAM, 29 TB of disk space); and is able to serve 200 requests per second (each request may hit different numbers of indices based on user input date range). is able to serve 200 requests per second (each request may hit different numbers of indices based on user input date range). Final Architecture", "date": "2020-08-06"},
{"website": "Ebay-Engineering", "title": "Celebrating 20 Years: eBay’s New APIs Enable Developers to Create Modern Buying and Selling Experiences ", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/celebrating-20-years-ebays-new-apis-enable-developers-to-create-modern-buying-and-selling-experiences/", "abstract": "eBay launches APIs for Managed Payments, Seller Initiated Offers, Charity and more for developers to help their businesses thrive. APIs, or application programming interfaces, at eBay are the front door to our global marketplace platform. They enable our business to expand into new contexts, allowing third-party platforms to extend their value proposition while also bringing their customers to us. We give our developers data and capabilities at scale, and in turn they invent and create fantastic buying and selling experiences for their users. Despite the challenges of this uncertain time amid the global coronavirus (COVID-19) pandemic, eBay is open for everyone. We haven’t missed a beat on our deliverables. We are fully committed to our employees, our customers, our local communities and, of course, our developer community. And, we’re also using our platform for good as our founder Pierre Omidyar envisioned when he established our marketplace in 1995. Earlier this year, we partnered with the U.K.'s National Health Service to launch an online portal to help health care providers get personal protective equipment (PPE). The solution is 100 percent based on eBay buy APIs . This illustrates the quality of the API portfolio and the great flexibility it offers. We’re extremely proud to have the opportunity to help save lives and get PPE into the right hands. This year, we are celebrating the 20th anniversary of the eBay developers program . The intent behind our first set of APIs, launched in 2000, was to help sellers manage their eBay businesses at scale. These APIs were SOAP-based APIs, and initially, API access was granted only to a limited number of licensed partners and developers. We later expanded our API portfolio to include buyer experience capabilities. Many of these APIs are still heavily used, and while they continue to bring a lot of value to both eBay and our customers, they also show their age. That is why we decided to revamp the developers program in 2016 and deliver a new family of modern and consistent RESTful APIs. We are getting ready for our managed payments global expansion, beginning on July 18. To enable our developers around the globe to properly manage their sellers’ finances and bookkeeping, we released numerous API capabilities around eBay managed payments. In addition to details about program enrollments, the Account API now allows sellers to check whether they are eligible for eBay managed payments. It also surfaces steps that sellers need to take to complete the managed payments enrollment process. Know Your Customer (KYC) is a set of regulatory and compliance requirements that any financial institution must support. Increased selling activity and other events may require sellers to provide additional data to meet these requirements, and the Account API exposes KYC capabilities to sellers. The breakdown of taxes and fees is available in the Fulfillment API when retrieving order details. We also enhanced post transaction capabilities. Through this API, developers now have a way to manage their sellers’ external, buyer-initiated payment disputes. The Finances API provides insights into financial activities on the platform for sellers enrolled in eBay managed payments. The API now supports various transaction types, including shipping labels, fees and disputes. Starting this month, final value fees will be deducted from sellers’ sales proceeds instead of being charged to sellers’ monthly invoices.​ Support for active cross-border listings for sellers enrolled in managed payments is coming in August. We also continue to deliver new sell API capabilities to help our partners’ businesses thrive: Promoted Listings Supported in France, Italy, Spain, and Canada in Marketing API - Ad campaigns help sellers to stand out from the crowd. Promoted listings show up in sponsored placements on the eBay site, including search and listing modules, and across channels: desktop, mWeb and native apps. The Marketing API now supports promoted listings in France, Italy, Spain and Canada. We’ve also increased the  maximum number of items per campaign to 50,000. Promoted Listings Supported in France, Italy, Spain, and Canada in Marketing API - Ad campaigns help sellers to stand out from the crowd. Promoted listings show up in sponsored placements on the eBay site, including search and listing modules, and across channels: desktop, mWeb and native apps. The Marketing API now supports promoted listings in France, Italy, Spain and Canada. We’ve also increased the  maximum number of items per campaign to 50,000. Customer Service Metrics in Analytics API - This capability is for sellers to retrieve metric and benchmark data as well as the overall seller rating to understand if they are meeting buyers’ customer-service expectations. Customer Service Metrics in Analytics API - This capability is for sellers to retrieve metric and benchmark data as well as the overall seller rating to understand if they are meeting buyers’ customer-service expectations. Offers to Buyers in Negotiation API - We announced the Negotiation API last year at Connect and released it in September 2019. This allows sellers to send offers with customized discounts to interested buyers. Promotions are important. Buyers want deals and transparency on savings, and sellers want sales. In some cases, like high-priced unique items, offers to buyers are optimal promotions. Offers to Buyers in Negotiation API - We announced the Negotiation API last year at Connect and released it in September 2019. This allows sellers to send offers with customized discounts to interested buyers. Promotions are important. Buyers want deals and transparency on savings, and sellers want sales. In some cases, like high-priced unique items, offers to buyers are optimal promotions. Enhanced Aspect Guidance in Taxonomy and Compliance APIs - The Compliance API now surfaces listings at risk for becoming non-compliant against eBay aspect listing policies. Also, we recently enhanced the aspect guidance in the Taxonomy API by adding the expected required date for item specifics. This is to give developers more insight into our initiative to improve the shopping experience by enriching aspects across listings. The aspect relevance indicator is coming in Q3 this year. The Taxonomy API now provides an easy way to retrieve aspect metadata across categories via a new bulk method. Enhanced Aspect Guidance in Taxonomy and Compliance APIs - The Compliance API now surfaces listings at risk for becoming non-compliant against eBay aspect listing policies. Also, we recently enhanced the aspect guidance in the Taxonomy API by adding the expected required date for item specifics. This is to give developers more insight into our initiative to improve the shopping experience by enriching aspects across listings. The aspect relevance indicator is coming in Q3 this year. The Taxonomy API now provides an easy way to retrieve aspect metadata across categories via a new bulk method. Feed API - This is a new API in our portfolio that enables sellers to upload input files and download reports and output files. Both upload and download files are processed asynchronously. Currently, supported use cases are downloading order reports and acknowledging fulfilled orders. Customer service metrics and remaining feeds and reports available via Merchant Integration Platform and Large Merchant Services will be supported in the future. The capability to schedule reports and feed tasks is coming later this year. Feed API - This is a new API in our portfolio that enables sellers to upload input files and download reports and output files. Both upload and download files are processed asynchronously. Currently, supported use cases are downloading order reports and acknowledging fulfilled orders. Customer service metrics and remaining feeds and reports available via Merchant Integration Platform and Large Merchant Services will be supported in the future. The capability to schedule reports and feed tasks is coming later this year. Auctions, Scheduled Listings and More in Inventory API - Sellers can now specify charity donation percentage when they create their offers. Support for auctions, secondary category, scheduling listings and specifying availability across warehouse locations is coming soon. Auctions, Scheduled Listings and More in Inventory API - Sellers can now specify charity donation percentage when they create their offers. Support for auctions, secondary category, scheduling listings and specifying availability across warehouse locations is coming soon. Consumer Selling API - We announced this API at eBay Connect last year and released it in October 2019. The API allows users on partner marketplaces to create eBay item drafts starting from the partner platform's information. After creating item drafts, sellers use the listing experience on eBay to finish and publish their items. The eBay consumer selling flow provides guidance and recommendations on pricing, aspects and more to help sellers optimize their listings in search results. Consumer Selling API - We announced this API at eBay Connect last year and released it in October 2019. The API allows users on partner marketplaces to create eBay item drafts starting from the partner platform's information. After creating item drafts, sellers use the listing experience on eBay to finish and publish their items. The eBay consumer selling flow provides guidance and recommendations on pricing, aspects and more to help sellers optimize their listings in search results. We have announced many enhancements across the buy APIs. Global Shipping Program import charges, coupons, auto-spelled keyword corrections, eBay guaranteed delivery and eBay Plus eligibility are enabled in Browse API . Goods and Services Tax (GST) for our Australian marketplace is supported across the Buy APIs. Deal API is a new API in our portfolio on the buy-side for affiliates and other partners interested in eBay deals and sale events, and it allows sales event discovery and retrieving items associated with such events or other deals on eBay. Our commerce API family includes capabilities that span both buy and sell activities. In addition to Taxonomy API enhancements, in April this year we released a new Charity API . This API enables buyers and sellers to discover charitable organizations supported on eBay. We are incredibly proud of our eBay for Charity program. There are 83,000 charities enrolled on our platform, and the total funds raised by eBay for Charity since 2003 has crossed $1 billion. Our marketplace users have an easy way to support their favorite charities when they buy or sell on eBay. On the RESTful API side, we have all of the capabilities now to enable sellers to sell for charities and to allow buyers to shop for charities. Translation API supports new language pairs and HTML markup in item descriptions. Translation capabilities are critical to our seller community to cross the language barrier and amplify reach. The machine translations are based on in-house trained models and state-of-the-art algorithms optimized for the ecommerce context. We continued releasing SDKs that simplify integrations with our APIs. We open-sourced these SDKs to give developers full transparency into their integrations, and we welcome contributions from our developer community. Our SDKs are consistent with the APIs and easy to use. To further simplify aspect adoption, we recently released the Taxonomy Metadata SDK that compares item specifics across two category trees. This tool goes well with the new bulk capability in the Taxonomy API . We are still a proud member of the OpenAPI Initiative . For all our new modern RESTful APIs, we have published OpenAPI documents, and we support both the 2.x and 3.x specifications. At eBay Connect 2018 , we demonstrated that integration with our read-only capabilities only takes a few minutes when leveraging contracts based on the OpenAPI specification. We suggest that developers leverage OpenAPI documents to simplify and speed up their integration with eBay APIs. Since October 2016, eBay’s Buy APIs have generated $3B* in Gross Merchandise Bought (GMB) globally. eBay serves ~150 - 250M API calls per hour. In an average week, eBay serves ~30B API calls. In Q1 of 2020, external developers used our public Sell APIs to: Create more than 500,000 new listings Create more than 500,000 new listings Manage about 2x that number Manage about 2x that number * $3B in GMB is as of Q1 2020.", "date": "2020-07-13"},
{"website": "Ebay-Engineering", "title": "The Journey to Integrating Android App Bundles", "author": ["Christopher Bowling"], "link": "https://tech.ebayinc.com/engineering/the-journey-to-integrating-android-app-bundles/", "abstract": "Over the last few years, our Native Mobile Architecture team has been reshaping our native app experience to better align with evolving mobile standards. Throughout the past four years, our app journey has evolved, adapted and accelerated in sync with today’s growing mobile ecosystem and innovations. In 2016, our Native Mobile Architecture team introduced new architecture goals and designs for our eBay app. About two years ago, Google shared new requirements about native application architecture with the release of its Android Architecture Components Library . Our app vision harmonized nicely with Google’s new stance on app architecture, paving the way for features like Dynamic Delivery , a mechanism for deploying specific features of the application independently. Earlier this year, we fully integrated Android App Bundles to help strengthen our customer experience in using our app . Our initial, native Android architecture served eBay extremely well by providing a stable foundation for features to be written on. As time passed though, we identified additional areas of opportunity for our Android architecture to further evolve, as teams focused on test automation, Continuous Integration/Continuous Delivery (CI/CD) and global infrastructure limitations. Our goals focused on modernizing the tech stack; decoupling the code; and providing for areas with low Internet bandwidth. We outlined a plan to move towards a proven, reactive-based Model-view-viewmodel (MVVM) architecture, which drew clear lines between often CPU intensive background work and UI updates that demanded seamlessness. Source: https://developer.android.com/jetpack/guide With this approach, the application will comprise independent and interchangeable modules, each rigorously tested. Highly decoupled layers will have clear boundaries, a clean API and a single responsibility. This is preferable to monolithic framework libraries that are tightly coupled. Decoupling leads to separation of concerns , resulting in code that is easier to understand; easier to transition to from an unknown application state; and easier to evolve as requirements and technologies evolve over time. This work aligned well with Google’s Application Architecture recommendation when it was released in 2018. We worked with Google architects and engineers through their various programs as we created modules, moved to Kotlin and port over existing features into modules with the new reactive-based, network-agnostic architecture. During our discussions, we noted the impacts of an increasing Android Package Kit (APK) size and the cost to customers to download large APKs. Here are some impacts of APK size: ●      For every 6 MB increase to an APK’s size, we see a decrease in the install conversion rate of 1 percent. ●      The download completion rate of an app with an APK size of around 10MB will be about 30 percent higher than an app with an APK size of 100MB. ●      Approximately 70 percent of people in emerging markets consider the size of an app before downloading it out of concerns for data cost and phone storage space. Source: “Shrinking APIs, growing installs ” An APK is a compressed file that contains all of the executable code and resources necessary to run an app on native devices. When an APK is built and delivered to devices, it contains all the possible configurations for the diverse Android ecosystem. In other words, when we build an APK, we are building for every supported Android device, from a high-powered flagship mobile to a budget-friendly device. However, the cost of this approach is extra bloat, packaged into the APK to serve all devices, resulting in increased device data usage to download and more required storage space. App Bundles solve this by requiring users to download the specific libraries and configurations needed for their device. Source: “ How Android App Bundles and Dynamic Delivery will change the way your phone gets softwar e ” To provide device-specific downloads, our builds now produce an Android Application Bundle (AAB) format. When a user downloads the eBay application, Google generates the specific APK needed for that device. The download has the specific language, native libraries, image sizes and layouts that are needed for that device's configuration and resolution. Below is what that means for our eBay application: We saw a 30 to 50 percent total disk space savings going from roughly 94 MB to 59 MB. This has a very real impact on download and install conversion for every customer. However, reducing APK size opens entire global markets where data can be expensive and users limit app installs based on download size and cost. eBay was one of the partners called out in this year at Google’s Android 11 Beta event Now that we have begun to leverage App Bundles and have decoupled our code into feature modules, we are able to take advantage of Google’s Dynamic Feature Modules. Built on top of the same delivery mechanism as App Bundling, we can now begin to craft features built for specific users and adapt automatically as their needs change. For example, if a user has limited Internet connectivity for their device, perhaps they may simply want the ability to search, view and purchase an item. Since they are not interested in heavyweight libraries, such as AR/VR, we wouldn’t have to force them to download those capabilities. Should their needs change, the features can be downloaded dynamically, in real-time at some point in the future. This will greatly reduce the initial download size of the app, providing a better global user experience regardless of device level or bandwidth. The benefits of Dynamic Delivery are enjoyed by every customer, especially given our future desire to power new machine learning data models on device. These models can be quite large and Dynamic Feature Modules will allow us to only download what is needed. The rearchitecting of the eBay native apps has been a multi-year journey. As the pieces fall into place, we have been able to accelerate development to the benefit of our eBay teams and customers. This architecture will power the future of native eBay features and create exciting possibilities for our product teams and customers. We will continue to update our progress and deep dive into other areas of the architecture in future posts.", "date": "2020-08-11"},
{"website": "Ebay-Engineering", "title": "How We Used Our Buy APIs, Catch Platform to Build a New Portal for the NHS", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/how-we-used-our-buy-apis-catch-platform-to-build-a-new-portal-for-the-nhs/", "abstract": "In partnership with the U.K. National Health Services, we leveraged our Buy APIs and Catch platform to build a new portal to deliver personal protective equipment to frontline health care workers. For 25 years, we’ve been committed to developing technology that enables opportunity at scale — in line with our founder Pierre Omidyar’s vision when he established our platform in 1995 on the basis of using tech for good. Amid the global coronavirus (COVID-19) pandemic, it’s important now more than ever to tap into our ecosystem of technology, tools and resources to support our international community. One recent global collaboration illustrates how we have realized this goal with a life-saving solution using our innovations in API technology. In March, our eBay teams partnered with the U.K. National Health Service (NHS) to launch an online portal for distribution of personal protective equipment (PPE) to frontline health care workers across the country. As a result of our joint efforts, as of July 2, more than 18.5 million PPE have been shipped through the portal to the existing 21,000 health care providers who have been invited to join the platform. When we were first contacted by the NHS, we quickly rose to the challenge. Our teams across eBay built a platform using our existing technology that was flexible and elastic enough for us to leverage for this particular project. At the end of this short period of time, we had the portal up and ready for production testing. Teams worked long hours across eBay offices in San Jose, Portland, Salt Lake City, Berlin, London, Dublin and Zurich to make this integration happen. We are all honored to be part of this project and extremely proud to have the opportunity to help save lives. Figure 1: Commerce and Buy API families This particular case study illustrates the complete flexibility and adaptability of the new API portfolio for all sorts of integrations. This specific portal is 100 percent based on our Buy APIs . APIs, or application programming interfaces, are the foundation on which today’s businesses are built. Buy APIs enable eBay to acquire new buyers, sellers and partners; grow GMB* and revenue; empower online buying from anywhere in the world; and further amplify our global reach. We launched these APIs in late 2016 to allow partners to integrate with eBay in an entirely new way and to create innovative experiences. In the case of this project, our teams were able to adapt other components of our technology and tailor them to solve the particular needs at hand. The NHS PPE portal is actually a clone of the Catch platform that eBay launched in Germany in late 2018. We reused the Catch platform and customized it to create a portal with access limited to authorized British health care providers, ensuring that the PPE items are delivered to workers on the front lines of the fight against the COVID-19. In consideration of the size and scope of each health care provider, we also instituted order quantity constraints and velocity checks. These quantity constraints were to ensure the PPE items were distributed to the front line workers that needed them. Integration with eBay is via Buy APIs. Inventory is pulled from eBay from authorized sellers, then curated and made available on the PPE portal platform. Authorized U.K. health care providers can browse for PPE inventory on the portal and submit their orders, which are then placed via buy Order API. The items are all shipped directly from eBay sellers to the health care providers. Figure 2: NHS PPE Portal integration with Buy APIs One of the most inventive challenges we had to solve for the portal was how to enable zero-cost transactions of PPE items via Buy APIs . This issue necessitated innovative solutions, especially considering that we are currently in the midst of getting ready for our Managed Payments platform to launch. Our approach was to partner with our payments engineering teams to slightly modify eBay’s internal checkout capabilities. Another challenge was to make PPE items exclusive to the NHS portal. We made sure that our infrastructure capabilities can support surfacing such items and enabling purchases in a controlled way. This year, we are celebrating 20 years of the eBay Developers Program . We launched our first APIs along with the developers program in November 2000, pioneering public API development. Though we started the journey to revamp the eBay Developers Program in 2016, our vision throughout has remained the same: to continue building a quality API portfolio for developers around the globe that enables us to expand our eBay business into new contexts. Now, two decades after we started the Developers Program, we have an adaptable API portfolio that adds value and benefits to our customers, sellers and buyers through this large, powerful ecosystem of developer applications. Our recent partnership with the NHS highlights the ease and agility with which our platform and technology can be used for a good cause and help respond to crisis situations. Another example of the agility of our API capabilities can be seen in our eBay for Charity program, in which developers can enable users to support favorite charities whenever they buy or sell on eBay. Overall, APIs are the building blocks of the digital economy and an essential part of growing a successful business. The possibilities are endless; we look forward to seeing what the imagination and ingenuity of developers help build next. * GMB is Gross Merchandise Bought", "date": "2020-07-09"},
{"website": "Ebay-Engineering", "title": "Software Quality: Elevating the Game", "author": ["Lakshimi Duraivenkatesh"], "link": "https://tech.ebayinc.com/engineering/software-quality-elevating-the-game/", "abstract": "Learn how software product quality goes well beyond simply avoiding bugs. Quality can be measured in many different ways across industries. For instance, an article’s quality can be based on several factors, including its grammar, accuracy, clarity and conciseness. In the manufacturing industry, a product’s quality can be benchmarked against its lack of deficiencies, defects and variations. In the world of software engineering, the following definitions on software quality are considered a general standard: Software functional quality reflects how well it complies with or conforms to a given design, based on functional requirements or specifications. It is the degree to which the correct software was produced. Software structural quality refers to how it meets non-functional requirements that support the delivery of the functional requirements, such as robustness or maintainability. It has a lot more to do with the degree to which the software works as needed. For software development, quality should be a foundational step in building a company’s core product. For any company, quality must be a shared understanding across product, business and engineering. It is more important to focus on whether the customer outcome and business impact were clearly delivered upon, not just releasing bug-free code. The general tendency across most organizations is to focus on testing at the end of the software development life cycle (SDLC), and spend significant time and effort to ensure a quality product is delivered. However, the final product quality is a sum total of the quality of deliverables in each of the upstream steps of the SDLC, which makes it hard to quantify and optimize. eBay is consistently at the forefront of quality in execution and delivery. To take quality to the next level, we looked at the whole lifecycle of software development and identified key factors that typically have a cascading effect: The need to accomplish everything Planning and change management Prioritization of work When looking at end-to-end (E2E) customer experiences, quality checks happen at different phases. eBay is a massive platform, with a very complicated mix of old and new technology stacks. Any sustainable process that spans E2E experiences needs to account for possible imperfect scenarios, but at the same time can’t come in the way of agility and speed. With the intention of elevating our standards and operationalizing a process-oriented approach, we started with the most obvious gating factors for quality, but with a twist. In a phased manner, we implemented a two-pronged approach: We first implemented an extremely detail-oriented Release Approval Process , which forced teams to take a step back and double-click on each step of a release life cycle, making sure all approvals/dependencies and mitigation strategies are in place. With the successful implementation and execution of the release approval process, we then instituted a Quarterly Release Planning (QRP) process at the start of each quarter to be methodical about each release, identify dependencies up ahead, and have an effective execution plan. The above two processes significantly improved our agility and speed of delivery by almost 20 percent and has provided an even better standard of quality for each release. Within the next section, I dive into the release approval process and the nuances that have led to its success. As is common across the tech sector, not every part of a company’s technology stack and features are worked on frequently, resulting in fragmented tech stacks and code bases. Despite this, we still need to deliver a quality product and a smooth customer experience every release. Simply taking a step back and looking at each release from an end-to-end perspective has helped teams deliver features with superior quality. At eBay, we do hundreds of releases a month and many interdependent subsystems and features are well separated by interfaces and services. Given that the customer experience is a cohesive end-to-end journey, with each journey possibly split into multiple different possible scenarios, there is a possibility of a kink in the chain. When we discuss a kink in the chain of a journey, we are not always talking about bugs. A suboptimal customer journey is also considered a break in quality from a product perspective. This thought process that we need a holistic view of every release — whether it’s a major feature release or just a bug fix — led to the inception of the release approval process. A combined look at technical design of each release, automation coverage, accessibility checks and security to name a few checkpoints formed the crux of this release approval process. None of these checkpoints were new for the teams. As basic SDLC steps, these checkpoints are followed to this day for each feature as part of its definition of done (DoD). However, when looking at a release in its entirety, this process gave the team leads a chance to look at everything in one shot, to make sure it made sense and to check that we have not missed anything. Developed as a bottom-up organic process, based on input from the teams on what usually goes wrong in a release at ground level, this DoD became the holy grail for teams to ensure a smooth release. The process in its entirety acts as a congregation of many different definitions of done into a single focal point: release definition of done. The harder you push, the harder the system pushes back.That truth from Peter Senge’s laws of system thinking held true during this implementation. With a solid leadership commitment to uphold quality standards, inviting engagement from all sections of the community and creating a safe environment where we acknowledge that mistakes happen. At the same time, learning and course correcting appropriately, this process became widely adopted and popular. Looking at a release from a holistic perspective gave the team leads an opportunity to identify potential issues ahead of time. With every aspect of a release documented in one single place, it was possible to chronologically track every micro-feature in a release. We were able to identify inefficiencies related to automation and tooling that were affecting the overall agility of releases, and consequently prioritized and invested effort in these areas to further improve quality. Because of the extensive documentation in one place, this enabled business, product and engineering teams to triage any release features dependent on other features at a rate 10 times faster than before. We saw rapid, quantifiable and obvious benefits of the release DoD: A 25 percent increase in release volume. Across the board, 79 percent reduction in overall bugs. 35 percent reduction in prod P1 and P2. This also ensured that dependent teams that worked on the same code base for cross-functional features adhered to a common prescribed release standard, allowing for easier testing of E2E experiences. This is an evolving process, and there are a lot of moving parts. But what has become apparent are the significant improvements in release quality by instilling awareness of quality across the whole life cycle of a feature — from inception to how the customer’s experience is impacted. It should be noted that as engineers are becoming more confident about each release, there is less churn and stress about potential mitigation strategies needed, and there is a huge increase in the general morale of everyone involved in a feature release. Over time, seeing the benefits of this process, teams became motivated and invested in the success of this process. They proactively worked on homegrown productivity tools to automate the process and help evangelize it across the entire company. What started as a gating factor at the end of the line quickly evolved into transparency and more confidence with each release. In combination with the meticulous, detail-oriented quarterly release planning (QRP) that is done at the start of every quarter, the definition of quality across the organization has changed; with every individual including product managers, engineers and quality engineers taking ownership of E2E customer experiences. While every tech organization has its own context, challenges and processes, I am sure everyone is continuously implementing, adopting and changing to improve the quality of customer experiences. I would be very interested to hear thoughts and ideas that have worked for you. I will leave with this saying from Aristotle that has been the cornerstone and yardstick for our focus on quality: “Quality is not an act, it is a habit.”", "date": "2020-09-14"},
{"website": "Ebay-Engineering", "title": "High Efficiency Tool Platform for Framework Migration", "author": ["Paul  Zhang", "Vivek Gadiyar"], "link": "https://tech.ebayinc.com/engineering/high-efficiency-tool-platform-for-framework-migration/", "abstract": "The eBay framework team has implemented a brand new tooling platform to speed up framework level migration. It supports highly efficient migration of applications running on legacy stacks to new platforms. This has been proven by the success stories migrating Batch, SOA, Trading/Shopping API, and messaging application migration. On the eBay developer platform, there are four application frameworks that are used to develop applications– V3, Raptor, Raptor.io and Node.js. V3, the oldest framework, has been part of eBay for more than fifteen years. Raptor and Raptor.io are the latest offering based on Spring and Spring Boot. eBay has made tremendous progress in rewriting hundreds of applications from proprietary frameworks to Open source Spring and Spring Boot. But a couple of hundred applications that are largely in maintenance remain and  still use the V3 framework to serve public traffic. The diagram below shows the V3 platform ecosystem. As illustrated in the diagram above, there are five flavors of applications in V3: ●  V3 SOA is a services stack based on SOAP ●  V3 Trading and Shopping are a service stack based on Axis, and mainly serves APIs for 3rd party developers of eBay ●  V3 BES is a messaging stack built on a relational database ●  V3 XSL/V4 Pres is a web stack for the presentation layer ●  V3 Batch is a batch stack for offline processing All legacy applications are deployed on a vendor supported technical stack (JDK, container and OS) that is fast approaching their End-of-Life. This has created a sense of urgency to move all legacy applications out of these vendor supported stack. The preferred destination at eBay is an Open Source based Raptor and Raptor.io that deploys applications on OpenJDK, Tomcat and Ubuntu. In order to facilitate a low cost migration from vendor supported tech stack to the Open source, we developed a tooling platform that is described in this document. This platform enables a central team to move the applications from the old environment to the new. Project structure, build systems, application packaging, deployment to the new environment are all taken care of by this central team using tooling. The application team is involved only in validation and the actual traffic migration. We call this approach “forklifting” because the application code is literally “forklifted” by a tool from the legacy to the new frameworks. Note that with this approach the application code is not changed. This was acceptable since our primary objective was to eliminate vendor supported software without an immense cost of a rewrite to application teams. Our first challenge was to make the migrated code work as it had before, and here’s how we did it. The following diagram shows the architecture after migrating to Raptor/Raptor.io. As mentioned earlier, we are not aiming to refactor or change the code of legacy applications. The code had to work as is in the new environment. Legacy applications run on legacy frameworks. This meant that we had to support these legacy application frameworks in our latest deployment environments. The main two tasks were: SOA/Trading/Shopping frameworks were ported to the Raptor.io framework and is available as a Spring Boot component SOA/Trading/Shopping frameworks were ported to the Raptor.io framework and is available as a Spring Boot component Batch/BES frameworks were ported to the Raptor framework Batch/BES frameworks were ported to the Raptor framework To build a migration tool, we first needed to identify the common patterns and the many differences between V3 and Raptor/Raptor.io. For each individual application’s migration, we considered two perspectives: resources and runtime. Resources ●  Code structure ●  App metadata, captured in proprietary XML files such as raptor_app.xml, etc. ●  Application configuration ●  Dependency changes (i.e. deploying on OpenJDK, Tomcat and Ubuntu may have different behaviors.) Runtime ●  Stack specific architecture differences (e.g. Ratelimiter is a local call in legacy stack, but a remote call in Raptor.io.) ●  Stack library backward compatibility (e.g. V3  Batch is an eBay proprietary  batch framework, while Raptor batch is based on Spring batch) ●  Framework  library backward compatibility (e.g. Raptor.IO rewrites the library to components and some classes' signatures are changed) ●  Third-party library backward compatibility (e.g. Jersey version is 1.x in V3 and Jersey version is 2.x in Raptor.IO) ●  Initialization mechanism difference (e.g. V3 is a proprietary initialization mechanism but Raptor IO uses Spring boot initialization) ●  Instance Lifecycle (e.g. Instance construction time point is different between the v3 container and Spring Boot container of Raptor.io) ●  Unexpected exceptions (e.g. same class from different libraries has different implementations) After we had a detailed breakdown, we categorized them into three levels: 1. Framework level problems ●  Code structure ●  App metadata ●  App configuration ●  Framework kernel library backward compatibility ●  Third-party library backward compatibility ●  Initialization ●  Instance lifecycle ●  Dependency changes 2. Stack level problems ●  Stack architecture differences ●  Stack library backward compatibility 3. Unplanned discoveries ●  Unexpected exception Based on the above analysis, we created the following architecture for our migration tool platform. There are two platform converters – one is for Raptor, and the other is for Raptor.io. Platform converters offer the lower level framework backward compatible layer. For stack level differences, we offer a plug-in to handle the differences. With this design, all the common problems are pushed down and handled by a central migration team, instead of training each application team to understand all nuances of the migration. The bulk of the work involved in the migration can be performed by a few team members who are able leverage the following : Built up expertise and a shared knowledge base as they tackle one application after another Similar methodology to handle problems arising in different stacks. Platform Converters There are eight logical components in the platform converter. Let us look at the code structure as an example. Code Structure V3 code structure was designed as a composite of applications. A deliverable in V3 is a binary package that is deployed to a cluster of machines in various environments such as staging, production, sandbox. A deliverable may consist of multiple applications produced by different teams or just one application produced by one team. On the flip side, a given application can be part of multiple deliverables. Below is an illustration: Therefore, the implementation needed to satisfy the following: Combination requirement: For some Trading/Shopping deliverables, a requirement was for all applications of one deliverable to be part of a single Raptor/Raptor.io application. We needed to modify our tool strategy to accommodate this case. Combination requirement: For some Trading/Shopping deliverables, a requirement was for all applications of one deliverable to be part of a single Raptor/Raptor.io application. We needed to modify our tool strategy to accommodate this case. Separation requirement: Some deliverables had strong requirements to be split into different applications. Separation requirement: Some deliverables had strong requirements to be split into different applications. Mixed application requirement: Some deliverables contain a mix of applications of different types (e.g. V3 SOA, Trading etc). Mixed application requirement: Some deliverables contain a mix of applications of different types (e.g. V3 SOA, Trading etc). To meet all the requirements, we needed to refine our solution. The applications of the deliverable were separated into two types. Applications that exist only in one deliverable were called non-shared applications. Applications that were included in many deliverables were called shared applications. The tooling supports parameters like  “-services” “-sharedservices” to handle these various scenarios. The migration tool platform has maximum flexibility at the design level due to the following code structures: We implemented five stack plugins in this tool platform – Batch Plugin is described below. V3 batch is based on a proprietary batch framework, while the target, Raptor batch, is based on Spring batch. Needless to say, there are significant differences. For example, the V3 batch task class is a Java class called Task, while the Raptor batch task class is a Java interface called Tasklet. Batch Stack Plugin had  to bridge these differences to minimize application code changes. The main objective of migration is to maintain the customer's business logic code. Batch stack plugin overcomes the difference and makes the existing code work as is. From the design pattern perspective, this is solved using the Adapter pattern, so that the application’s business code does not need to change. The migration team needed to build an adapter library. Therefore, the whole V3 batch core classes were considered as APIs, and class signatures were kept as before. It was reimplemented again using Spring Batch. It required the migration team to have thorough knowledge of the classes, lifecycle and initialization difference in V3 and Raptor batch core. Each stack of the framework has its features. The migration team had to take the following actions: For features that behave differently, we used the same behavior as V3, but reimplemented the feature in Raptor Batch. In the case where some features were supported in V3, but retired in Raptor, like DSF and ESF, different strategies were applied. In batch, all the run-time logic associated with DSF/ESF were email functions. Since the email team already offered replacements, the migration team offered a detailed solution to help migrate the  function. During the initialization, some ESF and DSF initialization modules were called by transitive dependency, but they did not interfere with the business logic. The team made configuration or code changes to bypass this kind of module. In batch, all the run-time logic associated with DSF/ESF were email functions. Since the email team already offered replacements, the migration team offered a detailed solution to help migrate the  function. During the initialization, some ESF and DSF initialization modules were called by transitive dependency, but they did not interfere with the business logic. The team made configuration or code changes to bypass this kind of module. We introduced the main design concepts for the two cases above. With this tool platform, code migration was no longer the long pole for the V3 migration project. Any application can be migrated within two minutes, and the migrated application is ready for testing on staging in about one day after the migration team fixes common problems. The framework level migration is a brand new methodology, which currently benefits V3 migration projects but also provides a reference for any future framework migration. This approach offers the following advantages First and foremost, it solved the key objective of the V3 migration program, which was to eliminate EOL vendor software at eBay. First and foremost, it solved the key objective of the V3 migration program, which was to eliminate EOL vendor software at eBay. Shielded consumers of the forklifted apps from having to make changes due to a replatforming of the service that they are consuming Shielded consumers of the forklifted apps from having to make changes due to a replatforming of the service that they are consuming Modernized the toolset (IDE, Project structure, build systems) of the forklifted code, which makes refactoring of the code more efficient in the future. (Note: forklift does not touch the legacy application Java code) Modernized the toolset (IDE, Project structure, build systems) of the forklifted code, which makes refactoring of the code more efficient in the future. (Note: forklift does not touch the legacy application Java code) All of this was achieved with minimal involvement of the application teams that owned the application.", "date": "2020-09-16"},
{"website": "Ebay-Engineering", "title": "An Automatic Mirror Testing Solution for Messaging Systems in eBay", "author": ["Tao Jin", "Vivek Gadiyar"], "link": "https://tech.ebayinc.com/engineering/an-automatic-mirror-testing-solution-for-messaging-systems-in-ebay/", "abstract": "A look at how we developed a solution for automatic mirror testing to overcome the challenges of migrating eBay’s proprietary messaging applications to a more modern technical stack. Business event stream (BES) is a reliable messaging framework developed at eBay that is widely used by nearly all domain teams in eBay. It is designed to provide a standard mechanism for propagation and asynchronous processing of events. A typical BES application workflow is shown below. Events published by PLSQL/API are populated into the database. Then, BES consumers will retrieve events from the database and process these events with specific business logic. In eBay, several BES applications still reside on the legacy V3 platform, which is out of support. These applications are being migrated to a new technical stack (newer JDK, app server and OS). After migration, the expectation is that the application will retain the same event processing semantics as before. The process of BES application migration must be transparent to end-users, which means all event process logic should be the same after migration without introducing any new issues. To achieve this goal, a well-defined testing strategy is necessary to guarantee quality. A typical way to test is by building up test events and checking the event status once they are consumed. However, there are four major challenges for migration with the traditional test approach. In a messaging framework, consumers are horizontally scaled to consume available events in parallel. It’s not easy to ensure specific events can be consumed by migrated consumers instead of legacy consumers. Legacy BES applications do not have enough end-to-end test cases to support mirror testing. Thus, there is no baseline for existing behavior that can validate behavior on the new platform. A lack of an automatic way to test large numbers of events with different types and generate test results. The inability to detect any performance impact for BES application changes. The BES mirror system is a brand new verification solution. It is designed as a centralized, independent system. It supports mirroring the same events to two instances based on separate platforms (V3/Raptor) and provides end-to-end comparison automatically. The system provides four capabilities to solve the challenges in testing. BES offers an affinity functionality that allows for specific events to be consumed by an appointed instance. To enable affinity, it is required to change centralized configuration and push changes to both publisher and consumer. Then, events from this publisher will have an affinity value and can only be consumed by consumers that can recognize this value. The mirror system leverages this functionality and provides a non-invasive way for BES applications to test specific events. It adds affinity value on target events directly with no publisher side code changes involved. BES leverages existing events from the database directly and republishes them with affinity will fill the gap that the lack of test cases on legacy applications. The solution tracks the whole event process flow and obtains all related information (e.g. event status, details logs and exceptions) to do the comparison. This approach can clearly expose any behavioral changes. By aggregating and comparing the performance metrics after mirroring large numbers of events, BES indicates whether performance is impacted because of migration. The workflow of the BES mirror system is shown below. The detailed, mirror steps are: Select two boxes. One is from the legacy V3 BES pool and the other is from the migrated Raptor BES pool. Enable affinity on both, and set host name as the Affinity value. Retrieve existing events from the database by filters. For each event, add the hostname of V3 box as the Affinity value and republish it, doing the same for Raptor box. TheV3 and Raptor consumer will consume their corresponding Affinity value (hostname). Compare the processed event status, metrics and exceptions in the log system. Every behavior should be the same before and after. Generate mirror reports. Since all the events will be published into databases by the publisher, the BES mirror system leverages the existing events in the database and republishes them. The system simulates the process of affinity enabled on publishers instead of changing on the publisher side. Thus, it will be consumed by specific instances which also enable affinity (add the same affinity value). It is capable of comparing “instance to instance” with the same events consuming. Leveraging existing events also helps resolve the lack of test cases for the legacy V3 BES pool. In BES consumers, some event processing logic has the duplication check. If the events with the same payload are used to be processed, they will be skipped when republished, which causes major process flow that is not covered in testing. To solve this, the BES mirror system provides the capability to amend the payload for existing events to be republished. A sample event payload looks like below: userId:12345|eventTimeStamp=2020-05-01|siteId=0 Duplicated event payload may skip to be processed. The BES mirror system supports overwriting partial payload like user ID to a new value before republishing events. The major event process logic will be touched during mirror testing. BES mirror system supports mirror events by specific event types, time windows and consumers. After mirror is done, one report will be generated to show comparison results based on related metrics to indicate if mirror testing passed or not. There is one job framework inside of the BES mirror system to run each mirror test, so it can be scalable to support massive events mirror testing. The below diagram shows the architecture of the BES mirror system. Before testing, it will create a job configuration with primary and candidate boxes, the target consumer and target event types for comparison. Once the target time window is selected, one corresponding job will be triggered to enable affinity for consumers; retrieve existing events within the time window; and set affinity value for events properly. Then, it will keep polling events process status from two mirror candidate boxes. Once the events are processed completely, all related status, metrics and thrown exceptions will be collected and compared. Any differences in test results between primary and candidate boxes will indicate the behavior changes during migration and will need to be fixed. The testing will be re-triggered until all tests passed. With the BES mirror testing system, the couples of V3 BES applications have been migrated to the Raptor platform successfully without any site issues. It guarantees the migration quality and benefits the regression testing for normal BES application’s features development, platform upgrade and performance testing. The BES mirror testing system provides a successful testing solution for the messaging system in eBay, which already benefits legacy V3 BES migration. It can overcome the unprecedented challenges of lacking end-to-end test cases for legacy BES applications and the instance to instance comparison. The test system is able to create a sustainable testing practice for BES applications, which enables all eBay developers to verify their applications for quality assurance.", "date": "2020-09-24"},
{"website": "Ebay-Engineering", "title": "Our Online Analytical Processing Journey with ClickHouse on Kubernetes", "author": ["Sudeep Kumar", "Mohan Garadi", "Xiancheng Li", "Amber Vaidya", "Liangfei Su"], "link": "https://tech.ebayinc.com/engineering/ou-online-analytical-processing/", "abstract": "Learn about the latest evolution of online analytical processing (OLAP) data, now with ClickHouse on Kubernetes. Sherlock.io is the platform of choice for all eBay monitoring needs, including applications, networks and devices. The centralized Sherlock.io platform manages different monitoring signals, such as logs, metrics, traces and events. The platform handles about 8 million metrics per second, 1 billion events per minute and multiple petabytes of logs every day. To run and manage a platform offering at such a high-volume scale, our monitoring team strives to build generic solutions with high availability and immense resilience. Developers are familiar with the attributes and characteristics of log and metric signals. Logs typically represent a logline from an application or device logs and have a timestamp representing when the log record was made. Although logs are structured, performing log aggregation at scale can present challenges. Metrics are time-series data that have a name, a timestamp, dimensions (key-value pairs) and a numerical value associated with them. We have defined a new monitoring signal, called events, in addition to logs and metrics. Events exhibit characteristics of both logs and metrics. They are immutable and present key signals in the system, such as database failure and device failure that could happen in surges. Examples of event use cases could be application traces or online analytical processing (OLAP) data. Events are monitoring signals that are strictly structured, and therefore, adhere to a schema format. They are not limited by the cardinality of data, or unique metric series, that most metrics backend systems are typically sensitive to. Throughout this blog, we will outline our journey in using Kubernetes to migrate OLAP event use-cases onto ClickHouse , a column-oriented database. Having defined an event signal, we looked at various backend solutions that would help us create a scalable and resilient platform for events while also supporting alerts. Our legacy OLAP use-case was built on Druid . Our OLAP data has application statistics for all eBay applications hosted on our infrastructure, processing around 1 billion OLAP events per minute on the pipeline. OLAP data gives quick insights into application health and other key statistics, such URL or transaction counts. We’ve run OLAP on Druid for years, but as our platform has scaled and as traffic has increased on OLAP, we sought new solutions to reduce the cost of maintaining Druid and occasional availability challenges. We explored ClickHouse late last year and, based on documentation and extensive benchmarking tests, it seemed to fit our events use-case well and yielded impressive numbers. We found ClickHouse capable of handling high-ingestion volume without issues. We also did a cost comparison of infrastructure footprint and storage, which showed that we could cut back on our existing infrastructure used for Druid by over 90 percent. However, the out-of-box read performance for ClickHouse did not work well for us. We performed techniques like data sharding and materialized views to improve read performance. A general problem, is that achieving high-read performance means understanding the user data, which can be difficult while hosting multiple customers and their data sets on the platform. ClickHouse also has other external dependencies, such as Zookeeper, for replication. ClickHouse also requires concrete tables and other dependencies, such as a buffer or distributed table, for the data pipeline to work smoothly. All applications within eBay are moving toward being hosted by an internally managed Kubernetes platform. Our desire was to have ClickHouse clusters spread across data centers. Our data centers are on the west coast of the U.S. and we expected little latency issues for data sync or replication. As part of that goal, we decided to create an operator that works on a federated control plane (refer to Kubernetes federation documentation on kubernetes.io ) through which we can manage a single ClickHouse cluster across multiple regions. We created two, custom resource definitions on Kubernetes referred to as FederatedClickHouseInstallation (FCHI) and FederatedClickHouseCluster (FCHC). FCHI represents different clusters that are deployed in our internal Kubernetes infrastructure. The following custom resource we have on FCHI represents all ClickHouse clusters in infrastructure. By following the example below, we have three clusters OLAP, events and a query cluster. FCHI allows us to maintain all the cluster topologies at one place for discovery and join across multiple Clickhouse clusters. For each of the above CH clusters, we have an associated Federated ClickHouse cluster (FCHC) object. The CRD (Custom resource definition) for FCHC is quite extensive – consider the following instance of FCHC we have hosted in our infrastructure. FCHC is used to create the ClickHouse cluster on individual Kubernetes clusters in different regions using the open-source operator. The specifications on FCHC has information on cluster dependent Zookeeper nodes and a side-car that exposes ClickHouse metrics in Prometheus format. The custom annotation on our deployment — io.sherlock.metrics — allows us to write back exposed Prometheus metrics back into our metrics platform. As part of shard creation, the ClickHouse operator annotates pods, which can be used to create a cluster topology. We listen to kube events and get notified on any changes on pod objects. The ingestion (ingress) and query module (egress), that were built on top of ClickHouse, use a lightweight, custom Kubernetes-based discovery module to use this cluster view. Both the ingestion and query modules are also built and managed on Kubernetes. We also manage exclusive query clusters – these nodes do not take any write traffic. All distributed table definitions are created on the query cluster. Our ingestion layer (ingress) is schema-aware, ensuring every incoming signal adheres to predefined OLAP schema. The ingestion layer uses a discovery module that watches for all Kubernetes events for ClickHouse infrastructure, hosted on our internally managed Kubernetes platform. On the read path (egress), we provide three modes to consume OLAP events data via ClickHouse query language (CQL) and EventQL, our own custom egress APIs. Alerting is enabled by Prometheus alert manager and our support of promQL via remote read APIs allows this integration. Customers can create alerts via a service layer built on top of the Prometheus alert manager. Our ClickHouse clusters are deployed across different regions or data centers. Our ingestion and egress layers always prefer to write or read into the nearest ClickHouse replica for a shard. Users can visualize the data via Grafana panel or use the ClickHouse Grafana data source plug-in to point to our egress layer directly. We use 14 fields for OLAP data representation, which include details like source application, regions, error, type of event and more. On the ClickHouse backend, this schema translates into multiple tables. Our ingestion layer always writes to the local, concrete table appevent. There are additional buffer tables and a distributed table created on top of this concrete table. Our concrete table definition for OLAP data looks like the following: Some attributes are defined as LowCardinality on which we expect lower unique values. ClickHouse applies dictionary coding to LowCardinality-columns, and this increases performance of SELECT queries. All of our read queries must have a pool attribute,which indicates application name, and we took advantage of this pattern to create sorting order first on pool and then on source timestamp. Timestamp precision is always in seconds and other labels are also used in sorting order based on query patterns. We have used ClickHouse default value for index_granularity setting. On top of the raw concrete table, we created materialized views (MV) at intervals of one minute, 15 minutes, one hour and one day for an MV-defined table. Our 15-minute MV table representation is as follows: Each roll-up table has a different time-based partition key. Higher roll-ups occupy less storage on the disk and have different retention policies configured. Real-time data is always served from the appevent raw concrete table, while other MV tables are used based on the time range mentioned in the read query. For example, if we are querying for the last 12 hours data, we used a one-hour, materialized view table and so on. One-day, MV data is maintained for a period of one year and raw concrete table data is maintained for a period of two days. With our new, cross-region aware OLAP pipeline, we reduced our overall infrastructure footprint by over 90 percent. Auto-remediation and configuration management provided by eBay’s custom Kubernetes operators have greatly reduced DevOps activity. Our model of using real-time Kubernetes events for ClickHouse infrastructure discovery on our ingestion and query layer has helped to quickly detect changes in infrastructure and handle failures.", "date": "2020-09-22"},
{"website": "Ebay-Engineering", "title": "Customized Ad-Rate Recommendations Now Available for Promoted Listings on eBay", "author": ["Wenlong Luo", "MingYu Liu", "Shad Kirmani", "Bedabrata Patel"], "link": "https://tech.ebayinc.com/engineering/customized-ad-rate-recommendations-now-available-for-promoted-listings-on-ebay/", "abstract": "A new machine-learning tool helps sellers set competitive ad rates for their Promoted Listings, optimizing between cost and performance. Within eBay’s global marketplace, buyers have access to over a billion listings around the world. To empower sellers to boost their items’ visibility and to help shoppers find items they love, eBay offers an advertising tool, Promoted Listings, which helps participating listings stand out. One conundrum that sellers face is how much to spend promoting their items. The fee for Promoted Listings is based on an “ad rate”: Sellers choose the percentage of the final sale price they’ll pay if a buyer purchases the item within 30 days of clicking on the promoted listing. Even with access to detailed metrics and sales reports evaluating Promoted Listings campaigns, calculating the optimal ad rate for an item can still be tricky. To help sellers stay competitive, eBay used machine learning to develop an algorithm to provide an item-level suggested ad rate which helps balance cost and performance. To estimate the probability of an item getting a significant visibility boost with a specific ad rate, eBay built an Xgboost classification model via Krylov, an internal machine learning platform. “Success” is defined as an item with significant improvement in impressions by showing in a more competitive slot (higher rank) compared to its organic status in major channels, including the search result page and view item page. The model evaluates the item’s features (count of historical impressions, clicks, transactions, number of competing promoted items in the same leaf category, price, etc.) over a set of reasonable ad-rate candidates and then generates the corresponding probability score. Next, the model assesses all the ad-rate candidates and their probabilities for success to recommend the optimal rate which maximizes the formula: where “price” is the item’s price, “final value fee” represents the normalized eBay fee paid per transaction by the seller, “ad fee” equals the Promoted Listings ad rate multiplied by the item’s price paid on top of the final value fee, and “score” is the probability computed using the Xgboost model described above. The resulting suggested ad rate is designed not only to boost visibility, but also to help balance visibility with cost. This ad rate is then shared with the seller as the suggested item-level ad rate. Before launching this machine-learning model, eBay provided a “trending ad rate” for each promoted item. This was the weighted average ad rate across similar items sold via Promoted Listings. eBay compared the new suggested ad rate with the trending ad rate for over 81 million experimental items and made an important discovery. In contrast to the trending ad rate’s unimodal distribution, the new suggested ad rate has a multimodal distribution. This showcases that the model not only guides sellers to set effective ad rates, but also helps sellers reduce their advertising costs by recommending ad rates lower than the trending rates when appropriate. Meanwhile, another study showed that sellers adopting suggested ad rates saw a much higher average ratio of converted items (sold items in a day per total adopted items for each seller) than sellers using trending ad rates, with an average ratio increase of 2.85%. This data suggests that the new ad-rate guidance can offer valuable direction for eBay sellers. eBay also evaluated the effectiveness of the suggested ad rate through a backtesting analysis using historical impression, click and transaction data. First eBay split the experimental items into three segments depending on whether their current ad rate was lower, equal to or higher than the suggested ad rate. Then, eBay calculated each segment’s average number of daily impressions, clicks and transactions per item. The result, visualized by the bar plot below, shows that the items setting their ad rate equal to or higher than the suggested ad rate performed significantly better than the items bidding lower than the recommendation: The items setting their ad rate equal to the suggested ad rate have 78%, 76% and 38% increases in incremental daily average impressions, clicks and transactions respectively, compared to the items setting their ad rate lower than the suggestions. Furthermore, for items using ad rates higher than the suggested ad rate, incremental ratios grew even further to 188%, 238% and 48% increases in impressions, clicks and transactions respectively. By improving the adoption of suggested ad rates, eBay can help sellers bring more high-quality listings into Promoted Listings. In addition to increasing items’ visibility, expanding participation also enhances the overall buyer experience. eBay will continue to analyze different needs of sellers and incorporate more signals to make the recommendations more robust, transparent and accurate.", "date": "2021-01-04"},
{"website": "Ebay-Engineering", "title": "eBay’s Technology Transformation is Made for Evolving Customer Needs ", "author": ["Mazen Rawashdeh"], "link": "https://tech.ebayinc.com/engineering/ebays-technology-transformation-is-made-for-evolving-customer-needs/", "abstract": "How our advances in edge computing are empowering both buyers and sellers in times of great demand with ease and agility. As we accelerate the tech-led reimagination of eBay, our goal is to become the platform of choice for sellers and a lifelong, trusted partner for buyers. When our customers succeed, we succeed — and never has this been more true than now. Thanks to our tech transformation strategy and investments in our private cloud infrastructure, the eBay marketplace has met the sudden and rapid influxes in customer demands with ease and agility throughout the pandemic with an even faster, more modern and more seamless customer experience. We are similarly poised to meet the increased demands of the upcoming holiday season as the world continues to turn to ecommerce during this time of physical distancing. What's exciting to me is that on average in the first three quarters of the year, the traffic that eBay has seen is on par with our busiest shopping days during a normal holiday season. In an extraordinary boost of customers onto our marketplace, more than 85 days in the first three quarters have exceeded peak 2019 traffic. Our platform is able to handle this sudden rise in traffic and customers' needs in large part because of our tech transformation strategy, providing a seamless experience while smoothly adjusting to this rise in numbers and handling over 250 checkouts per second. We have kept our marketplace available for customers throughout the pandemic at a relatively flat cost, even as we welcomed new customers to our platform. Now, with retail predictions that more than 70% of consumers will do more of their holiday shopping online this year than previous years, with over 50% solely buying online, we have great confidence that our technology is ready to take eBay and our customers into the next quarter century. eBay serves tens of billions of requests daily through our platform. In 2017, we started an initiative to modernize our edge computing technology around the world by distributing services to reside within proximity of our customers. By creating this unique-to-us infrastructure, we provide a seamless and consistent experience on our platform regardless of  customers geolocation. Our edge technology can be deployed very quickly in a nimble and efficient way within a short period of time to allow for surges onto our platform so we can continue to scale our services and provide the best experiences for our global customers. Building this type of sophisticated, dynamic infrastructure in a relatively short time at that scale while keeping our marketplace running at optimal performance requires a level of attention to detail similar to changing the engine on a Boeing 747 while it’s in the air. The stack was created using a variety of open source software, such as Envoy proxy for data plane and Contour for control plane. This open source approach promotes collaboration with the open source community overall, enabling our engineers to learn while also giving back. In addition, extending eBay’s software-based edge infrastructure closer to the customer has helped us bring in capabilities that were previously considered difficult to integrate, such as dynamic caching — which accelerates loading of our pages by caching time-critical content incredibly quickly at the edge of our network. By doing so, we can observe and act on user experience on the platform in real time by collecting billions of data points and detecting and resolving any potential issues using machine-learning algorithms. Our roll-out and migration of traffic to the new platform was also innovative. Our teams created systems that used AI — computer vision — to test new features in volume. We built autonomous systems that let us integrate these changes seamlessly by monitoring large volumes of data, again using machine-learning algorithms to solve problems before they interfere with customer experience. This allows us to endlessly innovate on behalf of our buyers and sellers. What does this mean for our customer? Our edge technology provides increased service resiliency, improved latency and delivered consistent customer experiences. With faster and localized services, we can truly show up for our customers — wherever and whenever they need us. For the past 25 years, we have empowered sellers to thrive and helped buyers find what they want while creating economic opportunity for all. We have achieved advancements over the past few years in our platform through our tech transformation and innovation. Now, we’re seeing these investments pay off with economic benefits and opportunities for our sellers and quicker, more seamless shopping experiences for our buyers during these recent times of massive surges onto our marketplace. The richness and value of our marketplace offerings, our global reach and our customer centricity are targeted to meet this moment and help people still find meaning, connection and joy in their holiday season. This customer-focused approach will continue to help us empower and enable our communities to succeed.", "date": "2020-11-17"},
{"website": "Ebay-Engineering", "title": "An Introduction to Apache Flink", "author": ["Chenru Lyu"], "link": "https://tech.ebayinc.com/engineering/an-introduction-to-apache-flink/", "abstract": "How we’re using Apache Flink to handle big data streaming in real-time. As a recommender system team at eBay, we stress the user behavior during the recommendation process. The more user behavior we grasp, the more beneficial items we can recommend to each specific individual buyer. Flink is a very powerful tool to do real-time streaming data collection and analysis. The near real-time data inferencing can especially benefit the recommendation items and, thus, enhance the PL revenues. Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. The JobManager has a number of responsibilities related to coordinating the distributed execution of Flink Applications: It decides when to schedule the next task (or set of tasks), reacts to finished tasks or execution failures, coordinates checkpoints and coordinates recovery on failures, among others. The TaskManagers (also called workers) execute the tasks of a dataflow and buffer and exchange the data streams. Task Slot Each worker (TaskManager) is a JVM process and may execute one or more subtasks in separate threads. To control how many tasks a TaskManager accepts, it has so-called task slots. Operators transform one or more DataStreams into a new DataStream. Programs can combine multiple transformations into sophisticated data flow topologies. Takes one element and produces one element. Takes one element and produces zero, one or more elements. Evaluates a boolean function for each element and retains those for which the function returns true. Logically partitions a stream into disjoint partitions, each partition containing elements of the same key. A \"rolling\" reduce on a keyed data stream. Combines the current element with the last reduced value and emits the new value. A \"rolling\" fold on a keyed data stream with an initial value. Combines the current element with the last folded value and emits the new value. Rolling aggregations on a keyed data stream. Windows can be defined on already partitioned KeyedStreams. Windows group the data in each key according to some characteristic. Partitions elements randomly according to a uniform distribution. Partitions elements round-robin, creating equal load per partition. Useful for performance optimization in the presence of data skew. Partitions elements, round-robin, to a subset of downstream operations. Broadcasts elements to every partition. Begin a new chain, starting with this operator. The two mappers will be chained, and the filter will not be chained to the first mapper. Do not chain the map operator Windows are at the heart of processing infinite streams. Windows split the stream into “buckets” of finite size over which we can apply computations. Tumbling Windows A tumbling windows assigner sets each element to a window of a specified window size. Tumbling windows have a fixed size and do not overlap. Sliding Windows The sliding windows assigner sets elements to windows of fixed length. Similar to a tumbling windows assigner, the size of the windows is configured by the window size parameter. An additional window slide parameter controls how frequently a sliding window is started. Session Windows The session windows assigner groups elements by sessions of activity. Session windows do not overlap and do not have a fixed start and end time, in contrast to tumbling windows and sliding windows. Instead a session window closes when it does not receive elements for a certain period of time, i.e., when a gap of inactivity occurs. ReduceFunction A ReduceFunction specifies how two elements from the input are combined to produce an output element of the same type. AggregateFunction An AggregateFunction is a generalized version of a ReduceFunction that has three types: an input type (IN), accumulator type (ACC) and an output type (OUT). FoldFunction A FoldFunction specifies how an input element of the window is combined with an element of the output type. ProcessWindowFunction A ProcessWindowFunction gets an Iterable containing all the elements of the window, and a Context object with access to time and state information, which enables it to provide more flexibility than other window functions. Allowed Lateness Flink allows to specify a maximum allowed lateness for window operators. Allowed lateness specifies by how much time elements can be late before they are dropped. Window Join Tumbling Window Join When performing a tumbling window join, all elements with a common key and a common tumbling window are joined as pairwise combinations and passed on to a JoinFunction or FlatJoinFunction. Sliding Window Join When performing a sliding window join, all elements with a common key and common sliding window are joined as pairwise combinations and passed on to the JoinFunction or FlatJoinFunction. Session Window Join When performing a session window join, all elements with the same key that when “combined” fulfill the session criteria are joined in pairwise combinations and passed on to the JoinFunction or FlatJoinFunction. Interval Join The interval join connects elements of two streams (we’ll call them A & B for now) with a common key and where elements of stream B have timestamps that lie in a relative time interval to timestamps of elements in stream A. Both types of timers (processing time and event time) are internally maintained by the TimerService and enqueued for execution. The TimerService deduplicates timers per key and timestamp (i.e. there is at most one timer per key and timestamp). If multiple timers are registered for the same timestamp, the onTimer  method will be called just once. Flink’s Async I/O API allows users to use asynchronous request clients with DataStreams. The API handles the integration with DataStreams, as well as handling order, event time, fault tolerance, etc. Assuming one has an asynchronous client for the target database, three parts are needed to implement a stream transformation with asynchronous I/O against the database: An implementation of AsyncFunction that dispatches the requests An implementation of AsyncFunction that dispatches the requests A callback that takes the result of the operation and hands it to the ResultFuture A callback that takes the result of the operation and hands it to the ResultFuture Applying the async I/O operation on a DataStream as a transformation Applying the async I/O operation on a DataStream as a transformation The following two parameters control the asynchronous operations: Timeout: The timeout defines how long an asynchronous request may take before it is considered failed. This parameter guards against dead/failed requests. Timeout: The timeout defines how long an asynchronous request may take before it is considered failed. This parameter guards against dead/failed requests. Capacity: This parameter defines how many asynchronous requests may be in progress at the same time. Capacity: This parameter defines how many asynchronous requests may be in progress at the same time. Using Keyed State The keyed state interface provides access to different types of state that are all scoped to the key of the current input element. This means that this type of state can only be used on a KeyedStream. ValueState<T>: This keeps a value that can be updated and retrieved (scoped to key of the input element as mentioned above, so there will possibly be one value for each key that the operation sees). The value can be set using update(T) and retrieved using T value(). ValueState<T>: This keeps a value that can be updated and retrieved (scoped to key of the input element as mentioned above, so there will possibly be one value for each key that the operation sees). The value can be set using update(T) and retrieved using T value(). ListState<T>: This keeps a list of elements. You can append elements and retrieve an Iterable over all currently stored elements. Elements are added using add(T) or addAll(List<T>), the Iterable can be retrieved using Iterable<T> get(). You can also override the existing list with update(List<T>) ListState<T>: This keeps a list of elements. You can append elements and retrieve an Iterable over all currently stored elements. Elements are added using add(T) or addAll(List<T>), the Iterable can be retrieved using Iterable<T> get(). You can also override the existing list with update(List<T>) ReducingState<T>: This keeps a single value that represents the aggregation of all values added to the state. The interface is similar to ListState but elements added using add(T) are reduced to an aggregate using a specified ReduceFunction. ReducingState<T>: This keeps a single value that represents the aggregation of all values added to the state. The interface is similar to ListState but elements added using add(T) are reduced to an aggregate using a specified ReduceFunction. AggregatingState<IN, OUT>: This keeps a single value that represents the aggregation of all values added to the state. Contrary to ReducingState, the aggregate type may be different from the type of elements that are added to the state. The interface is the same as for ListState but elements added using add(IN) are aggregated using a specified AggregateFunction. AggregatingState<IN, OUT>: This keeps a single value that represents the aggregation of all values added to the state. Contrary to ReducingState, the aggregate type may be different from the type of elements that are added to the state. The interface is the same as for ListState but elements added using add(IN) are aggregated using a specified AggregateFunction. MapState<UK, UV>: This keeps a list of mappings. You can put key-value pairs into the state and retrieve an Iterable over all currently stored mappings. Mappings are added using put(UK, UV) or putAll(Map<UK, UV>). The value associated with a user key can be retrieved using get(UK). The iterable views for mappings, keys and values can be retrieved using entries(), keys() and values() respectively. You can also use isEmpty() to check whether this map contains any key-value mappings. MapState<UK, UV>: This keeps a list of mappings. You can put key-value pairs into the state and retrieve an Iterable over all currently stored mappings. Mappings are added using put(UK, UV) or putAll(Map<UK, UV>). The value associated with a user key can be retrieved using get(UK). The iterable views for mappings, keys and values can be retrieved using entries(), keys() and values() respectively. You can also use isEmpty() to check whether this map contains any key-value mappings. A time-to-live (TTL) can be assigned to the keyed state of any type. If a TTL is configured and a state value has expired, the stored value will be cleaned up on a best effort basis. The central part of Flink’s fault tolerance mechanism is drawing consistent snapshots of the distributed data stream and operator state. These snapshots act as consistent checkpoints to which the system can fall back in case of a failure. Barriers A core element in Flink’s distributed snapshotting are the stream barriers. These barriers are injected into the DataStream and flow with the records as part of the DataStream. Barriers never overtake records, they flow strictly in line. A barrier separates the records in the DataStream into the set of records that goes into the current snapshot, and the records that go into the next snapshot. Operators that receive more than one input stream need to align the input streams on the snapshot barriers. The figure above illustrates this: As soon as the operator receives snapshot barrier n from an incoming stream, it cannot process any further records from that stream until it has received the barrier n from the other inputs as well. Otherwise, it would mix records that belong to snapshot n and with records that belong to snapshot n+1. As soon as the operator receives snapshot barrier n from an incoming stream, it cannot process any further records from that stream until it has received the barrier n from the other inputs as well. Otherwise, it would mix records that belong to snapshot n and with records that belong to snapshot n+1. Once the last stream has received barrier n, the operator emits all pending outgoing records, and then emits snapshot n barriers itself. Once the last stream has received barrier n, the operator emits all pending outgoing records, and then emits snapshot n barriers itself. It snapshots the state and resumes processing records from all input streams, processing records from the input buffers before processing the records from the streams. It snapshots the state and resumes processing records from all input streams, processing records from the input buffers before processing the records from the streams. Finally, the operator writes the state asynchronously to the state backend. Finally, the operator writes the state asynchronously to the state backend. Operators snapshot their state at the point in time when they have received all snapshot barriers from their input streams, and before emitting the barriers to their output streams. At that point, all updates to the state from records before the barriers have been made, and no updates that depend on records from after the barriers have been applied. After the state has been stored, the operator acknowledges the checkpoint, emits the snapshot barrier into the output streams and then proceeds. The resulting snapshot now contains: For each parallel stream data source, the offset/position in the stream when the snapshot was started For each parallel stream data source, the offset/position in the stream when the snapshot was started For each operator, a pointer to the state that was stored as part of the snapshot For each operator, a pointer to the state that was stored as part of the snapshot All programs that use checkpointing can resume execution from a savepoint. Savepoints allow both updating your programs and your Flink cluster without losing any state. Savepoints are manually triggered checkpoints, which take a snapshot of the program and write it out to a state backend. They rely on the regular checkpointing mechanism for this. Savepoints are similar to checkpoints except that they are triggered by the user and don’t automatically expire when newer checkpoints are completed. Apache Flink is the most suited framework for real-time processing and use cases. Its single engine system is unique which can process both batch and streaming data with different APIs like Dataset and DataStream. It does not mean Hadoop and Spark are out of the game, the selection of the most suited big data framework always depends and varies from use case to use case. There can be several use cases where a combination of Hadoop and Flink or Spark and Flink might be suited. Nevertheless, Flink is the best framework for real time processing currently. The growth of Apache Flink has been amazing, and the number of contributors to its community is growing day by day. Happy Flinking!", "date": "2020-11-20"},
{"website": "Ebay-Engineering", "title": "eBay Launches Marko 5", "author": ["Ryan Carniato", "Michael Rawlings", "Dylan Piercey"], "link": "https://tech.ebayinc.com/engineering/ebay-launches-marko-5/", "abstract": "eBay’s open source JavaScript UI framework modernizes universal web development. eBay was founded with the core belief that we should use technology to empower and connect people globally. In the technology world, we’re a core contributor to and believer in open source technology. Not only does a company culture of open source help us empower our developers, but it also enables our technologists to collaborate both across the organization and with peers throughout the industry. A key pillar of eBay’s participation in the open source software community is our most popular open source project, Marko . Initially developed by eBay in 2012 to transition from our Java stack to a Node stack, it now powers the majority of ebay.com . While Marko was transitioned to the OpenJS Foundation , eBay still actively maintains the project. Marko is a JavaScript framework for building universal websites: You write your code once, and it works seamlessly across both the server and the browser. Where Marko stands out is its incredible server rendering performance , and smart compilation to generate the optimal code for both the server and the browser. This has been vital to reaching our performance goals for the eBay platform, and is the culmination of years of battle-tested experience at a global scale. Since Marko’s last major release in 2017, the modern web has evolved at breakneck speeds. The JavaScript ecosystem has boomed, and tooling has grown in capability. And so Marko has changed with it. With recent focus returning to server rendering and server components – topics core to its DNA – Marko already delivers on next generation features. Instead, Marko 5 focuses on improving the developer experience by providing a foundation built on modern JavaScript tools, a better integration story with third-party libraries, and an easier path to getting started. The Marko 5 compiler now runs on Babel . This means immediate support for all current and future JavaScript language syntax and features by leveraging their huge ecosystem of presets and plugins. This also brings easy interop with the modern JavaScript ecosystem. We’ve updated our third-party integrations and added a few new ones: Bundlers: You can now build your Marko projects with Webpack or Rollup , in addition to eBay’s Lasso . Testing: Marko fully integrates with Jest and Storybook . Test with confidence with the Marko Testing Library . IDE: Marko’s language server integrates seamlessly in VSCode, bringing powerful code completion and navigation; syntax highlighting; and real-time error messages. We will continue to work to incorporate support for tools like TypeScript, Prettier and ESLint. Getting started with Marko has never been easier. The Marko CLI generates a ready-to-go project with built-in routing. Simply adding your pages and components to the folder generates a universal server and browser application. And that’s it. Marko automatically detects your components so you can use them without importing. These CLI projects come with the best practices for performance built in. Marko automatically detects the components that are only needed on the server and produces an optimal browser bundle for you. Asynchronous streaming lets your pages arrive in the browser as quickly as possible while the rest of the data loads. We’ve been leveraging these techniques for over half a decade at eBay in production and now you can, too ( read more here ). To get started, simply run the command from your terminal and follow the interactive CLI. Marko is a powerful, markup-based language that embraces the complexity of the client while being as easy to write as HTML templates. Our new compiler is designed to be able to target other platforms or even other UI frameworks. We have already begun to leverage it in developing our next-generation reactive runtime . It’s never been a better time to give Marko a try. You can find the new version on our GitHub and our updated documents at www.markojs.com . See for yourself what it is like to use the JavaScript framework built from the ground up with both the server and the browser in mind.", "date": "2021-02-11"},
{"website": "Ebay-Engineering", "title": "How eBay’s Distributed Architecture Surfaces More Item Listings for Buyers", "author": ["Vikas Singh"], "link": "https://tech.ebayinc.com/engineering/how-ebays-distributed-architecture-surfaces-more-item-listings-for-buyers/", "abstract": "With Buyer Demand Data, eBay sellers can access information on the item specifics buyers are using in their search queries. This insight increases sellers’ completion of item specifics, which in turn further enhances listings’ quality, visibility and sales. eBay’s global marketplace connects people with items they love. With so many choices in the marketplace, one challenge for our sellers is how to have their listings stand out and inspire buyers to make a purchase. In this post, we present a distributed application architecture built using event-driven systems, microservices, recommendation systems and a data aggregation layer. A good quality listing helps buyers get to know the product. It includes information about the item — ranging from the condition to shipping — through a descriptive title, item specifics, photos and a detailed product description. But creating an excellent listing is only half the equation for a successful sale — sellers must also surface their items to their most interested buyers. Buyers search in two different ways: by keywords and/or by applying left-hand navigation filters to their query. Even if a seller creates a good-quality listing that would be a perfect match for a buyer, the listing won’t surface in the search and perform to its full potential if it doesn’t map to buyer tokens (or keyword terms) and filters. Sellers often try to improve the search performance of their listings by tweaking the wording to optimize their position in search results, but they may not be aware of the important keywords or filters buyers are using that might impact their item’s visibility. Now, eBay sellers can access Buyer Demand Data in a simple and elegant way right in the listing flow, to help surface items that buyers will love. This insight into what buyers want helps drive seller engagement and increases the completion of item specifics, which in turn further enhances listing quality, visibility and sales. Buyer Demand Data is the number of times buyers have searched for a particular item specific over a defined period of time. Buyers tend to use consistent search terms for used or commodity products, whereas for new or in-season items there might be some fluctuation before a search pattern emerges. Buyer Demand Data captures these trends and helps sellers target their products to buyers. Some of the ways eBay sources the data include: Extracting item specifics from buyer keywords. For example, a buyer might use the search bar and type in “Apple Macbook Pro 2019.” Counting clicks on search filters; such as a buyer filtering search results by the item specific “Release Year.” Analyzing browsing/navigation patterns. For example, buyers browsing Electronics → Computers, Tablets & More → Laptops & Netbooks → Apple Laptops → Release Year. Important item specifics are the ones that buyers use more often in their searches and filtering. For instance, a typical buyer may search for a Macbook Pro using “Apple Macbook Pro 15” as keywords, and then applying filters such as Release Year, Processor and SSD Capacity. If we encouraged sellers to include this information in their listings, their items would have a higher chance of showing in the search results — significantly improving listing visibility and the overall conversion rate. Figure 1: Item specifics user interface for sellers The following diagram shows how data flows through the system. As buyers search for listings, their queries are analyzed and broken down into structured signals allowing downstream systems to capture buyer demand. This includes extracting item specifics from the keywords based on past searches and buyer behavior, using machine learning, as well as factoring in additional filters selected by the buyer. For instance, analyzing the sample query “Apple Macbook Pro 2019,” we can tell that buyers are searching for Brand = “Apple,” Model = “Macbook Pro” and Release Year = “2019” in the “Apple Laptops” category. Figure 2: “Buyer Demand Data Service” microservice These signals are tracked and stored in a data warehouse and subsequently processed as aggregated demand over time (e.g. 2,342,535 searches for the item specific “Model” in the last 30 days as shown in Figure 1). This aggregated demand is then pushed as a feed to transactional systems for real-time guidance to sellers around important item specifics they should provide to maximize their listings’ visibility. The real-time guidance includes showing Buyer Demand Data to sellers at the time of creating or revising a listing, as well as promoting the visibility of high-demand item specifics in the seller experience to drive upfront engagement and the item specifics completion rate. This is achieved via the microservice “Buyer Demand Data Service” in Figure 2, which acts as a single source of truth to deliver the demand data and important item specifics throughout the cross-platform selling experiences. This integration covers both the listing creation process and the post-listing management process, where sellers are provided guidance on existing listings that are missing important item specifics. Sellers positively respond to this valuable information, resulting in them filling out more item specifics and engaging more deeply with the platform, ultimately creating higher-quality listings that drive conversion and sales. Guiding our sellers by transparently sharing the marketplace dynamics has allowed them to improve their listings’ visibility in search, driving conversion which is 5% higher globally in the test group compared to the control group. Besides higher sales, we also observed a 15% increase in the item specifics completion rate globally during the same period. Data-driven insights go a long way in connecting buyers and sellers. This experience is live on eBay’s web and native ( iOS and Android ) selling apps in all major markets, as well as available in our public Taxonomy API . As a pure online marketplace, eBay is committed to the success of its buyers and sellers. By transparently sharing these dynamic demand insights, we have empowered our sellers to make the best use of the platform’s capabilities to grow their businesses. Interested in pursuing a career at eBay? We are hiring! To see our current job openings, please visit: https://careers.ebayinc.com/ This article was initially published on The New Stack on Feb. 17, 2021.", "date": "2021-03-03"},
{"website": "Ebay-Engineering", "title": "How Creating an Agile Code Base Helped eBay Pivot for Apple Silicon", "author": ["Wyatt Webb"], "link": "https://tech.ebayinc.com/engineering/how-creating-an-agile-code-base-helped-ebay-pivot-for-apple-silicon/", "abstract": "eBay’s native code base has grown and evolved for over 13 years, but is still nimble enough to quickly incorporate the latest features into the app. As one of the pioneer e-commerce sites and very first companies to launch an iPhone app , eBay’s native code base — encompassing our iOS and Android applications — has grown and evolved for more than 13 years. But it is still nimble enough to quickly incorporate the latest features into the app. Spanning diverse marketplaces in disparate geographies, the stack is both voluminous and complex. Over the years, our eBay team has consistently found ways to keep pace with the changing technological landscape and deliver a seamless mobile customer experience. How did we create a stack which is flexible and adaptable enough to power a consistently top-rated app? I’ll explain in this article. At eBay , the Mobile Architecture team is responsible for planning and building the foundational code of our native mobile apps. Part of our job is to try to peek over the horizon and see where Google and Apple are headed so that eBay can best take advantage of new technologies as they arise. eBay’s tenure in the tech sector has taught us how to make our codebase as nimble as possible — by sharing code where it makes sense and maintaining consistency in architecture across the app. For sharing code, we isolate changes to a single location that can be quickly validated and seen across the app, without much intervention into each section. For example, using a shared user interface (UI) component library in our app allowed us to adopt Dark Mode very quickly in 2020. We were one of the first amongst our competitors and industry peers to support this feature. Where sharing code isn’t feasible, building and maintaining a consistent architecture helps facilitate planning for the future. When an app lives as long as eBay has under constant development, architectural patterns can change and divergences happen. It takes effort to avoid having code evolve to a state where two or three different architectural patterns are being used at a time. Since old code and new code have to then be patched through layers of translation, the situation makes it more difficult to adopt broad new features without having to also update the older parts of the app. Lastly, adopting new technology means keeping up with new technology — which may sound tautological, but it means having a model of constant adoption. You cannot wait until a new game-changing feature is announced at a developer conference; you need to dedicate time to lay the groundwork. Chances are that new releases assume you have already been keeping up with earlier — but perhaps more mundane — updates. If you ignore technologies A and B because there was no immediate perceived value, you will have a lot of work to do when technology C is announced with great fanfare (with dependencies on A and B). Therein lies a little bit of prediction, though: You need to be a student of your platform and recognize where updates are leading, and make an educated guess about where they will go next. Then you can decide where to place your bets and spend time for a later payoff. For example, the retrofitting we had done to support iPad Multitasking in our iOS app recently paid dividends when we chose to support Apple Silicon. Multitasking allows our customers to do things like comparison shopping or to reference external information when creating a listing, and eBay was happy to support the experience even though it was not necessary for any of our major initiatives or features. However, for the architects working in the code every day, it was clear that Apple was laying the groundwork for some major changes — and it was important for eBay to stay in sync with those upgrades. Under the hood, supporting multitasking required eBay’s code to stop assuming that the screen size was constant on any given device. In the old days, an app could check at startup if it was on an iPhone or iPad and then run accordingly without change. However, this model assumed the user cannot adjust the window size. As the number of device sizes increased and patterns of usage changed, Apple introduced the idea of size classes — where an iPhone was considered to have a “compact” width and an iPad had a “regular” width. This allowed developers to describe layouts relatively and by rough device size. Furthermore, support was added for a size-class change happening at runtime. So, eBay’s code had to evolve to depend less on the actual device and more on the size class; as well as to react to size-class changes at any point. This was not a simple change, but it made the entire app more flexible. While these changes were predominately in service of multitasking, we were also planning for the future. Implementing multitasking and being prepared for major changes paid off this past summer, when Apple announced their plans for Apple Silicon at the Apple Worldwide Developers Conference in June 2020. By using their own chips for desktop and laptop Macs, Apple was able to share the CPU architecture across their Mac and iOS product lines. With that came the very intriguing idea of running iOS apps on Macs with little to no change. Now, what does this have to do with multitasking? This ability to run our iOS app on desktop and laptop Macs is exactly the kind of future tech we cannot precisely predict, but we at eBay want to take advantage of it when it arrives. Without multitasking, eBay’s app would be available to these new machines, but the window size would be fixed to the size of an iPad — which, on modern displays, feels relatively small. That would make the app feel inflexible and less useful. But because the Mobile Architecture team had already tackled multitasking, eBay’s iOS app users can resize the window on a new Mac and take advantage of the extra screen size. The Mobile Architecture team was able to very quickly evaluate eBay’s app on a pre-release Apple Silicon machine, make some minor changes, and then approve the distribution of the iOS app for Apple Silicon at the end of the year. The cost to release this version was small and the opportunity to learn more about our customers will be big. The ability to resize our app on a bigger workspace has already piqued the interest of our design team, which is extending eBay’s tablet/iPad designs to accommodate even larger screens; and is also starting to consider what eBay’s app would look like with multiple windows. Getting buyers and sellers to try the app on a Mac will help us understand their preferences better and how they use extra screen real estate on a Mac. These insights will help eBay modernize our UI design and feature adoption. It is even possible that features unavailable on iPhones will be added for users on these larger displays if there are advantages in doing that. Simply supporting multitasking has opened many avenues for investigation and possible innovation in the future. Part of good architecture is planning for the future, even when that future is influenced by platform vendors. We at eBay are very proud of our high rating in the App Store (4.8 as of this writing!). Our customers expect a stable, feature-rich application that adopts the new technologies that eBay’s platform vendors announce. It is an important part of our development process to continue building in a way that never leaves us far behind and allows us to pivot quickly when the time is right. Interested in pursuing a career at eBay? We are hiring! Please take a look at our current job openings. This article was initially published on The New Stack on Feb. 25, 2021.", "date": "2021-03-11"},
{"website": "Ebay-Engineering", "title": "Kubernetes Secrets: A Secure Credential Store for Jenkins", "author": ["Vasumathy  Seenuvasan", "Ravi Bukka"], "link": "https://tech.ebayinc.com/research/kubernetes-secrets-a-secure-credential-store-for-jenkins/", "abstract": "At eBay, we containerized Jenkins to provide a continuous build infrastructure on Kubernetes Clusters to power the ecommerce marketplace experience. Our goal was to leverage the capability of Kubernetes secrets, for managing the Jenkins credentials. eBay.com is powered by applications that are built with Jenkins as the opinionated choice of build-machinery. We run Jenkins instances as a cloud native workload on the Kubernetes Clusters with server-agent mode, all of which are managed by the eBay cloud team. The Jenkins servers provided to eBay development teams have a default credential plugin installed. The credentials stored on these Jenkins instances are used for various purposes, such as connecting to GitHub and third-party APIs. The credential plugin stores the user credentials, as well as the encryption key, on the disk in an encrypted format. These credentials are very sensitive in nature and need to be stored safely and securely. However, storing the key and the credential on the same disk poses a risk. Jenkins instances are also generally shared across a team with ADMINISTER privileges , which increases the chances of team members knowing each other’s credentials. Initially, we considered setting up a Vault 1 store, which is generally used in the community to secure the credentials in Jenkins. However, that would be a vendor lock-in. To remain vendor-neutral, we chose a different strategy. However, eBay has its own proprietary key management platform, which integrates with Kubernetes cluster’s control-plane and mounts secrets in a secure manner to the containers leveraging Kubernetes API standards. Since we are running Jenkins as workloads on Kubernetes, a simple solve was to standardize by using Kubernetes Secrets. Another benefit of this approach is that the same secrets can also be backed up to the in-house key management solution. The challenge here was to integrate the Jenkins application with this proprietary key management platform. To solve the challenge, we developed a new plugin - Jenkins Credentials Secrets Plugin - which replaces the default credential plugin and is now available as an eBay open source project. This article explains the design and process of a more secure way of storing Jenkins credentials as secrets on Kubernetes Clusters. We have enabled this plugin to store credentials as Kubernetes Secrets on ~6000 Jenkins instances at eBay. We have also migrated all the credentials currently stored in these instances to Kubernetes secrets with ZERO down time for the end users - eBay application developers. 1.     The Jenkins master runs as a container in Kubernetes Cluster. 2.     The Jenkins master has a Kubernetes service account mounted to it. Only credentials under the Jenkins Global domain are supported by the plugin at this moment. When a credential is created by the user from a Jenkins UI or API call, a Kubernetes Secret is generated in a namespace (as specified by the plugin user) in the cluster with the required information from the credentials. The Secret specification’s “data” will hold the credential’s “sensitive” information. The Secret will have labels and annotations in the spec to store the below details. The secret name is generated as “sec”  + “-” + “UUID” + “-” + “JENKINS_INSTANCE_NAME” Example : sec-<<UUID>>-myCI. We use UUID because the Kubernetes Secret object’s naming convention does not support all the characters allowed by Jenkins credentials (e.g. whitespaces and underscores). The credentials ID stored by the user is captured in the Secret’s labels, as shown in the above chart. We also capture other credential information, like description and type, and this list can be extended to attributes added in the future as well. The core component of this approach is the “Secrets-to-credentials converter” module. On Jenkins start up, the plugin gets the list of secrets that have the Jenkins name label selector. This is necessary so only the secrets belonging to the Jenkins in question are pulled. Converters for all of the below Jenkins supported credentials are implemented. Username with password Docker Host Certificate authentication Kubernetes Service Account OpenShift OAuth token OpenShift Username and Password SSH Username with Private Key SSH Username with Private Key and Passphrase Secret file Secret text Certificate The credential type of a secret is identified from the label (jenkins.io/credentials-type), and the corresponding converter implementation is invoked to convert a secret to a credential. This credential is added to the Jenkins credentials Map offered by the Jenkins credentials plugin. This makes the approach transparent to the users, as they just see the same credential information on Jenkins UI, and all the secret conversion happens in the background. ➔ Create: When users add credentials in the Jenkins UI, secrets will be created on the corresponding Kubernetes cluster in the namespace provided, and credentials will no longer be stored in `credentials.xml` on the disk. ➔ Read: On Jenkins startup, credentials of the particular CI are loaded from the Kubernetes cluster (using label selector “jenkins.io/ci-name”). ➔ Update: When a credential is updated, the corresponding secret is updated (using label selectors “jenkins.io/ci-name” and “jenkins.io/credential-id”). ➔ Delete: When a credential is deleted, the corresponding secret is deleted (using label selectors “jenkins.io/ci-name” and “jenkins.io/credential-id”). Username Password credential Type: The rest of the credential type YAML specs are available in the source code repository here: https://github.com/eBay/kube-credentials-plugin/tree/master/credentialspecsamples Base Credentials plugin - 2.1.19 Kubernetes plugin (dependency) - 1.1.4 We have open sourced this project, and the git repo can be found here: https://github.com/eBay/kube-credentials-plugin We welcome any PullRequests (PR) or github-issues on this repo. We have listed to-do items on the repo and welcome PRs to address them. 1 Vault is a product from HashiCorp: https://www.vaultproject.io/", "date": "2020-07-27"},
{"website": "Ebay-Engineering", "title": "eBay Motors & State Management", "author": ["Larry McKenzie", "Corey Sprague", "Maksim  Zadorskii"], "link": "https://tech.ebayinc.com/engineering/ebay-motors-state-management/", "abstract": "Learn how we avoided the state management debate when building the eBay Motors app. When we discuss the eBay Motors app, the question we are most often asked is, “Which state management solution does eBay Motors use?” The simple answer is we mostly manage state in StatefulWidgets, using Provider or InheritedWidgets to expose this state down the widget tree. However, we think there is a more interesting question we should be asked: “How did eBay Motors design their codebase in a way that the choice of state management tool does not matter?” We believe that choosing the right tool, or applying a single design pattern, is much less important than establishing clear contracts and boundaries between the unique features and components in the application. Without boundaries, it becomes too easy to write unmaintainable code that doesn’t scale as the application grows in complexity. “Unmaintainable code” can be defined in several ways: changes in one area that creates bugs in seemingly unrelated areas; code that is difficult to reason about; code that is difficult to write automated tests for; or code with unexpected behaviors. Any of these issues in your code will slow down your velocity and make it more difficult to deliver value to your users. Creating clear boundaries between different areas of the code reduces these problems. This approach encourages you to break down large, complex problems into smaller, more manageable pieces. It encourages different domains to communicate via abstractions, and allows for private implementation details to be encapsulated. It also reduces unexpected coupling and side effects, ultimately leading to a more flexible design that is easier to change. Most importantly, establishing these contracts forces engineers to really understand the problem space and the features they’re building, which always yields better results. Most of our team had already worked together for several years on codebases we inherited, and we knew from experience that it was important to add clear domain boundaries from the very start. As a team, we agreed the best way to start codifying our boundaries was to create separate Flutter packages for each of the major screens in our app. This was a forcing function that served multiple purposes. First, it allowed our team members to work independently on separate screens without stepping on each other’s toes. We wanted to provide space for experimentation and for engineers to discover which patterns worked best for us in a new technology stack. Second, it supported our team’s goal of ensuring that all behavior was covered by tests. In order to pass continuous integration (CI) checks, each package needed to be fully covered by automated tests. Our boundaries forced each package to be independently testable, which increased our confidence as we developed. The APIs for these packages were often simple and straightforward. Each package exposed a widget that represented the entire screen, and it would define the dependencies it needed to fulfill its purpose. Everything else in the package was private. As a result, the look and feel, user interactions and state management of the screen were implementation details that could freely evolve without impacting other parts of the application. As we began coding, we stubbed out the few packages that represented our first set of features. This included making skeletons for a Home Screen , a Search Screen and a Vehicle Details Screen for viewing more information about a listing. Our top-level app package focused on properly stitching these packages together to create a functional user flow. With this in place, we could divide and conquer and easily work in parallel. At this point, our team members continued to test and learn, identifying what worked best for us while using Flutter. We started almost exclusively using the BLOC pattern in each of these packages and explored adding other design patterns we had been accustomed to from traditional native development. Throughout this early period, the only things that remained constant were our package boundaries and a focus on achieving 100% test coverage via each package’s public API. After a few weeks, our understanding of Flutter’s widget tree grew, and we started to recognize that the patterns we were applying for state management weren’t serving us well. They forced us to create extra layers of abstraction, unnecessary boilerplate and made the codebase too complex. In many cases, we learned that we could solve the same problem with a simple StatefulWidget and far less code. At this point, the value of our testing strategy became apparent. Because we were testing via the package’s public API, our tests were not coupled to the implementation details, but instead were focused on asserting the behavior of the package. This allowed us to ruthlessly refactor and swap out layers of code wholesale, often without changing a single line of testing code. As the app grew in both size and complexity, the number of packages we have created has grown. Today, after two years of development, our monorepo consists of about 240,000 lines of Dart code and 5,500 tests spread across 80 packages. Over the past 24 months, a few patterns have emerged for us with regards to our state management. A fair amount of state gets created during app initialization that needs to survive for the life of the application. Our Application Package is responsible for initializing and holding a reference to this state, often with a StatefulWidget near the top of our Widget Tree. It then dependency injects these classes or behaviors down the widget tree via Provider or InheritedWidgets . Within each domain’s package, there is often state that is scoped to a particular screen. We intentionally do not apply a consistent pattern here. Each package has evolved to use whichever state management solution is appropriate for the job. We’ve applied many patterns with success (and some without success!), including BLOC, InheritedModel and exposing Streams and ValueListenables via InheritedWidgets. In many cases, we’ve swapped out state management tools, and in many more, we plan to. Our approach has been to listen to the code, and choose the best tool that fits the needs of that particular domain. It has been more a matter of style, than a key architectural decision. To better understand the approach we’ve taken to our package structure, let’s look at an example. One of the key features in our buying flow is the ability to search for vehicles on our Search Screen , and navigate to a Vehicle Details Screen to learn more about the vehicle and purchase it. If we break these screens down to their simplest requirements, they look something like this: Search Screen Integrates with Search API Provides Infinite Scrolling of Listings Provides mechanisms to filter and sort through results Needs to navigate to another screen upon tapping a listing Vehicle Detail Screen Integrates with a Listing Details API Provides rich content about a particular listing Needs to navigate to other screens in order to transact on the listing (chat, buy, place bid, etc.) These two screens feel distinct and have very different reasons to change. They are ideal candidates to be separated by clear boundaries. Let’s start by modeling the contract for the Search Screen . For this screen to be independently tested, two dependencies should be injected. In this example, we chose to inject these dependencies into an InheritedWidget that sits closer to the root of the widget tree than our Search Screen widget. Note: In this example, we are only injecting a few dependencies. In our app, we inject many dependencies into our domain packages such as APIs for analytics reporting, feature flags, platform APIs, etc. This enables any widget within the Search package to access these dependencies using a BuildContext: SearchDependencies.of(context) . This is conceptually no different than accessing Theme.of(context) or any of the other built-in InheritedWidgets. From a testing perspective, we can simply inject whatever fake implementations are needed for a given test case and can fully test the behavior of the Search Screen package. While we sometimes use this strategy to unit test individual widgets, we often test the top level public widget of the package. This helps ensure that our tests validate the overall behavior and aren’t coupled to implementation details. We even use this strategy to provide mock data to perform full screen screenshot tests. You can read more about that in our previous blog post: Screenshot Testing with Flutter . This approach to dependency management has other benefits as well. We use the same approach for our Vehicle Detail Screen . While the primary use case is to render a live eBay listing, we have a feature in our selling flow that allows for a seller to preview their listing before publishing. This preview functionality was easily achieved by wrapping the Vehicle Detail widget with different dependencies for that use case. Now that we have the public API for our Search Screen package, let’s look at how we might integrate it within our application. In this simple example, a stateful widget lives in our Application package and constructs the concrete implementation of our API client. This is one of the root widgets of our application and is responsible for constructing our MaterialApp. Note, we are configuring the dependencies and placing them above the MaterialApp. This is critical because MaterialApp provides our root navigator. This means as we navigate to new routes these same dependencies remain available from context because they are at the trunk of the widget tree. We would then add a simple integration test in our app package to validate that we’ve wired up our packages correctly. You may have also noticed that each package exposes its own localization delegate. In order for each package to be independently testable, the package needs to fully own all of its resources: images, fonts and localized strings. Obviously, this example has been heavily simplified. If taken at face value, this package structure could seem excessive. However, in our codebase, our Search Screen package has grown to 17,000 lines of code and over 500 tests – it is large enough that we are actively working to decompose it into smaller, more manageable pieces. In practice, this package boundary allows developers working on other features to completely ignore all of this complexity. Likewise, when someone does need to work on search, they are able to work exclusively in the Search Screen package and ignore the entire rest of the application. This approach provides a foundation to scale out our codebase in a manageable way. We can easily have multiple large scale features being simultaneously developed with minimal friction. Working in a smaller package gives focus and improves developer turnaround times. If a developer makes a change, they only need to re-run the tests in the affected package, or occasionally in the app package if their change impacts the public API. We’ve also completely avoided singletons and global state by always managing our state via the widget tree. Because of this, Flutter’s stateful hot reload works consistently throughout the entire application. It enables us to add options to our in-app developer menu to switch to our QA environment — forcing all API dependent state to be discarded and for the entire app to seamlessly switch environments without being recompiled. This has allowed us to avoid adding environment-specific build flavors. The only compile time variation we have is an optional build argument to include the developer menu. Having decoupled packages has also led to huge benefits in our CI pipeline. If we were to build, analyze and test all of the code in our repository, it would take over 20 minutes. However, because each package is independently testable, we’ve optimized our CI pipeline to build and test packages in parallel across our build servers. We’ve gone a step further and for our pull requests (PRs) we only build and test those packages that are impacted by the affected files. We do this by evaluating which packages transitively depend on the changed packages. This means our CI turnaround time is often under five minutes for most pull requests. This, however, has not come for free; it has required continual iteration and optimizations along the way. If you plan to have a monorepo with multiple packages, you should plan to invest some of your time in developer tooling and CI automation. Getting the right package boundaries is not always easy. The example we walked through is cut and dry, but the problem becomes much more nuanced with complex features that span multiple packages. Consider a feature where the user can “like” or “favorite” a vehicle on both the Search Results Screen and the Vehicle Details Screen. Sometimes we didn’t discover the right package boundary until late in feature development. Reworking these boundaries takes time and effort, and it can be tempting to kick the can down the road and defer cleanup. It can also be tempting to shove reusable code into a Common, Shared or Utilities package. However,  the “easy” way out almost always results in the accrual of tech debt. For a long time, we resisted creating single-purpose packages under the false assumption that adding more packages was bad. We’ve finally moved past that assumption and have since almost finished decomposing the last of our dumping ground packages and couldn’t be happier. For our team, breaking down and applying domain modeling to our application has been far more important than choosing the right state management tool. State management fads come and go, but if your app is to survive, modeling is forever.", "date": "2021-04-01"},
{"website": "Ebay-Engineering", "title": "Building a Product Catalog: eBay's 2nd Annual University Machine Learning Competition", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/research/building-a-product-catalog-ebays-2nd-annual-university-machine-learning-competition/", "abstract": "Participating universities will structure listing data to help solve a real-world ecommerce challenge. After last year's success, eBay is once again hosting a machine learning competition on an ecommerce dataset of eBay listings. This challenge is open to college and university students, and the winning team* will be offered a 2021 summer internship with eBay. We invite students to start using our dataset to solve a real-world ecommerce challenge. There are many datasets out there, but the primary focus has been recommender systems, price estimation, computer vision, Natural Language Processing (NLP), and more. None have been at a scale pertaining to mapping unstructured items to well-cataloged products. Like last year, we sincerely hope that making this real-world dataset available will entice students to explore the ecommerce domain further and come up with novel approaches to solve complex problems that can positively impact our platform and services. Problem The question we invite students to address is how to identify two or more listings as being for the same product by putting them into the same group. We call this Product Level Equivalency (PLE). That is, if a buyer purchased two items from two different listings in a single group, and assuming the items were in the same condition, they would assess that they had obtained two instances of the same product. PLE is defined over manufacturer specifications. That is, offer specific details such as condition, or item location are to be ignored. For example, a broken phone and a new phone with the exact same specifications (make, model, color, memory size, etc.) are considered to be Product Level Equivalent, while a golden and a gray phone of otherwise the same make and model are not considered Product Level Equivalent. The objective is thus to produce a clustering of the listings according to PLE. More mathematically, let L be the set of all listings. A clustering C is a partition of L into disjoint subsets: Ideally, all listings in each C i are Product Level Equivalent, and listings from different clusters are not Product Level Equivalent. The measurable objective, evaluation, submission format, and other details are available on EvalAI. Data The data set consists of approximately 1 million selected unlabeled public listings. We also provide an Annexure document that describes the columns and parsing logic. Approximately 25,000 of those listings will be clustered by eBay using human judgment (“true clustering”). These clustered listings will be split into three groups: a) Validation set (approximately 12,500 listings), b) Quiz set (approximately 6,250 listings), c) Test set (approximately 6,250 listings). The validation set is intended for participants to evaluate their approach. Anonymized identifiers and cluster labels will be provided to the participants. We will release the validation set along with the main dataset. The quiz data is used for leaderboard scoring. The test set is used as a factor to determine the winner. For the quiz and the test datasets, neither the listing identifiers nor the cluster labels will be provided to the participants. Hosting The challenge will be hosted on the open-source platform EvalAI. College and university students will submit their entries through EvalAI, which will be evaluated for leaderboard scoring. Please checkout the EvalAI challenge page for more details. Timelines Dates are subject to change, but expected deadlines will be: August 24th, 2020 – Challenge begins. Access to the dataset is granted. We start accepting submissions through EvalAI and begin the evaluations. February 1st, 2021 – Challenge ends. February 22nd, 2021 – We announce winners. Participation Criteria and Prize Teams (no more than 5 members per team) must only include students who are interested in an internship. Assuming eligibility criteria are met, members of the winning team will be offered an internship for Summer 2021 at eBay Inc. eBay’s internship program is a combination of real work experience plus a robust program giving interns exposure to various business verticals, executives and networking opportunities. The internship will also be an excellent opportunity for students to put their ML models into real use. Further details on the participant eligibility criteria, internship prize eligibility criteria, official contest agreement, and rules for the competition, as well as other details, are available as part of the official contest rule package. See eBay Contact details below to receive the official contest rule package. eBay Contact To find out more about how to participate in the challenge and receive the official contest rule package, please reach out to MLChallenge@ebay.com. *Teams should be no more than five members", "date": "2020-08-25"},
{"website": "Ebay-Engineering", "title": "How eBay’s Buy APIs Hit $5 Billion in Gross Merchandise Bought", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/how-ebays-buy-apis-hit-5-billion-in-gross-merchandise-bought/", "abstract": "eBay’s Buy APIs enable third-party developers to surface eBay inventory in their shopping experience, allowing consumers to purchase items without visiting an eBay site. At eBay, we have a large, powerful and vibrant ecosystem of third-party applications. For us, APIs are the foundation of the business-to-developer model and are instrumental in achieving business objectives and overall success. We treat our APIs as first-class products that democratize access to our marketplace platform — and we share the success with our partners. We launched the developers program in November 2000 , and the initial set of APIs brought a lot of value to our platform. Some of them are still heavily used and are important to our customers, but changes in the e-commerce space made it clear that it was time to revisit our portfolio to accelerate growth in the space. That’s why in 2016 we decided to completely revamp our developers program and deliver new, modern RESTful APIs. In addition to agreeing on standards and patterns, the essential step was defining the API taxonomy. We classified our capabilities into four contexts: Sell — includes APIs for sellers to manage their eBay business at scale. Buy — encompasses capabilities that allow surfacing eBay items on third-party shopping sites and placing orders from anywhere online. Commerce — consists of capabilities that benefit both buy and sell integrations. Developer — provides insights to developers into their integration with our APIs. By the end of 2020, our Buy APIs reached a significant milestone, surpassing more than $5 billion in cumulative GMB (Gross Merchandise Bought), as calculated from January 2017 to December 2020. Here is how we achieved that. eBay brings more than one billion listings together to surface the greatest inventory selection to our partners. This includes new, everyday items as well as rare, unique finds. If something exists in the world, you can most likely find it on eBay. Our focus is on enabling a shopping experience that is simple and data-driven, allowing buyers to find, compare and purchase items they need and want. The requirements for high-volume and low-latency buying radically influenced our API capabilities. Four Buy APIs contributed most to the overall GMB milestone. The Browse API gives partners access to eBay’s powerful in-house search capabilities. The API also supports creating a search refinement experience by further filtering responses by key aspects (e.g., brand, color, size). The Feed API allows authorized developers to curate, mirror and surface eBay inventory at scale. The Order API enables secure checkout for guests and members from anywhere online, without visiting eBay sites or using eBay’s native apps. Also, it provides updates on order status, shipment tracking details and expected delivery dates. The Offer API allows buyers to place proxy bids and retrieve bidding details. In addition to the above four APIs, partners leverage other APIs in our portfolio to integrate with various parts of their business — like the Deal API to retrieve deals and sales events, and the capabilities from our Commerce family of APIs. The Taxonomy API allows developers to understand the eBay taxonomy and choose the best categories to surface for their end-users. The Translation API is based on in-house models and state-of-the-art algorithms optimized for e-commerce that translate item titles and descriptions, helping cross the language barrier. Finally, the Charity API enables partners to discover the 101,000 charitable organizations supported on eBay, and to contribute to the more than $1 billion in funds raised by eBay for Charity since 2003. What brings value to the APIs are successful integrations. APIs are the building blocks that developers use to create great experiences for their users. Not all API integrations are created equal, and third parties innovate by putting these building blocks together in unique ways. While delivering APIs, it is crucial to partner with trusted developers and solicit direct feedback to help shape strategies and roadmaps. The design-first method allows the API providers to continuously evolve the interface while iterating with consumers. Initially, we released all our Buy APIs as a beta. We focused on building relationships with trusted partners, traversed the globe to meet them, and gathered feedback as often and as early as possible. Indirect feedback is equally important. The API usage analysis points to workarounds and missing capabilities in the API portfolio. With that, accurate insights — including operational and business metrics — are among the main drivers to enhancing API capabilities. Delivering an API is only half the work. It’s important to consider the overall developer experience, by enabling tools that improve developer productivity and simplify integration in order to boost adoption. We joined the OpenAPI Initiative in 2017 to make it easier and faster for developers to integrate with our APIs. The OpenAPI specification is the industry standard for describing APIs. For all our RESTful APIs , we have OpenAPI documents published — which allow developers to call our APIs in no time . Software development kits (SDKs) are another set of tools that improve developer productivity. In our case, they abstract some of the concepts — like the integration with our authorization services. An example is the Feed SDK. Since our data feeds are large and include the greatest selection of eBay inventory, the Feed SDK abstracts the file manipulation and simplifies the inventory curation. Our SDKs are open-sourced, to give developers full transparency into their integration with our APIs (and we appreciate contributions from our developer community). Keeping your developer partners informed and connected is key to a successful partnership. Since 2017, we have been hosting the eBay Connect conference, where we showcase our new APIs and eBay’s top initiatives to our developer ecosystem. We have had numerous developer events worldwide — including in Japan, China, the U.K., Germany, Australia and Israel. The global pandemic did not prevent us from connecting with our top developers. Over the summer, we held our fourth — and this time virtual — Connect Developer Conference . Despite all of the challenges, we look forward to another great year of events and partnerships in 2021. Delivering successful APIs for our developers is only made possible when the team is empowered to innovate. In the Developer Ecosystem organization, we nurture a blameless culture that is beyond just DevOps. Our team members are encouraged to innovate boldly on our customers’ behalf every day. Flexibility and joy are key to people feeling safe to take risks and try new things. As an example of our innovative culture, last year we leveraged our API portfolio for a good cause, by partnering with the U.K. National Health Service (NHS) to launch a portal for distribution of personal protective equipment (PPE) to health care providers across the country. The integration with eBay is via Buy APIs , and our team worked diligently to deliver this solution within days. This integration was recently recognized by Modern Retail Awards in their Most Responsible Retailer category. But most importantly, we are all incredibly proud to have delivered more than 1 billion PPE items since launching in April 2020 and to have the opportunity to help save lives. 2020 was the year of integration. The global pandemic turned many aspects of life digital, and continuous APIfication allows organizations like eBay to interact with customers more efficiently and more frequently. Getting great at digital enables agile business and adds a lot of flexibility. APIs play a critical role at eBay, so we will continue giving developers tools to propel their business and create great experiences for their users — who, in the end, are our customers. Our goal is always to share the success with our partners. To stay informed, please visit eBay’s developer portal . This article was initially published on The New Stack on Feb. 4, 2021.", "date": "2021-02-24"},
{"website": "Ebay-Engineering", "title": "eBay Launches Secure Local Pickup To Simplify The Shipping Experience", "author": ["Shuja  Rahman"], "link": "https://tech.ebayinc.com/product/ebay-launches-secure-local-pickup-to-simplify-the-shipping-experience/", "abstract": "The enhanced feature enables buyers to purchase items online and pick up directly from sellers in their vicinity. This May, we enhanced the Local Pickup experience on the eBay mobile app. Local Pickup provides an alternative shipping solution to transactions facilitated within the same region, enabling buyers to purchase items online and pick up directly from sellers in their vicinity, saving them money on shipping fees. This allows for a more streamlined delivery process. We updated the Local Pickup feature by securing the transaction with a formal tracking event. By using the code scan functionality, the transaction history is immediately updated with a \"Picked Up\" tracking event. This confirms that the exchange took place and formally closes the transaction. In the midst of a global pandemic, promoting the health and safety of our users is crucial to us at eBay. To prevent the spread of COVID-19 during a Local Pickup transaction, we recommend following appropriate social distancing protocols. eBay seller Trad’r Don has been selling through his eBay store Garbsafari since 1998. Hoping to stay competitive during the pandemic, he conducted his first socially distanced, face-to-face transaction using the updated Local Pickup feature. “This was the first time that I was able to use eBay’s QR code for Local Pickup,” said Trad’r Don. “It is very intuitive and gives me access to even more buyers that are right in my backyard. Avoiding a high shipping cost and a means of safely completing a legitimate transaction makes local pickup an attractive option for nearby customers.” Now live on the eBay mobile app, the updated experience leverages camera application and QR code scan capability to confirm and finalize the transaction. For Buyers To pick up a package, a buyer and seller communicate using eBay’s member-to-member platform to identify a meeting location for the transaction. Upon arrival, the buyer provides the seller with the purchase confirmation and a secure eBay QR code. At this point, the seller scans the QR code using the eBay App on their mobile device. For Sellers By scanning the QR code, the seller marks the transaction as completed — in this case, “Picked Up.” The tracking API then generates an eBay Tracking Number with both date and time stamps, and propagates it across eBay selling tools, APIs and buyer delivery experiences. The updated feature ensures a safe transaction for both buyers and sellers, while notifying eBay that an item has been securely exchanged. With this solution in place, eBay provides fulfillment flow clarity and the added security of a pickup confirmation signal, allowing for increased adoption across our marketplace. Disputes and claims are handled easily with the provided tracking data, and any money transacted on eBay is now protected from fraudulent chargebacks. The enhancement to Local Pickup is the brainchild of eBay’s 2018 Hack Week. A team of six employees worked diligently to build an end-to-end shipping solution that would benefit buyers and sellers worldwide. The feature is already enhancing the eBay shopping experience for millions of customers across the U.S. and will continue to roll out in the U.K., Germany, Australia, Canada, France, Italy and Spain this summer. The feature will be available to all buyers and sellers across iOS and Android devices.", "date": "2020-07-06"},
{"website": "Ebay-Engineering", "title": "eBay Makes Search More Efficient Through Personalization", "author": ["Shreya Raval"], "link": "https://tech.ebayinc.com/product/ebay-makes-search-more-efficient-through-personalization/", "abstract": "eBay introduces Best Match to personalize buyers’ search feeds. When buyers search on eBay, the default order of results is called Best Match. It's designed to show the most relevant listings and incorporates a variety of quality, demand and market-driven factors, such as item popularity, pricing, shipping, regions, seller details, etc. eBay has significantly invested in efforts to enhance the machine learning algorithms to improve the quality of search results for buyers. The most recent update to Best Match personalizes the search results for each buyer. eBay’s marketplace is extremely diverse. In a single category you can find inventory ranging from a few cents to hundreds or thousands of dollars. There are 174 million active buyers on eBay, and every individual has their own criteria to define a perfect item and price range for the product they are shopping for. Previously, when users searched for a product on eBay, the Best Match algorithm would show the exact same results, irrespective of the individual needs. We introduced the price propensity feature in search ranking to customize the search results based on a user’s price preference. Price propensity takes into consideration a user's past purchases at eBay, as well as the inventory they were interested in. Based on this information, a machine learned model tries to predict user’s price propensity for future purchases and uses this to surface search results that are personalized to the user’s price preferences. As an outcome, we see customized search results for individual users according to their preferences and higher engagement with those results. Price propensity is just a first step toward personalization in ranking. There are many other factors that we are exploring as part of this work stream. With these features in place, we hope to make the eBay shopping journey for a buyer a simple and efficient experience.", "date": "2020-06-23"},
{"website": "Ebay-Engineering", "title": "eBay Makes Promoted Listings in Search Results More Relevant and Dynamic", "author": ["Shreya Raval", "Yi Liu", "Gajanan  Adalinge"], "link": "https://tech.ebayinc.com/product/ebay-makes-promoted-listings-in-search-results-more-relevant-and-dynamic/", "abstract": "The Promoted Listing algorithm continues to build revenue for the marketplace and makes sellers happy. The success of any ecommerce marketplace is dependent on the successful transaction between the seller and buyer. There are millions of active sellers on eBay, and they constantly try to increase visibility of their listings in order to drive sales. Like any other big marketplaces, eBay also provides sellers the ability to promote their inventory via different programs, one of which is Promoted Listings. By paying an additional ad rate fee, it enables sellers to make their item stand out among billions of listings on eBay. When a seller opts into the program, their item shows up as sponsored on search result pages at a higher rank, increasing the chances of that item selling faster. The program has grown more than 100% over the past couple of years. Implementing a unified ranker for both non-promoted and Promoted Listings has driven the most growth. It focuses on how and when we show Promoted Listings to our buyers and addresses some of the key customer problems. Prior to the new unified ranking, Promoted Listings (PLs) and organic search results were served on the Search Results Page (SRP) from two different tech stacks with different underlying algorithms, which resulted in a sub-optimal buyer experience. Some Promoted Listings were showing up in fixed slots. The queries with poor quality Promoted Listings were showing irrelevant or low-quality results in those fixed slots. eBay strives to provide an equal opportunity for all sellers to gain visibility while also preserving the buyer experience. The unified ranker introduces the ability for non-promoted and Promoted Listings to compete for top search slots with the objective to maximize relevance and balance GMB (gross merchandise bought) cannibalization. It ranks promoted items by looking at several factors, including but not limited to, relevancy to the search term, item’s (and seller’s) past performance and the ad rate that the seller has set for the item. We continue to refine our algorithms to give the optimal weight to each factor. For some queries, this means promoted listings on top of search. This usually happens when the competition is high (meaning a large number of items which are relevant to the query are participating in promoted listings, and the ad rates are high). Sometimes, zero promoted listings are in the top of search. This usually happens when there are not enough relevant items to the query, and participation in Promoted Listings or the ad rates are not high. The examples below show two queries with different numbers of Promoted Listings based on the above criteria. The relevance of Promoted Listings increased substantially, which lead to increased sales for sellers who participated. This result was a winning combination of happy buyers, happy sellers and a significant increase in eBay’s advertising revenue. This is one of the big product wins eBay had in 2019. This is our first step towards surfacing PLs in floating slots on Search Results Page. Also, this is a new concept introduced in eBay’s dynamic marketplace, wherein promoted and non-promoted listings compete for SRP placements based on quality, conversion and potential revenue. Most other marketplaces like Amazon, Etsy etc. have promoted listings on fixed positions/placements on the Search Results page. As a next step, we are also working to understand the impact on the buyer from Promoted Listings at the top and will optimize the top of search experience based on buyer feedback. On the seller side, while a majority of the sellers have said that Promoted Listings is a great tool to increase visibility and gain traction in the market, we do recognize that some sellers can take advantage of this program by bidding high and dominating the top of search. We will continue to invest in algorithmic improvements that improve the quality and relevance of promoted and non-Promoted Listings at top of Search.", "date": "2020-07-16"},
{"website": "Ebay-Engineering", "title": "eBay’s Image Clean-Up Feature Brings the Power of Image Processing Algorithms to Android", "author": ["Dzmitry  Lebedzeu"], "link": "https://tech.ebayinc.com/product/ebays-image-clean-up-feature-brings-the-power-of-image-processing-algorithms-to-android/", "abstract": "This feature enables our sellers to create cleaner listings. Editor’s Note: Image Clean-Up launched on iOS and Android devices in February 2020. This article provides several updates for the feature on Android. Powered by computer vision, eBay’s Image Clean-Up feature solves two issues. First, the importance of quality, visual information is increasing every day. eBay needs to constantly improve the visual representation of listings on the platform. Second, we found that most users do not have access to professional photo equipment that can be used to take photos of items on a white background, nor do they have the ability to successfully use photo-editing software. White backgrounds are important for listings to be optimized for Google Shopping, and they improve the shopping experience for buyers by making search look and feel more streamlined. One of the most important decisions in the implementation of Image Clean-Up was to make the entire feature deploy locally on the device with the elimination of any service interaction. Image data is heavy, and we didn’t want to transfer such large amounts of data between client and server. It would also introduce a significant lag for each operation, which is not acceptable due to how critical interface performance is for the user (slower application is one of the top reasons for uninstalls). By removing back-and-forth, client-server communication, our approach allowed for faster image processing. This was made possible by our close partnership with the Computer Vision team, who developed a powerful image processing algorithm and built a native library responsible for processing images on mobile platforms. The feature consists of two paths – auto-cleanup and manual touch-up. This allows us to meet the needs of all user categories, providing the functionality in both automated (for quick and easy editing) and manual (for more fine-grained image tuning) modes. From a technical perspective, the feature on Android is isolated in a separate module, which increases build speeds and testability. The module can also be made dynamic in the future, which would enable the feature to be downloadable on demand. The addition of a native C++ library into the application and the need to support multiple processor architectures like ARM and x86 (both of which have 32- and 64-bit versions and require a separate build of native code) made us rethink how the app is packaged. The result was the implementation of app bundling – the distribution of the app via a bundle with Google Play, building a specific version of the app for each specific device. This allowed for a significant reduction of the app size for each specific user. The Selling team worked closely with the Foundations team to help with app bundling implementation. Since launch, Image Clean-Up has been a huge success: 76% of sellers that used the feature published the listings with the images processed by Image Clean-Up 1 . Image Clean-Up is now live on Android for all users in 16 countries. 1 According to data collected between April 1 - 22, 2020", "date": "2020-07-21"},
{"website": "Ebay-Engineering", "title": "eBay Helps Buyers More Easily Discover Competitively Priced Items", "author": ["Prerna Dubey", "Manoj  Kannadasan"], "link": "https://tech.ebayinc.com/product/ebay-helps-buyers-more-easily-discover-competitively-priced-items/", "abstract": "eBay recently launched a new feature that helps buyers find things easily on the Search page by highlighting competitively priced items from trusted sellers. A core eBay advantage is the ability to offer a highly diverse and competitive selection of inventory. Buyers choose eBay for prices and selection, including brands at discount. With 1.5 billion listings to consider, eBay recently launched a new badge feature called “Great Price” that helps buyers find things easily on the Search page by highlighting items which are competitively priced from trusted sellers. “Great Price” should become a trusted signal that will help users discover good prices, narrow down their consideration set and make a purchase decision easily. The new badge called “Great Price” highlights items which are competitively priced amongst similar items from trusted sellers. We have seen an increase in buyer engagement with “Great Price” items that can help drive higher velocity for sellers. We are launching this feature on certain products that are high in demand, based on sales history, impressions and engagement. The appearance or placement of the “Great Price” badge on listings will depend on a variety of factors, including but not limited to: Listings from sellers that meet Above Standard or Top Rated seller status; Listings with condition new and matched to a product in our catalog ; Competitively priced across similar listings on eBay; Item’s price and shipping cost, listing format, terms of service, history and relevance to the user query; and Buyer’s Search terms, browsing site. “Great Price” is not a paid feature and the placement of listings in search results remains subject to Best Match factors. Promoted Listings remains a separate program. If a listing qualifies for both the Promoted Listing and “Great Price” programs, the listing will show both the “Great Price” and Promoted Listing badges. To become part of this program, we encourage sellers to adopt the eBay catalog, price your listings competitively and offer a great service for your buyers! This feature is now live in the U.S., U.K., Germany and Australia on desktop, mobile and native apps (iOS and Android).", "date": "2020-09-11"},
{"website": "Ebay-Engineering", "title": "eBay Virtual Tracking Number Now Live in the UK and Australia", "author": ["Brian Pao", "Shuja  Rahman", "Irene Keay"], "link": "https://tech.ebayinc.com/product/ebay-virtual-tracking-number-now-live-in-the-u-k-and-australia/", "abstract": "eBay’s unique code flows through Royal Mail and Australia Post networks, automating provision of eligible tracking and event data for buyers and sellers. We are committed to helping customers around the world get exactly what they want, when they want it. Now, eBay is making it easier for sellers and buyers in the U.K. and Australia to have visibility of their packages with ease and confidence. This month, we are launching eBay Virtual Tracking Number (eVTN) in the U.K. following a successful rollout in Australia earlier this year. The new feature creates a unique eBay identifier — either on the shipping label itself (in Australia) or within the pre-advice and barcode data (in the U.K.) — allowing Royal Mail and Australia Post to automatically identify them in their networks as eBay packages due to the deep integrations with these carriers. What this means is that, for the first time, sellers using Royal Mail or Australia Post services through in-scope shipping channels don’t have to worry about forgetting to upload their tracking information back to eBay. While sellers are still highly encouraged to upload tracking information, Royal Mail and Australia Post will pick up eBay’s “virtual” tracking number and associate it with the parcel’s ID, allowing them to upload the tracking data on the seller’s behalf, with no further effort needed. In turn, buyers will be updated on the status of shipments 1 posted using eligible services. 2 The new feature associates a unique code (eBay:XYZ1234) with each order at checkout, so when sellers print postage labels off-platform on eligible Royal Mail or Australia Post shipping systems, the carrier can pick up the unique code from the metadata in the address, label barcode or pre-advice. eBay is given the corresponding tracking number with no further seller intervention needed. Directly integrated with our platform, these local carriers are able to provide eBay with available tracking and event data. 2 This means that sellers using Royal Mail or Australia Post will have their tracking numbers uploaded on their behalf 1 if they forget to do so themselves, creating a more seamless shipping experience. The new feature enables eBay sellers and buyers using Royal Mail and Australia Post to have more visibility of their shipments. For sellers who do not currently upload their tracking information onto the eBay site, eVTN will automatically add it for them. 1 On the other end, buyers will be able to view any available tracking and event data on eBay as opposed to tracking off-platform. The code improves the post transaction experience for buyers, increasing visibility of any eligible event data for their shipment’s journey. On a broader scale, eVTN presents new opportunities for eBay to solve seller and third-party shipping solution behaviors with improved tracking and event data. Sellers in the UK can find more information on eVTN, here. Sellers in Australia can find more information on eVTN, here . 1 For Australia Post, only articles received with an electronic manifest are automatically identified. This does not apply for articles lodged over the counter, such as prepaid satchels, that sellers will still need to enter tracking numbers on eBay for. 2 Only tracking and event data visible on the Royal Mail website will be shared with eBay, and only eligible services will have tracking and event data provided. For certain services, tracking and event data will only be made available after an item has been delivered.", "date": "2020-09-03"},
{"website": "Ebay-Engineering", "title": "eBay Motors: Accelerating With Flutter™", "author": ["Corey Sprague", "Larry McKenzie"], "link": "https://tech.ebayinc.com/product/ebay-motors-accelerating-with-fluttertm/", "abstract": "The UI software development kit enables a consistent user experience across iOS and Android. In December 2018, our team was asked to develop a new Android and iOS experience for buying and selling vehicles on eBay, catered to auto enthusiasts. We were given autonomy to research, design and build the eBay Motors App as we saw fit. However, we needed to go from concept to both app stores in less than a year, and it had to include a feature set that existing eBay users had come to expect. We were both excited and a little intimidated by this opportunity, and we wanted to exceed expectations. Our team had been building native apps together for many years. We knew what we were capable of, and we also knew that this would be a huge undertaking. Building two separate apps with a small team would be nearly impossible if we stuck to traditional native development. In the past, we had researched cross-platform SDKs and had been left unimpressed. Fortunately, just weeks before our team was given its assignment, Flutter 1.0 was released, and it approached the cross-platform problem in a promising new way. Our team had been following the development and preview releases of Flutter, and we were well aware of the enthusiasm that was growing around the technology. The first stable release, as well as Google’s sound roadmap, suggested that Flutter might well be the solution for our team to deliver on both platforms in a compelling way under our aggressive deadlines. On the surface, Flutter certainly seemed to approach UI development in a way that addressed many of our concerns. Still, we knew going all-in on an unfamiliar technology was a risky proposition. Thankfully, our product manager Richard Sipe and designer Thai Dang needed time to develop a product vision, so we had a bit of leeway to properly evaluate if Flutter was the right fit. We scheduled team workshops to learn about Flutter and the Dart language. We approached this with a healthy amount of skepticism and bent over backwards to identify areas where Flutter might not meet our needs. To our delight, the technology stood up to each of our challenges. Before long, even the most skeptical engineers were excited. We found that Flutter enabled us to solve interesting problems faster than we anticipated. The development experience was far more enjoyable and we could quickly build a user experience that was very consistent between iOS and Android. Our team wanted to make a high-quality product and we knew that automated testing was an integral requirement. To our surprise, Flutter’s testing story was among the best we had seen. After several weeks of working with Flutter, we were confident it was our best chance to hit our goals on time. Fast forward nineteen months, our team was able to meet every deadline. Our first beta was in our CEO’s hands within three months of receiving our first product requirements, and a few months later we released our app to the public. We are excited to be the team that positioned eBay among the first enterprise companies to use Flutter in a production app. Our experience has demonstrated how Flutter provides speed and agility, even with all the overhead inherent with an enterprise product. That said, it was not easy. Our team worked hard and dedicated itself to being successful. Along the way, we uncovered unexpected benefits that continue to pay dividends as we have added new features to the eBay Motors App. Let’s get into some of the details. The first challenge was to scale and onboard the rest of our team. We knew we had to hit the ground running once our product requirements started rolling in. No one had prior experience with Dart and Flutter. Everyone spent time learning the new stack, and went through the excellent introductory course at AppBrewery. The harder, but more valuable learning was developing our team working agreement. We had to take two teams, from different backgrounds with different approaches to solving problems, and bring them together into one cohesive unit. This was an arduous process that caused us to shed many of our prior beliefs about developing apps, and forced us to re-evaluate what we truly valued in building quality software. We had fierce discussions about design patterns and architecture, automated testing, and the guard rails that were necessary for us to move quickly, and innovate, without locking us into implementation decisions too early. We can’t overstate how valuable this upfront process was. It allowed us to truly start building this app as one team, and gave us a consistent voice in working with Product. As we began building our app in earnest, we were amazed at the amount of code sharing Flutter enabled us to achieve. Today, our repository consists of: 98.3% Dart code (~220k lines of Dart) 1.1% Scripts, CI, various Automation Tools for our development lifecycle .6% split across Kotlin, Java, Swift and Objective-C This level of cross-platform code sharing far exceeded our expectations. The reality is we rarely spend time thinking about or working on platform specific integration, and instead are able to focus almost exclusively on building product features. Early on, we expected to jump through hoops as we needed to access native capabilities. What we discovered was that it was trivial to integrate with device APIs to enable use of the camera, video capture, machine learning, geolocation, secure storage, and other device capabilities. The Flutter ecosystem provides great plugins for these services. On rare occasions, we wrote our own plugins to encapsulate native 3rd party SDKs. These tasks were approached with uncertainty, but they were all completed in hours. By being able to spend most of our time fully immersed in Flutter code, we realized the holy grail of cross-platform development: everything was truly “write-once.” This held true throughout the entire stack. Everything in the UI from layout, navigation, animation, and localization was shared. All of the business logic, domain models and network stack were shared. All of the table stakes like analytics, crash logging and accessibility features were implemented with shared code. We even got to share a single CI pipeline! Writing software in this fashion was a huge accelerator and allowed us to deliver new features very quickly. One of the benefits and surprises of this high degree of code sharing was the impact it had on our test automation. Flutter’s out-of-the-box testing support outperformed our expectations. We’ve been especially impressed with how deeply we can automate the verification of our app, from complex UI interactions to the finer details of an animation. This is a testament to Flutter’s architecture in that it fully controls the pixels that are rendered across all devices and platforms, but it is also because testing is a first class concept in the framework. You can read more about our testing experience with Flutter in our prior blog post . As we discovered the power of Flutter’s testing support, we agreed to a lofty goal: all production code must have 100% code coverage. While intimidating at first, we learned this was very attainable, and enforced the rule with automation in our pull requests. That automation is still in place today and has never wavered. This upfront discipline and our trust in Flutter’s test framework gave us the confidence to deliver quickly without fear of regression. It is common to build and test some features without ever manually testing on a device. We gained so much confidence in our test suite that our quality engineers started writing production code and became full-time Flutter engineers. The benefits of code sharing are not unique to Flutter, although we would argue that Flutter delivers on the cross-platform promise more successfully than its competitors. However, one of our biggest surprises was that, cross-platform benefits aside, developing in Flutter was far superior to traditional native development. We informally surveyed the team to help validate our decision to use Flutter, and the responses showed the majority of the team believed developing in Flutter was over twice as fast as developing for their prior platform. Perhaps more importantly, 100% of those surveyed enjoyed Flutter development more than iOS or Android. As it turns out, happy developers tend to be more engaged and productive. There are so many other reasons our decision to choose Flutter enabled us to move quickly. We worked alongside Product and Design as a single team, with the shared mentality we were building a single app. Requirements rarely diverged between platforms, and entire classes of work such as managing platform specific backlogs were eliminated. The reduced overhead impacted more than just the engineering team. In many cases, we were able to replace paper prototypes with functioning prototypes for user testing. We did not need to create unique designs based on each platform. Prebuilt Material and Cupertino widgets allowed us to prefer good defaults and create custom user experiences only when needed. This resulted in a consistent end user experience on every device without sacrificing the specific platform mechanics that eBay customers have come to rely on. Designs could then be iterated on in real-time with stateful hot reload, a Flutter feature that allows developers to quickly reload the code on a running device without changing the running state of the app. These unique and interesting changes to the way we work have resulted in increased confidence. We ship a new version of our app on Android and iOS at the same time almost every week. There is never any unintentional feature drift between the two platforms. Time savings layered on top of each other has allowed us to be nimble and quick. For example, in the past, we might be concerned if we rolled out the same feature in both the Android and iOS versions of an app, and our analytics showed the feature was used significantly more on one platform versus the other. In these instances, our first assumption was that the platform with the underused feature contained a programming error. However, with Flutter, the implementations on iOS and Android are identical. We don’t waste time and energy searching for programming errors that don’t exist. The team has continued to add features to the eBay Motor App at a rapid pace, such as a live-chat feature and an escrow account to securely transfer funds to the seller. Flutter has not only met our expectations — it has dramatically exceeded them. This has been much more than a technology choice. In many ways, Flutter has changed and enhanced the way our team operates, making its members demonstrably more productive and happier in their day-to-day work. With speed and agility quickly becoming the new normal, development teams would be well served to look into Flutter for production apps of any size.", "date": "2020-09-08"},
{"website": "Ebay-Engineering", "title": "eBay Partners with Google Assistant to Bring Voice Control to eBay’s Android App ", "author": ["Evan Thomas"], "link": "https://tech.ebayinc.com/product/ebay-partners-with-google-assistant-to-bring-voice-control-to-ebays-android-app/", "abstract": "A new partnership launches voice control capabilities on our marketplace to meet the needs of our customers by creating a more seamless, modern experience. Our focus is always on our customers, and we are constantly innovating and collaborating to provide a simpler, more modern experience for both buyers and sellers on our marketplace. Starting today, our Android customers can now seamlessly blend voice querying with our eBay app experience to facilitate an end-to-end shopping experience on their Android device. This is our first mainstream experience in the voice sphere: integrating the eBay app with Google's Assistant . eBay is also one of the first ecommerce companies to integrate with Google for this voice capability. In 2020, voice activation as a user experience and interface method has reached a precipice. First of all, COVID-19 has forced many of us to stay at home all day, where we are always in close proximity to our mobile devices. At the same time, the rapidly changing pace of current events has increased behaviors, such as obtaining news or instigating calls or texts to connect, through voice activation. Indeed, the very premise of “tap” or “click” shopping is losing ground. The Smart Audio Report conducted by NPR examined the effect of COVID on voice-controlled devices: 51% of Americans use voice assistants on their smartphones, compared to 61% that use voice assistants overall, representing 85% market usage. This is well more than those who use smart speakers, TVs or cars, which represent the other 15%. 52% of customers are asking their voice assistants one or more queries per day, up from 46% pre-COVID (13% increase). Now, 63% of the U.S. population say they use voice-activated assistants of any kind. In addition, this desire for voice control of all devices is building on an already-growing trend that we’ve seen over the past few years. In the second quarter of 2018 alone, the global market for voice devices grew by 187%. From 2019 to 2020, it’s estimated that 111 million customers in the U.S. used voice control via voice assistants. So when Google approached us as an early adopting key partner for voice control, we were thrilled to explore the space with them. As one of Google Assistant’s first integrated ecommerce partners, we collaborated with Google using our deep linking architecture on four main use-cases, each of them central to our core shopping journey and asking the following such questions: “Hey Google, search for a light blue Sneaker on eBay” will open the app, and search for the query voiced; “Hey Google, find an Omnath, Locus of Creation magic card on eBay” will open the app, and locate the specific item the customer is seeking; “Hey Google, show me my eBay Watch List” will open the app and drop the customer in their watchlist; and \"Hey Google, let me see my eBay purchases” will open the app and drop the customer in their purchase history page. To achieve this voice control access to our marketplace, we exposed entry points in our application to Google Assistant using an actions.xml file . Then we handled requests defined in that file to link to different flows. Google mapped voice queries based on the contents of our actions.xml. By defining the type of action we allow our customers to do, and on which landing page, we can harness Google’s robust capabilities to map that action to human language. With the help of Google’s natural language processing technology, variants of each of the above, in many languages, will be understood and enacted in the experience overall. This enables us to engage with customers all over the world extremely easily. This will make our customers’ experiences more seamless when they have their hands full but want to continue shopping on eBay. We’re excited to find future synergies with Google Voice in the near future and to use this partnership as one of the many ways we are accelerating our tech-led reimagination of our marketplace in service of our customers. The feature is now available worldwide. So for all of our Android customers — we hope you’ll start chatting with your Google Assistant to find your passions on eBay! Sources: Google, Voice Assistant Insights eMarketer Report: US Voice Assistant Users 2019 Quoracreative, Voice Trends Mobile Marketer, The Mobile Voice Study Brafton, Voice Search Statistics in 2019 The Smart Audio Report, NPR & Edison Research Statista, Number of digital voice assistants in use worldwide 2019-2024 Voicebot.ai, Voice Shopping to Reach $40 Billion in U.S. and $5 Billion in UK by 2022", "date": "2020-10-09"},
{"website": "Ebay-Engineering", "title": "eBay Launches Targeted Auto Retry", "author": ["Evan Pierce"], "link": "https://tech.ebayinc.com/engineering/ebay-launches-targeted-auto-retry/", "abstract": "eBay’s open source Swift package provides powerful resilience against flaky test steps. As eBay accelerates its tech-led reimagination , having a way to quickly vet and release code changes is key. Building on a foundation of 25 years of code requires thoughtful integration and detailed testing, which can be time-consuming – especially for intermittent errors which are difficult to replicate. To help developers focus on creating features which customers will love rather than troubleshooting regression test errors, eBay engineered a new way to quickly identify bugs so the code can be routed for updates. eBay’s new open source Swift package, dubbed “Targeted Auto Retry,” is a simple, lightweight, but powerful solution which provides resilience against flaky test steps. Mobile teams at eBay that have adopted it have seen their flaky test problems virtually disappear overnight – with up to 99% of flakiness eliminated from their entire regression test suites. This targeted approach has a number of advantages over the more common “global” auto-retry (a heavy-handed solution which automatically retries the entire test if it fails anywhere): By only focusing on specific steps prone to flakiness, Targeted Auto Retry greatly increases the chances of catching important intermittent bugs, instead of glossing over them. This immediately unlocks significant potential value from existing automation suites without any effort. It’s always an explicit and intentional choice to include Targeted Auto Retry for a test step. It’s surprisingly lightweight and easy to use. The package is hosted on eBay’s open source page for Swift. Most Swift developers / testers should be able to get it working and see immediate benefits within a single afternoon. Console logging is already baked into the solution through XCTContext.runActivity, so relevant, actionable information about Auto Retries will be easy to find in any test reporting system without needing to sort through multiple test run logs. These logs also provide flags set in the test steps, which can be used in test reporting solutions to pull out actionable metrics around the retry attempts: ○      [RETRY INFO: \\(retryAttempt)]… ○      [NEW RETRY ACTION: \\(retryAttempt)]… ○      [SUCCESS CONDITION: \\(retryAttempt)]… ○      [RESET STEPS: \\(retryAttempt)]… ○      [FAIL]… Since it’s targeted, the execution time is faster than global solutions. Also, the solution is more configurable / optimizable for potential flakiness in each test step. Targeted Auto Retry is a powerful solution. By design every use is intentional, so it’s unlikely that it will accidentally gloss over an important intermittent bug in the code. However, if misapplied there’s still a chance it could re-try a code segment with an intermittent bug until it works. Be sure to only use Targeted Auto Retry in cases where: You have exhausted attempts to fix the flakiness directly. The test step being auto-retried is part of the setUp / tearDown for the main focus of the test. If there is a real intermittent bug in the test step, you don’t want your test to catch it. (Or, there is a separate test focused on this test step without auto retry, which will catch real intermittent bugs.) Instead of retrying entire tests on failure, Targeted Auto Retry focuses on retrying just the steps which are most likely to cause issues with flakiness. You identify a test step which is prone to flakiness (e.g. app launch). The mainAction for the test step runs (e.g. XCUIApplication().launch()). A successCondition validates whether the mainAction step was successful (e.g. XCUIApplication().buttons[“My Awesome App”].exists). If the successCondition is true, you’re all set and you move on to the rest of the test. If the successCondition is false, that means the mainAction test step failed (or was flaky), so you automatically circle back to retry the mainAction test step. In some cases, you may need additional steps to reset the state of the test to just before the mainAction was attempted before retrying it, so an optional resetSteps code block is available to handle that if needed. If the mainAction continues to fail consistently (meaning the successCondition continues to be false) eventually it stops attempting retries and just fails the whole test. Since adopting Targeted Auto Retry, mobile teams at eBay have virtually eliminated flaky automation tests and have seen a number of improvements to their testing: Code coverage numbers increase substantially when flaky tests no longer artificially drag them down; Less engineer time is spent maintaining and troubleshooting failing flaky automation; Existing automation catches critical (but difficult to reproduce) intermittent bugs early in the process, and the results are taken seriously; and QE / Developer trust in the automation tooling and results is enhanced. Having proven its worth across eBay’s mobile teams, Targeted Auto Retry has just been released publicly to eBay’s Open Source platform as a Swift Package , where it can be easily integrated into any existing Swift project. Step-by-step instructions for adding the Swift package are included. Most Swift developers or testers should be able to get it up and running and start seeing added value to existing automation in a single afternoon. Check it out, and discover what virtually eliminating flaky tests can do for your team.", "date": "2021-04-09"},
{"website": "Ebay-Engineering", "title": "An eBay Charity Perspective for Developers", "author": ["Shekhar Banerjee", "Diana Dukart"], "link": "https://tech.ebayinc.com/product/an-ebay-charity-perspective-for-developers/", "abstract": "This article explores charity support in Public APIs and hypothetical integration scenario walkthroughs for developers who wish to integrate with eBay for Charity in their eBay-powered experiences. The eBay for Charity program had its beginnings in the aftermath of the September 11th terrorist attacks. Formally launched on eBay’s global platform in 2003, the eBay for Charity program proudly celebrates the generosity of our community with over $1 billion raised towards charitable causes— including $112 million in 2019 alone. Designed to support a wide range of cause areas including disaster relief, child and youth development, animal programs and environmental projects, over 83,000 charities have been integrated onto the eBay for Charity program worldwide. Of these, over 8,000 charities directly sell to support their causes. In addition, over 360,000 eBay sellers have generously contributed from their own proceeds to support a charity. While this program primarily caters to the United States and the United Kingdom, it also has a presence in Canada, Germany, Australia, and Italy. eBay is actively working on expanding the reach of this program into these countries, while exploring potential expansion opportunities in new regions. In the sections below we will cover both the functional and technical aspects of the eBay for Charity program. The eBay for Charity program gives our buyers and sellers a way to invest in causes they care about. As the shadow of the COVID-19 pandemic spreads across the globe, thousands of our community members have stepped in to help others in this hour of need. You can find examples of eBay listings that support efforts to minimize the impact of the pandemic here . In addition to nonprofits that help with COVID-19 relief, the eBay for Charity program supports a range of other efforts. One such example is the \" More Than Words ” job training and youth development program. More Than Words empowers young people in the foster care system, court-involved, out-of-school or homeless to take charge of their lives by leading a business. As another example, human-I-T has been helping students learn at home by getting them the electronics and resources they need to succeed. The nonprofit has delivered thousands of laptops to school districts. Efforts by charities such as human-I-T help close the learning gap and provide less privileged children the opportunity to obtain the skills they need to qualify for technology jobs. You will find many more inspiring stories here . This section covers Charity for Selling, Buying and Fundraising. Charity for Selling There are several facets to the selling experience within the eBay for Charity program. Any eBay seller who wishes to support charity is called a community seller. A charity organization can also sell to support itself, in which case, it’s called a direct seller. In addition, there is support for charities to sell symbolic gifts to support causes with the Gifts that Give Back feature. In all cases, eBay provides fee credits to sellers. Community selling supports sellers who wish to donate anywhere between 10% to 100% of sales proceeds to a charity they love. In the Motors category, the donation can be as low as 1%. The process is straightforward. Whenever sellers are listing items, they simply select to donate, choose any one of the 83,000-plus charities supported by the program, add the percentage they wish to donate—and they are done! Charity discovery is enabled both on the web and through eBay’s portfolio of APIs . eBay also supports seller efforts by crediting a portion of the final value fees and insertion fees for listing for charity. For more information, go here . Besides eBay fee credits, search results will show that the listing benefits a charity. In the listing itself, the title will display a charity ribbon and the description will show a banner describing the charity. Sellers have reported an uplift in sale conversion for their listings supporting charity. One of our most-supported community seller charities, St. Jude Children’s Research Hospital, has raised a total of over $3 million on eBay. Direct selling supports charities to sell and raise money for themselves and their programs. They receive the same benefits as community sellers with the charity ribbon that displays on listings and the charity banner in the listing descriptions. Since these sellers always donate 100% of their proceeds to their charity, all of their selling fees are credited. For more information, go here. The Gifts that Give Back feature gives buyers a way  to provide direct donations for causes. These symbolic gifts are listed directly by charities to support fundraising efforts for current causes or for ongoing needs. A description of what the money will support is included in the listing description. These gifts have been invaluable in efforts to provide COVID-19 relief along with continuous support for ongoing programs. For more information, go here . Charity for Buying The eBay for Charity program allows buyers to channel their goodwill by purchasing inventory that directly or indirectly aids fundraising efforts for causes or charities they care about. The Browse API , for example, supports search capabilities to surface items that benefit charities. Charity Fundraising Buyers can also support charity during checkout where either a buyer's favorite charity or an eBay-recommended charity is displayed with a list of supported amounts. Buyers can choose a favorite charity on the eBay for Charity U.S. site or U.K. site . Favorite charities will also show up in selling flows for sellers to easily select. Favorites are currently supported only in the United States and United Kingdom. eBay users can manage their donation accounts from the U.S. donation dashboard or the U.K. donation dashboard . This dashboard displays pending donations, donation history, favorite charities and various other settings. The settings include creating an automatic donation payment method to automatically pay your donations 21 days after any sales that involve charity. The 21-day period allows any refunds to happen, if necessary. Sellers can also choose whether they want their contact information shared with the charities they are supporting. This section covers conceptual approaches to integrating Charity with Buying and Selling experiences powered by Public APIs. Selling—A Simple Two-Step Guide to Charity Integration Step 1: Enable charity discovery Enable easy discovery of a seller’s favorite charities using search by keywords or search by registration IDs (which would be EIN in the U.S. or CN in the U.K.) through an integration with Charity Search . Both calls return a paginated collection of charity organization details. The results are sorted by relevance in case of keyword search. Charity API also supports retrieving details for a charity organization for a given charity’s ID. Step 2: Use the selected charity Enable the use of charity ID field from the seller’s selection in Step 1 along with a seller-specified donation percentage while creating or updating an offer for their inventory. It’s as easy as that! Step 1: Enable charity discovery Enable discovery of a buyer’s favorite charities following a similar approach as in Step 1 of the previous section. Buyers should then be able to select multiple charities for Step 2. Step 2: Use selected charities Based on the buyer’s selections in Step 1, use the registration_id field values from the charity API response as charity_ids filter in the Browse call to display items for the selected charities. This filter could be used along with other supported search filters, parameters and field groups. We’re proud to spotlight eBay for Charity, a distinctive program with global impact that spans almost two decades, connects thousands of trusted, active charities with our beloved community and has raised over a billion dollars. eBay for Charity can stake its claim as a platform differentiator. True to the spirit of eBay's principles on powering the world’s largest marketplace through the simple philosophy of “people are good,” eBay for Charity combines goodness with the generosity of millions of our community members in a secure, transparent manner that guarantees trust. The addition of the Charity API to eBay’s portfolio of Public APIs allows developers to fully integrate charitable buying and selling into their eBay-powered experiences. As a reminder, all eBay RESTful Public APIs are released with OpenAPI 3.0 and 2.0 documents for easy integration across multiple technology stacks. As the world grapples with the unprecedented COVID-19 crisis, the eBay for Charity program has played a vital  role, enabling the generosity of our eBay community to seamlessly flow to the charities who have been untiring in their efforts to aid those in desperate need across the globe. There has never been a greater call to push charity to the front and center of buyer and seller experiences.", "date": "2020-08-17"},
{"website": "Ebay-Engineering", "title": "Introducing eBay’s New Time Away Feature to Help Sellers Manage Selling on eBay While on a Break", "author": ["Vikram Shandilya"], "link": "https://tech.ebayinc.com/product/introducing-ebays-new-time-away-feature-to-help-sellers-manage-selling-on-ebay-while-on-a-break/", "abstract": "Time Away is now available to all Sellers and automatically updates the shipping date to make the experience more seamless. Last month we launched the new Time Away feature for all sellers so they can let their customers know almost in real-time when they’re on vacation or away from selling. Like an “out of office” reply, the new Time Away feature allows sellers to schedule their absences and notify customers with an automatic message while they’re away, so buyers know what to expect. Additionally, through the tool, sellers can choose if they’d like to pause or continue their listings on eBay. For our buyers, they can see updated estimated delivery dates for items by a seller who has turned on the Time Away feature. As we innovate on behalf of our customers in creating features like Time Away, we strive to be the seller platform of choice across our marketplace as part of our tech-led reimagination. Sellers can access the Time Away feature through several ways, including their Account Settings, the Seller Hub and My eBay pages. For sellers who’d like to hide their listings while on break, they can use the Pause Selling options in Time Away to automatically hide and unhide their listings based on their time away start and end dates, with only 1-2 hours for the changes to take effect. Pausing sales will hide a seller’s fixed-price listings from search results on eBay and block checkout on their listings while they’re away. Please note that auctions will continue unless a seller manually cancels them. Sellers who’d prefer to continue selling while away will no longer need to manually update their shipping and handling time for their inventory. Our new Time Away feature will automatically adjust the handling due/dispatch timeline and the estimated delivery date so that buyers know what to expect. A message will be displayed on the listings letting buyers know about their absence, return date and the expected delays, including an updated estimated delivery date. Sellers also can adjust their settings, change their end date or cancel their Time Away status at any time. This feature rolled out in September as the holiday season approaches, so sellers can prepare in advance for any well-deserved breaks, allowing them to relax and enjoy their vacation. The new feature is available to all eBay sellers worldwide on desktop and is an update of our original “Vacation Settings” feature experience, which previously had only been available to Store subscribers.", "date": "2020-10-16"},
{"website": "Ebay-Engineering", "title": "How eBay Leverages Kubernetes, Helm Charts and Jenkins Pipelines to Deliver High-Quality Software", "author": ["Zhong Shen", "Rocky Shang", "Ramit Bedi"], "link": "https://tech.ebayinc.com/engineering/how-ebay-leverages-kubernetes-helm-charts-and-jenkins-pipelines-to-deliver-high-quality-software/", "abstract": "Learn how eBay has fully automated our software quality certification to enhance efficiency, minimize manual intervention, and scale for increasingly complex requests and combinations. At eBay , product teams can choose from multiple stacks (Java-spring/spring-boot, Nodejs, Python, etc.) to implement eBay’s 3000+ front-end UI, microservices, batch and messaging applications. The application platform team is a central team that operationalizes open source projects for use at eBay by integrating horizontal capabilities such as monitoring and observability, logging, tracing, security protections, authn and authz, and more. In addition, the application platform team also provides the underlying containers/environment on which the applications run. A bad release by the application platform team can break many applications when it upgrades to the latest version of the platform. Therefore, platform release certification is of utmost importance to the application platform team. This article describes how we have used Kubernetes operators, Helm Charts and Jenkins Pipelines, to achieve full automation of software quality certification and automated test result comparison. The types of changesets that need to be certified frequently: Framework releases: Sitewide upgrade releases Micro version patches Cross-team contributions Sitewide upgrade releases Micro version patches Cross-team contributions Runtime updates: JDK, Tomcat, Envoy and Node.js runtime patches JDK, Tomcat, Envoy and Node.js runtime patches OS updates: Framework certifications with new container images Framework certifications with new container images Others: OS certification: Kernel + OS eBay application container certification OS certification: Kernel + OS eBay application container certification We have architected the certification solution to be efficient and avoid any manual testing and analysis of results. Additionally, we also developed the solution to be self-serviceable and capable of performing any type of certification. The certification solution offers a standard automation template and can orchestrate and handle multiple complex certification requests and combinations. Well-defined certification suites are offered for different types of certification requests. A certification unit is a standalone unit which does one part of certification, including: Simple: Only the test case Test case + Test app + pipelines Performance test Traffic Mirror with response comparison Each certification category (Image Certification, Framework Release Certification, Kernel Upgrade Certification, etc.) has its own Helm Chart, which contains the templates of the certification unit and certification instances. Triggering the certification involves installing the corresponding Helm Chart with user-provided parameters into a Kubernetes cluster. Certification and chart CRD instances are created based on the Helm Chart and user input parameters. Certification Instance defines the Jenkins pipeline git repository and pipeline parameters, while Chart CRD instance defines common parameters, groups of certification units and their dependency relationships. Controllers are the main orchestrators of the overall automation solution. The certification chart controller is responsible for managing all certification unit instances such as dependency enforcement, pausing/resuming/aborting certification, aggregation of status and result of each unit instance. The certification controller drives the whole lifecycle of a pipeline job run through certification service. It also remediates each failed job through back-off retry in order to improve the probability of success. Certification Service is the coordination service between certification controller and the backend Jenkins service. It provides RESTful APIs to perform and query the action for controllers and delegates the request action to the backend Jenkins server for execution. All the Jenkins pipelines are created and deleted on demand. Certification service also saves the job logs before deleting the pipeline. The pipeline flow varies with each individual certification unit type. We have built modularized pipeline scripts as standalone steps so that different pipeline flows can be constructed by reusing them. We have also developed pipeline modules to perform response comparison by forwarding a percentage of live traffic to the target host with n+1 code, load and performance runs (using JMeter and Taurus), etc., and automatically compare the metrics (Transaction time, TPS, GC, Memory, etc.) Developing a unified scalable certification experience is a huge advancement toward our goal to achieve full automation of software quality certification and automated test comparison. We have built different functional modules (e.g. framework upgrade, code deployment, result analysis etc.) using reusable Jenkins pipeline libraries, which are used for instantiating certification units. A certification unit can be easily included in all certification charts just by adjusting parameters and failure thresholds. The self-service certification portal enables team members to trigger certification jobs without worrying about complex configurations. Notification mechanisms with configurable failure thresholds are in place for prompt troubleshooting. Our certification solution has also helped us reduce the complete certification time from days to hours. By leveraging the Kubernetes controller reconciling mechanism, we have added resiliency to network issues and unreliable dependencies, allowing less involvement from human team members. Design your technology with scalability in mind. Sometimes capacity and underlying infrastructure must be upgraded to meet your project’s needs — such as an improved provisioning mechanism, node selector, lightweight jenkins builders, failover clusters, etc. Be bold and try new things. Sometimes open source software is already solving a subset of problems and can serve as a helpful foundation, rather than starting from scratch. Finally, it is important to innovate constantly — with long-term benefits in mind — to create a flywheel effect that scaffolds all of your enhancements into a cohesive, streamlined software system. Improving developer velocity and empowering our engineering team with world-class tools is very important to deliver great experiences to our customers. We innovate constantly by evaluating and incorporating industry standards and best practices in our SDLC and are working on future enhancements for software quality certification: We have plans to leverage Tekton, a powerful and flexible open source framework for creating CI/CD systems, for pipelines and to integrate it with CRD controllers. We are working on the classification of failures, root-cause analysis and predicting future failures using machine learning. Interested in pursuing a career at eBay? We are hiring! Please take a look at our current job openings. This article was initially published on The New Stack on March 22, 2021.", "date": "2021-04-05"},
{"website": "Ebay-Engineering", "title": "AsyncAPI 2.0: Enabling the Event-Driven World", "author": ["Shekhar Banerjee"], "link": "https://tech.ebayinc.com/engineering/asyncapi-2-0-enabling-the-event-driven-world/", "abstract": "Learn about how eBay is standardizing on and publishing AsyncAPI Specification 2.0-based contracts for event notifications. Editor's note: The following article was originally published on The New Stack on April 29, 2021. Though RESTful APIs remain the mainstay of the programmable world, there is rapid adoption of reactive event-driven architecture and a distinct shift from the traditional polling-based legacy integrations. The considerations of an event-based approach are not just limited to the obvious candidates, such as designing a system that reacts to changes in real time, but also include things like increasing adoption in resilient, highly decoupled microservices architectures. When it comes to the asynchronous world, the complexity in designing a specification that abstracts (and thereby unifies) multiple protocols and supports diverse formats, brokers, channels and publish-and-subscribe perspectives is a formidable task — and an ask that AsyncAPI Specification 2.0 admirably meets. In this article, we will discuss AsyncAPI Specification 2.0 and its adoption in eBay’s developer ecosystem, why eBay chose AsyncAPI Specification-based contracts to power its latest notification platform, and how AsyncAPI Specification standardizes and simplifies representation of a highly decoupled and resilient microservices topology. AsyncAPI addresses the need for a unified, open source, protocol-agnostic asynchronous specification that is both human-readable and machine-readable, while also being backed by a diverse and rich tooling ecosystem. With its maturity and elegant abstractions, the AsyncAPI Specification has emerged as the industry standard for defining asynchronous, event-driven APIs. AsyncAPI started as an offshoot from the OpenAPI Specification and maintains compatibility with it. A discussion about the AsyncAPI Specification would therefore be incomplete without an acknowledgement of the contribution of the OpenAPI Specification to the industry in general, as well as eBay in particular. (Learn more about why eBay adopted the OpenAPI Specification for its RESTful APIs in 2017, and eBay’s subsequent journey, in a previous article .) Here are some of the key AsyncAPI Specification features that we found particularly useful: Clean separation between channels , operations and servers . This allows a complete representation of the event-driven topology (the producer , consumer and message perspectives), resulting in standardized and precise representation of a message-driven ecosystem. Support for defining bindings at the operation , broker , channel and message level. Support for traits and external references. This promotes reusability. Support for correlationIds using dynamic runtime expressions. Extensibility of the specification, for those one-off customization needs. Support for a wide range of asynchronous protocols to satisfy most industry needs. Compatibility with the OpenAPI Specification . This enables re-use of schema definitions from existing models, which leads to a short adoption cycle for organizations that have already standardized on the OpenAPI Specification. Late last year eBay initiated work on a new event notification platform, designed to meet current and future demands for asynchronous communication to API partners. Aside from the usual considerations of scale, delivery guarantees, monitoring, playback and recovery, the new platform was also designed for data security; Elliptic Curve Cryptography- based message integrity verification; and support for multiple protocols and payload schema versions. Some of the advanced features also include multi-tenancy with isolation guarantees between different “tenants” of this system, as well as internally separating use-case-specific concerns from core platform concerns. When it came to exposing the event streams of this new platform as a contract to eBay’s external developer community, the Async API 2.0 Specification was the unanimous choice. In March 2021, eBay launched the first AsyncAPI-based contracts for the new business event notification capabilities . The AsyncAPI contract for eBay’s initial marketplace account deletion use case based on HTTP is here . This contract addresses the following integration concerns: Topic available for subscription; Supported protocols; Schema definition for the payload; and Message headers. We found AsyncAPI extremely helpful because of the ease of use that results from delineation of channels, protocols and bindings; separation of concerns between publish and subscribe; and separation of protocol-specific and application-specific headers. Most software systems start as a monolith and evolve over time to a distributed microservices-based approach. The first phase of this transition is the decomposition of the monolith to granular distributed services, with an entity services layer representing persistence. A typical outcome of this phase is often a business process tier orchestrating calls to a single or multiple underlying service tiers in a synchronous manner. Orchestration, however, has issues of scale due to the inherently synchronous nature of this approach. An unexpected aggravation on any of the dependencies, for example, can cascade to a major degradation of functionality without adequate safeguards. A high degree of coupling between the components also introduces significant maintenance overheads when specific components or dependencies change. The next phase of this evolution is a transition to an event-driven paradigm. An event-driven system entails a set of autonomous components that react to specific events or state changes. At a high level this system could be thought of as a set of event emitters and listeners that interact though a message bus. As a consequence of the decoupling, the emitters are not burdened with the listeners of the events they produce, nor do the listeners need to know about the event producers. Each can independently scale. Some of the major pitfalls of an orchestration-based approach are addressed. For further information, here is a great article by Martin Fowler on event collaboration . A simplistic illustration of decoupled microservices is given below: Figure 1: A typical decoupled microservices layout as a set of event emitters and listeners. The illustration above shows a set of emitters and listeners interacting with a message broker. However, representing a complex system of such autonomous components can be daunting. The event streams or channels at the core of this reactive system need to be represented along with details of the mechanism by which information is exchanged — for example, payload schema definitions, protocols and associated headers. Message brokers need to be presented along with URIs, protocol, security configuration and bindings that apply. Operations need to be unambiguous. The AsyncAPI Specification adequately covers all of the key elements in the preceding paragraph, simplifying the representation of this complex topology. An event stream is represented as a channel . The message definition allows both payload schema and header information. The message bus (or broker) is represented in an AsyncAPI contract as the server . Servers can have associated security requirements . The set of event emitters are defined as producers and listeners as consumers . For industry adoption, however, it’s not just the richness of the specification that matters — it’s also the tooling that comes with it. As with OpenAPI , AsyncAPI tooling incorporates powerful visualizers that allow architects and engineers to collaborate on the design. A complex layout can be simplified. Collaboration on design is simplified through standardized taxonomy and AsyncAPI visualizers . Because AsyncAPI provides a machine-readable contract, it means that it can provide code generation for producers and consumers. Here are a few references: Java-spring-cloud-stream-template is an awesome codegen utility to spring up microservices from an AsyncAPI contract. It also comes loaded with quite a few useful customization options. Microcks is an open source Kubernetes mock-and-test framework that supports AsyncAPI. Solace offerings include AsyncAPI support. Postman has long enabled the API world and recently partnered with AsyncAPI. There are more tooling references here . We expect that eBay’s decision to standardize on and publish the AsyncAPI Specification 2.0 -based contracts for event notifications will make integrations simple for eBay’s external developer community. And from our experience with AsyncAPI so far, we definitely recommend using it. Interested in pursuing a career at eBay? We are hiring! To see our current job openings, please visit: http://ebay.to/Careers .", "date": "2021-05-13"},
{"website": "Ebay-Engineering", "title": "Anomaly Detection — Product of Data Refinery", "author": ["Ahmed Abdulaal", "Subrahmanya Harve"], "link": "https://tech.ebayinc.com/engineering/anomaly-detection/", "abstract": "Large scale applications are ubiquitous in today's world, processing hundreds of billions of events and producing thousands of metrics. Sifting through these metrics to surface actionable insights without using scientific methods can be challenging. In this blog post, we explore an introduction to the realm of predictive analytics in the context of anomaly detection. \"Data is the new Oil.\" The quote is credited to Clive Humby , who is believed to have used it first in 2006. It is widely accepted today that any data \"refinery\" will emit insights in the form of metrics. At eBay, applications emit thousands of metrics from various domains such as Checkouts, Search, Payments, Risk, Trust and Shipping to name a few. Maintaining a robust marketplace means systems must be monitored in real time. Essentially, this means tracking the behavior of metrics over time (short/long term) to generate quick and actionable feedback to systems. However, even with domain focus, the volume of metrics is so high that we need to put systems and processes in place that help call our attention to anomalous events in metrics. In this blog post, we explore a basic introduction to the realm of predictive analytics for metrics in the context of Anomaly Detection based on Models and also present a software design for detecting anomalies in metrics. More specifically, we explore time series forecasting as an effective way to achieve next step prediction for metrics. Further, this article will focus on metrics that may be represented as a sequence of scalar observations over discrete and regular time periods, a.k.a. univariate time series. Typically, anomaly detection involves taking historical metric data into consideration, training a model on the data, describing the pattern as a function of historical data points, which is applied in the form of hyper parameters for the model and making a prediction. The prediction is usually in the form of a band of lower value and upper value. On observing a new value in the time series, we identify if the actual value is outside the predicted range and classify the new value as anomalous. Below is a pictorial depiction of this process. A time series is comprised of three components: trend, seasonality and noise. A time series may be described additively as y(t) = Trend + Seasonality + Noise Alternatively, it may be described multiplicatively as y(t) = Trend * Seasonality * Noise Time series forecasting is a vast subject that is continually undergoing research, and new models and methods are being created. In our case, we start with a popular model called ARIMA (Auto Regressive Integrated Moving Average). ARIMA is also referred to sometimes as Box Jenkins model. Most of our metrics exhibit seasonal behavior and in some cases multi-seasonal behavior. As a general rule, we pick a seasonality that is closer to our prediction frequency, since that is the most influencing factor in our seasonal prediction. Examples of such seasonality may be weekly or in the case of intra-day predictions, we pick 24-hour seasonality, assuming that data behaves in similar ways at the same time on any two consecutive days. Next, we have a choice to introduce an external influence to the data in the form of another time series. We refer to this external influence as an exogenous variable. Put together, the model is known as SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with eXogenous variable support). Let's have a look at the mathematical representation for ARIMA. AR is a representation of a data point in terms of time-lagged versions of the point until p points: y t = ∅ 1 y t-1 + ∅ 2 y t-2 + ∅ 3 y t-3 … + ∅ p y t-p y t = ∅ 1 y t-1 + ∅ 2 y t-2 + ∅ 3 y t-3 … + ∅ p y t-p I represents order of differencing to achieve stationarity Δy t = y t - y t-d Δy t = y t - y t-d MA is a representation of past errors that help carry forward lessons learnt from past until q points: y t = Θ t ∈ t-1 + Θ 2 ∈ t-2 + Θ 3 ∈ t-3 … +Θ q ∈ t-q y t = Θ t ∈ t-1 + Θ 2 ∈ t-2 + Θ 3 ∈ t-3 … +Θ q ∈ t-q Together: ∆ y t = Σ p i=1 ∅ i ∆ d y t-i + Σ q j-1 Θ j ∆ d y t-j ∆ y t = Σ p i=1 ∅ i ∆ d y t-i + Σ q j-1 Θ j ∆ d y t-j SARIMAX is simply a product of ARIMA with non-seasonal hyper parameters and ARIMA with seasonal hyper parameters. For simplicity, we will only provide an abstract representation of SARIMAX below: ARIMA(p,d,q) (non-seasonal) X (P,D,Q) S (seasonal) ARIMA(p,d,q) (non-seasonal) X (P,D,Q) S (seasonal) The hyper parameters of ARIMA are p, d, and q. The seasonal hyper parameters are denoted by P, D, Q. S refers to seasonal period for the time series. In addition to the above, the model also needs another parameter that describes the trend of the time series. Usually, the trend will be a constant or observed to vary linearly. Before we can fit the model to the series, we must choose the right hyper parameters. We can select the values of the hyper parameters in one of three ways: Plot the ACF (Auto Correlation Function) and PACF (Partial Auto Correlation Function) and visually determine the values. Use Auto-ARIMA, which is a version of ARIMA that automatically determines the best hyper parameters based on AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) values. Grid search the hyper parameters based on evaluating model performance by minimizing a pre-determined error criteria. The error criteria can be one of RMSE (Root Mean Square Error), MAE (Mean Absolute Error), MAPE (Mean Absolute Percentage Error), MASE (Mean Absolute Scaled Error), ME (Mean Error), MPE (Mean Percentage Error). In our implementation, we selected option 3 for selecting hyper parameters to avoid erroneous and subjective interpretations of ACF and PACF. The statistical measure, which we aimed to minimize the error of, is the mean, which is also the measure we output as part of the predicted values. These are also referred to as forecasts. (Here is a great article describing grid searching for SARIMA model: https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/ .) In addition, we also needed to decide at confidence interval at which the predicted mean needs to be output. In our case, we chose a 95% CI. What is an anomaly? Once the model is deployed, it outputs the forecasted values as a band, bound by the upper CI and lower CI. The forecasted values are compared with the actual values as they arrive in real time. A data point is said to be an anomaly if the actual value is observed to be outside the CI band. Let's consider a real-world metric and apply what we've learned so far. The metric we are using as an example represents counts at daily intervals of time. Below is a snapshot of the last 15 days of data. However, the more historical data we have leads to a more accurate prediction. Decomposing the time series into its constituents of Trend, Seasonality and Noise, we can see that the time series has weekly seasonality, which is useful while grid searching for hyper parameters. Feeding the time series to the grid searching routine to estimate hyper parameters, we get the following output. The output consists of 3 top models given by the lowest RMSE for mean out of thousands of combinations in the search space. The best performing model has p,d,q = 1,0,1 P,D,Q = 2,1,0,7 Trend = n, no explicit trend defined (\"[(1, 0, 1), (2, 1, 0, 7), 'n']\", 9221.353200844014) (\"[(2, 0, 0), (2, 1, 0, 7), 'n']\", 9280.010864427197) (\"[(1, 1, 0), (2, 1, 0, 7), 'n']\", 9280.970349459463) Over a period of time as the time series collects more historical data, we'll need to re-perform grid search to tune the hyper parameters and accommodate new data behavior. The entire process flow is described as a sequence of steps shown in the following diagram. The following line chart compares how actual values of the metrics have moved over time in relation to a forecasted range defined in terms of upper CI and lower CI. We can see that for most of the duration, the actual values are mostly within the forecasted range, which means our model has been forecasting accurately. Towards the end of the timeline, we do see potential anomalies that have been investigated and classified as true or false anomalies appropriately. Now that all the details of the model are in place, let's explore the engineering design to deploy in production. We'll also look at how the design and deployment is scaled for more metrics. Below is a simple design showing the logical components at play. The four key blocks represent the Data Source, Data Processing, Data Sink and Data Visualization. Data Sources may be comprised of a variety of databases that ideally support fast aggregation queries. For intra-day anomaly detection, we expect to have access to real-time data. Data Processing involves the following functional blocks The Scheduler issues batch triggers to drive the Query Manager. Batch triggers are configured for frequencies at which we need to predict metric values, e.g Daily, Hourly, Three-Hourly. The Query Manager manages the selection and execution of predefined aggregated queries specific to metrics. A good software design support a degree of configurability. The Hyper Parameters and Alerts are externalized into a configuration file. The Model Executor reads the configuration for metrics and generates the model and the next-step forecast for the metrics. Forecast comparison and Alerts compares the forecasts with the actual values, and if the actual value of the metric is outside the Confidence Interval band of the predictions, a notification is sent based on the alert configuration. The Scheduler issues batch triggers to drive the Query Manager. Batch triggers are configured for frequencies at which we need to predict metric values, e.g Daily, Hourly, Three-Hourly. The Query Manager manages the selection and execution of predefined aggregated queries specific to metrics. A good software design support a degree of configurability. The Hyper Parameters and Alerts are externalized into a configuration file. The Model Executor reads the configuration for metrics and generates the model and the next-step forecast for the metrics. Forecast comparison and Alerts compares the forecasts with the actual values, and if the actual value of the metric is outside the Confidence Interval band of the predictions, a notification is sent based on the alert configuration. The predicted values and the actual values are useful to understand how the model has been performing and therefore, we store the data back into a Data Sink, in this case Elastic Search. Data Visualization is a key component with numerous dashboards with graphics representing raw data to be compared with anomaly data. This allows us to understand how actual values are moving in reference to forecasted values. Physically, the components of the Data Processing block reside inside a pod in a Tess cluster. Tess.io is eBay's unified cloud infrastructure based on Kubernetes. Multiple pods are then deployed on the cluster with each pod containing Docker containers for various services. It is recommended that a pod host metrics pertaining to a specific domain such as Risk, Payments, and so on offering containment of failures at pod levels. For fault tolerance, we recommend deploying pods into several regions/DCs in Active-Passive configuration. A false anomaly is one that is essentially a false positive. Even though the actual value was outside the forecasted CI range, the value is attributed to a known/valid reason. In case of a false anomaly, we'd need to let the model learn from this behavior so that the particular false positive may be accounted for for further forecasting. In other words, we continue to use the actual value in further forecasting. A true anomaly is one that is essentially unexpected and we'll need to investigate further to attribute the right reasons to the anomaly. In this case, we need to prevent the model from learning this behavior since we successfully detected an anomaly. Hence, we replace the actual value with the predicted value, thereby ensuring that a similar data point deviation will continue to be flagged as anomalous for predictions. Time series forecasting is a complex subject with new research and methodologies being invented regularly. This article presents ARIMA as one of the more basic models in practice that allows for quickly generating returns. It is worth noting that ARIMA is technically a statistical model as opposed to a pure ML model. Additionally, metrics with low dimensionality and low cardinality are best suited to deploy ARIMA model. For metrics with high dimensionality and high cardinality of dimensions, anomaly detection scales better with the use of a model closer to pure Machine Learning deployed as a Recurrent Neural Network. https://otexts.com/fpp2/seasonal-arima.html https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/ https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html https://people.duke.edu/~rnau/411sdif.html https://kubernetes.io/docs/tutorials/#basics https://docs.docker.com/", "date": "2020-02-12"},
{"website": "Ebay-Engineering", "title": "Coding 4 Kids Inspires Future Engineers", "author": ["Marios Georgiou"], "link": "https://tech.ebayinc.com/engineering/inspiring-future-engineers-with-coding-4-kids-workshops/", "abstract": "Today's children are exposed to technology from a very young age, and on many occasions, 2-year-olds seem to understand the iPad better than adults. Is this enough to encourage and influence them to pursue a career in STEM (science, technology, engineering and mathematics)? make the ghost appear and disappear repeat the same behavior for the same action continue changing the position it appears on the page keep score", "date": "2020-02-20"},
{"website": "Ebay-Engineering", "title": "From Vendor to In-house: How eBay Reimagined Its Analytics Landscape", "author": ["Medha Samant", "Valerie Steinbrugge"], "link": "https://tech.ebayinc.com/engineering/from-vendor-to-in-house-how-ebay-reimagined-its-analytics-landscape/", "abstract": "Learn how eBay transitioned its analytics data platform from a vendor-based data warehouse to an open-source-based solution built by the team. To set the path for more tech innovation at eBay, we recently completed a transition of our analytics platform, moving from a vendor-based data warehouse to an open-source-based solution. This transition was no small undertaking and was considered challenging – or even impossible – by many in the industry due to its scale and complexity. With thoughtful strategy, breakthrough technological innovations and tight teamwork, eBay got it done. eBay’s journey with a commercial data warehousing platform began in the early 2000s. Since then, the platform steadily grew in size, amassing over 20 petabytes of data that includes web analytics and transactional data, such as bids, checkouts, listings, users and accounts. The platform was supporting thousands of eBay analysts, engineers, product managers and leaders across the globe. It not only served as the system of record for eBay’s financial reporting, but was also the preferred platform for all advanced analytics and business intelligence. Why did eBay decide to transition out of this platform, and how was this complex shift achieved? Key factors that influenced this move were cost, innovation and more control over eBay’s tech journey. The vendor system posed constraints on eBay’s scope of expansion and became increasingly expensive. Simultaneously, eBay’s technology stack was undergoing a transformation. With a growing focus on data governance, platform reliability and availability coupled with the rapidly evolving data security landscape, it was imperative for eBay to have full control of its technological innovation. eBay began exploring alternatives, with open source as the top contender. Here’s how we did it. Approach At the foundational level there were two main systems: one supporting large data and batch processing, and the other supporting fast interactive querying and analytics. Both systems had thousands of extract, transform, load (ETL) jobs running on a daily basis. These datasets were being used by thousands of users at all levels of the organization. eBay teammates in search, marketing, shipping, payments, risk and several other domains directly consumed and interacted with these datasets every second of the day. Whether a team wanted to execute a simple “select * from” SQL command or build a complex machine learning model, they had to touch the data residing in one of these two systems. The use cases were seemingly endless, and they all had to move to the new platform without any disruptions. The new ecosystem needed to provide the same capabilities as the existing solution. Users’ tolerance for any degradation in their experience was very low. Detailed analyses yielded the following factors as crucial to providing a seamless transition to the new platform: Executing interactive queries: The new solution had to match industry standards for SQL execution speed at scale. Irrespective of the number of users accessing the system simultaneously, every user would expect their queries to be executed in a matter of seconds. Feature parity: The eBay analytics community had a massive inventory of customized SQL scripts, reports, applications and complex processes that leveraged features and functions that were not provided by the new Hadoop platform. These needed to be able to be migrated with minimal changes. Connectivity patterns: The Hadoop environment was expected to support established connectivity patterns that had evolved over the years while adhering to new, more stringent standards. Tools integration: The new solution needed to be able to connect to a software where users could write and execute code like SQL and Python as well as connect to vendor business intelligence and data science applications. Therefore the migration objective of this program was staged into the following steps: Build the Hadoop infrastructure and clusters; Enable ETL batch processing on Hadoop; Replicate jobs running on the vendor platform on Hadoop; Build a dedicated computing cluster for interactive queries; and Migrate users from the vendor platform to Hadoop. SQL-on-Hadoop Engine eBay’s new SQL-on-Hadoop solution offers high availability, security and reliability. The primary goal of the engine was to replace a proprietary software that specializes in speed, stability and scalability. It was built on Apache Spark 2.3.1 with rich security features, but there was a gap in performance of open-source SQL on Spark. To close this gap, the following optimization strategies were incorporated: Custom Spark drivers: By introducing a custom Spark driver that functions as a long-running service, the engine was able to support a high volume of concurrent Spark sessions, thus increasing the elasticity of the system and providing isolated session management capabilities. This reduced the connectivity and initialization speed from 20 seconds to one second. Transparent data cache layer: Scanning the vast Hadoop Distributed File System (HDFS) cluster would introduce instability and degrade the performance of the cluster. To tackle this, a transparent data cache layer with well-defined cache life cycle management was introduced. This enabled automatic caching of the most-accessed datasets in the SQL-on-Hadoop cluster. The cached data would automatically expire based on the Spark runtime as soon as it discovered that the upstream data had been refreshed, prompting the cache to be rebuilt. This quadrupled the scan speed. Re-bucketing: Most of eBay’s data tables have a bucket layout and are more suitable for “sort-merge joins,” since they eliminate the need for additional shuffle-and-sort operations. But what happens if tables have different bucket sizes or the join key is different from the bucket key? The new SQL-on-Hadoop engine can handle this scenario with the “MergeSort” or “Re-bucketing” optimization feature. Bloom filter indexes: This feature allows data pruning on columns not involved in buckets or partitions for faster scanning. Bloom filter indexes are independent from the data files so they can be applied and removed as needed. Original design manufacturing (ODM) hardware: The full effect of software optimizations can be realized only if the hardware has the capacity to support it. At eBay we design our own ODMs and were able to leverage a custom-designed SKU with a high-performance CPU and memory specs tailored for the SQL-on-Hadoop Spark engine, providing maximum computing capability. ACID (Atomicity, Consistency, Isolation, Durability) support: Traditional commercial databases with ACID properties provide CRUD (Create, Read, Update, Delete) operations. The current Hadoop open-source framework lacks ACID properties, supporting only Create and Read operations. Not providing Update and Delete operations would have required thousands of analysts and engineers to learn and adopt heavy Hadoop ETL technology to perform their day-to-day functions. This was a deal-breaker for eBay, so the SQL-on-Hadoop tool was enhanced to provide the Update and Delete operations. Using Delta Lake , Apache Spark was enhanced to fully support the Update and Delete operations, including using these operations in complex statements that include joins. SQL Authoring Tool The other key component of migrating from a vendor warehousing solution to an on-premise one was building a SQL authoring capability. This was particularly crucial since it is the interface between users and the underlying warehouse. Analytics users at eBay can be categorized into “data analysts” and “data engineers.” Analysts primarily deal with SQL code, while engineers work with Python and Java in addition to SQL. With these two roles in mind, a new SQL authoring solution was introduced, which would eventually go on to offer more capabilities like metadata management, advanced analytics and toolkits for efficient data operations. The tool was designed to provide SQL development capability, and it leverages Apache Livy for connectivity to the underlying Hadoop data platform and two-way transfer of data. It also provides a centralized toolkit to support the development life cycle for engineers. The platform has become a powerful SQL authoring solution also providing the following capabilities: Data exploration; Interactive query analytics; Data visualization; Collaboration and sharing; Schedule as a service; and Operation as a service. Batch Workloads With over 30,000 production tables on the vendor system, the first task at hand was to determine the critical tables and establish a clear scope for migration. This provided the opportunity to clean up several legacy and unused tables, resulting in a final set of 982 production tables to be migrated to Hadoop. All other tables were retired. Personal Databases and Interactive Queries The vendor solution provided a custom feature that allowed users to create personal databases which they could use as sandbox environments for testing temporary use cases. However, several users leveraged this feature for their day-to-day analyses and reporting, so it was a critical component of what needed to be migrated to Hadoop without any loss of data or functionality. Transferring these databases posed a few challenges: From a platform perspective, there were thousands of such databases. Each of them served a unique use case into which the platform team had no visibility, and had to rely on each user to determine the criticality and decision to migrate to the new environment. Several databases were created for one-time use. Some of the users owning these databases had left the company, posing a communication and outreach complication. To tackle these challenges, the platform team built a self-service tool that would allow users to migrate their personal database tables from the vendor system to the new Hadoop platform. To address the problem of completion, the platform and migration teams analyzed the full list of databases on the system to eliminate all the tables unused for at least 365 days. The resulting list of databases was further analyzed for active usage, thus leading to a smaller set of databases to be migrated. This effort involved changing systems for users from a wide range of roles and responsibilities with diverse skills. Most users were accustomed to the vendor-provided ecosystem and found it challenging to reimagine their day-to-day tasks in a new environment. To facilitate the transition, the project team worked to address gaps in skills and to help users develop familiarity with the new system before encouraging them to make the move. The migration team established a dedicated track to develop training materials for various levels of user experiences and technical complexity – not only through wiki pages and training videos, but also through live zoom classes and training drives with full-fledged course offerings tailored for users across the globe. Several other learning and development avenues were established through custom office hours (for each topic of concern for users at large); dedicated Slack channels; and 24x7 level 1 and level 2 support with clearly defined SLAs for ticket acknowledgement and resolution. By the end of the project close to 2,000 migration-related tickets were resolved. In some cases where a team needed dedicated support for the migration, temporary working groups (dubbed “tiger teams”) – including engineers from all levels of the stack – worked closely with the user team. Together they navigated the deep end of their processes and dependencies on the vendor platform, rebuilding them in Hadoop to offer a similar – if not better – performance and experience. Another key aspect of making the move was the speed at which eBay’s engineering teams could roll out features and upgrades as users were making the transition. eBay is a highly agile organization, and this migration was particularly achievable within the planned timelines because of the velocity of the product’s maturity. For example, the SQL-on-Hadoop engine served as the system that several users directly interacted with on a daily basis. As people were making the transition and exploring the new system, they discovered several features that were essential to their activities. The platform team was able to gather these requirements periodically; design and develop the change; and roll it out in production within one or two sprints on average. This not only gave users the confidence that their requirements were being addressed, but also helped mature the product quickly. By eliminating vendor dependency, this migration puts eBay in full control of its innovation, preparing users for the future of analytics. Some of the biggest wins include: A highly resilient analytics data platform modernized across the tech stack; An open source solution with full flexibility over features, supporting tools and operations; No vendor dependency or constraints to the platform’s growth and innovation. This monumental migration not only resulted in significant cost savings, but also helps drive eBay’s renewed tech-led reimagination strategy. Most of all, it exemplifies the strength of collaboration and reaffirms the technical expertise at eBay for significant undertakings of this size and scale in the future. Vendor-to-in-house tool migration stats Vendor-based vs. in-house ecosystems at eBay", "date": "2021-05-10"},
{"website": "Ebay-Engineering", "title": "PyKrylov: Accelerating Machine Learning Research at eBay", "author": ["Selcuk Kopru", "Jingjing Jiang"], "link": "https://tech.ebayinc.com/engineering/pykrylov-accelerating-machine-learning-research-at-ebay/", "abstract": "A recent eBay Tech Blog article 1 presented the Unified AI platform called Krylov. In this article, we show how Krylov users interact with the platform to build and manage powerful workflows in a pythonic and efficient way. The experience while accessing the AI platform and running machine learning (ML) training code on the platform must be smooth and easy for the researchers. Migrating any ML code from a local environment to the platform should not require any refactoring of the code at all. Infrastructure configuration overhead should be minimal. Our mission while developing PyKrylov was to abstract the ML logic from the infrastructure and Krylov core components (Figure 1) as much as possible in order to achieve the best experience for the platform users. Figure 1. Simple layered representation of Krylov components. PyKrylov is the pythonic interface to Krylov that is used by researchers and engineers company wide to access eBay’s AI platform. PyKrylov was built by researchers for researchers, and has increased the productivity of researchers wanting to use  Krylov’s powerful compute resources. Onboarding new and existing code to the platform is as easy as writing a few lines of additional code to the ML logic without changing the existing implementation. The overhead that comes with PyKrylov is minimal, as shown in the below Hello World! example. The user can develop and start the code in her local environment, but the hello_world function will be executed on the Krylov platform and not in users’ local environments. Model training is usually a multi-step workflow that is represented using a collection of tasks and dependencies specified as a directed acyclic graph (DAG). Creating workflows in PyKrylov can be achieved in a very natural way. Figure 2 shows a simple sequential workflow where the execution starts with data_prep and ends with the finish task. Figure 2. A sequential workflow representing a simple ML pipeline. In PyKrylov, the above workflow can be simply created using an OrderedDict class that comes with Python. In the below code fragment, this time the Session.submit() function submits the workflow to the AI platform instead of submitting a single task. It is also possible to submit tasks that are implemented in a programming language other than Python. Through a bash script, the ShellTask class in PyKrylov enables the user to run code in any programming language preferred by the user. Additionally, you can convert the sequential workflow into a parallel workflow using the parallelize() function that comes with PyKrylov. The DAG representation of parallel_wf generated after the above code snippet is shown in Figure 3. The workflow starts with the data_prep task and after the completion of the task, three parallel flows are started with the train function. The finish function is executed only after all three instances of the test function are completed. Figure 3. A simple parallel workflow for hyperparameter tuning. It is also possible to define parallel workflows from scratch with OrderedDict definitions. However, PyKrylov users prefer to use the workflow modification functions to create parallel workflows. Bigger workflows can be created by chaining the parallelize() function for every hyperparameter (e.g. batch size and dimension). Another way of easily creating hyperparameter tuning workflows is to use the grid_search() , random_search() and parameter_grid() functions that are implemented in PyKrylov similar to the scikit learn package. The final workflow looks like the DAG depicted in Figure 4. Figure 4. A complex parallel workflow for hyperparameter tuning. In PyKrylov, tracking and managing workflow status after submission are straightforward. Session.job_show() shows the status of each task and the overall status of the run. Session.job_pause() , Session.job_stop() , and Session.job_resume() allow the users to pause, stop, or resume the runs. When a task is pending, Session.job_info() is very useful to peek at what is going on, e.g. if it is waiting for resources. Distributed Training leverages multiple machines to reduce training time. Krylov supports popular frameworks like TensorFlow, PyTorch, Keras, or Horovod, which support distributed training natively. Krylov provides stable IPs for pods in a distributed training job, and if a pod goes down during training, Krylov brings it back and provides the same IP so that the training can resume. The pykrylov.distributed package allows users to launch distributed training workflows on Krylov with their distributed training code in the framework they like. The experience is similar to launching non-distributed training workflows, but PyKrylov automatically generates the configuration files needed for parallelism and services which come with the stable pod IPs. The DistributedTask class enables users to run distributed training implemented in Python, and the DistShellTask class enables users to run distributed training implemented in other languages, as long as it can be started in a shell. Below we show two sample code snippets, one submitting a DT run from Python implemented function mnist_train , and the latter creating a DT run from shell scripts run_chief.sh and worker.sh . Experiment Management System (EMS) The search for the best model includes multiple iterations of hyperparameter tuning and running multiple experiments in parallel, and comparing the results obtained in each of them. Before the Experiment Management System (EMS), Krylov users had to do manual bookkeeping of the hyperparameters, workflow information, and other metadata related to the training. EMS provides the ability to track the experiments, manage logs, and manage generated models — regardless of whether the model will be picked for production or not — and visualize training status and logs on Krylov dashboard. Moreover, users can record and visualize computed metrics such as loss and precision values with timestamps. pykrylov.ems provides a simple pythonic way to create and update experiments, and associate metadata, logs, models, metrics or other files users generate as assets with the experiments. Trained models need to be accessible for inference in a production environment. Versioning and tagging the models, as well as recording the metadata associated with the model (e.g. accuracy, training dataset, hyperparameters) is necessary. Without a Model Management System (MMS), this task can become daunting for data scientists, as it requires manual and complicated solutions. For this purpose, the Krylov MMS system is developed to provide a centralized solution to store models, where versioning models and bookkeeping metadata are supported and are seamlessly integrated with training and inferencing. With the pykrylov.mms module in PyKrylov, data scientists can push models to MMS during training at ease. The pykrylov.mms module can also be used locally to upload and download models to/from MMS. The module also provides model discoverability capability to users. We have presented PyKrylov and shown how it accelerates machine learning research at eBay. Submitting ML tasks is simplified and configuration overhead is reduced. The user can onboard her code to the platform in a few lines of Python code. In our journey to democratize machine learning, this is only half of the story. Next step for us is to provide researchers the necessary tools for specific domains like NLP and CV. We will provide more details about this in another blog article. 1 eBay’s Transformation to a Modern AI Platform", "date": "2020-02-03"},
{"website": "Ebay-Engineering", "title": "A Human-centric Approach for Evaluating Visual Search Models", "author": ["Michal Romi", "Michael Ebin", "Chantal Acacio"], "link": "https://tech.ebayinc.com/research/a-human-centric-approach-for-evaluating-visual-search-modelsnew-blog-post/", "abstract": "Part of our mission within Core AI at eBay is to develop computer vision models that will power innovative and compelling customer experiences. But how can we compare several visual search models and say which of them works better? This article will describe a method that is tackling this problem directly from the eyes of the users. Since our main objective is to create compelling customer experiences, this article will describe a method that is tackling this problem directly from the eyes of the users. We are using a fixed set of n randomly selected user loaded images that will serve as our query images for both models during this evaluation. These images were not part of the training set that consists of eBay’s active listings, but are reflective of the true images our buyers use to search for eBay products. For each query (i.e. anchor image) we call a model, obtain the top 10 results per anchor image, and then collect 10X n images per model output for our evaluation dataset. Once we have the evaluation dataset, we upload these images to FigureEight (i.e. crowdflower), a crowd tagging platform that we use to collect responses on how well the output of a model compares to the anchor image given (see Figure 1). Figure 1. FigureEight demo. Since images are extremely subjective to evaluate, we decided to incorporate dynamic judgments in order to establish a confidence score for every pair of questioned images. We start by asking three people the same question and reviewing their responses. If they all answer the same, we keep this answer. If they answer differently, we will ask two more people (totaling up to five) to ensure a high confidence of this response. Our evaluators are also being tested while answering these questions. There are test questions, handpicked by our team, that every evaluator must go through in order to qualify as a valid labeler. Their accuracy on these test questions will be linked to their trust score . They must score at least a 70% on the test in order to be accepted to complete this task. In addition to the pre-test, there are test questions distributed throughout the task that could result in their trust score falling below our designated threshold of 0.7, which would result in these labelers being removed from the task. The overall confidence score per each answer is calculated by the level of agreement between labelers and their assigned level of trust. For example, if there were two types of answers selected for the same question, we will take the answer that has a higher confidence score overall.  Only questions that have a confidence greater than or equal to 70% are being evaluated (see Figure 2). Figure 2. Confidence score This process is done in order to obtain a score per each of the models we are evaluating so we can do a fair evaluation between them and decide which one users might prefer. We are using DCG (Discounted Cumulative Gain), which is a standard metric for ranked results (see Figure 3). Figure 3. Discounted cumulative gain. The weights we are using are described in the following table. Once we have all the answers from the crowd, we can assign the relevant numbers to the formula and accumulate the total score per each model. A model with a higher score means a model that produced more relevant search results per this 10X n evaluation set and thus will be the chosen one.", "date": "2020-03-23"},
{"website": "Ebay-Engineering", "title": "Prefetch Caching of eBay Items", "author": ["Ramesh Periyathambi", "Vineet Bindal"], "link": "https://tech.ebayinc.com/engineering/prefetch-caching-of-ebay-items/", "abstract": "With 1.4 billion listings on eBay, improving the speed to render these listings at scale and accuracy is a huge engineering challenge. We have taken major steps to cache item data, which brings great speed improvements. Author's Note: This article follows “ Speed By A Thousand Cuts ” as the second installment of the series. A majority of the traffic to item pages comes from search results page. (The item page is the page buyers see when they find an item they want to buy.) If we can do any improvements to help the item page speed during or after a search page load, without degrading search performance, it will be a good win for the business and for user experience. In past years, we prefetched selective item images shown on the item page, after search page is loaded, which helped to minimize throbbers and show item images instantly on item pages. This time, we used predictive item data prefetch caching, along with user context content, to serve the entire item page for traffic from search results page. Based on business metric values and the additional capacity required for prefetching, usually the top 5 or 10 items in search results are chosen for prefetch caching. These are the items that typically convert better and have a good chance of user clicking on those item URLs. What is predictive item data prefetch caching: In simple words, for certain search request we intelligently prefetch top item pages and cache it. When user clicks on one of these items from search result, the item page is fetched from cache instantaneously rather than going to downstream services. The architecture of item prefetch caching allows us to prefetch the item pages in the background and store them in the server-side cache. The time taken by the search service and the front-end search page is used to render the search page request to prefetch the cache internally and make it ready. When a user performs a search, the request comes to search service to fetch a list of items to be shown. While processing the search results, search service triggers an asynchronous call (1) to item cache service with set of predictive item URLs. Item cache service, in turn, triggers individual prefetch calls (2) to item service. The prefetched item service response data is then stored (3) in a memcache cluster, “CACHET.” Search service then attaches a flag to each prefetched item and returns a response to the client indicating cache eligibility. Figure 1. Cache creation. When a user clicks on any of the items on the search page, the item page request goes back to item service with a special header indicating this item can be prefetched and cached. Item service then checks for this header and reads (2) the cached item data from the memcache cluster. If a cache is found, it will be returned back to the user to render the item page. In case of a cache miss, item service will fall back (3) to its original dependencies to retrieve the item data from the database and other micro services. Figure 2. Cache hit or miss. Cache Storage Selection Any caching storage we choose had to meet the below criteria so that we can build a reliable system: Insert: Our use case demands millions of insertion per hour to cache. Read: Our use case demands under 15 millisecond of SLA with millions of lookups. Delete: Should be able to support millions of deletes per hour TTL: Should be able to support a TTL from under a minute up to 3 days. Scaling: Should be able to scale without compromising any of the above metrics as the data grows from few million or hundreds of millions or the storage grows from few GB to TBs. Building such a prefetch system can be costly due to the cost for performance and capacity involved. We wanted to stay frugal wherever possible and optimize the system to get the best out of it while minimizing the cost. So any system we choose had to give us performance benefits with minimum investment. We analyzed and defined the below additional parameters to optimize for performance and cost. Async Insertions: Inserts have to be async and as fast as possible to reduce resource consumption and save compute cycles on thread wait time. Data Persistence: The primary goal of this cache is to optimize speed and return faster responses. Since we have a reliable fallback (a.k.a Item Service) in case of a cache miss or if prefetch wasn’t available, data persistence is secondary here. We don't need persistence because the system can be fail-open. Consistency: Since the system is fail-open and we have a fall back, async eventual consistency works for us within an SLA of few hundred milliseconds before the user click happens and the GET request arrives. Secondary Replicas in same Data Center: We wanted something without the need of a replica. By comparison, for example, Couchbase needs a replica in each region. That would balloon cost and footprint. Availability: There are no replicas to reduce cost within the same data center, because we can still rely on multiple data centers to improve availability. Tight SLA: Our SLA is very tight (15 ms). The system should be able to meet this so that there is a minimum impact on speed in case of a cache miss. To meet all the above criteria for cache storage, we use a lightning fast, low latency, in-memory key-value store solution named CACHET [memcached + mcrouter] cluster, which is fully managed in the cloud and distributed across multiple data centers. A CACHET memcached cluster consists of mcrouter and memcached nodes that are integrated with eBay’s logging and reporting tools. Applications send a request to a mcrouter, which then sends the request to one or more memcached instances based on the defined mcrouter pool routes. Figure 3. CACHET memcached clusters. Performance CACHET is configured to serve millions of requests within an SLA of <10ms. The following statistics show the number of item requests and 95% read latency per minute (average). Figure 4. Data Centers: lvs - Las Vegas, slc - Salt Lake City, rno - Reno. Figure 5. r1vicache - Item Cache Service pool. Data Storage, Retrieval and transmission For write requests, the payload is zipped before storing into cache to save storage memory. For read requests, the payload is retrieved from cache in zipped format and transmitted in zipped format back to the application pool to save zipping and unzipping time on cache pool and save time to transmit less number of bytes with zipped data for optimization. Key Generation MD5(hex) keys are generated for incoming load and read requests based on {itemid}:{siteid}:{locale}:{userUniqueIdentifier/sessionId}:{userid}:{urlQueryParams} . This makes the cache specific to a particular user. MD5(hex) is used purely to save storage space needed for keys. Figure 6: The cache hit ratio per minute (average). Predictive item prefetch caching using server-side prefetching has many advantages over other techniques. The search service can trigger an asynchronous call to the ITEM CACHE service, which does the predictive prefetching of item content. Since this call is asynchronous, there is no notable latency to users in the search experience. Since the call to ITEM CACHE is triggered even before search service returns back for browser to render the search page, the whole prefetching and server side caching can complete and will be ready before the user hits the item URL in the search page. This is not device dependent and can scale for low-end devices or slow phone networks. To ensure consistent user experience with prefetched and cached item content, the state of the item and user is saved along with the cached item content. During cache read, all the below conditions are checked for cached data validity. Whether user’s sign in state has changed Is the item revised by the seller Has the item-to-user relation changed Does the user locale or language preference changed Also certain scenarios where items should not be prefetched are validated. Some of them are listed below. Item ending in few mins Item already ended Item belonging to certain categories Complex scenarios with product experience where content is very dynamic To minimize the impact of stale data on the page, the TTL of the cache is set to a few mins. This is based on the average time a user spends on the search page and the time taken to requery items on the search page, as shown in the following graph. Figure 7. The average time a user spends on the search page and the time taken to requery items on the search page. When a user accesses the prefetched cached item data, the response time of item service decreases by several milliseconds (~400ms). Since prefetch is done for the top 5-10 items allowing the optimum balance of impact and cost, it covers 70% of search to item page traffic. Taking this ratio into consideration, the overall speed for above the fold rendering time has improved by 10%. This feature is launched in Android and iOS native platforms for U.S. U.K., Germany and Australia markets. A variant of the same server side prefetch caching combined with client side prefetch is launched in desktop platform for Australia market. We are seeing good speed and business wins in all markets. A future improvement of this prefetching architecture is to move the server-side cache to the edge close to the users to save the network latency and bandwidth. Also moving towards a cache which is generic across multiple users (known as non user context cache) using the same prefetch architecture will further reduce the operations cost and improve cache hit efficiency. Thanks to our leadership Lakshimi Duraivenkatesh and Senthil Padmanabhan for guiding us through out this initiative.", "date": "2020-02-25"},
{"website": "Ebay-Engineering", "title": "New & Improved Terapeak Research 2.0 in eBay Seller Hub", "author": ["Senthilkumar Gopal", "Cem Gemici", "Lucan McRandall", "Shawn Zhang", "Yury Elizarov"], "link": "https://tech.ebayinc.com/engineering/new-improved-terapeak-research-2-0-in-ebay-seller-hub/", "abstract": "Follow the journey of integrating Terapeak into eBay Seller Hub, making it available to millions of eBay sellers. Terapeak has been the seller’s darling when it comes to providing key insights for businesses of all sizes and utilized heavily to evaluate sourcing opportunities, pricing guidance, market trends and analyze their competition. These excellent attributes made Terapeak a match made in heaven for eBay. This post chronicles the journey of integrating this excellent tool into eBay SellerHub making it available to the millions of eBay sellers and describes the technical challenges, transformations and architectural changes performed to achieve the milestones in this transformative journey. Terapeak, acquired by eBay in 2017, is a search tool that allows sellers to derive insights on what to sell, when to sell and how to sell a product on eBay.  Using Terapeak, sellers can determine the best price for their listings, the best shipping policy, the expected sell-through rate, and whether or not the product their selling is subject to seasonal trends. They can gain insights into the keywords that top-selling listings include in their titles, providing them with keywords suggestions that will surface their listings in the search results, putting them directly in the buyer’s line of sight. These features make Terapeak a perfect addition to the Seller Hub, which serves to help sellers make better decisions by directly providing them with eBay’s data, including inventory management, performance insights, and growth opportunities. Sellers can read more about Terapeak on the Terapeak help page and it can now be found in eBay’s Seller Hub . From the day eBay acquired Terapeak, eBay’s seller growth team began working hard to integrate the core capabilities of Terapeak into eBay's Seller Hub. This work began with re-building Terapeak's large data backend on eBay’s core technology stack in preparation of exposing the actual features in Seller Hub for sellers to use. Last year at eBay Open 2019, the initial features formerly part of Terapeak was officially launched in Seller Hub to all US sellers who had an eBay Store Subscription. The first release supported these capabilities: Conduct a keyword search for matching titles Conduct a keyword search for matching titles Conduct a product ID search for MPN, UPC, EAN, ISBN and EPID Conduct a product ID search for MPN, UPC, EAN, ISBN and EPID Search across any eBay site Search across any eBay site Select from pre-set date ranges of 7, 30, 90 and 365 days Select from pre-set date ranges of 7, 30, 90 and 365 days Filter their results based on price, condition, format and buyer or seller location Filter their results based on price, condition, format and buyer or seller location View aggregate metrics for Average Price, Shipping Policy Coverage, Shipping Cost, Total Items Sold and Total Sales View aggregate metrics for Average Price, Shipping Policy Coverage, Shipping Cost, Total Items Sold and Total Sales View detailed listing-level insights for all matching listings View detailed listing-level insights for all matching listings View real-time information that provides insights up to the most recent hour for all eBay transactional data. View real-time information that provides insights up to the most recent hour for all eBay transactional data. Following closely from the successful launch at eBay Open , Terapeak spent the remainder of Q3 2019 developing features that brought the new version closer to parity with the legacy application. These included: Custom date picker that allows sellers to define and research any date range over the past 365 days Custom date picker that allows sellers to define and research any date range over the past 365 days Aggregate metrics for the Number of Sellers Aggregate metrics for the Number of Sellers Design updates that allow for easier search input and more intuitive insights Design updates that allow for easier search input and more intuitive insights Figure 1: Terapeak today — Custom Datepicker Figure 2: Aggregate Metrics from Terapeak For the holiday release in 2019, Terapeak capabilities in SellerHub gained critical features such as type-ahead keywords/category suggestions and the category selector breadcrumb. Looking forward to 2020, the team is feverishly working towards delivering critical trends and metrics information to further drive feature parity and ensure a seamless transition to the SellerHub application for sellers. Figure 3: Typeahead keyword/category suggestions Figure 4: Category selector in breadcrumbs Additionally, the team is working on further data enhancements that provide unsold listing data for the complete 365-day date range. Not only will this provide the sell-through rate demonstrating the percentage of listings sold, but will also prove a major improvement over the legacy application by providing this sell-through rate for any date range, instead of only the last 90 days. The team is also working to add more data visualizations similar to the ones found in the legacy application’s Trends tab, while providing additional visuals and insights where possible. Figure 5: Data visualizations for Items sold and Average price In 2020, the team will also launch significant improvements to the way Terapeak search works in the Seller Hub, providing sellers with the ability to search using item specifics such as color, size, style, material and other important details that may not be included in the listing title. The architecture of the legacy Terapeak application was built on two core infrastructures. First, a robust Elasticsearch cluster for transforming data to a more retrieval-optimized structure enabled fuzzy matching and quick retrieval. Second, the application layer for Terapeak was based on a cloud native architecture with a React based frontend application and a JVM-based backend service to communicate with the data layer. Figure 6: Legacy Terapeak Data pipeline Architecture Above is a simple overview of the data platform architecture utilized by the legacy Terapeak application. The ingestion pipeline consumes the data from eBay and populates the Elasticsearch clusters. The data feed processor is responsible for validation and conversion of the ingested data for the optimized document format with PostgreSQL being utilized for storing and retrieving metadata such as category trees, exchange rates, etc. The API nodes represent the application backend services utilized for reading the data. Figure 7: New SellerHub Terapeak Data pipeline Architecture eBay hosts a myriad of application types managed using Kubernetes . eBay’s internal platform team specializes in managing Elasticsearch workloads , ensuring high availability and scalability of these clusters. With the advantage of a fully managed infrastructure and deeper integration with the eBay ecosystem, the data platform evolved to become faster and nimbler to handle changes better than before. With the platform team handling all the operations aspects of the cluster management, the engineering teams could focus more on application building with deeper insights and generating complex models based on near real-time data inputs. The modern architecture after the migration of Terapeak capabilities on to eBay tech stack consists of a tighter integration for data ingestion directly from the item-based data sources detailing the transaction information and direct access to the data warehouse for gathering category and other related information. eBay leverages the Openstack Swift storage system for quick loading of data files and transformation using Apache Spark for historical data. Apache Kafka is utilized for message resiliency to record real-time updates. All this means sellers get analytics data that is minutes old vs days old as was the case on the external Terapeak.com site. With this new and improved architecture and its closer proximity to the data source, Terapeak is able to provide same day transactional data to the user generating near real-time insights. With increased storage and processing capabilities, the new system has the ability to analyze data over a full year rather than just 3 months provided by the legacy system. With nearly 2 Billion documents, the new system is one of the biggest ES clusters within eBay serviced by around 130 nodes per cluster housing around 3.8 TB of RAM in just one data center. The near real time pipeline processes up to 6 million events in a day updating the cluster with more up to date information  to get these valuable insights into the seller's hands. Figure 8: Legacy Terapeak Application Architecture The legacy Terapeak application stack was constructed as an evolving application stack with various technologies based on their relevance at the time of development.  It is loosely constructed as a multi-tier, service-based architecture with the critical analytical data being stored in Elasticsearch clusters with a PostgreSQL-based database for all metadata and configuration. While it met the needs of the past, it was showing its age internally in terms of flexibility, opportunities to scale,  and total overall cost of ownership. With the integration into core eBay architecture, some of the key changes were the transformation from a simple service-based architecture to a microservice environment. This transformation was crucial to enable a multi-device strategy and provide future integration capabilities for multiple form factors. The microservice architecture we used was based on the Experience Services framework, which eBay has started adoption site wide to accelerate multi-screen deployments. Evolving into this Backend for Frontend (BFF) design pattern allows us to develop self-contained modular code that can be easily made available in various parts of the user experience with enriched feature sets at the same time using shared modules . The original backend service was built as a sheer monolith where the service was responsible for multiple actions to build the data response. This inhibited the cycle time for feature releases even for small changes involving schema updates to the Elasticsearch cluster or adding new data elements from the generated data responses. eBay Seller Hub was established with a few architectural guidelines that helped resolve this tight coupling and also helped disintegrate the monolith into self-contained microservices. This also provided a good mechanism to decouple the user experience from the data layer, allowing us to spin off public APIs for these data points in the future as part of our API strategy . The eBay platform services enable end-to-end detailed tracking and robust authentication capabilities. The Seller Hub infrastructure services provide the ability to generate the skeleton modules personalized for users based on their preferences, and subscription services enable quick lookup to ascertain the sellers’ subscriptions to enable targeted experiences. This architecture enables Terapeak to be bootstrapped into Seller Hub quickly and also without the need to replicate multiple core functionalities. The Experience services are built using Scala, which eases asynchronous programming and makes the orchestration of various services a breeze to implement. The Central Application Logging (CAL) system provided by the eBay infrastructure ensures complete logging capabilities while the global config management system helps migrate all application configurations as a simple one-time exercise. Figure 9: Experience Service Architecture and Components Seller Hub inherently processes heavy data loads to provide meaningful statistical guidance to the users. To ensure that users do not wait for a long period of time, one of the key design aspects  of Seller Hub architecture is to progressively render the application with data whenever they are made available. Terapeak fits hand-in-hand with this strategy and uses MarkoJS progressive rendering and client-side rendering for asynchronous loading on the browser, while utilizing Scala/Java futures to provide the data from the services side. This helps provide a seamless user experience with the search results and aggregated data being generated and rendered progressively so the user can visualize and interact with them right from the first response byte. Seller Hub on the other hand has adopted a more modern approach to use server-sent-events (SSE) to push updates from the server as soon as the data is available. This has many advantages over regular long polling as the data is immediately available, the data size is typically smaller (as SSE has no headers,) and using SSE provides users the ability to inspect the data immediately. Note: The SSE architecture is used for the front-end server to communicate with the experience service and they are restricted to communicate only with the front-end server to prevent any unwarranted access. A simple depiction of how SSE is used to build the new UI rendering for the Terapeak Product research page on Seller Hub is provided below. Figure 10: SSE Architecture for building User Experience Learn more about the differences between async responses and SSE in this Stack Overflow post . Another key change in this migration is the adoption of MarkoJS for the frontend application stack. This decision was based on factors including: Seller Hub follows the design standard defined by eBay skin . Seller Hub follows the design standard defined by eBay skin . UI core components were readily available and work hand-in-hand with eBay Skin. UI core components were readily available and work hand-in-hand with eBay Skin. These features allow us to build modular experiences that can be easily plugged in various flows. These features allow us to build modular experiences that can be easily plugged in various flows. These features include constant support and updates. These features include constant support and updates. MarkoJS also allows us to iterate faster and integrate seamlessly with the existing components, helping us deliver features faster and with more completeness with regards to accessibility, speed, and compatibility across devices and platforms. There were few components such as the datepicker that were not readily available as part of the eBay core component system that needed to be built from the ground up using MarkoJS.  This provided us the opportunity to build reusable feature-rich components for the whole eBay ecosystem and contribute back to the larger developer community. It also helped us gain crucial insights into the internal workings of MarkoJS and a deeper appreciation of its core architecture. Pros and Cons Clean syntax Clean syntax Fast rendering Fast rendering HTML first language (as opposed to JavaScript first JSX) HTML first language (as opposed to JavaScript first JSX) More established frameworks already have date-picker component More established frameworks already have date-picker component Smaller community Smaller community For more details see https://markojs.com/docs/marko-vs-react/. The  migration from Terapeak’s legacy implementation to a new modern approach on eBay’s technology stack  positions us for a faster and more feature-rich set of capabilities than Terapeak had ever been able to offer before. This includes the ability to offer more detailed and near real-time insights with higher levels of fidelity along with improved quality,  reliability, and more sustainable operational support. With the entire Terapeak data platform entirely rebuilt and the foundational features of Terapeak now hosted on a modern, more nimble, optimized tech stack atop eBay's technology platform, it has positioned the team to deliver not only the existing value sellers had come to love and expect from Terapeak, but also an ability to quickly deliver an onslaught of new capabilities for sellers that eBay will be rolling out heavily over the first half of 2020. Stay tuned for more updates on Terapeak in Seller Hub in future editions of this blog.", "date": "2020-03-04"},
{"website": "Ebay-Engineering", "title": "eBay Uses Computer Vision to Enable Sellers to Create Cleaner Images", "author": ["Ellis Luk"], "link": "https://tech.ebayinc.com/product/ebay-uses-computer-vision-to-enable-sellers-to-create-cleaner-images/", "abstract": "We built an algorithm that lets users change the background of their listing photos. You only get one shot at a first impression, and listings with a clean white background have the potential to convert better and sell faster. Today, we’re introducing a new feature that uses computer vision technology to make sellers’ listing photos easier on the eyes and more effective in Google Shopping for both Android and iOS. The feature removes the background of a seller’s listing photo and replaces it with a white background, optimizing listings for Google Shopping, and improving the shopping experience for buyers by making search look and feel more streamlined. eBay is an open marketplace where sellers can take their own photos and post them to the platform. We have millions of C2C and small business sellers who may not be able to use a professional photographer or have time for photo-editing software. Because of that, not all inventory is listed and photographed with optimal lighting or a clean white background. The resulting background — often clutter on a kitchen table, closet door or store shelf — creates noise and impacts results when a buyer shops using Image Search to find an item. How It Works Sellers can easily use this feature by going into the Sell flow from their mobile device. After they take or upload their photos within the eBay listing flow on their Android or iOS device, the seller can leverage the background removal tool to make a first pass at adding the white background for any of their photos. The seller can also touch up any missing areas or use the photo as is. The Technology Behind Image Clean-up Our computer vision algorithm processes the photo completely, using the processor on your mobile phone, to separate the foreground from the background clutter. Doing so enables us to change the background to a uniform white for a consistent look and feel. The current approach is based on these assumptions: ●      Pixels along the image border are predominantly background ●      The foreground and background have sufficient contrast to indicate different coloring We built color models for the foreground and background and solved for unknown pixels in the mask using conditional random fields. The output of the algorithm is a mask made up of the probability of foreground for each pixel. For example, when this is 100%, the pixel is entirely associated with foreground and when it is 0%, it is entirely associated with the background. This mask is then used to blend the foreground with a white image resulting in the desired image. The confidence in background removal is measured by a factor that we call “separability,” which estimates how difficult it is to separate foreground from background. The closer the separability score is to the maximum value of 100%, the more likely the algorithm can easily separate foreground from the background. We use this to guide the tool on whether it should show an auto-cleanup result to the user or let the user do manual touchup to remove the background. Low contrast images and images with background clutter will typically produce a low separability score. Images that score close to 100% on separability will trigger a bonus flow through Automatic Cleanup, in which the background removal tool will attempt to remove the background without seller input. The seller can then choose whether they want to use the photo as is, or make edits and do touch ups of their own. The idea for Image Cleanup was conceived during eBay’s Hack Week, an annual company-wide competition challenging our technologists to innovate and reimagine the ecommerce experience. Leveraging the latest advances in computer vision and AI, we continue to work on additional features that will make our users’ lives easier. This feature is now rolling out on Android and iOS devices in U.S., U.K. Germany and Australia and will be rolling out to all other regions in the next month.", "date": "2020-02-26"},
{"website": "Ebay-Engineering", "title": "Know Your Developer (KYD)", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/know-your-developer-kyd/", "abstract": "The vision of any API program is to deliver a world-class developer experience that enables partners to be inventive, provide their users with fantastic experience, and propel their business into the future. Learn the guiding principles behind eBay's Developers Program. APIs connect businesses, people, and things. They are everywhere nowadays, allowing developers to unlock new opportunities for innovation. The vision of any API program is a powerful developer ecosystem that enables organizations to connect with the world digitally. Both modern, as well as legacy, APIs encapsulate the business. In our case, they are the front door to our marketplace platform. eBay developers leverage our APIs to manage their business. They trust us, so our APIs must be reliable digital assets that enable growing a successful business. Every digital technology comes with risk. Now is the age of the API gold rush, where APIs play an integral role in the digital world. We also live in the age of digital ethics and privacy. With live integrations, it is very challenging to make changes to address security concerns without impacting partners and their businesses. On the other hand, legacy APIs show their age, which requires implementation of new security principles. It makes sense to apply the concepts of a networking zero-trust model, which never assumes trust, to API programs as well. None of the entities are trusted by default. When translated to developer ecosystems, this refers to: APIs, both legacy and greenfield, actors, applications, security protections implemented in the past, systems, and humans who assess the value of integrations. The new model relies on all sorts of data and continuous auditing of APIs. Look beyond the numbers. Around them. Through them. - Al Harrison Ongoing monitoring and API usage analysis is essential to any API program. Near real-time scanning and alerting is vital to keep the APIs up and running. It is not always simple to estimate the usage of APIs. At eBay, it is common to have sudden seasonal and other spikes related to business events. On the other hand, an unexpected surge in traffic coming from certain integrations is often a good indicator of anomalies. The API usage analysis is beyond operational metrics and pure insights into the API availability. Mining API traffic data for patterns is what enables API providers to have visibility into the way developers use the capabilities. By doing this, organizations can identify normal behavior, detect anomalies, assess the value the APIs bring, and understand benefits coming from third-party integrations. (If you cannot measure it, it does not exist!) And data ages like wine — the more data, the better the behavioral analysis. In general, entities are real-world people, businesses, or objects. They have their characteristics — entity attributes. Entity references are sets of attribute values. Entity resolution is the process that resolves entity references into entities. It answers whether the two references point to the same real-world person, business, or object. Entity and identity resolutions are not the same. Identity attributes represent a subset of entity attributes that identify an entity. In the case of real-world people, identity set also includes biometric attributes like human fingerprints. This illustrates well the difference between entity and identity resolutions. For example, if two sets of fingerprints are found in two different rooms, simple comparison determines whether they belong to the same person. That is entity resolution. But we still do not know who that person is. Looking up fingerprints in a particular system reveals the identity of a real-world person. That is identity resolution. Figure 1: Real vs. digital world In the marketplace world, it is important to resolve all product references to product entities. Similar applies to user accounts — buyers and sellers. This concept is also applicable to any developer program. Third-party developers are entities in the API ecosystem. To know your developer, all analyses and assessments should be done at the customer level. It is true that understanding the API usage pattern is necessary, but it is equally important to understand who is using the APIs. In our case, this includes relationships across eBay realms: Developers Program, eBay Partner Network (EPN), and Marketplaces, which adds significant complexity. To succeed, planning alone is insufficient. One must improvise as well. - Isaac Asimov Although the focus is on being predictive rather than reactive, in risk assessment strategy, both preventive and reactive approaches are essential. Risk decision systems assess the risk to prevent fraudulent activities. Such decisions are driven by regulations and long-term policies with less volatile rules. On the other hand, fraud decision systems detect fraudulent activities. These decisions are highly volatile due to fraudster behavior changes. Whenever you think your security practices are in place, very motivated bad guys find all sorts of loopholes and another way to achieve what they need and want.Keeping the bad actors out requires constant rule modifications, iterations, and improvements.In general, all serious risk management systems follow the triple-A approach: Agile Agile Adaptive Adaptive Analytic Analytic Agile requires a quick response to new circumstances and an easy way to change the decision rules while staying compliant with laws and regulations. Adaptive is to find new approaches, manage trade-offs, optimize policies, run simulations, and experiment, test and learn. Analytics is to use historical data to predict the future to reduce fraudulent activities. When predictive models are combined with other data points and rules, they typically improve risk assessment and management. Any API program should have a mechanism in place to prevent and detect evil intent and API misuse. (Otherwise, the API program becomes a playground for fraudsters.) Such a system should be on 24x7 and real-time responsive. Besides, full transparency is needed to confirm that decisions implement business requirements and that they are compliant with regulations. This also provides insights and visibility to Developer Technical Support and third-party developers, when needed. There are two primary sets of activities here: Prevent bad actors from entering the ecosystem Prevent bad actors from entering the ecosystem Continuous evaluation of existing developers and their integration Continuous evaluation of existing developers and their integration The first one is to control the front door to the eBay Developers Program by letting the right people in. This is also to prevent bad actors from recreating developer accounts and applications upon deactivation. Understanding related developer accounts allows evaluating risk assessment at the third-party developer-level — the developer as a customer here rather than a single account integration. The ultimate goal for sure is to take good care of good actors and avoid limiting their business. All we want is for the sky to be the limit for our trusted developers. On the other side, good actors could turn bad overnight. It is a game of cat and mouse that requires both continuous API auditing and continuous evaluation. Forces are always pushing one way or another. It is essential to find a balance between boosting adoption and maintaining fine-grained control over data and capabilities. So, it is all about balancing recall and precision. How would one prevent crime in a town? Arrest everyone. That is not the perfect strategy. Our choice is precision. Higher precision implies more good developers in the program and higher revenue in the long run. After all, the goal is not really to count the bad guys we caught. It is crucial to prevent the good actors in the ecosystem from being impacted by bad actors’ behavior. And that is what we measure. To continue with my analogy between cooking and integrating with APIs , APIs are healthy ingredients to enable the healthy business. We live in a connected digital age, and data sharing is part of our lives. So, it is essential for any API program to implement data protection principles and to address privacy concerns. APIs could easily and quickly turn against organizations. A continuous API security strategy is crucial to ensure the APIs are reliable digital assets. Keeping the APIs safe and treating API security as an enabler rather than a gate leads to preventing good actors in the API ecosystem from being affected by bad actors. Our goal is to continue opening the marketplace and at the same time, keep the eBay Developers Program secure. This is not a mountain you climb, reach the top and then sit, relax, and enjoy the view. Securing the developers’ program is a progressive journey rather than a one-time exercise. In general, the road to excellence is always under construction.", "date": "2020-03-09"},
{"website": "Ebay-Engineering", "title": "A Learning Culture to Propel Innovation ", "author": ["Mazen Rawashdeh"], "link": "https://tech.ebayinc.com/product/a-learning-culture-to-propel-innovation/", "abstract": "A test-and-learn culture is key to building the best experiences for our customers. Editor’s Note: Our newest innovation documentary series takes a look at how anyone can shape the future at eBay. The new series follows our technologists over six months as they create, test and develop their prototypes through our Innovation Program. Customers are at the heart of every new eBay feature or project. We know that what we do impacts the lives of millions who depend on our platform for their livelihood. And, we are passionate about ensuring that eBay buyers and our sellers have a seamless experience. One thing I have learned throughout my career is that as teams strive to deliver for customers, culture and technical skills are equally important. When a culture is unhealthy, new ideas can be stifled. When a culture is healthy, it speeds innovation, encourages collaboration and champions creativity. In a changing landscape, you have to be adaptable to changes around us in order to continue to compete and win. At eBay, we embrace a test-and-learn culture. It is a trusted and transparent environment where our technologists can succeed by trying new things or taking new approaches to solve real problems and improve the customer experience. No matter where someone sits on the team, their ideas and unique views are valuable. Our Innovation Program I love how we bring culture and creativity together in our annual Innovation Program . It is a sandbox-type environment where employees work cross-functionally to make the impossible possible. The journey starts every summer during Hack Week, with thousands of our brightest minds participating worldwide — building, testing and developing hundreds of new prototypes. Following Hack Week, teams submit their projects for eBay’s Expo in the fall, where they pitch their ideas to business leaders in hopes of landing their ideas on our roadmap. Teams collaborate to solve both customer and complex engineering problems at scale. eBay has 1.2 trillion data queries each day! Our data footprint is more than 700 petabytes, which is equivalent of 1.4 trillion songs, 3.5 million hours of movies and is enough to back up the American Library of Congress over 420 times. In 2019, more than 250 working prototypes were created through our Innovation Program, and many of these are incorporated into existing company initiatives. The program celebrates eBay’s test-and-learn culture and our teams’ visionary innovations with honors like the Innovation Rotation Award. Winners are granted four months to step away from their current assignments to focus on developing their projects. I’m very proud of the culture we foster. By rewarding people who aren’t afraid to challenge the status quo, to think the impossible, to dream bigger and to allow their curiosity to roam free, we are building a better eBay in service of our customers, and helping to improve lives around the world. Watch our new innovation documentary series at ebayinc.com/innovation.", "date": "2020-03-04"},
{"website": "Ebay-Engineering", "title": "Homepage Speed Improvements on eBay Android", "author": ["Billy Sword", "Viswa Vaddi"], "link": "https://tech.ebayinc.com/engineering/homepage-speed-improvements-on-ebay-android/", "abstract": "In this blog, we discuss strategies to improve application speed. Speed means better customer experience and engagement. There is tremendous value in identifying application speed issues early in the development lifecycle. At eBay, we have a company-wide speed initiative on most visited pages like the Homepage, Listing page and Search pages. With the introduction of speed budgets at eBay, we were tasked with a mission of ensuring that the homepage load times were consistent with every global Android release. Below are the steps we took to achieve the speed goal. Measure Application Speed The eBay Android app uses a backend service to measure the real-world speed metrics. The native app makes calls to this service to drop a speed beacon at critical points in the flow and uses this data to compute a metric called Virtual Visual Complete (vVC). This speed beacon contains timestamps when key transitions occur in the Android lifecycle. For example: lifecycle_create - beginning of the Android Activity onCreate lifecycle method lifecycle_start - beginning of the Android Activity onStart lifecycle method activity_first_render - completion of Activity onCreate These metrics are used to derive the time it takes the page to render ready and can respond to user interactions. Simply, vVc is computed as ‘end time’ minus ‘start time’. Based on the sample data above, it is computed as (activity_atf_render - lifecycle_create. We measure vVC for various sites and app versions. This type of historical data is used to compute the speed budget that the eBay Android home page must adhere to and optimize for future releases. eBay’s homepage is unique - while we load the homepage, we are also accountable for the app initialization that happens during first launch. To make that possible, we look at both cold boot and warm boot times on Android. To better understand how we can improve our load times, we decided to profile our app initialization logic. The aim of application initialization is to do the minimum amount of work required to show the homepage, so that the user can start interaction with the app right away. For this, we used the profiler that comes with Android Studio (more details here). Below are a few tips that worked well within our application. Of note, the best results were with Sampled Java and an API 26 or newer. ●      Launch the application using the profile run configuration. ●      Stop profiling once the homepage load completes and wait for the call charts to populate. ●      Look through the call stacks for Async Tasks, onCreate() of various activities. ●      Look at onCreate() as a starting point for profiling for the two activities that are required to launch the homepage. Success Stories Parallel vs. Sequential The homepage response is cached on the app for a few minutes in order to avoid too many disrupting home page refreshes. After the homepage backend call succeeds, we write to the cache, which can take time because this is an I/O operation. To circumvent the slow speed, we parallelize the work needed to write to disk for caching, populating and creating the user interface. Concentrate on Populating Above the Fold Modules First One of the “below the fold” homepage modules is populated through a call to a third-party API. In this case, the homepage backend sends a placeholder to where the content would go. The native app makes the third-party API call to populate the content. To decrease the load times, we decided to delay third-party API calls until they were required. Since this module is meant to be below-the-fold, we decoupled the homepage load and below-the-fold content. Content Comprise for First Launches When we profiled the application, we noticed that the first call to fetch the Google device identifier was much slower than the subsequent calls. This identifier was needed for a homepage module. This meant that first launch was getting bogged down until the device identifier was fetched. We made a compromise that we will not show this module on first launch, but we will make the call to fetch the device identifier and not wait for the result. Result The updates were available on app versions eBay Core v5.28.x+. We were able to achieve ~350-400ms (28%) improvement in our overall load times. Future This activity has provided encouraging feedback. To further our success, we use continuous integration tests to monitor app speed daily and during app releases. We also have nightly runs in QA on the native branches where we are constantly measuring speed. These synthetic tests run a few scenarios to replicate real world traffic and provide indications about any site speed degradations. The goal is to identify and fix speed degradations early in the development life cycle. This blog summarizes our experiences trying to improve application speed - a complex problem with no simple solution. It’s important to note that this article strictly covers the client side optimization efforts. As we iterate over more releases, we may find additional ways to achieve speed optimization.", "date": "2020-04-01"},
{"website": "Ebay-Engineering", "title": "eBay Motors: Screenshot Testing with Flutter", "author": ["Maksim  Zadorskii", "Corey Sprague", "Larry McKenzie"], "link": "https://tech.ebayinc.com/engineering/ebay-motors-screenshot-testing-with-flutter/", "abstract": "The team continues to update the eBay Motors App using Google’s UI toolkit, Flutter. Move Fast and (Don’t) Break Things When building rich user interfaces, validating the correctness of the UI across all form factors and variations has always been a challenge. How the UI looks on a single device is only one consideration. From text size to accessibility to different languages, engineers must think about – and test for – a seemingly endless number of possibilities. This was a critical consideration and a top priority as we continue to develop and update the eBay Motors App . For this project, we used Flutter, Google’s UI toolkit for building beautiful, natively compiled applications for mobile , web , and desktop from a single codebase. As one of the earliest large companies to adopt Flutter in a production environment, we had to think through many different UI factors that could affect the experience for our diverse, global customer base. We knew from experience that automation was the only scalable way to protect against regression as we rapidly iterated on the eBay Motors App. Typically, when most engineers think about UI test automation, they think about behavior testing: does a button perform the right action, will the screen be updated when a network call completes, etc. With this approach, you can test some of your most important requirements and feel confident that they will continue to work as expected as the software evolves. However, in a rich client application, there are many requirements that are visual: ensuring that content overlaid on an image has enough contrast, ensuring that multiple elements are visually aligned, asserting that certain data elements are properly emphasized, etc. These types of requirements are unnatural or often impossible to test using behavioral approaches. This is where screenshot testing comes in. Flutter provides out-of-the-box screenshot testing in the form of “golden tests.” During a test run, a Flutter Widget can be rendered and captured as a screenshot. That can be compared to a known good “reference” screenshot (aka Golden). If the screenshots deviate, the test automatically fails. This is a powerful tool, and is one that the Flutter team uses extensively in their own codebase. However, the out of the box support is fairly basic. Consequently, as we introduced Golden tests into our test suite, we found ourselves having to continually add redundant configuration code into every new test. In addition, we ran into hurdles achieving fidelity in our goldens, such as getting fonts and image assets to load correctly. As we added more tests,  and tried to test UIs that varied by data or by device, the number of goldens we created became difficult to manage. Fortunately, we started to recognize some common patterns that simplified the problem space. As these patterns evolved, we packaged them up into an open source package called the Golden Toolkit. Let’s take a look at some examples. A Dozen Reasons to Change Let’s take a look at an example from the Home screen of the eBay Motors app. The home screen is filled with cards that represent active vehicle listings on eBay. Our first requirement was to be able to display an “auction” listing. Here is a golden of a VehicleCard representing an auction. This is pretty straight-forward. The next feature was to emphasize the time remaining on listings that were about to expire. Then we were asked to show the current bid price once the auction’s reserve had been met. We then added support for non-auction listings. Then we wanted confidence that the widget was responsive for different widths. Before long, we ended up with nearly a dozen variations of this “simple” card. We started off capturing a separate golden for every one of these. This became unmanageable. Not only did it require a lot of redundant test code. But automated tests are intended to serve as documentation of the requirements for other developers, and having so many similar, but nuanced variations was making it difficult to see the big picture. What we wished, was that we could see all of the variations at once. Then we realized, with Flutter, we could. After all, Goldens are simply captured renderings of Widgets. We just needed to build a new widget that could display all of the widgets we cared about at once. Here is what we ended up with: This approach makes it easier to document and visualize all the requirements, while allowing code reviewers to understand the impact of a change in a pull request. In addition, it was easily extensible so that future requirements could be captured, with minimal effort. Putting It All Together Now we had a viable strategy for ensuring that individual UI components looked great and didn’t break. But could we use Goldens to help testing more complicated layouts, what about entire screens? One of our biggest pain points was ensuring that complex, full screen layouts continued to look correct on both phones and tablets. We arrived at a simple API for capturing goldens for full screens. Here is a multiScreenGolden test of the same screen on a phone versus a tablet. With multiScreenGolden, it is possible to simulate any device that is relevant for your test. Perhaps you want to make sure the content looks okay on a very small device, such as an iPhone SE, or maybe want to see how a screen looks in a different language, in dark mode, or with larger device fonts enabled. While you can always manually test those variations while developing, it’s not practical to ensure that they continue to work as expected as the software evolves. With GoldenBuilder, we had a strategy for writing visual unit tests for our individual widgets, and with multiScreenGolden() we had a strategy for writing visual integration tests that could verify that the final end user experience is correct. In Conclusion: Move Quickly With Confidence The Golden Toolkit’s simple to use APIs have allowed us to easily write UI regression tests across our entire app. As a result, we have been able to move quickly, without sacrificing quality; confident the app will look great on all form factors as the code continues to change and evolve. Developers who want more information about the Golden Toolkit, can go to: https://pub.dev/packages/golden_toolkit", "date": "2020-03-12"},
{"website": "Ebay-Engineering", "title": "Enabling HDFS Federation Having 1B File System Objects", "author": ["Ruchir Shah"], "link": "https://tech.ebayinc.com/engineering/enabling-hdfs-federation-having-1b-file-system-objects/", "abstract": "In this blog, we discuss our journey to enable NameNode federation for cluster with 4,000+ nodes, 1B file system objects and 170PB of storage. Today’s successful organizations are data-driven. At eBay, we have thousands of engineers, analysts, and data scientists crunching petabytes of data every day to provide a seamless user experience. Through the use of data, we can execute at a massive scale and connect our millions of users to global ecommerce. Apache Hadoop has been one of the most popular framework choices for Big Data analytics at eBay. We use Hadoop to generate value from data to improve search experience and identify and optimize relevant advertisements. This value also helps us enrich our product catalogs, and perform click stream analysis to understand how eBay customers leverage our marketplace. The Analytics Data Infrastructure (ADI) team at eBay is responsible for providing highly available and reliable Hadoop clusters for all customers. Currently, the team maintains 10+ Hadoop clusters ranging from hundreds to thousands of nodes. In this blog, we explore how we have enabled federation on one of our largest Hadoop clusters with 4,000+ nodes, 1B file system objects and 170PB of storage – all with the goal of improving our customer experience. Problem Statement One of our largest Hadoop clusters reached the magic number of 1B total file system objects. The NameNode process was running with 200GB of memory. Once file system objects count exceeds 950M, we began observing various scaling issues. 1)    The NameNode process was running with almost 90 percent memory leading to frequent full GC. Given we were already running with 200GB memory, we had reached the physical memory limit of enterprise node with no further scope of increasing memory. 2)    As shown below, we faced NameNode failover every other day due to high GC activities. We observed six failovers in seven days and the cluster was becoming unstable. 3)    The overall RPC processing time by NameNode increased significantly and resulted in slowness for various SLA jobs. 4)    We were running out of RPC threads on the NameNode which hindered our ability to add more data nodes to cluster. Under these conditions, we were not able to provide a highly available and reliable cluster. This forced us to consider scaling out a solution for name service by enabling federation for our cluster. Below, we discuss View FileSystem, namespace selection (the most important design decision), and basic ideas and outcome of the NameNode federation. We then conclude with some of our learnings and future plans. NameNode Federation Hadoop NameNode federation allows horizontal scaling of the name service. It contains several NameNodes or namespaces, each of which act independent of each other. These independent NameNodes are federated, which means that they don’t require inter-coordination. DataNodes are used as common storage by all the NameNodes, while each DataNode is registered with all the NameNodes in the cluster. For more information about NameNode federation design, please click here . View FileSystem The View File System (ViewFs) provides a way to manage multiple Hadoop file system namespaces (or namespace volumes). It is particularly useful for clusters that have multiple NameNodes and hence multiple namespaces in HDFS Federation. ViewFs is analogous to client-side mount tables in some Unix/Linux systems. ViewFs can be used to create personalized namespace views and also per-cluster common views. The team was particularly interested in using viewFS as it enabled our customers to use their existing code on the federated cluster with only config changes. For more information about viewFS, please click here . Selection of Namespace One of the important decisions for enabling federation is selection of namespace. As shown in the table below, we had a couple of folders which were candidates for separate namespace. One of the obvious choices was “/PathA” folder having the highest storage utilization. However, NameNode performance (in terms of RPC processing time, GC activities, etc.) directly depends on the amount of file system objects it needs to store in memory and the amount of operations performed for those objects. As the below shows “/PathB” folder used only 13.6PB storage, but it had the highest number of file system objects and operations being performed. Therefore, we decided to create “/PathB” as separate namespace and placed all the other remaining folders under another single namespace. Federation Outcome NameNode Heap Usage The most important benefit of federation is that the overall metadata will be reduced for NameNodes in terms of memory usage. As shown in the graph below, memory usage of existing NN reduced from 146GB to 88-95GB after the federation rollout. NameNode Metadata Size As heap memory usage reduces, it’s able to store more metadata per NameNode process with less memory requirement. The table and graph below show that currently, the new federation cluster is holding almost the same or slightly more metadata information than the old cluster. Though total file system objects for the new cluster is a little higher than the old cluster, overall heap usage is almost half. As shown in the below graph, heap usage for federated NameNodes is 85GB and 96GB. For the old cluster, it was 182GB even after full GC. Based on current metrics, we can easily double the number of file system objects for new clusters without impacting performance. GC Time for Each Cycle Amount of time spent for each GC also matters in measuring performance of application. As shown in the graph below, the GC time is around 100 ms for federated NameNodes. For the old cluster, the time was around 200 ms. Average Operation Time There was direct impact on average operation time performed by NameNode. One of the most important and costly operations performed by Namenode is \"GetBlocks\". As shown in the graph below, this operation takes around 50ms to 125ms for federated NameNodes. For similar operations in our older cluster, it took 300ms to 400ms. FS Image Size FS image contains all metadata information which is used in event of a NameNode restart. At the NameNode restart, the entire image gets loaded into memory and block reports are generated. NameNode stays in safe mode and no write operations are allowed during this time. This FS image size purely depends on metadata information. We now have almost half FS image size for federation NameNodes compared to the old cluster, which reduces NameNode startup time. Learning While enabling federation of our cluster, we came across four main issues: 1)    Default move operation is very strict in viewFs. By default, viewFs does not allow mv operations across mount points for the same namespace. This resulted in issues with Hive/Spark queries. The solution was to apply the HADOOP-14455 patch. 2)    Spark/Hive job failure because of ORC write error on viewFs. This was resolved with patch HIVE-10790 . 3)    We observed higher iowait caused by “du” command as HDFS needs to scan two 256 * 256 blockpools resulting in higher system load. All existing Hive/Spark tables locations had to be updated in Hive Metastore. We modified all locations from hdfs:// to viewfs:// during roll out. If you have a firewall in your system, then make sure to have all connectivity issues resolved for newly added federated NameNodes. After enabling the NameNode federation, we were able to provide a more reliable and performant cluster for our customers. Our selection for namespace was near-perfect as we were able to reduce heap usage, RPC processing time, and GC time by half for both NameNodes. We are now confident to add additional federated NameNodes in the future to increase our cluster capacity beyond 5,000 nodes. At eBay, we run our Hadoop 2.7.3 custom build on about 5,000+ nodes on our custom designed hardware (ODM) using Kubernetes in assimilation mode.", "date": "2020-03-16"},
{"website": "Ebay-Engineering", "title": "Building a Product Catalog: eBay's University Machine Learning Competition", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/research/building-a-product-catalog-ebays-university-machine-learning-competition/", "abstract": "Trade has played a critical role in the history of humanity and yet, data from ecommerce, the modern form of trading, has received limited attention from academia. We at eBay want to change that. At eBay, we use state-of-the-art machine learning (ML), statistical modeling and inference, knowledge graphs, and other advanced technologies to solve business problems associated with massive amounts of data, much of which enters our system unstructured, incomplete, and sometimes incorrect. The use cases include query expansion and ranking, image recognition, recommendations, price guidance, fraud detection, machine translation, and more. Though most of the above use cases are common among other technology companies, there is a very distinctive and unique challenge that pertains only to eBay — making sense of more than 1.3 billion listings, of which many are unstructured . Currently, we use our in-house machine learning solutions to approach this problem, but we also want to grow our community and future technologists that haven’t had access to this type of data. By working with universities, we hope that it will pique academic curiosity within ML, spur more research in the ecommerce domain powered by a real-world ecommerce dataset, and help us improve our platform. To support this idea, eBay is hosting a machine learning competition to structure listing data, in other words, producing a product catalog. We are very excited to partner with students at the following universities (list below), which now can start using a subset of our public listing data to help solve a real-world ecommerce challenge. We have more than 40 students from these universities participate as a team or at individual capacity. There are a number of teams competing from: NYU Stanford University at Buffalo The University of Texas at Dallas There are plenty of datasets out there, but the primary focus of those have been recommender systems, price estimation, computer vision, Natural Language Processing (NLP), etc. None have been at a scale pertaining to mapping unstructured items to well-cataloged products. We are using the EvalAI open source platform for hosting the challenge. Our main challenge page has all the relevant details. The question we want to address is how to identify two or more listings as being for the same product by putting them into the same group. We call this Product Level Equivalency (PLE) . That is, if a buyer purchased two items from two different listings in a single group, and assuming the items were in the same condition, they would assess that they had obtained two instances of the same product. The measurable objective, evaluation, submission format, and other details are available on EvalAI . The dataset consists of 1 million selected public data from unlabeled listings. Approximately 25,000 of those listings will be clustered by eBay using human judgment (“true clustering”). These clustered listings will be split into three groups: a) Validation set (approximately 12,500 listings), b) Quiz set (approximately 6,250 listings), c) Final submission set (approximately 6,250 listings). The validation set is intended for participants to evaluate their approach. Anonymized identifiers and cluster labels will be provided to the participants. The quiz data is used for leaderboard scoring. The final submission set is used to determine the winner. For the quiz and the final submission dataset, neither the listing identifiers nor the cluster labels will be provided to the participants. The challenge began on October 11, 2019. The partnered university teams can post their submissions anytime through EvalAI. The evaluation and leaderboard scoring will commence on or about November 8, 2019. The competition will run for about five months and end on or about March 4, 2020. We expect to announce the winning team on March 25, 2020. Students of the winning team will be offered an internship for Summer 2020 at eBay (subject to eligibility verification checks). The 12-week internship will take place at eBay’s San Jose, CA, headquarters and will be fully paid, including furnished summer housing. eBay’s internship program is a combination of real work experience plus a robust program that gives interns exposure to various business verticals, executives, and networking. The internship will also be an excellent opportunity for students to put their ML models into real use. From concept to creation, this challenge was an entirely voluntary effort from people across various disciplines. What started as a hallway conversation eventually ended up into a small group of likeminded enthusiasts. We formed an Operating Committee (OC) and met weekly to brainstorm ideas. Gradually the plans were put into motion, and now we are launching it. It has been an incredible journey, and I was fortunate to be part of the below team that made it happen. Engineering and Research — Roman Maslovskis, Uwe Mayer, Jean-David Ruvini, Anneliese Eisentraut, Akrit Mohapatra, Bennet Barouch, Pavan Vutukuru, Sathish Shanmugam, and Jon Degenhardt Program Management — Roya Foroud Legal — Brian Haslam, Brad Sanders, Sonia Valdez, and Kai Weingarten Recruitment — Cindy Loggins Comms — Melissa Ojeda We would also like to thank the EvalAI team for quickly responding to our numerous queries. And finally a shoutout to our senior leadership ( Mohan Patt and Ron Knapp ), who have been supporting this idea from the get-go. We sincerely hope that making this real-world dataset available will entice universities and students to explore the ecommerce domain further and come up with novel approaches to solve complex problems that can have a positive impact on customers and sellers alike. If you are a university student, researcher, or professor and would like to participate in future programs, please feel free to reach out to us.", "date": "2019-10-16"},
{"website": "Ebay-Engineering", "title": "Front End Debugging — Tips on Resolving Issues Quickly so You Can Move on to Better Things", "author": ["Michael Woo"], "link": "https://tech.ebayinc.com/engineering/front-end-debugging-tips-on-resolving-issues-quickly-so-you-can-move-on-to-better-things/", "abstract": "How did you get here? Have you been staring at the screen for days debugging a mysterious problem that makes no sense? The head-banging is probably making you dizzy and even more frustrated. Now, move away from that wall and read on. I am going to share some debugging tips that will help you. Software development would be quick and simple if only everything just worked — from network to services to libraries to our own code. Test environment is always up and fast. Data response follows the contract exactly. Libraries integrated without a hitch. All browsers follow standards perfectly and behave the same. The list goes on. Unfortunately, this perfect developer world does not exist. There are bugs, quirks, and problems. On any given day, few things work as expected right away. A good part of our development time is spent debugging. It can take hours or days, and sometimes, even weeks. But it doesn't have to be that way. Just like any skill, debugging is an art with its own techniques. I will present some that have helped me and others shorten debugging time and make the process less frustrating. I work in the JavaScript/Node.js world, but most of these tips can help in other development or debugging realms. (Note: The following techniques are tool-agnostic. They help whether you use Chrome DevTools, Safari Web Inspector, Firebug, command-line node inspector, Visual Studio Code built-in debugger, etc.) First, before you dive into a debug session, keep three principles in mind. #1: The computer ALWAYS does exactly what you tell it to. If it doesn't do what you expect, you didn't tell it correctly. I am paraphrasing here, but that's what my CS101 professor once said to the class. It's one of the few things I still remember from college. Whenever I am stuck, these words pop up in my mind. It reminds me that debugging is a solvable problem - there is hope and you will find a solution. (In a real development setting, your code may not be the issue, it may be an issue in the library or service dependency.) #2: Very, very, very few behaviors are intermittent or random. \"Intermittent\" issues can almost always be reproduced consistently using clearly written steps that are carried out precisely. They look intermittent only because the steps are not clear or complete enough, or the steps are not being followed precisely each time the test is run. #3: Don't start fixing until you have found the root cause. We often start \"fixing\" an issue before we have even identified a cause, let alone the root cause. Without knowing the cause, a \"fix\" is not a fix. It's a patch. And it will likely cause more issues than it fixes. Debugging may take a while. It may involve dozens of rebuilds, file saves, server restarts, browser refreshes, etc. It's tedious and soon becomes grunt work. So, every second or minute shaved from these actions will shorten iteration time and make it just a little more bearable. Some common time-savers that many overlook: Are you using too much mouse and too few keyboard short-cuts? Are you using node-watch? Are you using ESLint to catch syntax and other fatal but inconspicuous errors in real-time? Has the code been refactored to a good state for easier debugging? E.g., cache variables, little or no duplicate code. Do you have a step-by-step plan, as detailed as necessary, to test your code? Creating a detailed test plan may take 30 minutes, but for debugging that can take hours or days, the return on investment is high. It also forces you to think about the problem before jumping in. A few pointers: Check whether a bug happens on a common setup Does it really happen only on the German site using Edge 17 on a specific Android phone set to Italian? Or can you reproduce it on the US site using the latest desktop version of Chrome? Don't take the steps documented in a bug ticket at face value. Try to simplify the test flow. Do you really have to go from Home page to Search page to Item page to Checkout page, then add an address, add a coupon, change the address and then remove the coupon, and then add a credit card to see the issue? Perhaps you can see the issue simply by going from Item page to Checkout page and add a credit card? Reduce data input When you are testing the email field in a form, t@t.com probably works the same as fyodor.dostoyevsky@famous-writers.com Minimize typing Turn on browser auto-fill with pre-populated test data. Or copy and paste from a notepad. Just make sure to follow the test steps exactly. Use mock data Is a live service required to debug? Would mock data work? Write unit tests Write unit test to help debug? Yes. When your code change is related to business logic, it's worth the extra time to write unit tests. Make a change, run the tests. It is ALWAYS faster than manual testing. Besides, unit tests help ensure that your solution does not break existing functionality. A seemingly trivial inconsistency in a step during user interface debugging can make a behavior look intermittent. Typing one letter at a time; Control-C; Right-click -> Paste are not the same. How quickly did you click on a button after page load? Back chevron, browser back button, and keyboard combination all trigger different actions. Remember the speed you type, the speed you scroll. Does the plan say \"click, type, type, click\"? Or \"type, click, type, click\"? What command do you use to start the server? Do you delete your workspace or browser cache before you run the test plan? I often sit with other developers to debug together. Most of the time, one of these tips leads to a solution. Are you debugging the right file? In the right workspace? Correct URL? Ever spent hours making changes, but the changes don't seem to be reflected? You re-yarned or re-npm — that didn't work. You re-cloned the git repository — that didn't work. You cleared the workspace and browser cache — that didn't work either If in doubt, check the workspace folder name and the application test URL. Add an alert. Log something. Does it show up? Delete the workspace. Does the page still load? Is there an error? Did you read it? The error stack in the browser console or terminal may look long, cryptic, and scary. Many of us tend to ignore it until hours of fruitless debugging have passed. Don't do that. Try to read and understand it right away. It usually provides the exact place of the problem, or at least a hint on where to start. It can save you hours of aimless retries. Are you calling a library method correctly? Read (or re-read) the doc. Then, read the library code itself. If you have an idea, stop wondering whether it will work. Try it. Don't wonder. Don't wait. Just try it. A \"No\" means you can move to the next idea. The beauty of software (vs. hardware) is that if something breaks, you simply restart. No need to buy that expensive, hard-to-find replacement part. Is the response or input correct? Don't assume the service response or the input to a form or a function is correct or what you expect it to be. Network issues, service failure, or buggy upstream code can contribute to bad responses. A library method or your own code may have modified the input. Put logs at different points on the code path and verify. You can also set breakpoints to verify inputs and outputs, though my experience is that skimming and searching the log is usually faster. Did you debug that library method? Most applications will likely use open-sourced modules. In general, these modules, especially the popular ones, are of high quality, but any code can have bugs. If you suspect a library method may be the culprit, check it. Start removing code — binary elimination During debugging, nothing drains your enthusiasm more than staring at a large file or a method with dozens of paths. Isolate the problem by eliminating files or code half at a time. Less code, easier to isolate problems. Remove half the files/changes, test Remove half of the remaining files/changes, test again Continue halving and testing as needed Once the problem is fixed, restore the removed code. Test. Are you trying the same code change and hoping for a different result? Don't. It won't. Unless you are not running test steps consistently, which you should. The issue is caused by unstashed changes Sometimes, you come across a bug only after days of code changes. Thinking that you introduced it, you start to add and remove code to fix the bug. Stop. First check whether the bug was already there before your changes. Stash the changes to confirm. Corollary: When your code is in a good state, commit it. Small commits are good as long as each commit contains a coherent set of changes. You can always squash the commits later during code merge. Why does it work for your teammate but not for you? Do you have the same setup? Here is a checklist: Same environment? Same configs? Same branch? The right branch? Did you pull the latest code? Unstashed changes? Did you install dependencies recently? Did you clear your workspace cache or temp folder? Same version of node / nvm? Same version of yarn? Same code editor? Same version? How do you start your server? Same browser? Same device? Same URL? While developing and debugging, remember this: Test frequently after making small changes. Keep a record of what you have tried. Re-trying the same thing won't give you a different result, if you adhere to Principle #2 . When you think you have the fix: Don't check in yet. First, confirm that it is really fixed and not a fluke. Run through your entire test plan multiple times. Do this: Remove the fix, confirm it's broken. Add the fix back, confirm it's fixed. Remove the fix, confirm it's still broken. (Yes, again. You may be surprised.) Add the fix back, confirm it's still fixed. I have been at this as long as I can remember. I am still stuck. Please stop and take a break. If that doesn't help, call it a day. Go to bed. Don't think about it. You will have a different perspective, more ideas, and perhaps a solution the next day. Time to get help Finally, after hours or days of interminable frustration, walk over to a teammate and debug together. And as magical as it may sound, sometimes, the gremlins residing in your code may just decide to give you a break and everything starts to work suddenly, just for your walking over to someone. It does happen. I have been there. I hope these tips will help you resolve issues more quickly, so you can move on to better things. Perhaps a critical feature no one has time to pick up. Perhaps an interesting side project. Perhaps help your teammates debug using these techniques. With the time saved, what you do is up to you.", "date": "2020-04-07"},
{"website": "Ebay-Engineering", "title": "eBay’s New Approach to Managing a Vast Service Architecture", "author": ["Hanzhang Wang", "Chirag Shah", "Sanjeev Katariya"], "link": "https://tech.ebayinc.com/research/ebays-new-approach-to-managing-a-vast-service-architecture/", "abstract": "Learn how eBay's architecture knowledge graph was developed; the benefits eBay has received from it; and the use cases we see now and in the future for this approach. Governing and understanding the vast ecosystem in a service architecture is challenging – and with over 3,000 service application clusters in its production system, this is particularly true for eBay. Each application evolves independently with different features and development methods. Efficient development can be inhibited by lack of documentation and not having proper knowledge about internal customers. eBay’s vision – known as INAR, Intelligent Architecture – is to build sustainable service architecture by providing automated visibility, assessment, and governance Intelligence. In this pursuit, we developed a new approach to model and process the application ecosystem using a knowledge graph. A knowledge graph is a commonly used term whose exact definition is widely debated. Basically, a knowledge graph is a programmable way to model a knowledge domain using subject matter experts, interlinked data, and machine-learning algorithms. For eBay, the application/infrastructure knowledge graph is a heterogeneous property graph that improves architectural visibility, operational efficiency and developer productivity, eventually allowing customers to have a better experience when visiting the site. This article will explain how the eBay architecture knowledge graph was developed; the benefits eBay has received from it; and the use cases we see now and in the future for this approach. The Intelligent Architecture vision is aimed at addressing three key challenges of a service architecture: Blindness : It can be difficult to observe architectural issues such as inappropriate dependencies for software and/or hardware; or to envision the eBay infrastructure and ecosystem with customized search. This is an issue because popular software and services evolve frequently and become monolithic, resulting in redundant services and duplicated functions. Ignorance : Lack of measurability for service architecture or technical debts (additional rework that is required when you take an easier upfront approach that is worse in the long run) can prevent you from developing the metrics you need to improve operational efficiency. As business management guru Peter Drucker famously said, “If you can’t measure it, you can’t improve it.” Primitiveness : Diagnostic, engineering and run-time automation is not present. Consequently, artificial intelligence cannot be applied for IT operations, making it difficult to detect anomalies in operations. It was apparent we needed a clearer understanding of our ecosystem if we were going to serve the needs of our 183 million buyers. Our goal was to provide better visibility, provide pattern/anomaly detection, and automate and enhance IT operations. That led us to the idea of using a knowledge/property graph. The graph was constructed using real-time metrics, business features, and operational metadata. Ultimately, the purpose of this graph is to connect data sources and break the boundaries between isolated management domains. Here is a depiction at a high level: One of the first steps in developing a knowledge graph is to calculate the best application metrics and applied machine learning algorithms to automatically cluster the applications. We developed metrics that measured the popularity of applications based on real-time traffic flows and run-time dependencies. We calculated metrics for all eBay clusters and used techniques called K-means and canopy clustering to cluster all services and based on their popularity scores. This allowed us to organize the ecosystem into different categories, such as how active they are. We discovered that 77% of the clusters are labeled as low-activity. One of our goals for using a knowledge graph was to improve developer productivity and enable them to retrieve the information they needed more efficiently. Currently, developers have to go through many tools to receive the information they need. To improve productivity, we built a complete batching system which fetches data from different sources and builds a knowledge graph automatically. We also built an intelligent graph search that dynamically generates a query to explore the knowledge graph, including service metrics and intelligent layering. The following data schema was designed at application(pool)-level, and the boxes with bold or black borders are enabled as the very first “baby” step: By connecting cloud-native data, hardware, people, code and business, we gained better visibility of the ecosystem. The visualization provides rich information in a way that can be quickly understood and acted upon. In the following service dependency example: we randomly picked 18 services and visualize them by one of the default methods. The edge thickness represents edge properties (volumes). Node size represents the behavior metrics. The different colors represent teams or organizations (yellow, for example, is one domain team). The POC is adopted by the eBay dependency system “Galaxies” and now, the graph schema is extended as follows: We calculated metrics and intelligent service laying in more than 3,000 eBay production clusters. Three senior architects manually validated the initial results of the popularity metrics and automatic clustering. The results were surprising and informative. About 10% of the high-activity applications are running under an incorrect availability zone, which can impact operational performance and uptime. For eBay, the knowledge graph has become an important tool(galaxies) that allows us to provide customizable visualization, application metrics, intelligent layering, and graph search. The system provides top-down and bottom-up view of the application, along with the dependencies and increased accuracy; enrich data to enforce application compliance; governance with clear ownership details; and operational performance recommendations. Moving forward, we plan to enhance the graph to support site anomaly detection (an initial work ) by presenting suspected events on the graph with full causality details of each incident. We also plan to extend this graph to include service API metadata, which will enable service layering, recommendation and clustering. The knowledge graph promises to become a critical tool for understanding our ecosystem and meeting customers’ expectations for continually faster and better service.", "date": "2019-11-15"},
{"website": "Ebay-Engineering", "title": "Towards Agile AI", "author": ["Jean-David Ruvini"], "link": "https://tech.ebayinc.com/research/towards-agile-ai/", "abstract": "In this article, we propose a set of better practices, designed by and for eBay ML scientists, for facilitating weaving ML modeling into the cyclical Agile process flow. As artificial intelligence becomes more prevalent in our daily lives, machine learning (ML), its core enabler, is consuming a greater share of software development efforts across industries. Therefore, more machine learning tools, methods, and products are developed using the principles, processes, and tools of Agile methodologies like scrum, kanban, and lean. However, ML modeling (the tasks involved with identifying and implementing an appropriate machine learning algorithm; selecting the data, metrics, training; tuning the features and algorithm; and then producing the target model) is often conducted by data scientists who are not familiar with software engineering or Agile approaches and who have difficulty harmonizing their research activity with Agile project management, time boxes, or engineering configuration management. This article proposes a set of better practices, designed by and for eBay ML scientists, for facilitating weaving ML modeling into the cyclical Agile process flow. One important element of the Agile methodology is the Definition of Done (DoD) for “shippability” and keeping each modicum of incremental work shippable at all times. The DoD is a list of requirements, or acceptance criteria, to which software must always adhere in order to be called complete and accepted by an end user customer, team, or consuming system. However, standard acceptance criteria, such as unit test coverage, code reviewed, or functional tests passed, are inappropriate to ensure ML modeling quality, as they fail to address essential success criteria of the modeling task. Indeed, the absence of quantifiable requirements in modeling quite often leads to misunderstandings, sometimes to frustration (on all sides), when for example an engineering scrum master asks a data scientist “when will you complete your research?” We argue that well-known ML best practices can be very helpful in enabling Agile modeling if specified as requirements from the very beginning of a project and repeated throughout the entire life cycle of an ML model, from problem definition all the way through deployment to production, maintenance, refactoring, and end-of-life. More precisely, we believe that specifying and agreeing upfront on requirements elicits a discussion around their achievability which, in turn, naturally leads to an iterative mindset, a core tenet of Agile. Figure 1 highlights six important phases of ML modeling and their acceptance criteria. The rest of this article describes these requirements in more detail. Figure 1 . The six phases of ML modeling and their acceptance criteria. Before you start. At the beginning of an AI project, before any technical work has started, it is critical to get clarity on the business or technical problem for which the ML model will be applied and how the accuracy of the model’s predictions relate to the overall objective. More precisely, we have observed that answering the following questions before a sprint starts helps communicate precise qualitative requirements for the performance of the model: What business problem are you trying to solve? For which business measurement are you optimizing? Increased net revenue? Increased transaction rate? Increased market share in a different category? Acquiring new profitable, high-spending buyers or frequent shoppers? What are your scientific evaluation criteria? How does your scientific optimization correlate with your business optimization? What is your baseline? What is the industry baseline? What is the minimum viable performance you must achieve to declare this iteration of your model a success at the end of a time box? The answers to the last two questions are particularly critical. Understanding how the scientific metrics correlate with business metrics allows you to quantify the return on investment (ROI) of each measured increment of improvement of the scientific metrics. What would be the business impact of a model with an “accuracy” of 80%? 90%? 95%? Clarifying the current baseline helps define the minimum success bar. In a competitive market, it is important to understand how other companies perform compared to your current feature or service. If you are iterating on an existing model, you need to clarify how much better the new model must perform. If not, you must still quantify the minimum performance needed to reach a satisfactory level for success of your effort. While it is obvious that data quality is paramount for ML modeling, two aspects of data preparation are often overlooked: how the data is sampled and how it is split into training, validation, and test. Data sampling: don’t forget the body and the tail. Critically, the data used for training, tuning, and evaluating an ML model should be as close as possible to the production data and its distribution. In particular, attention must be paid to the head, body, and tail of the distribution of interest. Evaluating a model only on the head of the distribution is a common pitfall of ML modeling. Note however that some scenarios (unbalanced classes) require re-sampling the training data and purposefully training the model on a different distribution. Furthermore, evaluating a model on old test data should be avoided as they run the risk of rewarding old models for being outdated and punishing newer model for being current. And of course, seasonality and other time series patterns in data should accounted for when sampling data. Data splitting: no déjà vu! Any ML scientist knows that training, validation, and test data should not overlap in order to ensure a reliable estimation of the performance of the model on future unseen data. However, it is sometimes overlooked that real life data may contain identical or near duplicate samples. While this may be due to the nature of the underlying distribution governing the data, this deserves special attention to make sure that duplicate samples are not dominating the validation and test data and are not biasing the estimation of the performance of the model. To summarize, the following two questions must be addressed in the data preparation phase: Did you sample separately from the head, body, and tail of the distribution so that you can evaluate your model on each of these? Does your training data overlap with validation data or test data? Use industry standards! While the target metrics should have been identified in the Problem Definition phase, it is important to crystalize them before starting the training and evaluation phase, for two reasons. First, to ensure that industry standards are used. Table 1 lists some of the most commonly used ML metrics. While it is sometimes justified to create a new metric, standard metrics can be effectively used in a wide range of settings. Second, to ensure that the metrics used to evaluate the model and the loss function used to train it are consistent. In summary, the requirements for the metrics definition phase can be formulated as: Are you using industry standards? Are your metrics and your loss function consistent? Accuracy, Precision and Recall, F1, ROC curves, Precision and Recall curves. Regression Root Mean Squared Error,  Maximum Absolute Error Probability Distribution Estimation Log loss scores such as Negative Log Likelihood, Cross Entropy, KL Divergence. Ranking nDCG, DCG, AUC, Kendal Tau, Precision @k for various low values of k Language generation BLEU, TER, WER, ROUGE Table 1. Some of the most common machine learning success measurements. The training phase of an ML model preparation is mostly about hyperparameter tuning, the task of identifying the parameters of the learning algorithm that result in the best model. Hyperparameters should be tuned on the validation data only, not on the test data. Ideally, the test data should be used only once, to confirm that the model provides consistent performance on unseen data. There are two frequent reasons that performance is inconsistent. The most common cause is overfitting the training and validation data, which can be prevented using well-known techniques, such as removing features, adding training data, early stopping, etc. The second-most common cause is that the test data and the validation data are not within the same distribution, and one of them is not representative of production data. In the latter case, the data preparation phase must be revisited. Note that if error analysis is performed using the test data, the test data must be discarded (or added to the training or validation sets) and a new set should be generated to avoid overfitting the test data. In all cases, having a good synthetic data generator or a frequent feed of redacted production data for testing are invaluable. The requirements of the training phase can be summarized with one question: Did you tune your hyperparameters on the validation set only? It’s all about the baseline. We highlighted in the Problem Definition section the importance of identifying the strongest possible baseline. Of course, beating the baseline and achieving minimum viable performance is a requirement of the Evaluation phase. However, aiming initially at modest improvements over the baseline and iteratively publishing shippable models through successive refinement, is an invaluable and key benefit of the Agile methodology. Statistical significance. If the improvement over the baseline is small, statistical significance testing should be performed to ensure that the apparently superior performance of the model is not due to chance or noise in the data and is likely to be observed in production on unseen data. Student’s t-test, Welch’s t-test, and the Mann-Whitney U test are examples of well-known tests for regression; McNemar’s test and the Stuart-Maxwell test for classification. Confidence Score: “a reasonable probability is the only certainty” (E.W. Howe). If it is required that your model outputs a confidence score for its prediction, it is important to ensure that the score is a well-calibrated probability that means that the confidence score matches the true correctness likelihood. This consists of ensuring that when the model is p% confident about its prediction (say 80% confident), it is actually correct p% of the time. Confidence scores can be calibrated using a validation set. Don’t forget operating constraints. Finally, it is critical to ensure that the model also meets operating requirements early on. Examples of operating constraints include inference latency, throughput and availability, expected CPU, GPU, TPU, memory, SSD, HDD, and network bandwidth. To summarize, the following requirements must be met in the Evaluation phase: Do you exceed your baseline and reach minimum viable performance? Is your result statistically significant? Is your confidence score well calibrated? Do you meet the operating constraints? “ML models need love, too” (J. Kobielus 1 ). The task of model building does not end with a specific model being handed over for production deployment. It is important to establish and enforce a maintenance plan that ensures pro-active refresh of the model (as opposed to waiting until some metrics go down). Besides, formalizing such a plan forces a dialog between the modeling Agile team and the engineering Agile team and facilitates weaving modeling into the Agile work process. Before handing a model to production, the following questions must be answered: How will you monitor performances? How often will you retrain the model? Good times come and go but good documentation is forever! While the Agile manifesto favors “working software over comprehensive documentation,” ML modeling is not as self explanatory or reproducible as standard code and needs to be documented appropriately. In particular, we believe that the following should be archived and documented: The code used to sample the data (training, validation, and test). How to reproduce and operate the model. The test data. Hopefully, we have convinced you that specifying upfront clear and quantifiable requirements for each phase of the ML modeling process fosters model quality, quick iterations, better communication, and closer collaboration between the ML scientists and their partners, namely the business and the Agile engineering team responsible for building the inferencing engine and deploying the model in production. We intentionally kept these requirements simple and actionable, in the spirit of the Agile manifesto to favor “individuals and interactions over processes and tools.” However, acceptance criteria are just one of the tools that the Agile methodologies advocate. And if you have experience with extending some of these tools to ML modeling or data science, we would love to hear from you! Acknowledgements The author would like to thank all the co-workers that have been involved in the design of these best practices: Alex Cozzi, John Drakopoulos, Giri Iyengar, Alan Lu, Selcuk Kopru, Sriganesh Madhvanath, Robinson Piramuthu, Ashok Ramani, and Mitch Wyle. Special thanks to Robinson Piramuthu and Mitch Wyle for their careful review of the draft of this article. 1 J. Kobielus. “Machine learning models need love, too”. https://www.infoworld.com/article/3029667/machine-learning-models-need-love-too.html", "date": "2019-10-30"},
{"website": "Ebay-Engineering", "title": "eBay’s Multi-User Account Access is Now Live to Sellers ", "author": ["Lester Dorman", "Rekha Patel"], "link": "https://tech.ebayinc.com/product/ebays-multi-user-account-access-is-now-live-to-sellers/", "abstract": "eBay is enabling sellers to better manage their business by securely granting access to employees to create drafts, edit and launch listings on their behalf. Editor's Note: This article was updated June 1, 2020, with the most recent MUAA product features. eBay now has a solution for sellers that may need help managing their online businesses. Multi-User Account Access (MUAA) is eBay’s latest feature that enables sellers to grant access to Listings within their Seller Hub in a more private and secure way. Sellers can now add employees or delegates to perform seller activities on their behalf without giving full access to their eBay account. Here’s how it works: Sellers invite employees as users, granting each user specific permissions and access for tasks like editing, drafting and publishing listings. Employees then create an account and can access the Seller Hub in their own view. This provides both users — the seller and the employee — added security and privacy for their eBay account. Check it out on the eBay Account Settings today! If you’d like to learn more about how we implemented this feature, read this Engineering blog post . In 2020, we added two new permission areas, View Orders and Terapeak Research, to the MUAA feature. The View orders permission allows sellers to grant employees access to view their incoming orders in read-only mode. It provides enough information to facilitate the picking and packing of orders to get them ready for shipping. The Terapeak Research permission gives employees access to their employer’s Seller Hub Research tab in order to conduct market research on their behalf. Terapeak Research was fully integrated into Seller Hub earlier in March 2020 and full details of this integration and the capabilities of the product are available in this Tech Blog post . In addition to these new permissions, eBay has expanded MUAA availability to all the sites where Seller Hub is available: United Kingdom, Germany, Australia, Canada, France, Italy and Spain. Sellers on these sites can now delegate access to employees registered in the same country.", "date": "2019-09-19"},
{"website": "Ebay-Engineering", "title": "eBay Helps Sellers Better Manage and Grow their Business with Multi-User Account Access", "author": ["Lester Dorman", "Dave Kamalsky", "Karthik Thavamani", "Mallikarjuna Potta", "Rekha Patel"], "link": "https://tech.ebayinc.com/engineering/ebay-helps-sellers-better-manage-and-grow-their-business-with-multi-user-account-access/", "abstract": "Learn about how eBay enabled sellers to grant access to their accounts in a private and secure way. Giving employees access to their employer’s account should be an easy task. Little did the team know when they started this project that the larger challenge lay in the backend. Earlier this year, eBay launched a new feature, Multi-user Account Access (MUAA) , to help sellers scale their business and improve business efficiency without exposing their password and critical business information to designated users. eBay users that are opted into Seller Hub can now navigate to the “Permissions” section under “Account Settings” in My eBay and grant other users access to their account to perform specific functions on their behalf. Previously, sellers didn’t have a way to let other users access their account. To overcome this limitation, we considered several different implementation options, including: Creating sub-accounts under the seller’s account Creating different types of new users/accounts that could only perform limited tasks Linking a seller’s account to other users’ accounts in a way that permits them access to limited functionality only The first two proposals were ruled out due to the massive amount of change they would have required to eBay’s underlying identity model and user flows. The development time and cost would have been prohibitive. The concept of sub-accounts didn’t exist in the eBay architecture, and all of eBay only understood the notion of single-identity-single-account. Likewise, creating limited-function accounts would have been a completely new concept, and would have required extensive changes across the entire site. That left account linking via a permissions system as the most viable candidate. This was solved elegantly by adding an additional MUAA user context under the currently logged-in user account. The main advantage of this approach is that the eBay ecosystem already knows how to deal with logged-in users. We didn’t have to change the entire site all at once to recognize the MUAA context for an account. We only needed to enhance the parts we wanted to enable with MUAA functionality, and we started with Seller Hub Listings. Since we’re using regular accounts, the rest of the site works just the same as it always did. Over time, we can increase the MUAA footprint to include many more Seller Hub capabilities, such as delegating order management, CRM, and even expanding beyond selling use cases. We could also expand it to the buying side for parent/child or teacher/student types of scenarios. Since Seller Hub already provides a comprehensive and well-encapsulated selling experience, we were able to leverage its framework and deploy the MUAA experience within it. All Seller Hub users can delegate access to other users. We decided to start with a limited MUAA-enabled capability in the initial implementation, due to the complexity of introducing the concept of linked accounts. It provides the ability for delegated users to create and edit listing drafts, and/or to publish and revise listings. While the initial rollout is available to U.S.-based users only, we plan to expand to other countries soon. When a delegatee logs in and switches to another user’s account, they see a limited view of their Seller Hub and can access only those functions to which they have been granted permissions. The main design principle of MUAA is to grant multiple users access to a single eBay account, with specified permissions granted by the owner of the account. We knew from the outset that we would need to build several new Identity-related capabilities to make the account linking approach work, so we started there. Some of the key new Identity management components we built to enable MUAA include: Permissions catalog Permissions management and enforcement Account context switching and user session management Activity logging and reporting tools A way to invite users via email to access a seller’s account An email invitation acceptance mechanism A way to recognize existing eBay users when they sign in to eBay and route them accordingly to the desired MUAA experience A way to route new users to the registration flow, and then route them accordingly Adding MUAA specific navigation features to the My eBay and Seller Hub menus A way for sellers to view all the delegatee activities The entire MUAA components system is built using a modern Java/Node.js tech stack, with some added eBay contextual frameworks. MUAA is built on a role/permission-based access control model on top of the existing eBay user architecture. The complete life cycle of MUAA was built from scratch and all the existing eBay platform business logic components were enhanced to understand the MUAA context. We also needed to create the selling experience for MUAA delegates that mapped to their permissions level.  That entailed: Providing limited views of the seller’s Seller Hub that matched their delegates' permissions levels Disabling some existing features of the listing flow to prevent delegates from changing key business settings Implementing a new banner in the page header to let a delegatee user know which account they were active in Implementing permission limitations in Seller Hub Listings tab UI Implementing behind the scenes limitations to prevent delegates from accessing functionality not granted to them We have designed and built MUAA with specific principles in mind. These principles are at the core of the decision-making process for MUAA platform. Here are some of these principles: Do not reinvent the wheel and leverage existing systems as much as possible Make the MUAA experience easy for our users Make integrating with the MUAA platform easy for developers when adding new features The MUAA journey starts with the seller inviting the employees to act on the seller account’s behalf: The seller sends an invitation to the email address the employee will use to access their account, with certain permissions specified. The employee ‘Accepts’ the invitation by clicking on a link in the email.  If they already have an account using that email address, they log in and are taken to the employer’s Seller Hub Listings tab.  If they don’t have an account, they land on the Registration page and create one with that email. Once logged into the employer’s Seller Hub, they can perform the activities the employer granted them permission to access. The employer and employee each have a view of their account access status under the ‘Permissions’ section, and the employer can change permissions at any time. The changes take effect immediately. When a MUAA-enabled employee is in Seller Hub, they see a blue-bordered banner informing them which account they are working on. This banner also provides links to switch back and forth between accounts. The employee can switch back to their personal account. Or, they can have access to more than one seller account, and switch between them too. Sellers can view an employee’s activity on their account. They can see when invitations are accepted, when listings are published and revised, and they can filter searches on several criteria, such as Item ID or type of activity. We have designed MUAA to be easy for eBay application developers to integrate with. Permissions can be integrated declaratively for all our application technology stacks.  An eBay web application developer can specify the permission required for a resource through page metadata. And, a service developer can specify the permission required for the resource through annotations. When an employee is acting on another account, MUAA components in eBay web/service application stacks populate the employee and seller account identifiers in user access context. Then, the permission enforcement system in the application stack evaluates the permission specified for the resource against the user access context. If the employee and seller accounts are associated for the specified permission, the employee is allowed to perform the corresponding activity on the site. The user access context is propagated/evaluated from the application layer to the service, and throughout the chain of  services in the service layer. In the eBay ecosystem, there are a small number of web applications and services still utilizing legacy application stacks, and these are integrated with MUAA programmatically. The data model and framework is built to support mapping between an account, delegatee, and permissions that has the capability of going as granular as down to the field level or as flexible as dynamically computing permissions based on rules. For instance, a seller could limit the listing actions of a delegatee by category, price or location. These permissions can be aggregated into roles for efficiency and ease of management. The foundation has been built to cater to all the use cases that sellers may bring up in the future to increase their productivity and take advantage of business opportunities. eBay's sellers are excited about MUAA and have been eagerly awaiting it. We have kept their needs front and center.  It has been designed and built to support scale, resiliency, performance and ease-of-use. And as we ramp MUAA up, permissions management, policy enforcement and activity logging will support hundreds of millions of calls on a daily basis. For the first MUAA use case of Seller Hub Listings, changes were implemented across multiple technology stacks, and multiple technology generations, to seamlessly consume and validate the permissions and render the trimmed-down version of Seller Hub to delegatees.  It was a complex undertaking that we’re very proud to have implemented successfully. Since we first announced MUAA at eBay Open 2019 in July, we’ve seen steady organic adoption of the feature in the U.S., and we plan to roll it out to sellers in other regions in the coming months. We’re delighted to be able to provide something that sellers have been requesting for a long time.  They love what we’ve done with MUAA so far...and they want more! “Love the new feature of the Multi User Account Access.” “I really love how seamless it is to use. My employee is only able to make drafts as I want to review and make any final edits on the complete listing. I appreciate the fact that they can do the critical part without having full access to my site and risk being able to purchase things under my account, edit my PayPal (to ensure it’s going in my account), or see the volume of sales I have. Since they’re only able to make drafts, they can only see that aspect of eBay seller hub. It’s phenomenal.“ “It has made it easier for me to focus on other things for my business. This is the first time I have hired someone to create drafts for my account as I am a small time seller and I take a very hands on approach.  Due to personal reasons, the last two weeks I was unable to work. I literally got over 20 drafts created while I was out of commission. I wish I had given my employee more items!” “I really love this feature because the lister I am using now is someone I didn’t know before so I would hate to give her access to my entire account.” Our sellers have also made it clear to us that there are other important selling tasks they want us to MUAA-enable, such as order management, shipping, returns, cancellations, and member messaging, and we’re planning to implement those in the near future.", "date": "2019-09-19"},
{"website": "Ebay-Engineering", "title": "Resiliency and Disaster Recovery with Kafka", "author": ["Engin Yoeyen"], "link": "https://tech.ebayinc.com/engineering/resiliency-and-disaster-recovery-with-kafka/", "abstract": "Multi-Region Kafka Setup Using MirrorMaker 2.0 High availability, resilience and scalability have become the standard when building user-facing applications. Cloud computing has been a contributing factor to that. Previously, having multiple data-centers was a major up-front capital investment — both skill-wise and in terms of hardware. Today, having a disaster recovery plan requires less effort and no up-front investment, thanks to cloud computing. As a result, many consumer-facing applications are now set up in a geographically dispersed fashion. In case of major failure, users are not affected, and latency is reduced for users across the globe. The Motors Vertical team is part of the eBay Classifieds Group (eCG). As a global organization, we run our software in multi-region (multi-datacenter) setup, where regions are geographically dispersed. From time to time, we do failover exercises where we take down a whole region to verify how systems respond to the outage. We also have automatic failover, so if services have problems in one region, we can divert all traffic to another region. Many of the technologies we use enable us to run applications in a multi-region setup very efficiently and easily, while a small set of technologies makes that difficult. One of the most critical components in our infrastructure is Apache Kafka ®. — But as important as it is in our architecture, it’s not yet that easy to run a multi-cluster active/active Kafka setup. Although the recent release of Kafka 2.4 has greatly improved support for multiple Kafka clusters with MirrorMaker 2.0 , depending on your use cases it could still give you a headache. In this article, I will outline a technical scenario that requires ordered events, highlight several challenges, and present possible solutions for running a multi-region Kafka setup. At Motors Vertical (MoVe), we deal with product listings. A listing (or “ad”) is a non-personal message to promote or sell a product, either by a private individual who wants to sell a single item, or a professional organization who sells at a higher quantity. Listings are the core data structures of most classified ads, and everything in our system depends on them. Several actions are taken whenever a listing is created: Check for fraud Notify the user that the listing has been successfully published Create an index, so the posted listing can be shown in search results Add promotions to seller listings This list gets longer, based on whether the listing has been created, updated, deleted, etc. So each state that the listing is in could trigger a different action. A high-level view of the system looks something like this: Figure 1 – Use Case Overview: Posting a listing The listing service has its data storage, which stores listings and serves as the source of truth for that domain. Once a client posts a listing, it is stored in NoSQL data storage, and published to the Kafka brokers. There are a set of clients which consume those messages and take appropriate actions. If we were running our system in a single region, the setup above would be fine. If we run our software in multiple regions, we might replicate all the software we run in one region to another, so if region-1 fails, region-2 could take over. In that scenario, the system might look like this: Figure 2 – Use Case Overview: Posting a listing with multi-region setup A setup like this allows iOS, Android, Web and API clients to connect to multiple regions. Traffic is routed to the different regions based on load balancing rules. For the sake of simplicity, let’s assume that we are using the round-robin load-balancing method here. So requests will be handled by multiple regions for the same client. Any events received in region-1 are processed in region-1. The problem with this approach is that some of the services require a complete data set to function properly. For instance, for the search API to deliver correct results, it has to index all the listings. Search indexes are not shared, so each region has to hold its own search index. All the data must be available to deliver the same result on both regions. Since we rely on Kafka Streams , for some join operations we require the whole data set. Otherwise joins would not take place, and the data that is produced would be misleading. From a very high level, services can be grouped into two categories: 1.     Services that need to consume the data at least once from the region that they are in 2.     Services that need a complete data set, meaning all events across regions With that in mind, now the diagram should look as follows: Figure 3 – Listing consumption based on the use case So there are two technical challenges that need to be overcome: 1.     Make sure events take place in a region processed by local services (which is done by the design above) 2.     Make sure events are shared across regions (not hard but very problematic) Before we move on, it’s worth noting that a Kafka cluster gives you the following guarantee: Messages sent by a producer to a particular topic partition will be appended in the order they are sent. That is, if a record M1 is sent by the same producer as a record M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log. (See Kafka Guarantees ) It’s worth emphasizing that this guarantee is for a single cluster —not for a multi-cluster setup. This is important to keep in mind, and we’ll come back to this point later… So with our use case and technical challenges in mind, the question we want to answer is: How to run multi-cluster Kafka in a multi-region setup, when event ordering is a must-have feature? While it’s possible to run an active/active Kafka cluster, it’s very problematic with ordered events. Internally, here at MoVe we had our own custom kafka-mirror-service, which was born out of necessity because MirrorMaker 1.0 could not fulfill our needs. But in this article, I will focus on the capabilities of MirrorMaker 2.0. With MirrorMaker 2.0, topics can be mirrored between clusters. MirrorMaker handles the topic creation and synchronization on its own, which is very very handy. For instance, if a topic called listing exists in “region-1” and “region-2”, MirrorMaker creates new topics on both Kafka clusters, prefixed by the defined name: Figure 4 – MirrorMaker 2.0 Cross-Cluster Replication Now all the listing items from region-1 will be mirrored to region-2 and vice-versa. This solves the problem of accessing complete data sets, as services that require it can now consume multiple topics with a regular expression in the topic name, like .*\\.listings , which returns the whole data set. With replication, we can re-draw Figure 2 as follows: Figure 5 – Listing consumption by local and cross-region This looks like a solution. We can satisfy both groups of services, including those that 1) consume events in the local region at least once, and 2) consume all data across regions. We do have use cases where the order of events are irrelevant and this solution will work fine for that. But the listings scenario requires events to be consumed in the correct order. Additionally, we use topic compaction for the listing topic, so this solution becomes quite problematic. For instance, let’s assume that the listings client is initiating the update to a listing, where each update is different than the previous one and contains more data than before, such as: Figure 6 – Event Order #1 represents the listing ID, which we will use as a Kafka message key, while the payload is an updated version of the event. Events are processed by different regions: v1, v3 end up in region-1, whereas v2 and v4 end up in region-2. Now the Kafka topics cluster and topics look like this: Figure 7 – Different versions of a message with the same key in different regions We do use log compaction, so assuming topics get dirty enough to be eligible for log-compaction after a while, once log compaction takes place, the system state looks something like this: Figure 8 – Different versions of a message with the same key in different regions, after log compaction Now Kafka clients will read the data from those topics, but Kafka clients cannot give you the guaranteed ordering between different topics. So although this approach is great, it solves the problem of unordered events, but not ordered events. This setup gets problematic when you start using tombstones. Let’s assume that region-2 gets the delete event, and the service request is routed to a local Kafka cluster. When events are replicated, we still have the copy of the old version (not even the latest) in both regions, like this: Figure 9 – Different versions of a message with the same key in different regions, after tombstone event While MirrorMaker 2.0 does the heavy lifting, it does not solve the ordered event problem in this scenario. This setup is relatively simple — topics where the order of the events matters are written to a single Kafka cluster, regardless of the region in which they are produced. Data will be replicated via MirrorMaker 2.0 to other clusters in different regions. It will look as follows: Figure 10 – Write Global / Read Local — Kafka Cluster With Mirroring Topic Services in different regions will consume multiple topics via .*\\.listings, but as a producer will connect to only one region, data is ordered in a single region. This is convenient if one Kafka cluster fails — the producer only has to change the broker and consumers do not need to do anything. This forces all events to be ordered, in both regions as events are written to a single Kafka cluster and data is only replicated. Now all the events are ordered, but we have two other problems that we need to solve: 1.     Some applications now consume and process the data unnecessarily. For instance, for webhooks, we consume the same event at least twice. At least once is great, but at least twice has financial consequences: we rely on third-party services and cost-per-call models, and we just doubled the cost of those services. So while we solve the ordering, we are now forced to process events at least twice. 2.     In the case of cluster failure, there is a potential data loss in two separate scenarios. First, applications that are publishing messages will not be able to communicate with the cluster, which could lead to data loss. Second, MirrorMaker 2.0 data replication is an asynchronous process, so when a cluster fails, there is no guarantee all data has been replicated before the cluster failed. Which brings us to the conclusion that recovering from failure is not easy as it seems. Latency is also a big issue in this scenario. Data replication between clusters is asynchronous, which means when data is written to region-1 it will take some time to replicate this to region-2. This replication time is affected by several factors: Connection between regions Amount of data that has to be replicated So this solution causes the problem that data that is immediately available in one region can take longer to be available in the second region. This is relatively easy, and solves a bunch of problems, but has other caveats. In this solution producers & consumers connect to the same Kafka cluster, as shown in the following diagram. Figure 11 – Active/Passive with Replication In this case, events are ordered as they are not being written to the same cluster, consumers have access to the full set of data, and they are consumed at least once. This rather easy. The caveats of this approach are: Idle Compute Resources The whole Kafka cluster has to wait in an idle state, which wastes resources. It is used only in case of failure. There is a continuous data flow between regions, just in case something fails one day. Latency While applications in the same region with the active Kafka instance work fine, others are punished with latency penalties while reading and writing. This problem can get bigger or smaller based on the distance between regions. Consumer-Group Offsets Each record in Kafka is assigned an offset number, which is used to identify the record in a partition of the topic. So when consumers import records, Kafka stores a set of offsets that indicates which messages have been consumed. This helps, as the next time the same consumer tries to read the data, Kafka can send only new records. This is called consumer-group offsets. However, offsets of a topic are not guaranteed to be the same, even if data is replicated via MirrorMaker 2.0, which means the consumer-group offset may not be the same between two clusters — even if they hold 100% identical data. For this reason, offsets are not shared directly. In case of failure, all the services now have to connect to the Kafka cluster in region-2. The problem is because they’ve been consuming data from region-1, the offsets are those assigned by region-1. So services have to start to read messages from an offset that is nowhere near the real offset. This means some messages may be consumed more than once. However, MirrorMaker 2.0 emits consumer-group offset checkpoints from one cluster to another. MirrorMaker 2.0 uses an internal checkpoints topic to store offsets for each consumer-group in the source cluster for the destination cluster. Furthermore, it uses an offset-sync topic to store cluster-to-cluster offset mappings for each topic-partition being replicated. So although offsets are not the same between clusters, MirrorMaker 2.0 tracks the nearest consumer-group offset. This process is not automated, so services still have to take the responsibility to find the right offset when connecting to a different cluster in case of failure. MirrorMaker 2.0 provides partial data, so services can do that if they need to. For this purpose, Kafka 2.4.0 offers RemoteClusterUtils, described in KIP-382 : A utility class RemoteClusterUtils will leverage the internal topics described above to assist in computing reachability, inter-cluster lag, and offset translation. It won’t be possible to directly translate any given offset, since not all offsets will be captured in the checkpoint stream. But for a given consumer group, it will be possible to find high water marks that consumers can seek() to. This is useful for inter-cluster consumer migration, failover, etc. Let’s consider this scenario: You have two regions. A Kafka cluster in region-1 (or the entire region-1) has failed. All the applications now have to connect to a new cluster. Most likely services load the cluster configuration on bootstrap, so when the configuration changes, you need to restart services. Services have to have the appropriate code — they have to be aware that region-1 has failed, and that they are now connecting to region-2 and have to look up the offset with the help of RemoteClusterUtils to get the appropriate offset from the new Kafka cluster. Services should only have to do that once — if you re-deploy a service, it should not perform the same operation again. This rule applies to all of your services — so they all need to be aware of the regions they were connecting to and the regions to which they will connect in case of failure. This is a valid scenario (assuming your cluster doesn’t fail every day), but if not configured properly, services may end up re-reading all the data from the beginning of the queue. I can definitely see use cases for syncing offsets between clusters, so I believe RemoteClusterUtils will be very useful for integration scenarios and other challenges. But using it for cluster failures requires a lot of effort, coding and automation, and the merits of such an effort are questionable. This is a rather expensive solution, and debatable whether it’s really worth it, but for the sake of completeness, we’ll outline it here. This is an extended scenario of Solution 1, in which messages are written to the local cluster. Each local cluster has a service which leverages Kafka Streams . The job of this service is to order the events and write them to a new topic. Figure 12 – Active/Active with Kafka Streams to order events In this scenario, each document has a version. Every time there is an update, this version number is increased. When the document is published to Kafka, the version number is sent as a Kafka message header (see KIP-82 ). The application can sort the messages based on header value, and write them into a new topic. Initially, messages will be distributed as in the following diagram: Figure 13 – Different versions of a message with the same key in different regions After a service processes and orders the messages, the latest version will be written to a topic called listing.ordered. Figure 14 — Active/Active with Kafka Streams to order events Using message headers with version information solves the following problems: Events can be ordered regardless of their source Tombstone events that mark items for deletion are also versioned, so the latest message will be the tombstone. This looks like a good idea at first, but tombstones from different regions may cause problems. Although the resulting topic will have a tombstone message for a while, eventually those tombstone messages will be removed. Alternatively, delete as a cleanup.policy instead of compact would be a reasonable decision, as it solves the tombstone message issue — and it might even be a better solution. So while this is viable, there are tradeoffs: It requires significant engineering effort to get everything up and running and tested well It introduces additional latency. In an ideal scenario, the publisher publishes the data and consumer consumes it. This is relatively fast. However, appending data replication + data processing to a pipeline for every message increases latency. For instance, MirrorMaker 2.0 has to replicate the data to another region first (this is an acceptable lag). But then the Kafka Stream service has to consume and publish the data in the correct order. The Kafka Stream application requires some fine-tuning and a good understanding of how Kafka Stream works, such as data storage and how to minimize the latency of task failover (see Standby-Replicas ). In high throughput scenarios, Kafka Stream requires a good deal of resources to run, which may be expensive in the long run. A similar solution could be implemented using message timestamps as well as message keys — but time is a very tricky concept, so I wouldn’t recommend it (see There is No Now ). There are several possibilities to run Kafka in a multi-region setup to share data across regions. Each of those, however, has consequences which should be acknowledged. The engineering effort to get it up and running, testing, cost of infrastructure, time that is lost, and so on. I wouldn’t argue that the effort is unnecessary, but it should be well thought out. It is also possible to run a single Kafka cluster in a single region and share data between two regions. Most cloud regions have three or more Availability Zones (AZ). Each AZ is isolated and physically separated, with independent power, cooling, and connected via redundant networks. For some business use cases, this could be enough high availability, and acceptable latency. However, for our latency-sensitive business, multi-region architecture is a better choice. Given all that, it is worth mentioning that MirrorMaker 2.0 does a decent job. But it should also be noted that replication is an asynchronous process, which means MirrorMaker alone cannot guarantee that all the data is replicated successfully in a target cluster if the source cluster fails. Kafka is a great piece of software. It is performant, well-abstracted, very cheap (meaning modest hardware can do millions of read/write operations per second) and it gives us great flexibility. These characteristics of Kafka are based on simple design choices. Those design choices, however, do not play well in multi-region active/active or active/passive system design, for a very simple reason: There is an obvious data transfer latency between regions. Achieving low latency, and strong consistency while surviving region failure is not an easy challenge to solve.", "date": "2020-05-08"},
{"website": "Ebay-Engineering", "title": "GRIT: a Protocol for Distributed Transactions across Microservices", "author": ["Gene Zhang", "Mohammad Roohitavaf", "Jung-Sang Ahn", "Kun Ren"], "link": "https://tech.ebayinc.com/engineering/grit-a-protocol-for-distributed-transactions-across-microservices/", "abstract": "eBay technologists recently showed off a distributed transaction protocol called GRIT, for distributed ACID (atomicity, consistency, isolation, durability) transactions across microservices with multiple underlying databases. This article describes the basic ideas of the GRIT protocol, which was announced at the IEEE International Conference on Data Engineering (ICDE) 2019, and provides an example of using part of the protocol for implementing a transactional storage backend for JanusGraph. This example focuses on a system with only a single database, but as we said, GRIT can support ACID transactions for systems consisting of multiple databases. In a microservice architecture, an application may invoke multiple microservices, which are usually implemented in different application languages by different teams and may use multiple underlying databases to achieve their functionality. This popular architecture brings new challenges for consistent distributed transactions across multiple microservices. It is a real requirement to support ACID transactions in the context of microservices, but is very hard to achieve using existing technologies, since distributed transaction mechanisms designed for a single database cannot be easily extended to the cases with multiple databases through microservices. In environments that involve multiple independent databases, the traditional two-phase commit (2PC) protocol 1 was essentially the only option for distributed transactions by the system without additional application effort. However, it does not work well in a scale-out platform due to long paths of potentially many coordinating participants and the locking required over the phases. On the other hand, using a transaction log executed by a framework 2 such as Saga will incur complex compensating logic by applications and may have business implications due to irreversible partially successful transactions. To address these issues, we developed GRIT , a novel protocol for globally consistent distributed transactions that cleverly combines ideas from optimistic concurrency control (OCC) 3 , 2PC, and deterministic databases 4,5 to achieve, for the first time, high-performance, globally consistent transactions across microservices with multiple underlying databases. The following diagram illustrates the GRIT protocol in a system of microservices with two databases. The GRIT components, including GTM, GTL, DBTM, DBTL, and LogPlayer, are shown in the center. Without the GRIT components, the diagram represents a system of plain microservice architecture with two scale-out databases. They consist of the following: Applications: invoke microservices to achieve their functionality. Microservices (Entity Services): building blocks to provide business-oriented service for applications to implement business logic. Each DB may have support for multiple microservices, and each microservice is likely independent of the other. DB Services: provide DB read/write interface and directly access DB servers. When supporting transactions, it also caches the read/write results of each transaction during the execution phase and sends them to its DBTM for conflict resolution at commit time. DB shard servers: the backend storage servers for the database, usually replicated for high availability. The key components of GRIT include: Global Transaction Manager (GTM): It coordinates global transactions across multiple databases. There can be one or more GTMs. Global Transaction Log (GTL): It represents the transaction request queue for a GTM. The order of transaction requests in a GTL determines the relative serializability order among global transactions. Persistence of GTLs is optional. Database Transaction Manager (DBTM): The transaction manager at each database realm. It performs the conflict checking and resolution, i.e. local commit decision is located here. Database Transaction Log (DBTL): The transaction log at each database realm that logs logically committed transactions that relate to this database (including single database transactions and multi-database transactions). The order of transactions in a DBTL determines the serializability order of the whole database system, including the global order dictated by the GTM. A log sequence number (LSN) is assigned to each log entry. LogPlayer: This component sends log entries, in sequence, to the backend storage servers for them to apply the updates. Each DB server applies log entries of logically committed transactions in order. For the purpose of understanding the details of the protocol, we use the following diagram to show the main steps for a distributed transaction. In GRIT, a distributed transaction goes through three phases: Optimistic execution (steps 1-4): As the application is executing the business logic via microservices, the database services capture the read-set and write-set of the transaction. No actual data modification occurs at this phase. Logical commit (steps 5-11): Once the application requests the transaction commit, the read-set and write-set at each database service point are submitted to its DBTM. The DBTM uses the read-set and write-set for conflict checking to achieve local commit decision. The GTM will make the global commit decision after collecting all the local decisions of DBTMs for the transaction. A transaction is logically committed once its write-sets are persisted in log stores (DBTLs) for databases involved. This involves minimum coordination between the GTM and the DBTMs. Physical apply (steps 12-13): The log players asynchronously sends DBTL entries to backend storage servers. The data modification occurs at this phase. Overall, our approach avoids pessimistic locking during both execution and commit process and avoids waiting for physical commit. We take the optimistic approach and also make the commit process very efficient by leveraging logical commit logs and moving the physical database changes out of the commit decision process with deterministic database technology, which is similar to log play in replication. GRIT is able to achieve consistent high throughput and serializable distributed transactions for applications invoking microservices with minimum coordination. GRIT fits well for transactions with few conflicts and provides a critical capability for applications that otherwise need complex mechanisms to achieve consistent transactions across microservices with multiple underlying databases. As you can see, the GRIT protocol contains two parts: one for each database (or each database realm, which can be a set of partitions of a database) performed by DBTM, DBTL, and LogPlayer, and the other for cross-database coordination by GTM and DBTMs. In the following diagram, we illustrate the design of a transactional graph store backend (called NuGraphStore) for JanusGraph using the part of GRIT protocol for a single database. The following diagram shows how NuGraphStore is implemented with two availability zone (AZ1 and AZ2) deployment for illustration. There are a few components involved in the NuGraphStore backend for JanusGraph: Storage plugin: a custom storage interface plugin to interface between the JanusGraph DB Layer and the backend storage engine and transaction protocol components. DBTM: performs the critical conflict checking for optimistic concurrency control. This is part of the GRIT distributed transaction protocol on single databases performing OCC. LogStore: replicated log store for mutations from transactions. One entry for each transaction, indexed by Log Sequence Number (LSN). It acts as a WAL (write-ahead-log) in traditional database systems. The LogStore is the DBTL in our GRIT architecture. LogPlayer: applies log entries to the backend servers asynchronously. Storage engine: backend storage engine to store KCV (Key-Column-Value) data from JanusGraph. It performs reads and mutations and supports snapshots defined by the LSN. As an application is performing a transaction, it can read from the store and write to the store. For the read operations, the storage plugin directly communicates with the storage servers (except for reads that are found in the write-set for the transaction). The storage plugin also keeps track of the read-set as the application is reading from the store in the context of a transaction. The useful information for each read is the <key, lsn> pair, where lsn is the log sequence number reflecting the storage engine state when the key-value is read. An LSN is the log index of the entry for the mutations of a transaction. It is assigned by the LogStore and used to define the snapshot of the backend databases. A key being not found is also recorded as part of the read-set. Unlike reads, the storage plugin for writes does not directly communicate with the storage servers. Instead, the storage plugin buffers the writes in the corresponding write-set for the transaction. When a transaction commits, the storage plugin submits the commit request with the read-set and write-set it has captured for the transaction to the DBTM. The DBTM performs the standard conflict checking for OCC for the transaction. If there is no conflict, it will persist the write-set to the replicated LogStore (i.e., it sends the write-set to the LogStore replica set, so all the replicas keep the exact same log). At this point, the transaction commit completes logically, and the DBTM responds back to the storage plugin. The LogPlayers tail the LogStores and play the log entries to the backend shard servers based on the data distribution. It’s worth pointing out that the above description is a basic design with many opportunities to enhance for performance and reliability. It’s our belief that it is more productive to make the basic components mature before optimizing across the components or using replication for DBTM to achieve higher reliability. Also, there are different ways that we can capture the read-set and write-set. For a KV store, the simplest form we need for conflict checking is <key, lsn> pairs. To support more complex systems, however, the read-set may include ranges, or predicates, as described in. 6 As of this writing, NuGraphStore is going through the open source process. 1.  C. Mohan, Bruce Lindsay and R. Obermarck, “Transaction management in the R* distributed database management system” ACM Transactions on Database Systems (TODS), Volume 11 Issue 4, Dec. 1986, Pages 378 - 396. 2.  Pat Helland, “Life beyond Distributed Transactions: an Apostate’s Opinion”, CIDR 2007. 3.  H.T. Kung, J.T. Robinson, “On Optimistic Methods for Concurrency Control”, ACM Transactions on Database Systems 6:2, June, 1981. 4.  Thomson, Alexander and Diamond, Thaddeus and Shao, Philip and Ren, Kun and Weng, Shu-Chun and Abadi, Daniel J, “Calvin: Fast distributed transactions for partitioned database systems”, SIGMOD 2012. 5.  Kun Ren, Alexander Thomson, Daniel Abadi, “An Evaluation of the Advantages and Disadvantages of Deterministic Database Systems”, VLDB 2014. 6. Thomas Neumann, Tobias Mühlbauer, Alfons Kemper, “Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems”, SIGMOD 2015", "date": "2019-10-24"},
{"website": "Ebay-Engineering", "title": "Auto Effect Pattern with Redux and React Hooks ", "author": ["Nate Wang"], "link": "https://tech.ebayinc.com/engineering/auto-effect-pattern-with-redux-and-react-hooks/", "abstract": "A pattern for managing HTTP requests with Redux and React hooks. It separates concerns of data fetching and binding logic from components into hooks to make it easier to use. One of the most important tasks of creating front-end applications is fetching data and showing it to users. With React and Redux, we usually need to write a lot of boilerplate code to handle those requests. When multiple components need to access data from the same REST API, each component needs to check if data exists, and if not, then data needs to be fetched separately. To simplify this general process, we introduced a new pattern to provide a unified caching mechanism, error handling and loading status management. The pattern is widely used in building eBay internal cloud platform portals and it increases the development efficiency by simplifying code of components. We call the pattern Auto Effect. It makes Redux async actions of fetching data easier and re-useful in a web application built with Redux and React hooks. A Redux async action, either an existing one or a new one, can use the pattern to wrap it as a React hook to make it easier to use. Note this pattern is only useful for the scenario: Use React functional components Use Redux async action to handle HTTP requests Keep data, error and loading state in the Redux store Whenever a component needs to show some data from remote API, we usually need to implement the below things: Define a Redux action to fetch data, dispatch fetchDataBegin, fetchDataSuccess, and fetchDataFailure actions when request status changes. Handle those actions in the reducer, set fetched data, pending, error state in the Redux store. Connect the component to the store and render the UI based on the current state, like show loading, error or requested data. With this process, we create `action.js`, `reducer.js` and `Component.js` files. These are normal Redux practices, if you are not familiar with it you can see the async action sample from Redux's official documentation. First, let's see how we use the async action in a component without the Auto Effect pattern. Say that we've created an async action to fetch data. For example, it's named \"fetchData\", then we can use it in a React component: The most painful part of this approach is every component that needs the data has to implement the same logic when using the action: Use useEffect to call the action Implement cache logic, if data exists, then use the existing one Know the exact path of the data in the store so that it can use “useSelector” to bind components to the store We can move the logic in a hook so that it could be re-useful for all components. Then we can think of the REST API as a remote data source. From component perspective it only needs to care about three parts of any REST API: Data: the data returned from API when successfully. Pending: whether it's fetching the data. Error: if the request failed, we know it’s from the error state. Next let's see how to create a React hook for the async action. To simplify the usage of async action, we can create a hook named \"useFetchData\" to wrap the Redux action as a hook: In this hook, we guarantee the existence of the data. That is if it doesn't exist then fetch it, otherwise it does nothing. With this hook, we can treat any REST API as a data source which contains status if data is available: Then we can use the remote data via a React hook in any component: Then it is very easy to be used in a component with much less code. We no longer care about: Whether data exists. How to call API to fetch data. Where is the state saved in Redux store and how to connect it to the component? That's just why we call it Auto Effect Pattern. It uses useEffect to call API automatically to guarantee the data exists when the hook is called. By this approach, any component that needs the remote data doesn't need to implement any logic to call action to fetch data. The common logic is abstracted into the hook so that the action is more re-useful. To be simple, the above example async action has no arguments. However, in real cases some async actions receive arguments to pass to the REST API. For example, if a component shows an article by the article id passed in as a property. Then we just need to slightly modify the hook: By this approach, whenever article id is changed, the hook guarantees the API is called to fetch the new article. In the example, we can see the API is always called when the component is mounted. However, that doesn't always make sense, in some components, the data is only fetched when a button is clicked. So, we should also allow the hook to be called without auto effect enabled. To make this happen we can do some tricks to the arguments of the hook: Then if the hook is called without any argument then it doesn't call the action in the useEffect logic. If we look back to the hook implementation, we have also exported bound action in the hook, so we can use fetchData directly without dispatching the return value. Auto effect is only triggered if any argument is passed to the hook. Even if the action has no argument, we also need to pass it an empty array to trigger the auto effect logic. If there're arguments for the action, we need to embed all arguments in an array to be passed to the hook. By this trick, we can wrap actions with the auto effect hook safely. It doesn't bring any limitation to the existing code logic. We can either use the Redux in the old way or use the hook with auto effect. As mentioned, the auto effect pattern only seems to be useful for HTTP requests of get method. However, in the hook there are other logic besides auto side effect: Bind Redux action with dispatch Use “useSelector” hook to bind the component to the store In our practice, almost all Redux actions are wrapped in hooks even if there's no auto effect logic necessary, including both sync and async actions. It also makes actions easier to reuse because some common logic is implemented in the hook. Take a simple counter action as example, we wrapped it in a hook: The approach is useful because almost everywhere we use a Redux action we also need to access some specific value of the Redux store. So, it makes sense to encapsulate the logic of action binding and store binding in hooks. It looks like that the pattern could be provided as a library via npm. But we think the hook logic should be very flexible to meet different requirements rather than encapsulate everything into a module, like: Custom cache logic. For example, use reselect to improve performance More return value from the hook. In our scenario, we also return “dismissFetchDataError” to clear error state from the hook. So, you can customize the hook by your requirement. In this article, we introduced a new pattern for managing REST API requests with Redux actions and React hooks. The pattern is widely used for building eBay internal cloud platform portal and we created code generators to reduce the boilerplate coding effect. It separates concerns of HTTP requests from components into hooks. By thinking of the API as a remote resource, it makes Redux actions easier to use.", "date": "2020-04-15"},
{"website": "Ebay-Engineering", "title": "Scalability Tuning on a Tess.IO Cluster", "author": ["Yingnan Zhang"], "link": "https://tech.ebayinc.com/engineering/scalability-tuning-on-tess-io-cluster/", "abstract": "Tess.IO is eBay’s new unified cloud infrastructure based on Kubernetes. With more and more applications being deployed on the Tess cluster, the requirements for scalability and capability of the cluster are growing. This article describes how to achieve 5000-node scalability for the tess.IO cluster. Tess.IO is eBay’s cloud platform, with a vision of ensuring a world-class build, ship, and run experience for eBay’s applications at scale while ensuring high efficiency, security, and agility for the developers. Tess.IO leverages Kubernetes under its hood. As more and more applications are deployed on the Tess.IO cluster, scalability and capability of the cluster become more and more important. Kubernetes upstream has claimed it officially supports 5000 nodes. But there is a long journey to support it in a real production environment. To make the Tess.IO cluster production ready, on each node we deployed additional components, such as a network agent to configure the network for pods, beats, and a node-problem-detector for monitoring, etc. All these add-on components from every node need to interact with the control plane of the cluster. Meanwhile, cloud-native customer pods add more load against the cluster by using CustomResourceDefinition. All these factors restrict the scale of production cluster. This article describes how to achieve the 5000 nodes goal for the Tess.IO cluster. To discuss scalability of a cluster, we must first talk about the deployment architecture of a Tess cluster. To achieve the reliability goal of 99.99%, we deploy five master nodes in a Tess.IO cluster to run Kubernetes core services (apiserver, controller manager, scheduler, etcd, and etcd sidecar, etc). Besides core services, there are also Tess add-ons in each node that expose metrics, set up networks, or collect logs. All of them are watching resources they care about from the cluster control plane, which brings additional loads against the Kubernetes control plane. All the IPs used by the pod network are global routable in the eBay data center. The network agent on each node is in charge of configuring the network on host. Figure 1. Tess.IO cluster architecture First, we should figure out whether the etcd/apiserver with the current configuration and deployment in our Tess cluster can sustain the pressure from the components of the control plane and system daemon, because the connection/api load from the system daemonset and kubelet is increasing with the node number. This is the load from our cluster itself, so this is the baseline. Then, we should figure out the capability to respond to the change from the customer within the built-in base apiload. For example, we could create/delete 1000 pods at the same time to see the error rate, QPS, and latency in percentile. Besides this, we should also figure out the capability to withstand the stress from customer pods, for example, the upper bound of watcher or how many list requests apiserver can bear. In order to simulate 5000 nodes, we spun up 5000 kubemark pods in cluster B and registered it to the apiserver of cluster A. The benefits are: It spares 5000 VMs in the eBay datacenter. It is easy to scale out 5000 nodes with Kubernetes cluster B (private IPs). It saves public IPs of the target cluster. (A public IP is global routable in eBay datacenter.) It isolates the impact between kubemark nodes and target cluster A. Figure 2. Deployment of a test environment With these kubemark pods, we added 5000 fake nodes to cluster A. We should also simulate the behavior of other components; just abstract the logic related to the interaction with apiserver and build these as containers into the kubemark pod. One kubemark pod includes one fake node, and the cluster add-on daemonset is located on this node. When running the tests described in above section, the following issues arose. Failed to recover from failures on cluster with: 5k nodes, 150k pods. ETCD exhausts large amount of memory (around 100GB) in each master node. ETCD has frequent leader elections. In etcd log, it showed: 2018-07-02 17:17:43.986312 W | etcdserver: apply entries took too long [52.361859051s for 1 entries]\n2018-07-02 17:17:43.986341 W | etcdserver: avoid queries with large range/delete range! Apiserver containers keep restarting every few minutes. ETCD exhausts large amount of memory (around 100GB) in each master node. ETCD has frequent leader elections. In etcd log, it showed: 2018-07-02 17:17:43.986312 W | etcdserver: apply entries took too long [52.361859051s for 1 entries]\n2018-07-02 17:17:43.986341 W | etcdserver: avoid queries with large range/delete range! Apiserver containers keep restarting every few minutes. Pod scheduling is slow in a large cluster. When there are only 5k nodes, and no pods in the cluster, it takes about 20 minutes to schedule 1000 pods. The  average cost is 1~2s per pod. The average cost of scheduling one pod, when there are already 30000 pods, reaches up to 1 minute. When there are only 5k nodes, and no pods in the cluster, it takes about 20 minutes to schedule 1000 pods. The  average cost is 1~2s per pod. The average cost of scheduling one pod, when there are already 30000 pods, reaches up to 1 minute. Large list requests will destroy the cluster. In the Tess.IO cluster, a pod is a very important resource that represents the entity of applications. The indicator of the scalability of the cluster is not only the count of nodes, but also the count of pods. Pod information is important to the components of the cluster control plane and customer applications; there will be pod queries from the cluster control plane and customer applications. With the increasing number of applications onboard, the pod count becomes very large. The LIST requests for pod resources become Large Range requests. A large amount of concurrent LIST pod requests can exhaust the buffer window of the kube-apiserver and impact core probes. Based on the test results, the rate of sending LIST all pods is 50%, and Node PATCH and Node LIST nodes are impacted. In the Tess.IO cluster, a pod is a very important resource that represents the entity of applications. The indicator of the scalability of the cluster is not only the count of nodes, but also the count of pods. Pod information is important to the components of the cluster control plane and customer applications; there will be pod queries from the cluster control plane and customer applications. With the increasing number of applications onboard, the pod count becomes very large. The LIST requests for pod resources become Large Range requests. A large amount of concurrent LIST pod requests can exhaust the buffer window of the kube-apiserver and impact core probes. Based on the test results, the rate of sending LIST all pods is 50%, and Node PATCH and Node LIST nodes are impacted. Based on the test results, the rate of sending LIST all pods is 50%, and Node PATCH and Node LIST nodes are impacted. Etcd keeps changing leaders. The ETCD sidecar in a Tess.IO cluster takes etcd snapshots every half an hour. The ETCD cluster changes the leader while snapshots are being taken. The ETCD sidecar in a Tess.IO cluster takes etcd snapshots every half an hour. The ETCD cluster changes the leader while snapshots are being taken. Basically, there’s a prerequisite for a large-scale cluster, increasing the max mutating in-flight request to 1000. For 5k nodes, just considering path nodes requests from kubelet (patch node in each 10s), the average is 500 patch request per second (5000/10=500). There are 5 apiservers, so each apiserver will get 100/s ideally. Besides, Node-Problem-Detector and other core components also patch the node at the same time. If the connections to patch nodes are not even, if some apiservers are down, or if some long READ transactions impact patch, it’s very easy to hit the default inflight-limit of 200, and return request rejection (response code 429). Then clients will retry and bring an additional load to apiserver. On each node, there are several daemonsets to configure the network, collect metrics/logs, or report hardware information. So the kubelet is not only the component watching all the pods on this node; there are also these other daemonsets. Make sure daemonsets won't override ListOption In the default List/Watch mechanism, the first request is a LIST call. It sets ResourceVersion=0 to its listoption and gets the resource list from the apiserver cache instead of etcd. We should make sure all these daemonsets won’t override the ListOption when registering the customized ListFunc and WatchFunc. If not, all of them will go through the apiserver cache and hit etcd directly. If apiserver gets restarted or large amount of applications like the daemonset get deployed at the same time, all the LIST pods requests will hit etcd. This is a disaster to kube-apiserver. Return an error before watch cache is ready There are five daemons in each node to list/watch all pods on the node. These component query pods with fieldSelector (the first LIST request is send with resourceVersion=0, nodename=<nodeName>). Before the pod watchcache in apiserver gets ready, apiserver forwards these LIST requests directly to etcd. Because of the large amount of pods, it takes several seconds for the pod watchcache to get ready. Also when apiserver restarts, all the pod watchers will resend LIST requests in parallel. All these requests will hit etcd directly, which puts huge pressure on the etcd server. The solution is to return err before watch cache is ready. Store attributes in watch cache After the cache is ready, all the LIST requests are sent, in parallel, to apiserver again. Though the status of etcd cluster is normal, the stress is moved to apiserver. From the pprof, it shows that apiserver is busy filtering pods on the specified node over the 150k pods. GetAttrs gets fields and labels from the object and sets it into a golang map. Mapassign is a time-consuming operation. In the version before release 1.10, this function will be invoked in a filter loop. If there are 150k pods in each LIST request with fieldSelector/labelSelector, it will be called 150k times. The solution is to move GetAttrs to a unified place. When processing the object from etcd to the apiserver cache, it will get attributes and store them in the cache together with its objects. After all these changes, apiserver and etcd can tolerate a spike of large queries when apiserver restarts or clients resync list/watch. The pod in the following state draws down the performance of scheduling, even though there’s only one pod in such state. Pod is in Terminating state The node, where pod locates, is gone The node, where pod locates, is gone It is inevitable to have these kind of pods in a large cluster in a real product environment, so it’s important to mitigate this situation. Why did performance issues show up in this case? There’s one condition for scheduling any pod: checking whether scheduling the pod onto this node would break any anti-affinity terms indicated by the existing pods. For this condition, checking in a normal case, it will find all the pods in cluster that match the affinity/anti-affinity terms of the pod being scheduled once and store them in a metadata. Then, it checks the topology of the matching pods for each node in the cluster. But when one or more pods in the whole cluster enter into the above state, scheduler won’t create predicate metadata, then it will execute the slow path that invokes FilteredList() function to schedule any pods. This slow path goes through all existing pods while traversing each node, so the latency relies on increasing the count of nodes and pods. How to mitigate this situation? Create predicate metadata anyway, even if there are no nodes specs for a pod, so it will bypass this slow path. Enhance the slow path — FilteredList() function based on Kubernetes 1.9, which used for above test. Change Mutex to RWMutex Remove the string concatenation in a function invoked many times Avoid expensive array growth Change Mutex to RWMutex Remove the string concatenation in a function invoked many times Avoid expensive array growth Results? In view of Pods and Nodes are the two largest resources in a Tess.IO cluster. PATCH node is the most frequent request. In ETCD cluster, there is a global lock. Then, these two resources—pods and nodes—will race each other for the global lock, which will cause low throughput. Separating pod resources into a dedicated etcd cluster would help the scale. During the scale test before resources separation, when there are large volumes of LIST pods in parallel (hit etcd), node patching is affected. If the node cannot be patched successfully for a long time, the node will become NotReady, which is dangerous. In the test cluster, the kube-apiserver/etcd can stand 1000 concurrent requests on pod creation/deletion. After all the core components have SharedInformer/ListWatch refactored, the control plane can get 5 apiservers restarted at the same time (all the clients start sending requests, which hit the apiserver cache instead of bypassing it). The key is to set up the rate-limit to large LIST queries, which bypasses the apiserver cache, and avoids too much load on etcd. We implemented rate-limit based on throughput, and record and refresh the hot, heavy LIST request patterns and costs (received bytes from etcd and send out bytes from apiserver to client) in real time. Then it will predict the cost of new incoming LIST requests according to the pattern cache and enforce rate-limit by a customized quota pool. Etcd persists raft proposals to its WAL log; as we can see in the etcd.log above, the wal sync duration was larger than 1s sometimes. The upshot is that etcd may miss a heartbeat or fail to send out a heartbeat, then etcd followers will start a new leader election. All the timestamps of \"wal sync duration long\" fully match the timestamp of \"etcd backup.\" We check the iostat of sdb, which is the etcd data disk. Most of the time, write bytes per second is 20MB/s~60MB/s. While taking a snapshot, write bytes per second is up to 500MB/s and the io utilization is 100%. We guess it is caused by flushing the etcd backup file to disk. Currently, the etcd backup is sharing the same disk with etcd data. The snapshot size is 4GB in test, and the etcd backup occupies all the disk IO and impacts sregular ETCD actions — writing the wal log. The kernel version used by the tess.IO cluster is 3.10.0-862.3.2.el7.x86_64, the default IO scheduler is deadline, instead of cfq, which allocates timeslices for each of the queues to access the disk. IO scheduler \"deadline\" will aggravate the situation disk IO occupied by a flush backup. In order to verify this, test by taking a snapshot and writing the backup file into a separate disk from etcd data. After these tests, there are no spikes of WAL sync duration any more while taking a snapshot, and leader election doesn’t happen, either. Kubernetes currently claims support of 5,000 nodes in one cluster. However, a large number of existing resources (such as pods), different deployment topology, and different patterns of API loads in a Kubernetes cluster may restrict the cluster size. After the tuning/fixes, a Tess.IO cluster makes these claims true in a real product.", "date": "2019-09-25"},
{"website": "Ebay-Engineering", "title": "Push Notifications-based 2-step Verification", "author": ["Anand Bahety"], "link": "https://tech.ebayinc.com/engineering/push-notification-based-2-step-verification/", "abstract": "To create a trustworthy online marketplace, we need to ensure only authenticated and authorized users gain access to their accounts. Learn how eBay built a secure, easy-to-use, and robust authentication framework using push notifications and leveraged it for 2-step verification. The identity and authentication landscape is rapidly evolving and using passwords as a primary source of authentication is no longer a reliable option. Two-step verification or two factor authentication (2FA) help add an extra layer of security where users need to provide additional authentication factors before they can gain access to their account. However, the security and effectiveness of these mechanisms depends on the methods used and how they are implemented. Typical authentication methods used to offer 2FA are either less secure or susceptible to malicious attacks. SMS-based 2FA is widely adopted online due to its simplicity. But it is susceptible to man-in-the-middle (MITM) attacks and has been declared insecure by the National Institute of Standards and Technology. Authenticator-based apps or hardware tokens that generate one-time passcode require a common secret (seed) to be shared between the server and client, making it susceptible to server-side attacks. Relying on third-party authenticator apps requires users to download additional apps on their devices and can be ineffective if there is not enough ongoing support and maintenance to keep it up to date. Hardware tokens are also not scalable from a usability and cost perspective. With the increasing use of smartphones as personal devices, the widespread adoption of eBay mobile application on popular mobile platforms, and push notifications providing a user-friendly and seamless access to eBay application, push notifications become an attractive option to leverage for authentication. While designing a push notifications-based authentication framework, we had the following security considerations to begin with: No sensitive data to be passed over the wire No shared secrets between client application and server Leverage industry authentication standards wherever feasible Aim for reliable, easy-to-use and robust authentication framework. 2FA should be a use case to leverage this framework, but don't just build a 2FA-specific product. With these specific design goals in mind, we decided to use push notifications as a delivery mechanism and standard public key cryptography techniques on a user’s mobile device for stronger authentication. We use the FIDO UAF 1.0 protocol specification for registration and authentication to securely authenticate requests and responses between client and server. eBay mobile applications already have built client libraries implementing that protocol, hence it was an obvious choice for us to use the same technology for 2FA as well. An eBay application with the FIDO client library on mobile devices constitutes the client side, while a FIDO server hosted on eBay servers forms the server-side components. A user needs to be authenticated in the eBay application for this process to start. During this process, the mobile device initiates the registration request against the server to register itself to receive authentication-related push notifications. Once the user approves the enrollment process, the client generates an asymmetric cryptographic key pair, stores the private-key securely on the device and sends the public-key back to the server where it is stored against the authenticated user and device. This completes the process and marks the device as trusted and authorized to receive and approve push notification requests for authentication. We leverage the FIDO UAF registration request protocol for request-response messaging. In this process, first the user presents primary set of credentials (like username/email and password) to eBay sign in screen. On successful validation of this input, the server creates a unique, securely random, short-lived, and one-time use transaction identifier and links all the related data on the server side for the user’s attempt. It then sends a push notification to the user’s registered device with this identifier as part of the notification request payload only. On receipt of this push notification, the eBay application initiates the authentication approval request by verifying the incoming payload against the server to make sure it’s not a stale or invalid request. The eBay application then asks the user to approve the login attempt. On approval, the application uses the private key securely stored on the device to sign the request payload obtained in the first step and sends the cryptographic signature and transaction identifier back to the server for verification. Even if user wants to reject the request, the same process is followed to ensure that only an authorized device is able to reject a login attempt. The server verifies the signature with the public key stored in the database and approves the login attempt. The user then gains secure access to the account. We use FIDO UAF authentication protocol for request-response messaging. A user can use their mobile device to deregister, which causes the private key to be deleted on the client side and the public key on the server side. Another option is to sign in to your account on the web, which prompts 2FA, because the user still has it active and then deactivate it, which causes the server to delete the public key only, which indirectly renders the private key on user’s device not usable. Let’s take a closer look at the subtle nuances of this architecture to understand its benefits and also spot check to see whether we achieved the goals set forth upfront. During the registration process, the client never sends the private key (secret) to the server. Only the public key is shared with the server. Hence, no shared secrets. The server stores only the public key. Hence, the system is not susceptible to server-side attacks. During the authentication process, the client sends only signed data along with transaction identifier to the server. No sensitive data is passed over the wire. The push notification payload doesn’t need to be encrypted, as it contains only a transaction identifier, avoiding a complex set up of end-to-end TLS between client and server. Given that no secret or sensitive data is sent over the wire, this protocol is not susceptible to MITM attacks. FIDO is the current industry standard for simple and stronger authentication, and we leverage that for this framework. This framework forms the basis for several features like password less authentication, trusted device management, step-up authentication and many more. It’s not just a 2FA product. If you want to enable push notification-based 2FA against your eBay account: Download the latest version of the eBay app. Sign in to your account. Go to My eBay > Settings > Signing In > 2 step verification. Turn \"eBay Verify\" on. Congratulations!! You have just made your eBay account more secure. Here is a snapshot of the login experience once you have enabled it. Want to jump into the code and try out the FIDO UAF server or client side components? Check out eBay’s open libraries here . This article presents a secure, easy-to-use, and robust authentication framework using push notifications, which has helped enhance eBay’s current 2FA offerings, form a rock-solid foundation for authentication, and create a safe and trusted online marketplace experience.", "date": "2019-11-18"},
{"website": "Ebay-Engineering", "title": "Discovering Continuous Automation With Request Mirroring", "author": ["Lakshimi Duraivenkatesh", "Vineet Bindal"], "link": "https://tech.ebayinc.com/engineering/discovering-continuous-automation-with-request-mirroring/", "abstract": "Because eBay's item page updates frequently, and because it depends on hundreds of libraries and services, discovering the unknowns and automating testing for all use-case combinations from production calls for a different approach to testing. Each item page release needs an extreme cycle of QA and testing to prove feature functionality and reduce potential bugs in production. Each configuration change has to go through multiple layers of approval to make sure no unfortunate event happens in production that can impact users. All of this is costly, time consuming, slow paced, and not all the scenarios from a production environment are covered. QA cycles happen before every release with a defined set of use cases without randomization or going outside a defined set of rules. While in production, there can be use cases that are outside this defined environment. Today, there is no way to capture these unknown production use cases before releasing a new build. Testing happens in staging environments that might not have the correct data to test all the corner cases, and the code is tested against a static dataset in static conditions, while the same code runs against a different dataset in production in dynamic conditions. As the software gets bigger, the dependency graph increases, use cases expand, and the combination of all these scenarios grows exponentially, which adds uncertainty. Neo in action on Production Machine, (N)th build When a request is received by a production machine with the (N)th build of the item page, Neo intercepts incoming requests and assigns a “RequestMirrorId” to identify each request uniquely. Neo then mirrors each and every aspect of the incoming request in production and sends this mirrored data to a test machine with (N+1)th build. Neo then stores a copy of the production request in a file (let's call this file “ProductionRequest”) in a central storage location. When the production machine is ready to return the response, Neo also creates a copy of this response and stores it in a file (let's call it “ProductionResponse”) in central storage. When a request is received by a production machine with the (N)th build of the item page, Neo intercepts incoming requests and assigns a “RequestMirrorId” to identify each request uniquely. Neo then mirrors each and every aspect of the incoming request in production and sends this mirrored data to a test machine with (N+1)th build. Neo then stores a copy of the production request in a file (let's call this file “ProductionRequest”) in a central storage location. When the production machine is ready to return the response, Neo also creates a copy of this response and stores it in a file (let's call it “ProductionResponse”) in central storage. Neo in action on Test Machine, (N+1)th build Neo intercepts the mirrored request on a test machine with (N+1)th build and stores this request in a file (let's call it “MirroredRequest”) in central storage. When test machine is ready to return the response, Neo creates a copy of response and stores it in a file (let's call it  “MirroredResponse”) in central storage. Once the data is collected, Neo can compare these files based on “RequestMirrorId” by running different comparators and can than generate a report of the delta and mark any use cases that are not covered in comparators or automation. This comparison can be done all the way to last dependent service in the call hierarchy of dependencies. The following diagram illustrates how each payload is stored and compared using “requestMirrorId.” Neo then processes this delta against the acceptance criteria, and anything that is not acceptable or unknown is marked as a potential issue. ItemPage code base : As part of the tool, HttpRequest, HttpResponse interceptors were introduced in ItemPage backend code to intercept request responses to be able to mirror and make a copy of the data. We can choose and filter what requests to Mirror based on Header, queryParams or request path criteria. These interceptors are customizable with the properties shown in tables below. Picking machines from Production: The next step is to pick a couple of healthy production machines (Nth build) at random from a production pool and configure them with mirroring, and then point them to the test machines (N+1th build) to send mirrored data. This step is automated to encourage randomization and to pick up healthy machines in case previously chosen machines are not part of a pool anymore or are not available. The following configurations are set on production machine to start mirroring. MIRROR_ENABLED true MIRROR_TARGET_HOSTNAME \",\" separated target test machines MIRROR_TARGET_PORT target port MIRROR_PERCENTAGE % of traffic to be mirrored Connection to Central Storage: The next step in the pipeline is to change the configuration on production and staging machines to connect and store data from the Nth and N+1th builds in central storage. MIRROR_RESPONSE_DISPATCH_PATH is used to connect to central storage. The production machine will connect to the dispatch path with /BASE and the test machine will connect to the dispatch path with /NEW. Central Storage: Central Storage is a file-based system where production and mirrored requests, responses and header data are stored. Data from each service pool is stored in its own file and against the same “RequestMirrorId” that was assigned at the time of mirroring from a source production machine. Ex: “/production/ItemPage/”RequestMirrorId” vs “/test/ItemPage/”RequestMirrorId” Central Storage Wrapper: Each Service pool makes a call to “/Base\" and \"/New” services to set up data storage in central storage using the path provided in “MIRROR_RESPONSE_DISPATCH_PATH.” These APIs accept Nth and N+1th responses to map them against “/production” and “/test” folders, respectively. Comparators: These are a series of HTML, JSON, and header comparators that are integrated in a Jenkins pipeline that runs every hour and reads the data from central storage to compare and generate a temporary delta file. Report Generator: This is a separate Jenkins job that is kicked off at the end of the Comparator's pipeline to pick up the temporary delta file and generate an HTML readable report out of it. This report contains all the diffs in headers, requests, responses, etc. This diff is important, because headers can get encoded, additional request parameters can get added or missed, and responses can change while moving from one pool to another or by downstream services. Item page total production traffic is more than 350 million requests per day. It's not possible to mirror all the requests, because mirrored request adds load on all downstream services and storage as well. Typically we pick 2-3 machines randomly from a production pool and mirror their traffic. Mirrored requests can fire tracking events similar to production requests. These tracking events are suppressed by passing a “nocache=true” value in tracking header or by pointing test machines to test tracking pools. Important user-specific and sensitive information is masked out while storing HTML and JSON responses and creating reports.", "date": "2019-11-25"},
{"website": "Ebay-Engineering", "title": "Autofill on Browsers: A Deep Dive", "author": ["Shanmuga Priya Pandiyan"], "link": "https://tech.ebayinc.com/engineering/autofill-deep-dive/", "abstract": "The less you ask of users, the more inclined they are to complete a form, and faster form filling increases conversion. Browsers provide the autofill feature to help achieve that. In this article, we discuss how to effectively use autofill features on web forms. tel-country-code tel-national tel-area-code tel-local tel-local-prefix tel-local-suffix tel-extension", "date": "2019-12-03"},
{"website": "Ebay-Engineering", "title": "Buy the Items You Love Again and Again on eBay", "author": ["Asheem Sinha"], "link": "https://tech.ebayinc.com/product/buy-the-items-you-love-again-and-again-on-ebay/", "abstract": "eBay is taking the friction out of repurchasing a favorite find on eBay with the new Buy Again button. People come to eBay to get their favorite items from their favorite sellers. The strength of eBay’s marketplace relies on connecting sellers and their listings with buyers. Our customers have shown us they repurchase the same listings across all categories on the site. Whether it’s buying everyday essentials or business and industrial items, they trust the people they have bought from and the items they have received before. However, until now, it wasn’t easy for buyers to land on the same item page to make a repurchase. This is in large part because we have 1.4 billion listings sold by millions of sellers on our site. So today, we are announcing an all new feature called Buy Again. This new button takes the friction out of finding and repurchasing a favorite find on eBay. Now, when buyers go to their My eBay > Purchases page on iOS or to the eBay hamburger menu on Android, they can scroll through a list of their purchases and will see a Buy Again button. This button allows them to quickly purchase that item from the same seller that they have purchased from before. If the same item is not available from the same seller, buyers can use the Buy Similar button. eBay’s search algorithms will then surface similar inventory, using the purchase title. Buyers will see the same product sold by a different seller; the same product in a different variant (think of the same sneaker in different colors); or a similar product (like a different running sneaker from the same brand). It’s that simple. For buyers, our goal is to provide a seamless experience to help discover relevant items to repurchase. For sellers, this is another feature that brings return buyers back to your eBay listings. The Buy Again button is now available in all territories in Purchase history on eBay’s native apps for iOS and Android. It will be available on desktop in 2020. We will also test a dedicated repurchase experience via a Buy Again page, which will make it easier for buyers to find the items that they’d like to purchase again in one place.", "date": "2019-12-05"},
{"website": "Ebay-Engineering", "title": "Under the Hood of eBay Motors: New App Powered By Advanced Technology Like Flutter, ML and BFF", "author": ["Larry McKenzie", "Corey Sprague", "Andrew Chalkley", "Jake Hall"], "link": "https://tech.ebayinc.com/product/under-the-hood-of-ebay-motors-new-app-powered-by-advanced-technology-like-flutter-ml-and-bff/", "abstract": "eBay introduces the eBay Motors native app, now available for Android and iOS in the U.S. The app was built using cutting-edge tech including machine learning, Backend for Frontend architecture and Flutter, an open source, cross-platform SDK by Google. eBay has long been a place where automobile enthusiasts turn to buy or sell ’67 Ford Mustangs, ’69 Dodge Challengers and other classic cars that were elegant solutions equipped with the most advanced technology of their day. A similar philosophy was taken toward eBay Motors, a new native app for Android and iOS for the U.S. that was announced today. The app’s elegant and easy-to-use interface is built upon cutting-edge technology that is reinventing ecommerce, including machine learning, Backend for Frontend (BFF) architecture, and Flutter, the trending open-source x-platform development SDK created by Google. In the Big Data era, eBay Motors is using these different technologies to leverage massive amounts of information and providing a mobile foundation to simplify and improve the online retail experience. On the eBay site, the way that automobile photos are organized and tagged on a listing can have a significant impact on the buying and selling experience. Someone who wants to sell a car on eBay typically posts 15 to 20 photos of the vehicle. User testing of the eBay Motors app showed that photos which are well-organized into three buckets (car exterior, car interior, and engine) make buyers feel the listing is more trustworthy. Creating a narrative presentation, with well-organized photos and descriptions, can be a complex and expensive endeavor. In developing eBay Motors, we used edge-powered image categorization AI built with firebase autoML, enabling users to create compelling listings with little effort. It works like this: The car seller takes photos of their car on a phone, uploads the photos to their listing, and eBay automatically organizes the photos into the three groupings. This has a significant impact on the perception of the listing. During testing, users gave comments like “This listing must have been put together by a mechanic” due to the intelligent image categorization. The categorization model was built with Firebase AutoML Vision Edge, an easy to access tool for building models which can be sized for use on phones. The original algorithm was constructed by a single engineer in a couple of days, who trained the model using a few hundred photos from existing eBay car listings. The ease-of-development speaks to the maturity of AI and ML, which have moved beyond the province of data scientists to off-the-shelf tools which are available to software engineers. Another testament to the improvement of the technology was the accuracy of the models, which achieves > 95% accuracy after being trained on only a few thousand labeled images. Flutter, Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase, has generated a significant amount of interest. We instantly saw its value in developing eBay Motors. Instead of the traditional approach of using WebView or the OEM widgets that ship with the device, the technology uses its own high-performance rendering engine to draw widgets. With only a thin layer of C/C++ code, Flutter implements the widgets, framework, animation, compositing and other system elements in an object-oriented language called Dart. Developers have a significant amount of control over the system. The technology shares a single code base across Android and iOS apps, with a focus on speed and maintaining a native feel. Since we were developing eBay Motors with a relatively small team, the promise of Flutter to speed up the development of the mobile app and reduces the cost and complexity of app production across platforms was enticing. Implementing a new technology like Flutter in a large-scale, production environment always involves risk, of course. The technology moved from beta to Release Preview 1 only a year ago. We spent the first month performing due diligence, and found that Flutter was perfectly suited for a native development where the user interface was so fundamental to what we were doing. A key benefit was faster development. In normal Android and iOS development, for example, it can take several minutes between the time you write the code and see it reflected in the emulator. In Flutter, this step takes only a few seconds. Allowing developers to instantly access their handiwork is critical for implementing an appealing UI that is being iterated rapidly. An additional benefit of flutter is the maturity of its testing solutions. The test automation is so robust that we were able to convert our two QA engineers into software engineers. This allowed us to maximize the talent of our small development team, giving developers a more holistic responsibility for the app as they tested their work as they developed features. As we developed the new app that would add more features as user requirements became clearer, it was apparent that we needed a Backend for Frontend (BFF) architecture, which is ideally suited to create backends for client-facing mobile apps. In BFF, the mobile frontend architecture does not directly access backend services. Instead, the BFF acts as an aggregation and proxy layer between the client and the services that it calls. By definition, the only consumer of the BFF is the native clients; this allows us to place logic in this service which can minimize the number of calls from the client and ensure that payloads sent over the wire are optimized. These qualities of the BFF give users of the eBay Motors app a faster and smoother experience. The BFF architecture provides numerous benefits to a development team that is creating features that require a specific API. Critically, BFF allows the development team to define the API contract, giving them the ability to define the source of the data, how to aggregate it, and other considerations that normally require the assistance of a backend team. As a result, they can define and adapt the API based on UI requirements. By giving the development team this autonomy, developers could quickly iterate the app to provide the best experience for the users. This evolutionary design moves the system to a better, less-coupled state, as compared to a large single-purpose API. BFF, Flutter, and machine learning allowed us to quickly build and iterate on a cutting-edge app that dramatically improves and simplifies the experience of buying and selling cars on eBay. Early feedback from customers who have used the app have found it similar to a classic car – it’s elegant and powerful to drive, but what’s really impressive is what’s under the hood.", "date": "2019-12-12"},
{"website": "Ebay-Engineering", "title": "eBay Makes Mobile Web Login Easier", "author": ["Shikha Khanna", "Anand Bahety", "Md Kamal Hossen", "Neb Pesic"], "link": "https://tech.ebayinc.com/product/ebay-makes-mobile-web-login-easier/", "abstract": "eBay enables secure password-less login experiences on the web-based version of eBay with WebAuthn. Passwords as an authentication factor are failing in this day and age. The use of passwords present both a security and usability challenge for consumers. Password-less authentication using biometrics has been available for some time on native smartphone apps. eBay apps support biometric authentication on both Android and IOS platforms. Up until recently, biometric authentication has been missing on web-based applications. eBay has now enabled web-based biometric authentication using WebAuthn. WebAuthn is a new standard that allows biometric-based authentication on web thereby removing the need for passwords. Existing eBay users accessing eBay using the Chrome browser on Android devices will now be able to use biometrics to login to eBay. On supported devices, users will get an option to turn on biometrics (fingerprint or facial recognition, depending on what the device supports). Once enabled successfully, users can use biometrics to log into eBay the next time, instead of having to use their password on that device. The return experience for a user that has opted into this feature is shown below. With adoption of the latest industry standard for authentication ( WebAuthn ), eBay is also one of the first major ecommerce companies to enable biometric authentication as a first factor authentication on web browsers. We hope this feature makes using eBay easier and more secure for our buyers and sellers. Note: Supported devices include Android phones with biometrics enabled using the Chrome browser version 75 and higher. We plan to expand to more platforms in the future.", "date": "2019-11-21"},
{"website": "Ebay-Engineering", "title": "eBay’s Transformation to a Modern AI Platform", "author": ["Sanjeev Katariya", "Ashok Ramani"], "link": "https://tech.ebayinc.com/engineering/ebays-transformation-to-a-modern-ai-platform/", "abstract": "How the AI transformation at eBay was powered by a modern AI platform with a unified and open approach. Have you ever wanted to find an item and struggled to describe it in words? Now, with computer vision powered by eBay’s modern AI platform, the technology helps you find items based on the click of your camera or an image. Users can go onto the eBay app and take a photo of what they are looking for and within milliseconds, the platform surfaces items that match the image. The user has not only activated computer vision technology, but they have also tapped into some advanced AI capabilities, including deep learning, distributed training and inferencing. The computer vision algorithm sifts through more than half a billion images and eBay’s 1.4 billion listings to find and show you the most relevant listings that are visually similar. A primary reason why this can be done so effectively at scale and with precision is because of Krylov, eBay’s modern, state-of-the-art AI platform designed to boost eBay’s productivity with AI and accelerate time to market of AI models at scale. AI platforms are having a huge impact in leading companies across all industries. Public cloud providers like Google use AI platforms to provide many of their products and services.The AI platform at Facebook, called FBLearner Flow, personalizes news feeds and filters out offensive content. At Uber, the machine-learning platform Michelangelo powers the ability to give customers an accurate prediction of when a restaurant meal they’ve ordered through UberEats will be delivered. Similarly, eBay built Krylov from the ground up as a scalable and multi-tenant, cloud-based AI platform that powers a diverse set of AI use cases at scale. In 2019 alone, data scientists at eBay used Krylov to run thousands of model training experiments per month spanning AI use cases, such as computer vision, natural language processing (NLP), merchandising recommendations, buyer personalization, seller price guidance, risk, trust, shipping estimates, and more. Figure 1. eBay AI strategy. Before Krylov, data scientists building models needed weeks, sometimes even months, to become productive. They would need to procure and manage infrastructure, move data to the machines, and install frameworks –  and sometimes still encounter issues, leading to productivity overheads. Training models on large data sets cannot be scaled across nodes. Now that the infrastructure is available on demand on the AI cloud, data scientists have access to the latest software, hardware, models and runtimes, such as Notebooks, Tensorflow, PyTorch and H20. Through these runtimes we can train models like BERT (for language understanding) or ResNet (for Computer Vision) at scale on our inventory of 1.4 billion listings. Data scientists can train models on large data sets using distributed training. They can run experiments and hyper-parameter tuning in parallel; record and visualize the experiments; and deploy the best model experiments. For example, our AI researchers have used Krylov to train neural machine translation models, deep and wide models for recommendations as well as computer vision models to power image search. This is key to improving model precision as well as time to market for eBay’s machine translation technology, which is a significant contributor to enabling cross-border trade, which makes up 59% of eBay’s international revenue. Krylov allows our AI teams to maximize the power of the vast repositories of data, both batch and real time, that eBay has. If you think of data as the fuel for artificial intelligence and machine learning, Krylov is the sophisticated vehicle being powered by that fuel. And it’s a fast-moving vehicle. Today, data scientists can spin up an AI workspace with popular software frameworks (Tensorflow, Scikitlearn, Math libraries, Jupyter notebooks, etc.) on compute configurations of their choice (GPU, high-memory high-cores) in less than a minute. Previously, this process could take days. Data scientists can also run automated AI workflows (pipelines) using Python, Java or Scala interfaces to experiment with various approaches (hyper-parameters) and record their experiments/compare the output of the experiments. The ability to do hyper-parameter tuning and run distributed training on large datasets and models have resulted in marked improvements in the accuracy of models. eBay designed and built its own specialized servers to better manage the vast amounts of data that moves through its system. The new servers allow eBay data scientists and engineers to accelerate the production of new features, reducing development time from weeks to hours. The business impact is a dramatic improvement in time to deployment. eBay can now automate model training and deploy the models over individualized or a common inference as a platform in days, compared to the months that were once required. This has led to improvements to important functionality like Image Search , which allows shoppers to browse for an item they want by uploading a picture of a similar item. While Krylov is highly innovative, so was the way in which it was developed. A unified platform for eBay needs to scale across a diverse set of use cases, such as computer vision, natural language processing (NLP) and recommendations. Consequently, developers and data scientists had a diverse set of needs. This was a multi-year platform transformation. Implementing Krylov was an exercise in breaking down varied silos and coming together across functions and geographies to develop and execute on a common unified vision. To guide the project, we put together the Unified AI Initiative Core Team (ICT). The ICT included representatives from the AI platform team, which is the provider of the service, the owner and builder of the platform. Also represented were AI platform dependencies: hardware, compute, network, storage and data services. The third component of ICT was the AI domain teams, the internal customers of the platform, such as AI research and engineering in ads, computer vision, NLP, risk, trust and marketing. These AI teams have a vested interest in defining, shaping and adopting the platform for their everyday, AI lifecycle management. Together, these experts created a unified AI vision for eBay – the strategy, roadmap and key tenets of the platform. This was a hands-on process. At various points, researchers and engineers from the domain teams either contributed or embedded themselves in an internal open source manner to build parts of the platform. Since these engineers and researchers were closer to the domain problems (AI lifecycle) or had built frameworks/platforms for their specific needs in the past, they were able to provide critical input. In some cases, there were frameworks or platforms that were absorbed into “Unified AI Platform,” because they solved a specific problem really well and could help accelerate the evolution of the platform for the broader eBay AI community. Additionally, we also instituted an eBay Machine Learning (ML) Engineering Fellowship program, where any engineer at eBay could embed themselves into the AI platform team similar to an internship program to help build the platform features from the product backlog. This fellowship program aims to familiarize eBay engineers with ML concepts and technologies. Participants are mentored on ML engineering concepts by senior domain experts. The Internal Open Source model as well as the ML Engineering fellowship program helped in not only code contributions but also as a feedback mechanism to the development of the platform as we scaled up our scientists’ and engineers’ skill sets. In the discovery phase of building Krylov, global eBay teams across different geographies worked together to better understand the pain points and challenges of building eBay’s AI. This included understanding needs and wants; showing empathy to and appreciating the day-in-the-life of the AI researchers and developers; and researching the existing approaches in the industry. The phased strategy to build, adopt and transform AI over multiple years required: AI training cluster with easy, secure and performant access to data with powerful compute (GPUs, high-memory and high-cores) Training platform: automatable training workflows and interactive workspaces, SDK and clients (Python, Java, Scala, REST) AI model lifecycle management: model experiment management, model management service, deployment services, AI Hub (web-based UI) Model serving platform and feedback loop: deploy AI models as a service tied to the experimentation framework and monitoring systems (operational as well as model performance) Data lifecycle abstraction for modeling, deployment and inferencing lifecycle that consisted of data discovery, preparation, feature store and serving, and feedback loop In addition, the platform had to be built with a few key tenets to address the diverse AI use cases and operational patterns of data scientists and engineering teams at eBay. The key tenets we established were: Support for heterogeneous software frameworks — Tensorflow, PyTorch, Cafe, Notebooks, any framework of choice Heterogeneous hardware architecture — support GPUs, high-memory CPU based Built for scale Using open source technologies, in an open source manner Support for heterogeneous software frameworks — Tensorflow, PyTorch, Cafe, Notebooks, any framework of choice Heterogeneous hardware architecture — support GPUs, high-memory CPU based Built for scale Using open source technologies, in an open source manner Figure 2. End-to-end AI model lifecycle management using the AI Platform. Figure 3: AI Hub (UI for end-2-end lifecycle management of models) showing a model training experiment in AI project with collaborators. Figure 4: AI Hub showing comparisons between metrics for two model training experiments in an AI project. Figure 5: AI Hub showing visualization of an ML model training workflow (DAG), where the user can see the status as well more details of each task in the workflow. Users can also attach logs and assets, specify configurations, and view deployment status. As the platform was built, we would provide previews, alpha, beta access for the AI ICT teams to get early access and to test the platform. This iterative and collaborative engagement with a unified vision and execution helped build a unified platform by the eBay AI community for the eBay AI community. While the early results are promising, we are by no means finished. AI is an evolutionary journey with no final destination. With Krylov by our side, we are in a great position to evolve our use of AI as customer needs change and opportunities arise. Looking ahead, we will continue down the path of innovation through eBay’s AI-managed marketplace, knowing that the scope of what’s possible with AI expands every day. We’ll continue to share what we’re discovering and how we’re incorporating AI on our platform to create the most fulfilling commerce experience for our customers.", "date": "2019-12-17"},
{"website": "Ebay-Engineering", "title": " eBay’s Buy APIs Hit $1B in GMB", "author": ["Gail Frederick"], "link": "https://tech.ebayinc.com/engineering/ebays-buy-apis-hit-1b-in-gmb/", "abstract": "eBay’s Buy APIs enable third-party partners to expose eBay inventory on their site. The Browse API allows partners to create a rich selection of items for their buyers to browse with a keyword, category, or other designations, such as the charity’s registration ID. The Feed API allows partners to curate, mirror, and surface eBay inventory at scale. Partners can now store and synchronize millions of eBay items on their site, staying up-to-date with those listings as they change on eBay. The Order API offers secure checkout for guests and members as well as live order status, allowing shoppers to complete their checkout process without visiting the eBay website or app.", "date": "2019-12-10"},
{"website": "Ebay-Engineering", "title": "How eBay Governs its Big Data Fabric", "author": ["Alex Liang"], "link": "https://tech.ebayinc.com/engineering/how-ebay-governs-its-big-data-fabric/", "abstract": "At eBay, nearly everything we do is based on data. We deal with structured, unstructured, and semi-structured data, where Hadoop, as a big data platform, has provided key technology features. Keeping pace with the speed of innovation while continuing to help data consumers easily find and consume the data they need guides our architecture and investment in building out eBay’s Big Data Fabric. Advancements in big data technologies have enabled the processing and storage of massive amounts of data, with data lakes becoming an increasingly popular way to expose this data to users quickly. These provide increased agility and flexibility over traditional data warehouses. The term data lake is often associated with Hadoop-oriented object storage. In such a scenario, an organization's data is centrally collected and loaded onto Hadoop, and then business analytics and data mining tools are applied to the data where it resides. But the question remains: Is a “data lake” approach truly enabling faster data driven decisions? While it may be a more flexible way to consume data, does this hold true for insights and answers? Similar to big data, the term data lake is in many cases criticized as being a marketing label for Hadoop. It has been accepted as a way to describe any large data pool where the schema and data requirements are not defined until the data is queried. In reality, data lakes generate more questions for data users than answers. Data users cannot use data correctly without knowing what exists, how it can be used, what can be trusted, what it means, and how it was generated. Working in a data lake can be a daunting experience, absent any clear way to ask for help or even to find those who may have the answers. Even more, we all know that dirty data is not really dirty; it is just incorrect. Data cleansing consists of correcting mistakes in the data. But with an unmanaged data lake, the system will continue to get the garbage data in without cleaning, and keep them forever with no retention process and even worse, data quality issues are ignored at this stage. eBay has a culture of independence and innovation which needs an open approach that puts control into the hands of the data users and supports exploration and innovation. Towards this goal, we turned the concept of data governance on its head. Rather than focusing on control and limiting access, our data governance initiative is focused on gathering exhaustive information about each data element and making this available to users within their normal workflows in a programmatic way. In this way, data users are not working in a silo; they can make informed decisions about whether a given data element or object is the right fit to answer their business question. One key way to understand your customers is to understand their behavior.  At eBay, we track the full usage of our data assets, including how many times they are accessed, by whom, and how the asset was accessed (for example via report vs. manual querying), all the way down to the actual queries executed. This information is then used to for simple things like data retention (retiring unused assets), setting operational goals on levels of support, and even optimizing our big data fabric. Data governance is managed as a process and product, not a project. Providing value without limiting how we keep the data lake clean. We are able to adapt to changes in data and data needs and keep data up to date, which may mean deprecating or removing as well as adding. Policies and processes We implement policies and processes to manage the data lifecycle includes Release -> Monitoring -> Re-Certification -> Rationalization/Optimization. The Process assets are critical to ensure the right behavior conducted during the data development phase in the eBay holistic Big Data environment. In the meanwhile, internal audit and external (SoX) audit practices are the benchmark for process success. We also ensure that golden data assets are provided in a right-on-time and curated manner to support eBay business users and analysts for data analysis and business decision making. As the big data fabric governance team, we are always proactively working for Data Rationalization policies and other exercises to maintain the DW environment well-being. Data rationalization, as a key method, is always taken into consideration when bringing new data assets into the data lake. As a result, we gain agility and change management at scale. Tools We create tools to help us discover and understand data, manage inventory, analyze usage, and optimize the usage and capacity. As a result, we gain agility, change management at scale, and the ability to answer the questions such as What, How, Where, and When. Quality We create a quality platform to automatically validate and verify the data. Cross-platform data reconciliation Cross-platform data reconciliation Monitor data consistency across different data platforms Monitor data consistency across different data platforms Send out notification automatically when data is not in sync Send out notification automatically when data is not in sync Open web service allows data downstream to plug in quality check in downstream processing Open web service allows data downstream to plug in quality check in downstream processing Rule-based data quality check Rule-based data quality check Monitor data accuracy through data flow Monitor data accuracy through data flow Support both default and free format type of quality check rules Support both default and free format type of quality check rules Send out notification automatically when rule is triggered Send out notification automatically when rule is triggered Open web service allow data downstream to plug in quality check in downstream processing Open web service allow data downstream to plug in quality check in downstream processing As a result, users are able to answer the question for themselves “Can I trust the data?” Usage Analysis Usage analysis provides a 360-degree view on how the data assets are used by the customers. Self-Service tool to analyze data usage by drag and drop, sourcing from Teradata QueryLog Self-Service tool to analyze data usage by drag and drop, sourcing from Teradata QueryLog Understand who used what data and how frequent Understand who used what data and how frequent Understand how data products are used among different group of users Understand how data products are used among different group of users Understand data usage distribution among different platforms, locations, etc Understand data usage distribution among different platforms, locations, etc Understand who are active users of data so we can contact in case of data changes or data incidents Understand who are active users of data so we can contact in case of data changes or data incidents As a result, eBay is enabled to use data to power data products. S hared scorecards We created shared scorecards across the organization to create alignment and ensure the adoption of our processes internally and of our data products externally (to leadership). As a result, with Adoption comes subject and user expertise. That’s where the knowledge and collaboration come together to enhance user productivity. Metadata At the end of the day, It is all about metadata. Thanks to our talented engineering team, we have turned the concepts into an innovative product (DOE — Data Operational Excellence) to manage and integrate all metadata together, which can then be used to answer any questions for any specific production data assets. We were able to successfully deploy processes and policies with full coverage of EDW in both Teradata and Hadoop, with close to 100% metadata and lineage coverage. All critical data assets have a 24/7 monitoring process. This foundational work supports keeping the data lake \"beautiful,\" and the data users are able to get the full value of using the data. Our self-service data discovery platform reached a tipping point of adoption and continues to grow, with plans of expanding the data catalog to include real-time data, services, and data APIs, as well as enhanced discovery enabled by AI powered auto data tagging.", "date": "2019-04-10"},
{"website": "Ebay-Engineering", "title": "Failbot—Improving Visibility on End-to-end Tests", "author": ["Tony Da Silva Bernardino"], "link": "https://tech.ebayinc.com/engineering/failbot/", "abstract": "In eBay’s Global Shipping team, we use end-to-end tests to detect problems on eBay’s platform introduced by new developments. When those tests are failing, it is hard to see what is going on. From an intra-team effort to improve our visibility, Failbot was born.", "date": "2019-04-03"},
{"website": "Ebay-Engineering", "title": "Deriving Data Structures", "author": ["Sachin Tilloo"], "link": "https://tech.ebayinc.com/engineering/deriving-data-structure/", "abstract": "Data Structures are the basic building blocks of software. In this article, one eBay engineer gives his perspective on how they might have evolved over time, enabling us to build complex things out of simpler parts. When I started learning about data structures, I wondered how some pretty cool ones might have evolved over time and how things might have been invented on top of other ideas. It’s like someone invented a wheel with no knowledge that one day it might be used to build a car. But as with everything else, concepts start adding up together over time and we are able to build complex things on top of other things. It all started with arrays, one of the oldest data structures. A pretty neat idea: a collection of identical things, but, more importantly, stored in contiguous memory locations. Any particular index accessed within O(1) time resulted in fast random access. Why contiguous? We want fast random access, and the fastest way to do it would be for the compiler to: Get the base address of the array. Get the base address of the array. Calculate an offset by multiplying the array index by the size of each element. Calculate an offset by multiplying the array index by the size of each element. Add the base address to the offset to form the address of the desired array element. Add the base address to the offset to form the address of the desired array element. Load or store to the desired element using the calculated address. Load or store to the desired element using the calculated address. This allows us to get the exact memory location needed for random access. In fact, with index registers, the content of the register can be added to an immediate address (one that is part of instruction itself) to form the “effective” address of the actual data. It makes sense to have contiguous memory assigned in the case of arrays. Any addition on elements more than what we estimated at initial size means we need to copy over all of elements and find another free space in memory for all of the array. The first digital computers used machine-language programming to set up and access array structures. John von Neumann wrote the first array-sorting program ( merge sort ) in 1945, during the building of the first stored-program computer. Linked lists are little different. They just allow you to put data in and link them together like a chain. There is no random access, since you have to traverse from the start nodes. Again, you might ask: “Why would I need this? I have the array data structure to give me random access, so why should I live with this limitation?” The reason is that linked lists need not be contiguous in memory. You can allocate nodes and just connect them via pointers. Pretty cool! No need to be in order in memory. This gives you flexibility when you don’t need random access and have no idea how much data you have to store. Preserving memory would have been super important on older computers, so the linked list data structure allows us to just get the right memory for any dynamic arbitrary data set. In fact, linked lists were developed in 1955–1956 by Allen Newell , Cliff Shaw , and Herbert A. Simon at RAND Corporation as the primary data structure for their Information Processing Language . If you go through a few pages, it mentions “Flexibility of Memory assignment.” The language and structure the authors proposed was used by the authors to develop several early AI programs, including a chess program. 1 Then came the idea of hashing. What should be the fastest way to find a number in a list of numbers? Logically, a binary search after sorting the numbers in O(log n) time. The genius is to realize there might be a way better than O(log n): apply a function on the input number itself that will tell us the index to use to fit in the array. Also, instead of N entries, let’s make an array with M entries, where N can independent of M. Even if there is a collision, we can insert this entry and form a linked list of collision entries, and then sequentially search each collision. This method would be still faster than searching all N entries. Notice the brilliance of combining the array and linked list data structures with a mathematical function to come up with super cool hash table. So, we solved the problem of lookups efficiently in O(1) time via the miracle of the hash function to point us to the array index. Hashing was invented in 1953 by Hans Peter Luhn . 2 Now, let’s extend that thought even further. We can store millions of numbers in a hash table and quickly find out whether a number is present. What happens if we want to store a billion or trillion numbers, or instead of numbers, we want to store a larger data set, for instance, user addresses? In theory, we can still work with what was invented (hash table), but the problem would be the memory needed to fit everything in. Storing such a big list is too memory intensive and almost non-practical. What if we store only a bit of information instead of actual data? Since a hash table is backed by an array that holds the data, what if we have an array of bits? If we just want to find out if something exists or not, we don't need to store the actual data. We can just store a bit instead, like a bitmap. But now there is a problem. Remember when we had collisions in hashing, we stored entries as linked list that allowed us to search. What will happen in a bitmap now? We cannot know for sure whether the data exists, because there can be a collision case, and this entry was actually not present in original set. It seems we are stuck. But here comes another cool idea. What if instead of using one hash function, we use two? How does that help? Here’s how: one function maps the data to an array index, say index x. Now another function will map it to another index, say y. Now we apply the data to two hashes and set the bit of our bitmap at both locations x and y. What’s the benefit? Now if there were a collision for some data, the chances of having a collision for two hash functions are remote. If both hash function indices (x and y) have the bit set, there is a higher probability that data is present. We can increase our array size and have 3, 4, 5, or more hash functions for each input we get to set in the bitmap. This way we can be almost sure that the data exists — not 100%, but almost. We can tell if a particular piece of data is present in a large dataset with minimal memory overhead and with almost certain probability. This is nothing but bloom filters, which were originally conceived by Burton Howard Bloom in 1970. 3 The idea to combine multiple hashes into one data structure is ingenious. But what if we wanted to find not only whether something existed, but also how many times it exists? Let’s say I have a list of million tweets, and I want to know how many tweets came from one user. The idea is to use multiple hashes — (N) again — similar to bloom filter. But instead of backing it up with one array, as for a bloom filter, we back up with N arrays. In other words, if we have 3 hashes we use 3 arrays. Think of it as a table of 3 columns (one for each hash) and rows corresponding to entries of each of the arrays. Now we set a bit in each of the arrays for a given input by hashing it against them. In this case we set 3 bits for 3 hashes in 3 arrays. When we get new tweets with a given user ID, we do the same, but now if there was a bit set to 1, we increment the bit to 2. Every time we hash, we increment the array index value by one across all hash functions. How did that help? If we have to find out how many times something appeared, we need to look at the array indexes via applying our hash functions at each of the arrays and then take the minimum of these. Why the minimum? There is a possibility that it was incremented because of collision, so we can never be sure. But the minimum is the best possible guess we can make since the more hash functions we use, the probability of collisions decrease, meaning hopefully the minimum value had zero collisions and it is the right number of times that the data exists! This cool idea is what is called a count-min sketch. The count–min sketch was invented in 2003 by Graham Cormode and S. Muthu Muthukrishnan and described by them in a 2005. 4 So, we solved the problem of finding whether x exists and the counting how many times x exists. Now let’s move to a different one. How to find differences between x and data blocks. Imagine when one computer sends lots of data to another, how can another computer verify that all the data is accurate and not manipulated? We can use the same magic of hash, but now we can put all hashes of records and form a tree out of it, as shown in the following figure: Now if we send A, B, C, and D, and the top root hash is stored and can be verified — say from a trusted party — any computer can verify whether the data is sane or manipulated by comparing the top hash. If the root nodes hashes did not match, go down one level to find first mismatch. Keep going down, and you can find the culprit data block that was manipulated. This is concept is known as a Merkle Tree, which were invented and patented by Ralph Merkle in 1979. 5 Notice how we came from array and list and invented hash table, and using hash table, we invented Bloom filters, and then Count Min Sketches. Then we combined hashes with trees to invent Merkle trees. Pretty cool, isn't it? Essentially there are three different classes of data structures, and how they might have evolved: Core Data Structures — These are the classic structures NOT dependent on anything else. Array and Linked List fit this category. The two structures are as basic as can be and follow properties that we all know, and we know when to use which one, with their benefits and limitations. Even stacks, queues, trees, and graphs can fit in here. Core Data Structures — These are the classic structures NOT dependent on anything else. Array and Linked List fit this category. The two structures are as basic as can be and follow properties that we all know, and we know when to use which one, with their benefits and limitations. Even stacks, queues, trees, and graphs can fit in here. Partially Derived Data Structures — These are things like HashMap or HashSet. They introduced the concept of hashing, but they cannot exist on their own, since they need to be backed by the core data structures, which in this case is an array. People might have used it as an extension of arrays. Partially Derived Data Structures — These are things like HashMap or HashSet. They introduced the concept of hashing, but they cannot exist on their own, since they need to be backed by the core data structures, which in this case is an array. People might have used it as an extension of arrays. Fully Derived Data Structures — These are things Bloom Filters, Count Min Sketches, Merkle Trees, etc. that are fully derived from core and partial data structures. Fully Derived Data Structures — These are things Bloom Filters, Count Min Sketches, Merkle Trees, etc. that are fully derived from core and partial data structures. It’s interesting to think how these things might have evolved and what problems would have led to their discovery. Some of these structures are nothing but pure magic, and understanding them gives an “aha!” moment that reveals the genius behind them. What is your favorite Data Structure? Please share in the comments section below. References: Programming Logic Theory Machine Programming Logic Theory Machine Hans Peter Luhn and the Birth of the Hashing Algorithm Hans Peter Luhn and the Birth of the Hashing Algorithm Space/Time Trade-offs in Hash Coding with Allowable Errors Space/Time Trade-offs in Hash Coding with Allowable Errors An Improved Data Stream Summary: The Count-Min Sketch and its Applications An Improved Data Stream Summary: The Count-Min Sketch and its Applications A Digital Signature Signature Based on a Conventional Encryption Function", "date": "2019-04-24"},
{"website": "Ebay-Engineering", "title": "eBay Users Can Save With More", "author": ["Liv Ellingsen", "Steve Neola"], "link": "https://tech.ebayinc.com/product/ebay-users-can-save-with-more/", "abstract": "Volume Pricing saves you money when you buy more of the same thing from eBay sellers. There are things we always need more of when we shop: glass cups, ink cartridges, toilet paper. That’s why eBay recently introduced the Volume Pricing buttons, a faster way to order multiple items and save. Buying more than one of an item isn’t a new idea. As a buyer on eBay, you’ve been able to increase your quantity since we introduced fixed price items in 2002. It doesn’t matter if you’re shopping on desktop or on the app—there are options to type in or select the quantity you want. So why add the buttons? When we recently thought about how to provide valuable information to buyers through design, we thought about volume pricing. Originally the discount had been communicated to the buyers in a plain text box. While it was informative, if you saw it, it could easily be overlooked and was not at all interactive. So we took the original plain text design a step further, combining the information with buttons that stand out on the page and are both informative and actionable. Buyers click a button, see the price, see the discount, know how much more they can save when buying more, and add it straight to their cart. Now, when buyers are searching, they can see “Save when you buy more” taglines that indicates the seller has volume pricing available. If sellers can sell more than one item to the same buyer, it’s a bigger win. The seller spends less time picking and packing items, and can save on shipping costs as well. Plus, the seller is now the person who provided what the buyer wanted, when they wanted it, and with a discount. With 80 percent of the inventory on eBay being new items, there is plenty of scope for sellers to join in. Sellers with a store subscription and listings can add Volume Pricing to their fixed price, multi-quantity listings directly when listing with the tools in Seller Hub. Volume Pricing can also be applied across multiple listings by going to \"Promotions\" under the Seller Hub Marketing tab. If you sell one-off items or don’t list multi-quantities, there are other offer types of discounts that you can offer to your buyers. For example, if you would like to give a discount to a buyer that spends a certain amount with you or purchases three different items, you can do that too. We’re also enhancing Volume Pricing by giving sellers the ability to run this promotion for similar items with multiple variations — things like colors and size — within a single listing. We have made it easier for sellers to offer discounts multiple ways on the platform and for buyers to purchase the way they want. No matter which group you fall into, eBay is working toward a more frictionless ecommerce experience. You’ll see Volume Pricing buttons on mWeb, desktop, iOS and Android in the U.S, U.K, Germany and Australia.", "date": "2019-04-25"},
{"website": "Ebay-Engineering", "title": "Measuring Success with Experimentation", "author": ["Tianlin Duan"], "link": "https://tech.ebayinc.com/research/measuring-success-with-experimentation/", "abstract": "Tips from eBay's Experimentation Science team on how you can best leverage A/B tests to measure the success and health of your product. During a recent internal product conference, I had the honor of sitting on the Product Health & Opportunity Sizing panel. Along with four amazing product owners, we discussed the importance of opportunity sizing, and shared tips, tools, and challenges when measuring the success and health of products. As the Experimentation Science team, we have the great pleasure of working with product teams all across eBay to understand the impact of their products. In this blog, we would like to share with you some of our insider tips on how you can leverage experimentation to guide your product development journey. Step #1 towards effectively measuring the success of your product? Have a clear definition of success. Often times when product owners approach us to discuss the feasibility of an experiment, they already have a product or feature change in mind or already in development and at least a vague idea or vision of what impact this change might have. The very first thing to ask at this early stage would be, what is your definition of win, or what does success look like for this product/feature. This definition should cover both the potential business impact and how it might change user behavior. We currently put a lot of emphasis on the first part, and less so on the latter, but the two actually go hand in hand. Try to frame your definition of success in terms of the engagement you would like to see from the users, and then describe how that positive behavioral change may lead to desired business outcome. In sum, when building your business case, form a user story first. With a good user story comes a strong hypothesis and naturally follows your set of success metrics. Success is rarely defined by a single goal. Success can mean increasing conversion and decreasing friction; it can encompass both short-term win and long-term impact; it may also include keeping potential cannibalization to other parts of the site at an acceptable level. Each of these aspects of success corresponds to one or more metrics that have their own expected movement in response to the testing feature, and together they help you see the whole picture of the impact of your product. Brainstorming and doing your homework on all these components of success will prepare you for the upcoming test. You will know what might happen and what to look for. When finalizing the design of your experiment, we work with you to translate your clear and thorough definition of success into metrics that serve various purposes: some measure the direct impact of the testing feature, some measure actions that are a few more steps from the change, and others provide additional insight into your product. Next we’ll dive into three main categories of metrics that we focus on in experimentation context: primary success metric, guardrail metrics, and monitoring metrics. Let’s consider an example. Say you are testing different designs of the Search bar to improve its prominence. By drawing out the User Flow Diagram, you define a desired user path as noticing the more prominent design → perform a search → find the item they want or perform a new search until then → showing purchase intent by clicking on Add to Cart or Buy It Now → complete the purchase. This is the metric that measures the most direct impact of your product, and is thus by definition a product metrics, not a business one. It is also the metric that Touchstone, our experimentation platform, uses to determine the duration (or days remaining) for your test, and what you should focus on when interpreting the results of the test. In the Search Bar prominence example above, the expected immediate next action after users in test are exposed to the new design (definition of “treated”) is “perform a search”, which translates into the number of searches. If the number of searches per treated sample is higher in Test than Control, then you’ve realized your definition of success — making the Search Bar more prominent. These are metrics that measure the overall impact on the site or the business, and thus also the metrics you don’t want to break. They are generally business metrics that are usually a couple steps away from the immediate action your feature triggers, and thus they are noisier and take longer to reach statistical power. Business metrics include metrics such as Purchases, Bought Items (BI, number of items purchased in a transaction), and GMB (gross merchandise bought) fall into this category. In our example (or in every experiment that we run), the ultimate goal is to drive more purchases, have more purchasing users, and make more revenue eventually. But notice how many steps we have between performing a search and making a purchase? Chances are, if the new design is indeed working and we observed a statistically significant lift in the number of Searches, that exciting little lift might have already diminished before users reach the last step, completing a transaction. If you chose business metrics, especially BI or GMB, as your primary success metric, you will very likely be disappointed when the experiment ends after weeks in noise without a clear launch signal. These are metrics that measure the indirect impact of your feature and/or provides additional insights into how your feature is impacting user behaviors. Product metrics measuring events or user actions between the immediate next step (measured by your primary success metric) and end-of-funnel transactions (measured by your guardrail metrics), or cannibalization of your feature on related features on site all fall into this category. In our example, metrics like SRP (Search Result Page) to VI (View Item page) conversion and purchase intent metrics such as Buy It Now or Add to Cart clicks make good secondary metrics, and you may also benefit from tracking and comparing click shares on different components of the Global Header (where Search Bar lives) to understand how the more prominent search bar is affecting these coexisting features. Hopefully the above categorization of metrics provides some inspiration on how you want to measure the success of your experiment. Try identifying your primary, secondary, monitoring, and guardrail metrics for your next experiment, and if you’re new to the process or have questions about it, reach out to your experimentation team for help. Often times we jump right into the discussion on how to measure the success of your product, but we also need to think about whether that success is measurable. There are three important pieces of information that can help us answer that question: the baseline performance of your success metric, the expected lift as a result of your product/feature change, and your treated percentage. In this section, let’s deep dive into treated percentage. Treated percentage represents the percentage of users that would actually experience the product change you have in mind. For example, if you’re testing some changes on the checkout success page, fewer than 10% of our users who come to the site might actually have the chance to complete a purchase and really see the change. If you’re testing something on the Home Page, the treated percentage would be much higher. Having a decent treated percentage is key to make sure your definition of success is actually measurable. Here’s an example: let’s say you have one test group and a control and you’re hoping to test it on US Desktop, which usually has the least concern about traffic. Having a 70% treated percentage would let you identify a 1% lift on purchase and BI within the minimum test duration required by our Experimentation Platform. A 10% treated percentage would extend that time to months. A 1% treated traffic? Over a year! In case you wonder, it takes longer than 5 years to measure a 1% lift in GMB in this case. This is not to mention that we rarely see a 1% or larger lift in these business metrics, meaning that these numbers are, in fact, an underestimate of the actual duration your test is going to take. If you already know you will be dealing with small treated percentages, here are a couple options you should consider: If your treated percentage is extremely small, like <5%, don’t run an A/B test to measure business impact. Use your best product judgment to launch it. As you’ve seen above, it’s going to take months, if not years, to measure a 1% lift that rarely happens, so what usually happens is that the test would run for a month and end up having all metrics in the noise and underpowered. If your treated percentage is extremely small, like <5%, don’t run an A/B test to measure business impact. Use your best product judgment to launch it. As you’ve seen above, it’s going to take months, if not years, to measure a 1% lift that rarely happens, so what usually happens is that the test would run for a month and end up having all metrics in the noise and underpowered. If you have a relatively small treated percentage, like 10%, be our friend and do yourself a favor by choosing a product metric, especially one that measures the direct engagement with the feature, as your primary success metric. Also, A/B test in this case will only be efficient and helpful if you’re testing a high-impact change — if you’re expecting a 0.1% lift, even engagement metrics can take months to reach statistical power. If you have a relatively small treated percentage, like 10%, be our friend and do yourself a favor by choosing a product metric, especially one that measures the direct engagement with the feature, as your primary success metric. Also, A/B test in this case will only be efficient and helpful if you’re testing a high-impact change — if you’re expecting a 0.1% lift, even engagement metrics can take months to reach statistical power. From a company’s perspective, there is no doubt that revenue matters a lot. In eBay’s business context, GMB has always been a key metric that finance teams track and monitor, as it should be. It's a separate discussion about whether it’s the money or the users (buyers and sellers) that matters more for a company’s health, growth, and success, but in both experimentation and product development context, we would like to argue that the users and their behaviors are much more measurable and provide much more actionable insights for your product development journey. GMB is the most noisy metric among all and measures purchase behavior, which is the very end of the funnel. Remember that the more steps between the event measured by your primary success metric and your actual product change, the more noise you have in the data and often the longer the duration. If you don’t want to wait 6.5 years to get a read on your test, resist the temptation to define success as “boosting GMB by 1%.” A related case is driving conversion. One question to always ask is, what exactly is “conversion” in your case? Conversion is the general movement of users moving one step further down the shopping funnel, and thus could mean quite different things depending on the context: conversion for a personalized module on the homepage might mean users showing interest in the module and clicking to land on a VI page, while conversion on a VI page might mean a Buy It Now click or other actions showing purchase intent. Note that Conversion never meant GMB or revenue. It is widely accepted in the industry as “the percentage of users who perform a desired action.” If this desired action is making a purchase, then the Conversion Rate can be a percentage of users who completed at least one purchase during the test period, but not the average GMB generated by a user or the average GMB per session. The good news is, if defined properly, conversion rate can be a great candidate for a primary success metric, and it usually requires less duration to reach statistical power given its nature as a binary metric. Hopefully by now you have a much better sense of how to define the success of your product and feature, whether that version of success is easily measurable, and how to leverage different types of metric to get the whole picture when measuring success with experimentation. Thank you for reading and please feel free to leave comments, questions, or suggestions below! Have fun experimenting.", "date": "2019-06-12"},
{"website": "Ebay-Engineering", "title": "Akutan: A Distributed Knowledge Graph Store", "author": ["Simon Fell", "Diego  Ongaro"], "link": "https://tech.ebayinc.com/engineering/akutan-a-distributed-knowledge-graph-store/", "abstract": "We're excited to announce the public release of Akutan, a distributed knowledge graph store, under the Apache 2.0 open source license. Akutan is the result of four person-years of exploration and engineering effort, so there's a lot to unpack here! This post will discuss what Akutan is, how it's implemented, and why we've chosen to release it as open source. Akutan is a knowledge graph store, sometimes called an RDF store or a triple store. Knowledge graphs are suitable for modeling data that is highly interconnected by many types of relationships, like encyclopedic information about the world. For example, Wikidata is a great dataset that contains the structured data and relationships from Wikipedia and is a good fit for a knowledge graph. A knowledge graph store enables rich queries on its data, which can be used to power real-time interfaces, to complement machine learning applications, and to make sense of new, unstructured information in the context of the existing knowledge. In a knowledge graph, data is represented as a single table of facts, where each fact has a subject, predicate, and object. This representation enables the store to sift through the data for complex queries and to apply inference rules that raise the level of abstraction. Here's an example of a tiny graph: subject predicate object <John_Scalzi> <born> <Fairfield> <John_Scalzi> <lives> <Bradford> <John_Scalzi> <wrote> <Old_Mans_War> Akutan uses an RDF-like representation for data and a SPARQL-like query language. To learn about how to represent and query data in Akutan, see docs/query.md in the GitHub repo. Akutan is a distributed store. It's designed to store large graphs that cannot fit on a single server. It scales out horizontally to support higher query rates and larger data sets. Its write rates don't scale, but a typical Akutan deployment should be able to support tens of thousands of changes per second. We've run a 20-server deployment of Akutan for development purposes and off-line use cases for about a year, which we've most commonly loaded with a dataset of about 2.5 billion facts. We believe Akutan's current capabilities exceed this capacity and scale; we haven't yet pushed Akutan to its limits. Akutan's architecture is based around a central log, as shown in the following figure. Each box in the diagram is a separate process on a network. The central log isn't a novel idea (see Tango , for example), but it's often overlooked. All write requests are sequenced into an append-only central log. The log is a network service that is internally replicated for fault-tolerance and persisted for durability. Several view servers read the log and apply its entries in sequence, each deterministically updating its local state. Different view servers maintain different state. An API tier accepts requests from clients. It appends the write requests to the log, and it collects data from the view servers to answer reads. The central log imposes a fundamental bottleneck: the maximum rate of appends to the log determines the maximum rate of change to the entire dataset. In exchange, it makes many features simpler to implement, including cross-partition transactions, consistent queries and historical global snapshots, replication, data migration, cluster membership, partitioning, and indexing the dataset multiple ways. See docs/central_log_arch.md for more details. To be more specific, Akutan's implementation is shown in the following figure. The interface to the log is modular. Apache Kafka is the current recommended log implementation (when configured to write new log entries durably to disk before acknowledging them). Akutan currently includes a single view implementation called a DiskView, which can run in two modes: either indexing knowledge graph facts by subject-predicate or by predicate-object. A typical deployment will run three replicas of multiple partitions of each mode. The DiskViews store their facts in RocksDB (this, too, is modular). The API server contains a sophisticated query processor, which we'll discuss next, and the Transaction Timer is a small process that times out slow-running transactions in case an API server fails. The API server has far more functionality than that little box would imply: it contains an entire query processor, as shown in the following figure. Akutan's query processor implements a query language that's similar to a subset of SPARQL, which is analogous to SQL but for knowledge graphs. The query processor consists of a parser, a cost-based query planner, and a parallel execution engine. The parser transforms an initial set of query lines into an abstract syntax tree (AST). The planner combines the AST with statistics about the data to find an efficient query plan. The executor then runs the plan, using batching and streaming throughout for high performance. The executor relies on a View Client/RPC Fanout module to collect data efficiently from the many view servers. See docs/protoakutan_v3.md for more details. We learned a lot during our journey with Akutan. We first built an in-memory key-value store from scratch (ProtoAkutan v1), then iterated to a disk-based property graph (ProtoAkutan v2), then to a knowledge graph (ProtoAkutan v3). Then, we transitioned from prototype mode to writing production-ready code, with thorough documentation, testing, and reviews. We explored many interesting engineering trade-offs in the process, and we wrote up many detailed snapshot documents that show how our thinking evolved over time; see docs/README.md in the GitHub repo for an overview. The Akutan project has come a long way, but unfortunately, we can't continue working on it full-time to turn it into the polished system we had hoped for. We still think it's a really interesting project that has a nice foundation, and it may be useful for some people: Though we were targeting a production deployment of Akutan, lots of other use cases would not need that level of service. Akutan could well be used for offline, noncritical, or research applications today. Akutan would benefit from additional love, and we'd be excited to see your contributions continue to take the Akutan project forward. Take a look through the GitHub issues for ideas on what to contribute. Even without taking Akutan as a whole, it has many internal packages that may be useful in other projects, like its fanout and query planner modules. Finally, others might find Akutan an interesting project to study or learn from. It's an interesting case study of the central log architecture and an example of a fairly large Go project. We hope you'll take a look around the project and give it a spin. Please file bugs, feature requests, and questions as Issues on GitHub.", "date": "2019-05-01"},
{"website": "Ebay-Engineering", "title": "eBay Makes It Easier with Three New Ways for Sellers to Send Offers to Buyers", "author": ["Muthu Sundaresan", "Parin Jogani", "Nainesh Nayudu"], "link": "https://tech.ebayinc.com/product/ebay-makes-it-easier-with-three-new-ways-for-sellers-to-send-offers-to-buyers/", "abstract": "Sellers can now send offers to buyers with an exclusive discount. eBay is giving sellers more tools to reach out to interested buyers so that they can sell more and move faster. Buyers express interest in items listed on eBay in many ways, such as watching those items or adding them to cart. We’re introducing three new ways sellers can send an offer to interested buyers with Seller Initiated Offer — offers to watchers, offers to buyers who have an item in their cart, and offers in message. For example, if a buyer adds shoes to their cart or puts them on their watch list, that’s a signal that a buyer is interested in an item. With Seller Initiated Offers, the seller can go into the Seller Hub (Active Listings) to the specific listing and see that people have shown interest. In a matter of seconds, the seller can send an exclusive offer with a personalized message. This offer will go to buyers that are watching or have that item in the cart. That message reaches up to the 10 most recent people who expressed an interest in the item, ensuring that the offer is always relevant. Sending offers to watchers and offers in message have already launched. We plan to launch offers to buyers with an item in their cart later this year. In an instant, those buyers will receive a notification of the new offer in the app or via email and can make the purchase. We’re excited to create more engagement and opportunities for ecommerce between users. Our sellers asked for this feature through community groups, social media and other channels. The eBay selling team responded to that feedback with this functionality to address the needs of our sellers. Seller Initiated Offers gives buyers another reason to make purchases wherever they are in their day. When we created ​ Best Offer​ , we allowed a buyer to start negotiating on items. Seller Initiated Offers is a way to expand that economic opportunity and help our sellers do more and work faster. This will enable more engagement between sellers and buyers, embracing eBay’s mission of driving ecommerce through innovation. Part of these Seller Initiated Offer tools is offer in message, which is an expansion of how sellers can reply back to buyers. For a long time, buyers were able to send a message to sellers asking for a better price and sellers could reply back with a better offer. Before this update, buyers could only accept that offer through eBay’s website. Now we’ve enabled it through our mobile app, allowing more buyers to shop while they’re on the move. At eBay, we use the approach of continuous delivery and ​ Minimum Awesome Product​ (MAP) to build and test the product adoption, building something nimble enough to test but enjoyable from a user experience perspective. That approach was important to build something that would touch each of the 1.2 billion listings on eBay. After a few weeks of experimenting, we knew Seller Initiated Offers would be a winner with all our users. For buyers, this is another means to connect with our sellers and let them know they’re interested. For sellers, it’s another tool for sellers to use in their business. eBay is about connecting people through ecommerce. In the online world, we think it’s important for a seller to be able to reach out and say, “I want to make a deal. Let’s make it happen.” Seller Initiated Offers — offers to watchers and offers in message — have started to roll out in the U.S., U.K., Germany and Australia on desktop, mobile web and our native apps for iOS and Android. Offers to cart will be available in mWeb, desktop and our native apps on iOS and Android later in the year.", "date": "2019-05-09"},
{"website": "Ebay-Engineering", "title": "Swapping Fridays—Improving Customer Experience with Role Swapping", "author": ["Matthew Wood"], "link": "https://tech.ebayinc.com/engineering/swapping-fridays-improving-your-business-with-role-swapping/", "abstract": "Shutl has been running an initiative called Swapping Fridays. This involves software engineers and customer service agents swapping roles for a day. This article discusses how the initiative was run, the objectives and outcomes, and some challenges that were encountered along the way. Shutl consists of many departments; this article focuses on the engineering and the customer service (CS) teams. Historically, the two teams have always been based in different office locations, which contributed to them remaining quite separated. Recently the CS team moved to an office based within walking distance from the engineering team’s office. We wanted to use this opportunity to try to bring the two teams closer together. In comes Swapping Fridays. Swapping Fridays is an initiative that involves a single developer and a single CS agent pairing up for the day to swap roles (as far as practical). Here is how it works: on any given Friday, an engineer and a CS agent will be paired up to work together for the day. For the first half of the day, the engineer transforms into a CS agent. They will first sit in a short meeting where all the CS tools are demonstrated and explained. They will then take their position behind a computer and start going through emails and live chats with their assigned CS partner. Figure 1. A CS agent (left) and Shutl engineer (right) solving customer tickets This goes on up until lunch time, where the pair gets a little breather apart. Once lunch is done, it’s the CS agent’s turn to step into the engineer’s shoes. Together with the engineer, they go through the process of how an engineer might solve technical customer queries, or if there are none, what an engineer does on a daily basis. Figure 2. CS agent in the role of an engineer This continues until the end of the day. As you can imagine, it’s a fairly substantial time investment from both the engineering and CS teams. Let’s take a look at the outcomes we thought the initiative would produce. As previously stated, the major outcome we hoped to achieve was to help unify the engineering and CS teams. Before the initiative, we found that CS agents did not have a good understanding of how a technical issue was investigated, and the engineers struggled to understand the challenges that the CS agents deal with when solving customer issues. This contributed to a couple of behaviors. First, the engineers would occasionally be frustrated when a technical issue was raised, and there was not enough information to start an investigation. Second, the CS agents would occasionally get frustrated at the length of time it took engineers to respond to tickets. This was caused by a disconnect of expectations from one team to the other. After the initiative, there was a change in this attitude. We found that the engineers realized the work, effort, and skill that the CS agents required to continuously help customers with issues, and the CS agents realized the time investment for an engineer to properly investigate an issue. This led to better communication between the teams which, in turn, resulted in better and faster responses to customers. We also saw that the sharing of knowledge when the pair worked together improved the quality of work that the individuals produced. As an example, CS agents often showed ways of getting information that engineers were unaware of, and engineers could show how the CS agent could solve certain issues without having to escalate a ticket to the engineering department. Seeing as it’s the engineers who build the product for customers, but the CS agents who talk to these customers, there are tangible benefits in getting the engineers closer to the end consumer of their work. It’s possible for engineers to occasionally forget that the work they do can drastically affect the customer’s opinion of the product. We hoped that by allowing engineers to talk directly to customers, it would help to instill the impact that the engineers have on the customers on a daily basis. We hoped that with the combination of the above objectives, there would be a decrease in the number of CS tickets that were escalated to the engineering team. The following graph shows that this outcome was achieved. Figure 3. The number of tickets escalated to the engineering team in 2018 It’s important to note that the Swapping Fridays was not the sole reason for the drop, but it was also a catalyst that enabled people to come up with innovative solutions to improve the processes. For example, one solution was to filter all escalated tickets through the senior members of the CS team before they reached engineering. This was suggested after noticing that some escalated tickets were solvable without the intervention of the engineering team, but not all CS agents possessed that knowledge. As you can imagine, the engineering time saved from not having to look into as many tickets was substantial. This also had a big impact on response time to customers. An escalated ticket can take anywhere between 30 minutes and 1 week to resolve. As fewer tickets were being escalated, the response times back to the customer improved. This was an unexpected but welcome outcome that resulted from the initiative. We found that engineers easily recognized small fixes to the internal tools that would lead to big improvements to the processes the CS agents had to follow. One example of this was normalizing tracking numbers. This meant that CS agents no longer had to capitalize long tracking numbers or remove characters like dashes by hand. Although it was a trivial task for the engineer, it was a major improvement for the CS agents, who were therefore able to save time and provide an improved experience to the customer. I think it is also worth mentioning possible challenges that could be encountered when trying to get the initiative off the ground. One challenge is to ensure that the engineers and CS agents understand the benefits of the initiative. It’s important to keep in mind that you are asking people to do something that they might not feel comfortable doing. Briefing the pair before the day is vitally important to ensure that when they work together, they both take into account the level of expertise that their partner might have on the relevant topics. Another challenge, depending on team size, is the frequency at which an individual would have a Swapping Friday. If it happens too often, then the time investment starts to yield a lower payoff in terms of benefits gained for time invested. If they are too far apart, then it could become a novel event people forget about after a while. At Shutl, the frequency sits around once every 3 to 4 months for each individual. So far this has worked for us. Overall, Swapping Fridays has been a success. The benefits that have come from the initiative unquestionably outweigh the time investment required. A special thanks needs to be made to the engineering and CS management for their willingness and encouragement to get the initiative going. It is really great to work with management that openly embraces changes to try to improve the experience for customers. I would encourage any company to give the initiative a go if they are looking to improve engineer/CS relations and improve the experience they are able to provide to the end customer.", "date": "2019-04-17"},
{"website": "Ebay-Engineering", "title": "Going the Distance — Edit Distance 1", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/research/going-the-distance-edit-distance-1/", "abstract": "What is Edit Distance? How could it be used to measure quality? Find out the basics about this simple metric used for Machine Translation. Edit Distance has always been a great metric to measure the changes made to something, according to at least one person (me). In our MT world, we are usually talking about the MT output as a starting point, and a final version of the target language as the end point. This target language can be a post-edited version created from the MT output, or it can be a purely human translation created from scratch, without seeing the MT output. The Edit Distance measures the changes from starting point to end point. Edit Distance can be a metric for quality: if your starting point is of good quality, it needs few changes to get to the end point. So a low Edit Distance is good. It could also be seen as an indicator of productivity: if you need to make few changes, your work will tend to be faster than if you have to make lots of changes. Edit Distance is probably at the core of nearly all known MT quality scores such as BLEU, TER and others. All of them compare an initial version to a final version and measure what changed. But these metrics are adding more complex features to try to be closer to human evaluations. Edit distance may just be the simplest of all metrics. One word about all scores: whatever score you like, that score is usually much better when used to compare things than to provide absolute verdicts. Are you comparing multiple MT engines? Or different versions of one MT engine? You may get reliable results saying that engine X is better than Y or that version 2 is better than 1, or that Neural is better than Phrase-based. Or you may find out that Transformer s is the best movie ever is a better technology than other NMTs. But the absolute statement “this MT is good”, based on any score, has been usually harder to trust. Now that we know where we are, let’s go back to simple, and take a look at Edit Distance. The core of Edit Distance calculation is an algorithm called Levenshtein distance , which finds the minimum number of changes (additions, deletions or substitutions) to change something into something else. This is how it works: Let’s say you want to change characters. If I want to change Rose > Violet (you know where this is going): Change R to V = V ose Insert an i = Vi ose. Don't do anything to o . Change s to l = Viol e Don't do anything to e . Add a t = Violet . The Edit Distance is 4 for a total of 4 operations, or 4 characters. Funny enough, Violet to Rose is also 4: Violet > Riolet > Rolet >Roset > Rose. (And if you thought that “Edit Distance being the same in both directions” was really something “funny enough,” you may be a fellow nerd.) Now let’s change words : Roses are sometimes red > Violets are blue and you are sweet Change Roses to Violets = Violets are sometimes red Don't do anything to are . Remove sometimes = Violets are sometimes red Change red to blue = Violets are blue Add and = Violets are blue and Add you = Violets are blue and you Add are = Violets are blue and you are Add sweet = Violets are blue and you are sweet The Edit Distance is 7 for a total of 7 operations, or 4 words. The calculation is simple, but you see that there are already two variations in how we can calculate. So one question already is here: What am I gonna watch after Game of Thrones ended? Should we use characters or words? What is your take on this? You can find the first two articles in this series at Going the Distance — Edit Distance 2 and Going the Distance — Edit Distance 3 .", "date": "2019-08-08"},
{"website": "Ebay-Engineering", "title": "Monitoring at eBay with Druid", "author": ["Mohan Garadi"], "link": "https://tech.ebayinc.com/engineering/monitoring-at-ebay-with-druid/", "abstract": "At eBay, we switched one of our monitoring tech stacks from legacy homegrown architecture to a Druid-based real-time monitoring system. In this article, we discuss how we transitioned our journey to a new stack and also the benefits it has to offer. eBay supports millions of users every day for ecommerce transactions. Large user growth has also come with an explosion of data produced by various applications that support different products. Logs are the heart of the application that determine what an application is doing. It is very difficult to visualize logs as the application size grows. We have a centralized log store that processes all the logs. Harnessing useful information is difficult and not feasible in real-time directly from the logs. At eBay, the monitoring team visualized the problem in a different way. Extracting useful events from the logs and processing those events through a data pipeline is a better way to solve the problem. The number of events correlate directly to the amount of logs generated based on the current system’s traffic. Some applications may be generating hundreds to thousands of events while others may be generating millions of events. Our interest is to monitor how the individual applications are performing, based on the events extracted from logs and the ability to alert the users in case of an abnormal behavior when there are too many errors or anomalies happening in the system. Application events consist of error status codes, url transactions, command executions, and the build id for the application artifact running on different hosts. These events serve various purposes. These events are of interest to the individual app developers and site reliability engineering (SRE) teams to monitor performance of an application in real time. They are able to visualize how many errors are happening in the system, slice and dice those errors by command executions and build causing these errors, and then set up alerting based on error thresholds that can impact the application performance. This information provides critical insights when the application teams have to deploy new artifacts of the application in production. They will be able to do a sampled rollout of code on a small percentage of hosts and visualize real-time dashboards to determine the behavior of the new code with respect to errors generated, and then compare real-time data with historical data that gives a level of confidence. The legacy architecture was designed years ago when the number of events generated by the entire site were in the order of 10 of millions every day. This was scalable at that time and for a few years down the lane. There were some of the shortcomings of the legacy architecture over time: Cube generation was custom written code for each of the intervals. Generation of data for the current time used to take a few minutes, which was not acceptable for real-time monitoring. This delay increased with the increase in the amount of data. Horizontal scaling of the custom cube generation had less success over time as the amount of data increased. Slow generation or failure to create cubes in case of very high cardinality of the dimensions (a few hundred thousand to a few million combinations). In the new architecture, the Tibco dependency has been removed, and Kafka is used as a layer for persisting messages temporarily for consumption. Tranquility is used to consume data from Kafka and push into Druid. Key points of the new architecture: Minimal end-to-end latency from the event generation to realizing it at the egress (< 10 seconds at max for very large applications). Use Druid for processing multiple granularities of data such as 1 minute, 15 minutes, 1 hour and so on. Reindex data for 1 day interval. Kubernetes deployment enables us to delete the cluster and recreate it in a matter of a few minutes in case of upgrades or maintenance. It’s very easy to perform rolling updates with 100s of nodes. Druid efficiently handles high cardinality data. Even millions of dimension values are possible with Druid without incurring any additional delay as long as the horizontal scaling is provided sufficiently for indexing tasks, which is achievable with zero down time. (Tibco is an enterprise message bus for data transport. Tranquility is a part of druid-io which has APIs to send data streams to Druid.) Events comprise the things happening in a system that are sporadic in nature. A few apps generate a few events a day while others generate millions of events in a minute. Different types of events may be generated based on the purpose they serve. We discuss monitoring event in this context. In our use case, data has a fixed dimension keys (11 dimensions), a timestamp, and two metrics to be computed: Count and Latency. Count is the number of events happened on a host while data was collected at a particular timestamp. Latency represents the sum of latency across all the transactions. Hundreds to millions of events may be generated by thousands of hosts across apps and can contain different set of dimension values per event. The dimension values for each of the dimensions can vary from ten to a few thousand per application. The developers and SRE teams are interested in events like the above to find out the number of errors happening on the site for a particular application or across multiple applications when there is a large impact. Collecting a few million events per minute in real time into a central store and processing them comes with a set of challenges of accuracy, speed, reliability, and resiliency. Monitoring events are generated across the entire fleet at the rate of 8 million events/sec on an average to 10 million events/sec at peak traffic, from more than 5,000+ applications. Monitoring events require slicing and dicing across various dimensions, such as application name, application type, operation name, error status, build running the application, host and so on. All the data should be aggregated and available in near real-time service level agreement. There are 11 fixed dimensions, and dimension value cardinality across all dimensions is between 1.4 to 2 million unique combinations. Our Druid clusters are deployed across multiple availability zones for high availability and 2 copies of replicas per shard per data center are maintained. This allows us to have a total of 4 replicas available across 2 data centers. Each data center has a few hundred middle managers, 2 overlord + coordinator nodes, 15 broker nodes, and 80 historical nodes. Peak data traffic is shown in the following screenshot. Egressing of data is designed to keep the availability of data high at all times. A layer in front of Druid brokers is designed to query data from Druid to determine the health of each data center. We expect that the health of both data centers is always maintained at optimal and highly available. In case of any data loss in any data center, Egress switches to a cluster that has better data quality. We get the event counts from each cluster every minute to determine if both clusters have similar data (with a deviation of < 0.5% of the event count difference between the clusters). In case the deviation is too much, we pick the cluster that has better event counts. The calculations happen every minute, and we continue to update the cluster health to determine the best cluster that can serve the data for a time duration. We also mark down the cluster if any data loss is detected so that none of the queries go into the bad cluster’s broker node for querying. We support various granularities that earlier versions of Druid supported (1 minute, 15 minute, 1 hour, 1 day) depending upon the length of time for which the data is queried. This granularity selection happens automatically. When required, the granularity can be forced to fetch more granular data for a longer period of time at the cost of response time due to large volume of data queried. For site monitoring and event tracing use cases that carries a high cardinality data that requires aggregation in real-time or near real-time is critical for a big ecosystem like eBay to make data driven decisions. The capability for getting insights that an analytics store like Druid can provide is extremely valuable and important from the monitoring aspect, which a lot of teams and developers rely on to maintain the system availability and reliability for eBay’s customers. All references to Druid in this blog post refers to the open source version of Druid. Please refer to the following links: http://druid.io/ https://github.com/apache/incubator-druid/ Mohan Garadi — Software Engineer Premendra Singh—Software Engineer Mahesh Somani—Architect Saurabh Mehta—Engineering Manager Amber Vaidya—Product Manager Andy Santosa—Engineering Director Rami El-Charif—VP Infrastructure Engineering Our Infrastructure and Partner teams - Tess.IO, Capacity, SREs, Operations", "date": "2019-05-29"},
{"website": "Ebay-Engineering", "title": "Going the Distance — Edit Distance 2", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/research/going-the-distance-edit-distance-2/", "abstract": "If you change a sentence, should you see the characters or words that changed? Edit Distance is back to help you figure this out. Before we go any further, Edit Distance is, by definition, an absolute number, which is the number of operations for change. It is not a relative number. Can you do anything with absolute numbers? Absolutely! Spellcheckers look for words in a dictionary that have the smallest Edit Distance to the word that has been misspelled (or is it mispelled?). The smallest is 1 character, an absolute value. But most applications in MT will need a relative number, proportional to some form of length. The Edit Distance can be measured in characters or words. Which one should we choose? Some arguments in favor of calculating in characters could be: It can be used for any language, including Asian languages. It can be used for German, where a compound word could be equivalent to several English words, for example, throwing the word-based calculation off a little. This is more of an “against-words” argument than an “in-favor-of-characters” one. This is more of an “against-words” argument than an “in-favor-of-characters” one. It better represents minor changes to words such as adding an “s” for a plural. Change Rose to Rose s = 1 character operation Change Rose to Roses = 1 word operation Change Rose to Rose s = 1 character operation Change Rose to Roses = 1 word operation Some arguments against characters could be: Changing one Asian character is not the same as changing one non-Asian character. They carry more meaning and there are less Asian characters in a sentence compared to its equivalent in English, for example. So, you can’t really compare character distances across all languages. They carry more meaning and there are less Asian characters in a sentence compared to its equivalent in English, for example. So, you can’t really compare character distances across all languages. Some languages, such as Japanese, will have Asian and non-Asian characters in the same sentence. They would need to have different weights. And a reordering of one word from MT to PE: MT: The seller voluntarily refunded the buyer. > PE: The seller refunded the buyer voluntarily . The seller voluntarily refunded the buyer. The seller refunded the buyer voluntarily . The seller voluntarily refunded the buyer. The seller refunded the buyer voluntarily . The change of position of the word “voluntarily” means two word operations: the deletion of the word where it was initially, and the addition of the word where it is now. For 2 changes out of 6 words, we get 33%. But for characters, it looks like this: 1. The seller v oluntarily refunded the buyer. 2. The seller vo luntarily refunded the buyer. 3. The seller vol untarily refunded the buyer. 4. The seller volu ntarily refunded the buyer. 5. The seller volun tarily refunded the buyer. 6. The seller volunt arily refunded the buyer. 7. ... 11. The seller voluntarily refunded the buyer v . 12. The seller voluntarily refunded the buyer vo . 13. ... 22. The seller voluntarily refunded the buyer voluntarily . There was a lot of change, 22 out of 42 characters, more than 50% of the characters were moved. Does this look more like two changes of words or more like changing over 50% characters? So, certain changes are better represented as word changes. Some other possible arguments that could be made in favor of words : The “unit of attention” of a translator is a word and not a character. Nobody changes characters, they change words. Translators think of the meaning of the whole word and then apply a change in meaning, which may be just a character. So, the “effort of PE” is a thinking effort in words. Translators think of the meaning of the whole word and then apply a change in meaning, which may be just a character. So, the “effort of PE” is a thinking effort in words. It is easier to think of changing 2 words out of 5 than 17 characters out of 85. This image below shows the difference in Words – Chars looks like for a sample of about 1000 segments. The average % edit distance in words was 44% and for chars was 28%. The numbers I have seen usually seem to be around the edit distance per characters being smaller by about 35 to 40% of the edit distance value for words. If 44-28 = 16, then 16 is about 36 % of 44. Some automatic metrics are based on words, such as BLEU and TER . Other metrics, such as CharacTer and chrF++ have words and characters working together to produce scores. And some metrics are calculated to ignore changes in position, such as Position Independent Word Error Rate (PER), and BLEU to some extent. These are all attempts to better represent the changes. We should just calculate both edit distances (by characters and by words) for a while, until we get better numbers that help us choose one or the other in each situation. This list of arguments for one calculation or the other is by no means exhaustive. What others can you think of? There is one more choice about Edit Distance: how should we normalize the Edit Distance ? You can find the first two articles in this series at Going the Distance — Edit Distance 1 and Going the Distance — Edit Distance 3 . If you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2019-08-21"},
{"website": "Ebay-Engineering", "title": "WebAssembly at eBay: A Real-World Use Case", "author": ["Senthil Padmanabhan", "Pranav Jha"], "link": "https://tech.ebayinc.com/engineering/webassembly-at-ebay-a-real-world-use-case/", "abstract": "From the time it was announced, WebAssembly caused a huge buzz in the front-end world. The web community readily embraced the idea of taking code written in programming languages other than JavaScript and running that code in the browser. Above all WebAssembly consistently guarantees native speeds much faster than JavaScript. At eBay, we were no different. Our engineers were very excited about this idea and kept a constant eye on the spec and its evolution. Once WebAssembly 1.0 was shipped in all major browsers, teams around eBay were eager to try it out. But there was a problem. Though there are many use cases and applications that would benefit from WebAssembly, the scope of the technology within ecommerce is still primitive. We were not able to find a proper use case to leverage WebAssembly. A few suggestions came up, but we were better off with JavaScript itself. At eBay, when we evaluate new technologies, the first question we ask is “What potential value does this add to our customers?” Unless there is clarity around this, we do not proceed to the next step. It is very easy to be carried away by the new shiny thing, often forgetting the fact that it may not make any difference to our customers and only complicate the existing workflow. User experience always trumps developer experience. But WebAssembly was different. It has tremendous potential, we just did not have the right use case. Well, that changed recently. eBay native apps both iOS and Android have a barcode scanner feature in the selling flow. The feature leverages the device camera to scan a product UPC barcode and automatically fill out the listing, thus removing the manual overhead. This was a native app-only feature. It requires some intense image processing on the device to detect the barcode number from the camera stream. The retrieved code is then be sent to a backend service which, in turn, fills out the listing. This means that the on-device image processing logic has to very performant. For native apps, we compiled an in-house built C++ scanner library into native code for both iOS and Android. It was extremely well performant in generating the product barcode from the camera stream. We are slowly transitioning to iOS and Android native APIs, but the C++ library is still solid. The barcode scanner is an intuitive feature for our sellers, as it made the listing flow more seamless. Unfortunately, this feature was not enabled for our mobile web users. We already have a well-optimized selling flow for the mobile web, except that the barcode scanner was not available, and sellers have to manually enter the product UPC , thus adding more friction. We have looked into implementing a barcode scanner for the web before. We, in fact, launched a version of the barcode scanner with the open source JavaScript library BarcodeReader . This was 2 years back. The issue was that it performed well only 20% of the time. The remaining 80% of the time, it was extremely slow and users assumed it was broken. It was timing out the majority of the cases. This is sort of expected, as JavaScript can indeed be equally fast as native code, but only when it is in a “hot path,” i.e. heavily optimized by JIT compilers. The trick is that JavaScript engines use a lot of heuristics to decide if a code path is “hot,” and it is not guaranteed on every instance. This inconsistency obviously resulted in user frustration, and we had to disable the feature. But things are different now. With the web platform evolving at a rapid pace, the question resurfaced “Can we implement a consistently performant barcode scanner for the web?” One option is to wait for the Shape Detection API . This proposed web API brings many native image detection features to the web, one of which is barcode detection . It’s still in very early stages and still has a long way to achieve cross-browser compatibility. Even then, it is not guaranteed to work on every platform. So we have to think about other options. This is where WebAssembly comes into play. If a barcode scanner is implemented in WebAssembly, we can make strong guarantees that it would be consistently performant. The strong typing and structure of WebAssembly bytecode enable the compilers to always stay on the hot path. On top of it, we had an existing C++ library that was doing the job for native apps. C++ libraries are ideal candidates to be compiled to WebAssembly. We thought we had a clear path. Well, not exactly. Our engineering design to implement a WebAssembly-based barcode scanner was pretty straightforward. Compile the C++ library using Emscripten . This will generate the JavaScript glue code and .wasm file. Create a Worker thread from the main thread. The worker JavaScript will import the generated JavaScript glue code, which in turn instantiates the .wasm file. The main thread will send a snapshot from the camera stream to the worker, and the worker will call the corresponding wasm API through the glue code. The response of the API is passed to the main thread. The response can either be the UPC string (which is passed to backend) or an empty string if no barcode is detected. For the empty scenario, the above step is repeated until a barcode is detected. This loop is timed by a configurable threshold in seconds. Once the threshold is reached, we display a warning message “This is not a valid product code. Please try another barcode or search by text.” This either means the user is not focusing on a valid barcode or the barcode scanner is not performant enough. We track these timeout instances, as it is a good indicator of how well the barcode scanner is performing. WebAssembly Workflow Compilation The first step for any WebAssembly project is to have a well-defined compilation pipeline. Emscripten has become the de facto toolchain for compiling WebAssembly, but the key is to have a consistent environment that produces a deterministic output. Our frontend is based on Node.js, which means we need a solution that works with the npm workflow. Fortunately, it was around the same time that the article “ Emscripten and npm ” was published by Surma Das . The Docker-based approach for compiling WebAssembly makes perfect sense, as it removes a ton of overhead. As recommended in the article, we went with the Docker Emscripten image by trzeci . We had to make a couple of tweaks to the custom C++ library to make it compatible to compile to WebAssembly. That was mostly a trial and error exercise. Ultimately we were able to compile and were also able to set up a neat WebAssembly workflow within our existing build pipeline. It was fast, but… The way we calculate the performance of the scanner is by analyzing the number of frames the wasm API can process in a second. The wasm API takes in a frame, in this case, an image snapshot pixel data from the live camera stream, performs the calculations and returns a response. This is done on a continuous basis until a barcode is detected. We measure it in terms of the well-known Frames Per Second (FPS) metric. In our testing, the WebAssembly implementation performed at an astonishing 50 FPS on an average. However, it worked only 60% of the time with the current timeout threshold. Even with this high FPS, it was not able to quickly detect the barcode for the remaining 40% of valid scans and ended-up displaying the warning message. To put this in comparison, the JavaScript implementation that we tried earlier performed only at 1 FPS for the vast majority. So for sure, WebAssembly is faster (50x), but somehow it was not able to detect the barcode in the allocated time for nearly half of the scans. It should also be mentioned that in certain scenarios, JavaScript performed really well and was able to detect the barcode immediately. One obvious option would be to delay showing the warning message, but that would only increase user frustration, and we are not actually solving the real problem. So we dropped that idea. Initially, we were clueless about why the custom C++ library, which worked perfectly well for native apps, did not produce the same result for the web. After a lot of testing and debugging, we found that the angle in which we focus the object, along with the background shadow, determines the time for successful detection. Then how did it work in native apps? Well, in native apps we use inbuilt APIs to either autofocus or provide user tap focus to the center of the object that is being scanned. This enables native apps to send high-quality image pixel data (i.e. information only about the barcode) to the scanner library at all times. This avoids the blurry image situation. Hence the consistently fast response times. Now that we had an idea about what is going on, we thought maybe a different native library might perform better under varied focus conditions. The open source barcode reader ZBar is pretty popular and stable. More importantly, it works well with blurry and grainy images. Why not try that? Since we have a WebAssembly workflow already set up, compiling and deploying ZBar as WebAssembly was seamless. We then began evaluating the ZBar implementation. The performance was decent, around 15 FPS (not as good as our custom C++ lib). However, the success rate was close to 80% for the same timeout threshold. Definitely an improvement over our custom C++ library, but still not 100% reliable. We were still not satisfied with the outcome, but we noticed something unexpected. The scenarios in which ZBar timed out, the custom C++ library was able to get the job done very quickly. This was a sweet surprise. Apparently, based on the quality of the image snapshot, the two libraries performed differently. This gave us an idea. Multithreading and racing to the rescue You probably guessed it. Why not create two web worker threads — one for ZBar and one for the custom C++ library —  and race them against each other. The winning response (i.e. the first one to send a valid barcode) is sent to the main thread, and all workers are terminated. We set this up and started internal dogfooding to simulate as many scenarios as possible. This setup yielded us a 95% success rate when scanning a valid barcode. Much better than our previous success rates, but it still falls short of 100%. One weird suggestion was to also put the original JavaScipt library into the mix. This would make it three threads. We honestly did not think that this would make a difference. But it was easy to try out, as we standardized the worker interface. To our surprise, with three threads racing against each other, the success rate was indeed close to 100%. This again was totally unexpected. As mentioned earlier in the post, JavaScript did perform very well on certain scenarios, and this factor seemed to close the gap. So yes, “ always bet on JavaScript .” Jokes apart, the following diagram provides a good overview of the final architecture we implemented. Web-based barcode scanner architecture The following illustration shows a high-level flowchart: Barcode scanner flowchart A note about asset loading The assets required for the barcode scanner are prefetched after the main page is rendered. This is to ensure that the sell landing page is loaded fast and ready to interact. The WebAssembly assets (wasm files and associated glue code scripts) and the JavaScript scanner lib are prefetched and cached after the load event of the page using XMLHttpRequest . The point to note here is that they are not executed. This is to keep the main thread free for user interaction. Execution happens only when the user taps on the barcode icon. In case a user taps on the barcode icon before the assets are loaded, we load them on demand and execute immediately. The barcode event handler and worker controller are bundled as a part of the initial page load, but they are very small in size. After thorough testing and internal dogfooding, the feature was launched as an A/B test. The “Test” bucket in the experimentation showed the barcode scanner icon (screenshot below) and the “Control” did not. The final product The metric that was used to evaluate the success of the A/B test was something called “Draft Completion Rate.” It is the rate at which a listing goes from a draft stage to successfully completed and submitted. The Draft Completion Rate is a good indicator to support the notion that reducing friction and proving a seamless selling flow through a barcode scanner should enable more listings to be completed. We ran the tests for a couple of weeks, and when the results came back it was indeed very satisfying. It perfectly aligned with our original hypothesis. The Draft Completion Rate improved by 30% for the listing flow with a barcode scanner enabled. A/B test results We also added profiling to get the distribution on which type of scanner wins the race. The results were as expected, with ZBar contributing to 53% of successful scans, followed by the custom C++ lib with 34%, and finally JavaScript lib with 13%. The whole WebAssembly journey was a great learning experience for us. Engineers get pretty excited about new technologies and immediately want to try them out. If the same technology makes a positive impact on a customer-centric metric, it is a double delight. This alludes to an earlier point in this post. Technology evolves at a very rapid pace. Every day we hear new things getting launched. But only a few make a difference to customers, and WebAssembly is one of them. This was our biggest learning from this exercise — “Saying “No” to 99 things and “Yes” to the one thing that really matters to our customers.” As next steps, we are looking into expanding the barcode scanner to the buying side of the mobile web, which would allow buyers to scan items to search for and purchase. We will also look into augmenting this feature with the Shape Detection API and other in-browser camera capabilities. Meanwhile, we are happy that we found the right use case for WebAssembly at eBay and in bringing the technology to ecommerce. Special thanks to Surma Das and Lin Clark for their numerous articles on WebAssembly. It really helped us get unblocked on various instances.", "date": "2019-05-22"},
{"website": "Ebay-Engineering", "title": "ModaNet: A Large-scale Street Fashion Dataset with Polygon Annotations", "author": ["Shuai (Kyle) Zheng", "Fan Yang", "M. Hadi Kiapour", "Robinson Piramuthu"], "link": "https://tech.ebayinc.com/research/modanet-a-large-scale-street-fashion-dataset-with-polygon-annotations/", "abstract": "Searching for an ideal dress or pair of shoes sometimes could be challenging, especially when you do not know the best keywords to describe what you are looking for. Luckily, the emerging smart mobile devices provide an efficient and convenient way to capture those products of interest in your photo album. The next natural thing is letting an ecommerce app like eBay figure it out for you. Understanding clothes and broad fashion products from such an image would have huge commercial and cultural impacts on modern societies. Deploying such a technology would empower not only the fashion buyers to find what they want, but also those small and large sellers to have quicker sales with less hassle. This technology requires excellence in several computer vision tasks: what the product is in the image (image classification), where it is (object detection, semantic image segmentation, instance segmentation), visual similarity, how to describe the product and its image (image captioning), etc. Recent works in convolutional neural networks (CNNs) have significantly improved the state-of-the-art performance of those tasks. In the image classification task, ResNeXt-101 method has achieved 85.4% in top-1-accuracy 1 in ImageNet-1K ; in object detection, the best method 2 has achieved 52.5% mAP in the COCO 2017 benchmark for generic object detection; in semantic image segmentation, the top-performing method 3 has reached 89% mIOU in PASCAL VOC leaderboard for the generic object segmentation. Due to unique challenges in street fashion images, including wide variations in appearance, style, brand, and layering of clothing items, one remaining question is how well those object detection and semantic image segmentation algorithms perform on the street fashion dataset. By understanding the pros and cons of existing algorithms for object detection and semantic image segmentation on street fashion dataset, we would be able to provide the technology to eBay customers. Figure 1. Examples of Annotations in ModaNet dataset. These images contain pixel-level annotations for each product type. Yamaguchi et al. 4 created a street fashion dataset called the Paperdoll dataset with a few hundred pixel-wise clothing annotations based on super-pixels. We are introducing a new dataset called ModaNet, which is built on top of the Paperdoll dataset and adds large-scale polygon-based fashion product annotations, as shown in Figure 1. Our dataset provides 55,176 street images, fully annotated with polygons on top of the 1 million weakly annotated street images in Paperdoll. ModaNet aims to provide a technical benchmark to fairly evaluate the progress of the latest computer vision techniques that rely on large data for fashion understanding. The rich annotation of the dataset allows measurement of the performance of state-of-the-art algorithms for object detection, semantic segmentation, and polygon prediction on street fashion images in detail. Figure 1 shows a snippet from ModaNet. The ModaNet dataset provides a large-scale street fashion image dataset with rich annotations, including polygonal/pixel-wise segmentation masks, bounding boxes. It consists of a training set of 52,377 images and a validation set of 2,799 images. This split ensures that each category from the validation set contains at least 500 instances, so that the validation accuracy is reliable. It contains 13 meta categories, where each meta category groups highly related categories to reduce the ambiguity in the annotation process. The 13 meta categories are bag, belt, boots, footwear, outer, dress, sunglasses, scarf and tie, pants, top, shorts, skirt, and headwear. All images are annotated by human annotators. Annotators have been trained for 2 weeks before starting the annotating, and their annotation quality accuracy reached 99.5%. During the annotation process, the annotators conducted two tasks: (1) skip the images that are ambiguous to annotate, and (2) provide polygon annotations for individual objects of interest in the image and assign a label from a predefined set of 13 meta categories. The goal of object detection in ModaNet is to localize each fashion item from the image and assign a category label that can be further used for visual search or product recommendation. We chose three most popular object detectors to evaluate their performance on the ModaNet dataset: Faster RCNN , SSD , and YOLO . Both SSD and YOLO are single-stage, real-time detectors. Faster RCNN is the representative work for the two-stage approach, which aims to give more accurate results. Specifically, in our experiments, Faster RCNN uses Inception-ResNet-v2 as its backbone network, while we chose Inception-V2 for SSD and YOLO v2 network for the YOLO detector. As shown in our experimental results, we find that more effort should be put into developing detectors that can better handle small and highly deformable objects for fashion. Figure 2. Semantic image segmentation results on the ModaNet dataset. The first column contains output from DeepLabV3+. The last column contains ground truth annotations. Semantic image segmentation provides more detailed localization information. We considered several most representative approaches to evaluate on the ModaNet dataset. These approaches are: Fully Convolutional Neural Networks (FCNs), Conditional Random Fields as Recurrent Neural Networks (CRFasRNN), and DeepLabv3+. FCNs methods use a VGG network as its backbone network. We adapt the VGG network with batch normalization, which has obtained higher top-1 accuracy in the ImageNet-1K dataset. We also adapted the CRFasRNN module on top of the FCNs, and we obtain higher accurate results than FCNs. For DeepLabV3+, we take the publicly available TensorFlow implementation and ImageNet pre-trained Xception-65 model, and fine-tune on the ModaNet (see Figure 2). We find that DeepLabv3+ performs significantly better than the alternative approaches across all metrics. This shows the importance of the backbone network as well as the careful design in the CNN modules for semantic image segmentation. We also find that CRFasRNN helps to get a better shape of some objects like “outer” and “pants,” but performs poorer in small objects such as “sunglasses,” as show in Table 1. Table 1. F-1 score per category of evaluated semantic segmentation approaches. One immediate application of semantic image segmentation is to predict the color attribute name given a street fashion photo. We develop a prototype based on the models trained on the ModaNet dataset. We first conduct the semantic image segmentation and then predict the color attribute names by mapping the mean RGB values for each segment to a fine-grained color namespace. This gives interesting results as shown in the Figure 3. Figure 3. Color attribute prediction using semantic image segmentation. Mahajan,et al. Exploring the Limits of Weakly Supervised Pretraining. ArXiv, 2018. Peng, et al. MegDet: A Large Mini-Batch Object Detector. CVPR, 2018. Chen, et al. Rethinking Atrous Convolution for Semantic Image Segmentation. CVPR 2018. Yamaguchi, et al. Retrieving Similar Styles to Parse Clothing, IEEE PAMI, 2014. Ren et al., Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015. Liu et al., SSD: Single Shot MultiBox Detector. ECCV 2016. Redmon et al., You Only Look Once: Unified, Real-Time Object Detection, CVPR 2016. Abadi et al., TensorFlow: A system for large-scale machine learning. CoRR abs/1605.08695, 2016. Zheng et al., Conditional Random Fields as Recurrent Neural Networks. ICCV 2015. Long et al., Fully convolutional networks for semantic segmentation. CVPR 2015. Szegedy et al., Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. AAAI 2017. Deng et al., ImageNet: A Large-Scale Hierarchical Image Database. CVPR 2009.", "date": "2018-11-05"},
{"website": "Ebay-Engineering", "title": "Improving the Speed and Accuracy of the Item Page with Light Service", "author": ["Ramesh Periyathambi"], "link": "https://tech.ebayinc.com/engineering/item-fast-light-service/", "abstract": "The item page is one of the most critical pages in the eBay buyer experience. It's where a buyer make their purchase decisions. It's also one of the pages in eBay that gets the most user traffic. The speed of this page and showing accurate information to the user is of great importance. Item fast light service was developed to get critical item information as quickly as possible from the primary source tables. This service is currently used in a number of applications to achieve functionalities that needs 100% accuracy with lightning speed and performance of less than 10ms for an item. Item fast light service also supports functionality scalable to pull information for multiple items with the high consistency and performance. Based on the necessary information needed to support a required functionality, item light service is using a highly tuned SQL to retrieve only the needed information. Buyers tend to bid more during the last few minutes before an item listing ends. This makes the current price/next bid price of the item to go up quickly, and also makes the buyer's status on the item, such as high bidder/outbidder, change based on other user bids. To handle this functionality across multiple pages (item/product) and platforms (web/native), the item light service polls auction details every few seconds, based on the time left before the listing ends. The item light service obtains up-to-date item and buyer status information directly from the ITEM DB and, therefore, it is highly consistent. This enables buyers to see the updated information on the page quickly without refreshing the entire page, giving a better user experience. One of the top engineering efforts in eBay is to improve the site speed of critical page flows to provide a better user experience. The item page is one of the critical pages in the buyer experience. We explored multiple ways of improving site speed for the item page using this light service. The majority of the traffic to item pages comes from the search results page. One strategy is to find ways to improve the item page’s speed during or after the search page load, without degrading search performance. Using item light service, selected top item images to be shown on item pages (after a user clicks) are prefetched and cached in the browser cache after the search page is loaded. This helped us to show item images instantly when the item page is loaded, which helped to improve conversion and speed by 200ms above the fold, as shown in the following graph. The item image prefetch in the search page only covered a set of top hit items. The same item light service is used in the item page to get the image faster, before the main domain service returns to render the whole page. This quickly fetched image from the light service is downloaded before the item page is fully rendered, saving 100ms above the fold time on the item page. The item page for the mobile web (mWeb) platform uses the item light service to quickly render the above-the-fold information like images and item title. This gives the ability to render the item page much faster progressively, without waiting for the entire domain service completion to render the entire item page. Since the item light service gets the item information directly from the primary data source, the data is consistent and reliable. Using the item light service to render above-the-fold information on the item page gave incredible speed gains of ~1.5 seconds, as shown in the following graph. There are a lot of similar functionalities that needs quick and accurate information to give a better user experience. A few more capabilities will be added to the item light service to get the user status of the item, such as high bidder, outbidder, or buyer, which will be used to show relevant merchandising modules on item pages.", "date": "2019-07-24"},
{"website": "Ebay-Engineering", "title": "Beyond Logos and Patterns: How We’re Training eBay’s AI to Understand Brands", "author": ["M. Hadi Kiapour", "Robinson Piramuthu"], "link": "https://tech.ebayinc.com/research/beyond-logos-and-patterns-how-were-training-ebays-ai-to-understand-brands/", "abstract": "We’re researching how to recognize brands using computer vision by training our AI to look beyond logos and iconic patterns. Think of your favorite brands: How do you recognize them when you are shopping? Maybe it’s that iconic swoosh on your favorite sneakers or that distinctive plaid on your handbag or that apple on your phone. Brands express themselves in various visual forms. At eBay, we’re researching how to recognize brands using computer vision . We’re training our AI to look beyond logos and iconic patterns to dial in on the unique characteristics that brands use to create specific items. We’ve compared our deep learning model to human understanding in an experiment to validate visual perceptions of brands. As your brain evaluates a shoe or a bag, it’s taking in and processing all sorts of information—from the style to the pattern to the fabric. It makes a decision with its best hypothesis based on a variety of factors and insights learned over time. These are some of the elements that make up brand recognition. And when you’re shopping, this recognition is one of the key steps to finding the perfect item since brand encapsulates such rich information. We set out to understand how to identify brands visually by targeting unique designs, patterns, stitching or hardware. We also wanted to understand how deep networks distinguish between similar products and to compare our analysis to human perception. We investigated how our deep learning models build internal representations of brands and we examined how those representations vary over products. This allowed us to further understand the classification path that AI uses. We are using these representations to analyze visual embodiment of brands at large scale and to find the key characteristics of a brand’s visual expression in a single product, a brand as a whole and across categories. It’s important to note that one of the reasons eBay’s computer vision models are so powerful is because we are training the models on varying qualities of images—from professional or stock photos to amateur, dimly-lit photos with complicated backdrops. Our dataset consists of 3,828,735 clothing products from 1,219 brands spanning across a wide range of clothing types. These are real-world ecommerce images from our catalog. For every product, we collect an image, title and a set of attributes from which we extract the brand information. After training the model on a given image, we get outputs for probabilities of what brand is the most likely featured in the image. Then, we follow where the neurons at each layer are focused to make their decision through “attention” maps. Our goal is to visualize and interpret the deep model’s decisions in order to explain the visual characteristics of fashion brands. For instance, we saw that our AI was recognizing the three-stripes signature on Adidas products instead of the logo. As part of our work to understand brands, we’ve also analyzed our AI at the neuron level to get insights into different visual indicators for fashion brands. In our deep learning model, neurons collect and process information by classifying unique characteristics and assigning it to the most likely brands. We’ve created attention maps to interpret the visual characteristics of a brand present in a single image and model the general design direction of a brand as a whole. Even more exciting, we observed that certain neurons diverged into experts or generalists when learning over time. Some neurons leaned toward specialties and became decision makers while other remained generalists. For example, once a neuron learned to identify a color like purple and a pattern like paisley, it was more likely to be called on to identify purple and paisley characteristics in the future. This is important to answer which decision-makers in our neural networks make certain judgments and helps us come closer to answering the all-important question of why certain neurons in deep learning models become decision-makers and gain more authority over time. Our work analyzes the deep neural network model to understand and characterize the types of neurons that help predict brand from a single image. Brand prediction beyond logo is an example of narrow AI—or a place where AI is more efficient than humans at a specific task. Taking a step back, AI technology should be explainable since we use it to make big decisions. As we look to the future, this application of AI and our understanding of these neurons is paving the way for answers to specific and important questions that will help reduce bias, sharpen personalization and ultimately to the improve our recommendations. By understanding brand characteristics, we can further cater the shopping experience to individuals and serve them up a truly tailored experience where everything they see is personalized to them. eBay is not affiliated with or endorsed by Vera Bradley or Adidas. For a more in-depth look into our computer vision efforts, read our latest Tech Blog .", "date": "2018-11-05"},
{"website": "Ebay-Engineering", "title": "API Mindset at eBay", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/api-mindset-at-ebay/", "abstract": "APIs allow organizations to give their partners access to data and capabilities, at scale. Extensible and adaptable API ecosystems make it easier for developers to innovate. Building such an ecosystem is a progressive journey with many technical challenges. APIs represent business and enable it to expand into new contexts and experiences. The entire API portfolio is what brings value to organizations. Individual APIs are not sufficient to allow developers to be innovative. The APIs are powerful when used together, so the portfolio dimension is what matters. Combined, our APIs give access to valuable eBay marketplace capabilities. There are great analogies between cooking and integrating with APIs. We believe that we provide healthy ingredients. It is up to our developer community to be inventive and come up with great recipes to delight their customers. “Precisely one of the most gratifying results of intellectual evolution is the continuous opening up of new and greater prospects.” - Nikola Tesla The easiest way to do APIs is to design intuitive and straightforward contracts. APIs are interfaces for people. They are for human developers. (There is intelligence beyond artificial that still matters.) The goal is to integrate with APIs without spending too much time on documentation or filing developer technical support tickets. An API is like a joke. Need for a further explanation typically indicates unnecessary complexity. In general, APIs do three simple things. (I am going to skip here all of the technical challenges that are sometimes behind the steps below.) Perform actions and gather data Format data Serve data It takes time to assess the value of an API; to understand whether releasing an API was a success or a failure. It is essential to embrace change and emphasize an API ecosystem that is flexible and adaptable. If all you have is a hammer, then every problem looks like a nail. When it is challenging to meet developers’ needs, the API owners should change the perspective and look for a different solution to improve the API portfolio. Typically, there are many ways to solve the problem. Building a developer ecosystem is hard, with so many technical (and other) challenges, but if done well, it expands in the right direction. In the commerce world, disruptive forces are powerful these days. Augmented and virtual reality and artificial intelligence, with speech and image recognition, and all sort of self-learning systems, are everywhere. These innovations have the power to dramatically reshape the buying and selling experience and transform the way people interact with the digital world. At eBay, we are exposing such capabilities through APIs. Search by image and machine text translations released last summer is just the beginning. On the other side, sometimes API capabilities disappear for various reasons: business policy or strategy changes, not meeting business objectives, challenging to integrate with. So, there was a need also to standardize this aspect of the API lifecycle. At eBay, we have specifications for both API versioning and deprecation. Versioning is a controversial topic among developers. It is one of the most common debates in the API world. After dealing with improperly versioned SOAP-based APIs for 10+ years, we decided to change the approach with our new modern RESTful APIs. We follow semantic versioning and have alpha, beta, and general availability quality levels defined. Also, we support experimental APIs. The purpose of such capabilities is exploring, testing, and getting feedback from our API early adopters and partners. Our API deprecation standard is focused on both the API contract and runtime behavior. The deprecation specification applies to all API quality levels: alpha, beta, and general availability as well as experimental capabilities. Deprecating an API version, or entire API is the final stage of the API lifecycle. It is the best option when the API does not meet business goals. Some of the reasons for the API deprecation are: Business policy or strategy change Experimental capability/API no longer serves a business objective Inefficiency Low usage Elements of the API that could be deprecated are: Resource/method Field Enumerated value Query parameter HTTP header Behavior API (major) version Entire API across all major versions Deprecated API elements are maintained in their original form for 18 months. Exceptions to this policy are possible and apply in case of critical business decisions. A transparent public announcement accompanies deprecation of any API element. It contains detailed instructions to developers, including migration plans if applicable. The API deprecation announcement is published on our Developer Portal and may include other communication channels. In some cases, deprecated elements cause backward incompatible changes. It is on the API product owners to decide on the approach: proceeding with deprecation or releasing a new major version. We use the deprecated attribute from the OpenAPI specification to declare deprecated API elements: methods, fields, query parameters, and headers. Also, we change the OpenAPI specification description attribute to reflect the deprecation. The following example is for the deprecating actualDeliveryDate field. For any new deprecated API element, we release a new minor API version. Such a release requires an API documentation update, and it includes the new API contract and proper communication to developers. In runtime, we leverage a standard Warning HTTP response header with the 299 code to reflect deprecated API elements. The purpose of the Warning header is to be logged and monitored by developers. According to the HTTP Protocol specification , 299 code (Miscellaneous Persistent Warning) is not a dedicated code for deprecation. To simplify the consumption of this information on the client side, we use the 299 code for deprecated API elements only. In case of any API deprecation, the Warning HTTP header is present in every single applicable response irrespective of the HTTP status code. When a method, a query parameter, or an HTTP header is deprecated, the Warning header is part of every response coming from a deprecated method or a method having query parameter or HTTP header deprecated. In case of a deprecated field, enumerated value, or behavior, responses from all of the methods where these elements are used or relevant have a Warning header specified. When an API major version is deprecated, every method’s response includes a Warning header. In case of an entire API deprecation, this applies to all of the API major versions. API clients should log the HTTP deprecation header and have proper alerts and monitoring in place. Instructions and examples are documented on the eBay Developer Portal as part of Best Practices for Integration with eBay RESTful Public APIs . Warning: <warn-code> <warn-agent> <warn-text> [<warn-date>] <warn-code> A three-digit warning number. It is always 299 (Miscellaneous Persistent Warning) to reflect the deprecated API element(s). 299 code here is not HTTP response status code; it is just part of the Warning header content. <warn-agent> The warn-agent value is API hostname. Examples: api.ebay.com, api.sandbox.ebay.com <warn-text> The Warning header includes the warning text, which contains brief information about the deprecation. The warning text has a link to public API documentation explaining the deprecation. The warning text is intended to be presented to a human user or logged. <warn-date> The Warning header includes the deprecation date. The format is the same as HTTP Date header as defined by RFC 7231 Date/Time Formats . Multiple warnings, with the same 299 code and different warning text, may be generated in response. This is to reflect multiple API elements to be deprecated at different point of time. Return policy fields restockingFee and extendedHolidayReturnsOffered are deprecated in Account API. Below Warning HTTP header is present in all methods defined for return_policy resource. Version 1 of the Account API is deprecated. Below Warning HTTP header is present in all methods under version 1 of Account API. We considered standard Sunset HTTP header to convey the fact of API deprecation to clients but discarded that idea due to the following reasons: The Sunset response header is still in draft . The Sunset response header indicates the upcoming retirement of a resource or an API. It is not ideal for deprecation of other API elements: fields, enumerated values, and behavior. There is no support for multiple Sunset headers to communicate multiple deprecations happening at a different time. The Sunset response header contains a single timestamp when a resource is expected to become unresponsive. We are also following this year’s efforts to introduce (and standardize) a Deprecation HTTP response header to signal the API deprecation to clients. “We are what we repeatedly do. Excellence, then, is not an act, but a habit.” - Aristotle APIs are intermediaries that enable applications to interact. They are building blocks for developers to innovate and create their products. On the other side, SDKs are tools that simplify working with building blocks by abstracting some of the concepts like various cross-cutting concerns. SDKs enable developers to create new applications or new building blocks. This year, we continue with open-sourcing tools for APIs. We recently released C# and Python OAuth client libraries that simplify integration with our APIs. This is in addition to previously open-sourced Java OAuth client library . For proper authorization and to address data privacy concerns, we leverage the industry-standard OAuth 2.0 protocol . All eBay APIs require clients to be authorized to use our marketplace capabilities. When leveraging our libraries, a few lines of code are sufficient for the integration with eBay OAuth services. Feed API exposes the greatest selection of inventory to our partners. This includes new, everyday items as well as rare goods — if something exists in the world, it is most likely for sale on eBay. Partners use our data feeds to create a rich selection of items and enable their users to find their version of perfect. Feed SDKs abstract downloads and large file manipulation and simplify typically complex inventory curation. They allow authorized partners around the globe to programmatically access the vast selection of eBay items and curate the inventory of their choice to fit their business models. After last year’s Java Feed SDK , we recently released the Python client . At eBay Connect last summer , we announced that OpenAPI documents are available for our RESTful APIs . We demonstrated that integration with read-only capabilities takes a few minutes. Our libraries simplify this further. Instead of delivering black boxes, we open-sourced SDKs to provide full transparency to developers into what is happening in their integrations. (And we welcome contributions from the developer community!) Our API gold rush continues. We have passionate teams with a common goal: to craft APIs engineers love and to deliver a world-class developer experience that enables partners to innovate. We are announcing improvements and additions to our API portfolio next week at eBay Connect. Stay tuned! Wikipedia contributors. (2019, May 15). Law of the instrument. In Wikipedia, The Free Encyclopedia. Retrieved 05:47, May 25, 2019, from https://en.wikipedia.org/w/index.php?title=Law_of_the_instrument&oldid=897198823 .", "date": "2019-06-19"},
{"website": "Ebay-Engineering", "title": " eBay’s New APIs Enable Developers to Create Innovative Experiences at Scale", "author": ["Gail Frederick"], "link": "https://tech.ebayinc.com/product/ebays-new-apis-enable-developers-to-create-innovative-experiences-at-scale/", "abstract": "Managed Payments, Identity and Marketing APIs available today for developers APIs are first-class products at eBay. We continue to add capabilities into our public API portfolio so third-party developers in our eBay Developers Program can create powerful experiences and manage their eBay business at scale. Our goal is to empower developers, enhance the ecosystem of eBay applications, and add value for our buyers and sellers. Starting today, eBay is launching new payments API capabilities that will give our third-party developers access to eBay’s new managed payments program, creating more efficient ways to integrate their businesses. The new Finances API provides developers with a more holistic view of transactions on eBay to more seamlessly manage payouts to bank accounts for sellers enrolled in managed payments, get transaction details, and help sellers with accounting and reconciliation. Finances API is an alpha release intended for developer evaluation and feedback. The Account API adds a new capability to determine whether a seller has opted-in to the managed payments program. And, we added a new refund capability to Fulfillment API that enables managed payments sellers to issue full or partial refunds to their buyers. We have also announced enhanced Promoted Listings capabilities with the Recommendation API . This API provides guidance for Promoted Listings to help sellers optimize their advertising strategy by leveraging trending ad rates and recommended listings. Our third-party developers can use this new capability to competitively elevate sellers’ visibility in sponsored placements across eBay. Reebok saw great success in following eBay’s Promoted Listings recommendations. They promoted more than 100 items at eBay’s trending rate for their multi-quantity inventory. Over the past 5 months, Reebok has seen a 55% lift in overall sales and a 142% lift in sales for the items that were promoted. Additionally, we’re rolling out more new APIs to our developers to help them expand and evolve their offerings, including: Login with eBay - Sign in for new users can feel cumbersome if there are too many steps. We are taking away friction by allowing third-party developers to delegate authentication to eBay by simplifying their integration and improving their new user experience. No need for local authentication or managing identity moving forward, now using Login with eBay, developers will be able to tap into our authentication services to verify their user’s identity. Catch , a new shopping experience created by eBay in Germany, brings fun, inspiration and value by offering cool, trendy and unexpected products at the best prices. Catch has already integrated Login with eBay into its sign-in flow. Login with eBay is currently available for all developers. Login with eBay - Sign in for new users can feel cumbersome if there are too many steps. We are taking away friction by allowing third-party developers to delegate authentication to eBay by simplifying their integration and improving their new user experience. No need for local authentication or managing identity moving forward, now using Login with eBay, developers will be able to tap into our authentication services to verify their user’s identity. Catch , a new shopping experience created by eBay in Germany, brings fun, inspiration and value by offering cool, trendy and unexpected products at the best prices. Catch has already integrated Login with eBay into its sign-in flow. Login with eBay is currently available for all developers. Volume Pricing in Marketing API - Selling more of the same thing allows sellers to save money and time, and move product more quickly. Sellers can now offer a volume discount to their buyers when they buy multiples of the same item. If sellers can sell more than one item to the same buyer, it’s a bigger win. The seller spends less time managing orders, packing items, and can save on shipping costs. Volume Pricing in Marketing API - Selling more of the same thing allows sellers to save money and time, and move product more quickly. Sellers can now offer a volume discount to their buyers when they buy multiples of the same item. If sellers can sell more than one item to the same buyer, it’s a bigger win. The seller spends less time managing orders, packing items, and can save on shipping costs. Offers to Buyers in Negotiation API - This new capability allows Sellers to send offers to buyers that are watching their product with an exclusive discount. Buyers will instantly get notified on their mobile and desktop devices and can accept the offer or counter with the price they would like to pay to purchase. This creates more engagement and opportunities for ecommerce between our buyers and sellers. This capability will be available later this year. Offers to Buyers in Negotiation API - This new capability allows Sellers to send offers to buyers that are watching their product with an exclusive discount. Buyers will instantly get notified on their mobile and desktop devices and can accept the offer or counter with the price they would like to pay to purchase. This creates more engagement and opportunities for ecommerce between our buyers and sellers. This capability will be available later this year. Best Offer Support in the Inventory API and Merchant Integration Platform (MIP) feed solution - Selling applications that use the Inventory API and MIP will be able to adopt and offer eBay’s Best Offer capabilities to buyers. This capability will be available later this year. Best Offer Support in the Inventory API and Merchant Integration Platform (MIP) feed solution - Selling applications that use the Inventory API and MIP will be able to adopt and offer eBay’s Best Offer capabilities to buyers. This capability will be available later this year. Aspect Guidance in Taxonomy API & Trading API - eBay updated the Taxonomy API and Trading API to provide guidance on important item specifics that are required, recommended, and optional for sellers to add into their listings. Required item specifics are the most-searched-for aspects by buyers. Our guidance to sellers is intended to boost their listing visibility. Our algorithms use filled item specifics from these three classifications to power accurate search recall, create search-engine friendly links to seller’s listings as well giving those listings exposure in our search result filters on mobile and desktop. Item specific guidance takes the guesswork out of knowing which item specifics are important. Aspect Guidance in Taxonomy API & Trading API - eBay updated the Taxonomy API and Trading API to provide guidance on important item specifics that are required, recommended, and optional for sellers to add into their listings. Required item specifics are the most-searched-for aspects by buyers. Our guidance to sellers is intended to boost their listing visibility. Our algorithms use filled item specifics from these three classifications to power accurate search recall, create search-engine friendly links to seller’s listings as well giving those listings exposure in our search result filters on mobile and desktop. Item specific guidance takes the guesswork out of knowing which item specifics are important. Consumer Selling API - Earlier, we released the Buy API, that enables buyers to find and purchase inventory from the eBay Marketplace in partner experiences. This year, we are announcing a Consumer Selling API to be released later in 2019. This new API enables users on partner marketplaces to also list their inventory on eBay Marketplace, giving those sellers the additional reach of eBay’s 180M buyers. Consumer Selling API - Earlier, we released the Buy API, that enables buyers to find and purchase inventory from the eBay Marketplace in partner experiences. This year, we are announcing a Consumer Selling API to be released later in 2019. This new API enables users on partner marketplaces to also list their inventory on eBay Marketplace, giving those sellers the additional reach of eBay’s 180M buyers. Open Source Tools for Integration with APIs To simplify integration with APIs, eBay has released open source SDKs and client libraries for processing feeds and integration with eBay OAuth services. These releases complement our current open-source tools, Java OAuth client library and Java Feed SDK . We are excited to see how the developer community will innovate with these tools and look forward to partnering with developers to expand our open-source offerings. Additionally, each year we bring together our most influential and innovative third-party developers to learn about the latest APIs in our portfolio and how to integrate them into their eBay experiences. Today, eBay kicks-off its third annual eBay Connect developer conference at our San Jose Headquarters. eBay APIs/Developers By the Numbers Since October 2016, eBay’s Buy APIs have generated $550M in GMB globally. eBay serves ~150 - 250M API calls per hour. In an average week, eBay serves ~30B API calls. In Q1 of 2019, external developers used our public Sell APIs to: Created more than 700,000 new listings Managed about 2x that number In Q1 of 2019, external developers used our public Buy APIs to: Drive more than $154M GMB globally # of transactions: 3.2M", "date": "2019-06-26"},
{"website": "Ebay-Engineering", "title": "Universal Finder: Moving Toward One Platform for All Finders", "author": ["Sachin Tilloo", "Kishore Kumar Mohan"], "link": "https://tech.ebayinc.com/engineering/universal-finder/", "abstract": "Showing our buyers relevant inventory of what they want to shop for among eBay's vast billion plus inventory is always a priority. One of the avenues we use regularly for filtering is what call \"Finders,\" which are shown in the Search and Browse experiences. This article explains our efforts of unifying our finders and moving them towards a universal finder platform. If you’ve ever looked for a car, tire, or parts for your car on eBay you probably would have seen something like these on search, listing details, category browse, product pages, motors home page, etc. Figure 1. Find Results on the Tire Finder page Figure 2. Find Results on the Vehicle Finder page Figure 3. Finder on the Parts Finder page Figure 4. Finder on Category Browse page We call these finders , simply because they help make your shopping journey easy to “find.” Inspired by the experience service architecture ( Experience service 101 ), we started exploring how our existing legacy architecture should change to get the benefits of the new stack. There were lots of problems we had in the existing stack: Tight integration with individual domains. The code for the finder was in each hosting page's codebase search, view listing, etc. Each finder had small tweaks related to the domain, which meant any change would mean a rollout on each of the partner pools. For instance, the addition of new field—say “Drive Type”—would mean changing all the partner code base and rollout to partner pools (search, view listing, etc.). Higher operational and maintenance costs Slower time to market time for new features, experiments, and enhancements Inconsistencies due to all of the above in the experience We wanted to fully utilize the power of experience services and build modules, not pages (see “ Don't Build Pages, Build Modules “). With this in mind, we started exploring the module provider architecture to see how that would help us solve all the pain points mentioned above. This is how it looks like at a high level. Figure 5. High-level module provider architecture We have clients calling experience services, which call module providers that are responsible for a respective module that, in turn, can call the domain services. Breaking each of these down further for the finder use case: Figure 6. Universal Finder workflow The Universal Finder Module provider in the above diagram represents one experience module provider service that is called from different experience services for the hosting pages (search, view listing, category browse, product detail pages, etc.) depending on the underlying page. Since this is an experience module provider, it knows the underlying page and can render the finder component based on it. This way we have one finder that all the client/experience services integrate with, but can render any finder dynamically based on the inputs—encapsulating all the business/domain logic at one place as a component. This also aligned with the microservices architecture since we are separating out core logic for finder in a microservice whose purpose is do one thing across all pages. Figure 7. Separating out core logic for finder in a microservice At a high level, here is how the new architecture works: Clients (desktop web, mobile web, native) call the relevant experience service that is responsible for identifying (web vs native) and selecting the appropriate set of modules, taking experimentation into account. For each request, these experience services, in turn, call the universal finder module provider once they know that for this given input combination (category, keywords, experiment etc.), there might be a finder that needs to be shown. The Universal Finder module might have its own finer-grained experimentation, tracking, and localization data and calls the domain service to get raw domain data for the finder. Each of the domains (finder for parts, finder for tires, finder for vehicles, finder for electronics, etc.) registers its components and rules with the Universal Finder. The domain service has registered rules that trigger based on the inputs to a rule. These rules might be like : Check if input category is X Check if keywords(search) are in or not in Y Check the region of the request (US, UK, etc.) Check if the request is for EPID (product) listing Call a service to perform any complex logic as a rule Check if input category is X Check if keywords(search) are in or not in Y Check the region of the request (US, UK, etc.) Check if the request is for EPID (product) listing Call a service to perform any complex logic as a rule All the rules configured above per domain get fired in parallel and finally, for the given set of inputs, one domain is declared as the winner. Once a domain wins (say finder for tires) we rely on that component in the Universal Finder to orchestrate and make all the necessary finer calls to get the data it needs for serving the finder. For example, in case of the tire finder, it might call series of domain services to get the list of all vehicles, to get the list of all valid tire sizes, and maybe to get the list of any user-saved vehicles from previous sessions. The Universal Finder domain service then aggregates and sends the data back to the experience module provider, which would then send it back to experience services in a standardized schema for all the finder modules. The above architecture strives to isolate and encapsulate the core business logic for putting a finder from each of the domain pages (teams). The idea is to have one team responsible for the finder as a service that works with dedicated verticals (parts, tires, vehicles, etc.) to power the experience. This architecture drastically reduces time to market and release timelines for new finders features/enhancements.", "date": "2018-10-17"},
{"website": "Ebay-Engineering", "title": "Seven Tips for Visual Search at Scale", "author": ["Fan Yang", "M. Hadi Kiapour", "Robinson Piramuthu", "Qiaosong Wang"], "link": "https://tech.ebayinc.com/research/tips-for-visual-search-at-scale/", "abstract": "We present seven tips for visual search at scale, based on our KDD 2017 paper titled \"Visual Search at eBay.\" We had the pleasure of being part of the panel session on visual search at ODSC West 2018 on Nov. 2, 2018. In this article, we summarize our presentation, which was based on our paper “ Visual Search at eBay , ” presented at KDD 2017 . Imagine that you are in a store that looks like a large warehouse, with numerous aisles that are identified by a unique positive integer. eBay has over 1B live listings at any given time. So, if eBay were such a store, it could very much look like the infinite stockroom in the 1999 movie Matrix . You have a page from a printed catalog of plumbing parts and are on a quest to find one of them for which you do not have a model number. You seek a store assistant and point at the plumbing part on the page. The assistant directs you to few aisles, say aisles 183, 5276, 14098. Without this information, you would have to walk through every single aisle and compare the picture of the plumbing part with each item on the aisle. This comparison task gets very complex if parts look very similar. In this case, you would pay attention to every single detail such as color, shape, size, brand, packaging, etc. If the aisles were organized by the type of items, you would sample a few from each aisle and then spend more time at relevant aisles. This is a very complex task, where if you miss the item of interest, you may never find it in a single pass through all the aisles. Algorithmic search essentially simplifies this process when it comes to large scale search. As mentioned in our KDD paper, we train a neural network to predict leaf category from a given image (see Figure 1). These leaf categories are much like the different aisles in our “warehouse.” This neural network-based classifier acts like our store assistant who identifies the potential aisles. We use a top few potential leaf categories selected based on probabilities predicted by the softmax layer in our neural network. Figure 1. Listings are organized by a hierarchy of categories where the terminal entries are called “leaf categories.” These labels can be used to train a deep neural network to predict leaf category, given an image. Once we are at an aisle, we need to know how to compare two images (the query and an item on the shelf). We represent each image by a compact signature in the form of a series of numbers represented as a vector . This signature is also extracted by the same neural network using weak supervision. We extract a binary vector (made up of 1's and 0's) by training the network with a sigmoid layer to predict top leaf categories. It is best to use as much supervision as possible at all steps. Since we have a lot of diverse data for a leaf category, we train the network to predict the leaf category. You can look at “ Exploring the Limits of Weakly Supervised Pretraining ,” ECCV 2018 to appreciate the power of pretraining a large network and then transferring it to a different task. This weak supervision along with data augmentation (such as crop, flip, rotation) during training helps the network to discount the background and focus more on the important parts of the object that matter, giving rise to a compact semantic signature. These signatures can be easily compared. In the case of binary signature, as in our paper, we can count the number of matched bits based on Hamming distance . Matched items from the selected leaf categories can be ranked based on this similarity. The more matched bits the better. Key challenges we face in large scale visual search for commerce include Variation in image quality and composition Large inventory to compare Inventory is heavily fine-grained Need simple architecture and models for easy maintainability In the following, we summarize tips to build a large-scale visual search system. A lot of these tips could be applicable to non-visual or multimodal systems. 1. Understand the data, and use stratified sampling Data is principal in determining the strategy for the entire procedure. The quality of image can introduce complexities and needs to handled carefully. It is important to understand both the images in the inventory as well as the images typically uploaded by users during visual search. Figure 2 depicts a visualization of a subset of the eBay handbag inventory. It has a mix of complexities based on background as well as attributes of the handbag. Figure 3 highlights another situation where the same object can be captured so many different ways when it comes to camera pose. It is critical to use a diverse and representative set of images when we train our neural network so that these situations can be handled by the neural network. Figure 2. Visualization of images of handbags on eBay using t-SNE. Notice that about 40% of the images have a difficult background, slightly over 10% have a white background. User-uploaded images have even more variations, such as rotation. Figure 3. An eBay item titled “Ferrari Puma~Red Leather Sport Shoes Sneakers~Men’s Size 8,5 (Women’s Size 10).” Same item, very different pictures! The size of the training set is determined by factors such as the number of labels we want to predict, the diversity of data within each label, memory and compute constraints imposed by the training infrastructure, and the time budget to train such a system. When we create the training set for our neural network, we use a stratified sampling over leaf category, sellers, condition, brands, etc., and finally remove duplicates, in order to make sure we capture the richness in diversity of data. 2. Data augmentation is critical, specifically rotation Data augmentation is a critical step in training neural networks when the training data does not capture all variations that are likely to happen in real use case. When a user takes a picture of an object using a mobile phone, it is very likely that the object is not properly zoomed in/out, is cropped out, is rotated, is blurred, etc. Data augmentation (Figure 4) produces variations in data synthetically, especially when these variations are more likely to happen, but are not present in the training set based on eBay listings. It is not very likely that the object is rotated much in eBay listings. However, it is normal in user uploaded images. So, we observed that rotation is an important operation in data augmentation. Rotation is often ignored in data augmentation but is important for visual search. Figure 4: Data augmentation allows us to synthetically create more variations from a single sample. Here is a selected few data augmentation operations. Rotation is often ignored, but is very important. 3. Extract the semantic signature with as much supervision as possible As mentioned in the introductory text, it is very important to use as much supervision as possible. This helps in training the classifier to focus on informational content and discount other non-informational regions. It is best to leverage large, diverse data with low acquisition cost for strong supervision (such as leaf category prediction) when labels are not available for the actual task (measure similarity between pairs of images). 4. Analyze the entropy of signature This step is usually ignored in many system designs for large information retrieval systems. It is critical to assess if we effectively pack information within a given capacity of signature. For example, if we use 8 bits to represent the binary signature, we could represent up to 2 8 unique concepts. In the optimal case, each bit takes the value of 1 with a frequency of 50%. We could calculate the entropy of the system to get the effective bit length and compare with the actual bit length. It is good to allow some slack to account for redundancy in the system, in case some bits are affected by noise and perturbations (Figure 5). Figure 5. Optimal bit occupancy is vital for optimal information representation. This picture is from our paper and corresponds to ImageNet . 84.1% of bits are active (=1) on 45% to 55% of data. It is good to have some redundancy in the system so that it is not exactly 50% for all bits. 5. Within-class variance is important when labels are coarse We use coarse leaf category labels instead of product IDs to train the neural network. This is partly because leaf categories, although coarse, are more readily available. Several items in categories such as clothing and furniture do not have product IDs. Typical classification systems aim for minimal within-class variance. The ideal case is when variance is 0. Here, all samples from a class collapse to a single point (see Figure 6). For example, all samples from athletic shoes will be collapsed to a single point. But, there are numerous unique products that fall under the leaf category “athletic shoes,” and we want to be able to find them using signature similarity. So, we claim that when labels are coarse and fine-grained search is desired, between-class variance should be high, but within-class variance should also be high. This can be measured by looking at the entropy of the signature, as discussed in the previous tip. Figure 6. The recommendation for classifiers is to have small within-class variance and large between-class variance. However, we argue for a better fine-grained search, and it is important to have large between-class variance, but also large within-class variance. Fine-grained matching is not possible when the points in each cluster collapse to a single point. This figure shows five classes from ImageNet, where samples from the same class belong to the same type (equivalent to product), plotted using binary signatures. Within-class variance becomes more important for large scale visual search for commerce, where we use coarse class labels (leaf category) and aim for fine-grained search (product). 6. Process of elimination for speed and precision The process of elimination is very powerful when tailored for high speed and precision. For example, if the input image contains an athletic shoe, there is no point to search inventory for dress, table, computer. We showed in our KDD paper that it is very effective to use a strong classifier to predict the top few potential partitions (leaf categories/aisles) to reduce the search space and also to improve precision (dress signature will not get confused with shoe signature). 7. Absolute vs. cumulative top-k partitions We use a strong classifier, a deep neural network trained using strong leaf category labels, to predict the top partitions (leaf categories/aisles). When confidence of top prediction is high, there is no need to search other partitions. However, when top prediction is uncertain, it is best to include other competing partitions. So, we recommend to use cumulative top-k for better precision and absolute top-k only for those situations where exact match is desired even at higher cost. See Figure 7 for details. Figure 7. Assume that we use top-5 predicted partitions (leaf categories) to narrow down the search space by the process of elimination. We recommend using top-5 based on cumulative score rather than absolute score. Use absolute score only when precision is not as important as recall, such as in the scenario to find the exact match at any cost. You can find details in the KDD paper . We show three scenarios in this figure with a score threshold of 0.9. A cumulative score uses more categories (as shown by gray cells in a, c) when the confidence is low and uses few categories when confidence is high (as shown by gray cells in b). We presented seven tips above. In addition, we recommend minimizing humans in the loop (including for evaluation), retrain models periodically to cope up with shift in data distribution, and keep the number of models to a minimum for easy maintainability. Hopefully, this article gave you some insights into why these tips are important. Please read “ Visual Search at eBay ,” KDD 2017 for more details.", "date": "2018-11-29"},
{"website": "Ebay-Engineering", "title": "NuRaft: a Lightweight C++ Raft Core", "author": ["Gene Zhang", "Jung-Sang Ahn"], "link": "https://tech.ebayinc.com/engineering/nuraft-a-lightweight-c-raft-core/", "abstract": "We are excited to announce the public release of NuRaft, a lightweight C++ Raft core, under the Apache 2.0 open source license. NuRaft is based on the cornerstone C++ Raft implementation, but with various additions and changes, and is the result of over two years of development and testing for production use within eBay for storage server data replication. This post discusses what NuRaft is, and how it can be used. We are excited to announce the public release of NuRaft , a lightweight C++ Raft core, under the Apache 2.0 open source license. NuRaft is based on the cornerstone C++ Raft implementation, but with various additions and changes, and is the result of over two years of development and testing for production use within eBay for storage server data replication. This post discusses what NuRaft is, and how it can be used. A consensus protocol, such as Raft , can be used for strongly consistent replication by servers to achieve high availability and high read throughput with replicas. It can also be used to coordinate distributed computing agents with a globally consistent ordering. University researchers as well as developers at companies who need an efficient C++ replication protocol for their data replication or distributed log stores to support distributed transactions, as we do at eBay, can benefit from it. The NuRaft protocol requires at least three servers or virtual machines to tolerate one failure, although you can run three processes on a single machine for testing and learning purposes. eBay’s NuData initiative aims to develop and operate new-generation, cloud-native database services for eBay’s core businesses, leveraging open source and contributing to the open source community. NuRaft is the first graduate from our overall effort, which also includes a LSM-tree-based storage engine ( Jungle ), a replicated high-performance log store (which works with distributed knowledge graph store Akutan ), a multi-purpose log player, a multi-master NuKV service (with optional session guarantees ), a distributed transaction protocol ( GRIT ), a transactional graph store (NuGraph), a transactional doc store, global secondary indexes (GSIs), CDCStreamer for change data capture, NuSQL, and machine-learning-based anomaly detection and prediction (GRANO - demo at VLDB 2019 ). Over two years ago, when we started to look for a robust, efficient data replication component, we analyzed a few open source choices. Given our development environment, we needed a C or C++ implementation of a consensus protocol, such as Multi-Paxos, or Raft. After some hands-on prototyping and evaluation using some of the options, we settled on the cornerstone C++ Raft implementation. It’s lightweight, has the least dependencies (on ASIO only), and yet is functionally complete in terms of cluster management, recovery from peers, in addition to including basic replication features. It provides an interface for log store plugin, state machine, as well as configuration parameters. The following diagram illustrates the interface and its main calls. As you can see, NuRaft allows flexible design for users through User-defined Log Store and User-defined State Machine with a fixed set of APIs. Some significant enhancements in NuRaft over cornerstone Raft include: SSL/TLS support. Logical snapshot: original snapshot recovery from peers is based on the assumption that a snapshot is a file. We added a logical snapshot interface. Leader election: pre-vote and priority for leadership. Pre-vote addresses a problem of annoying members that interrupt the current legitimate leader from an isolated part of the network. Learner only member. Custom quorum sizes for commit and leader election. Asynchronous replication. The following diagram depicts the replication flows for both strongly consistent and asynchronous replications. The main differences between the two are when to execute the requests and return results to clients and quorum sizes. For strongly consistent replication, the requests are executed at commit time, while for async replication, the requests are executed at pre-commit time before real replication occurs. Logical snapshot support We needed a snapshot interface that is not file only, but a snapshot in a KV store. Physically creating a file as a snapshot is quite expensive and disruptive to the normal operations in many application scenarios. Instead, logical snapshots would be much efficient. For example, when we started implementing the replicated storage servers, we used ForestDB as the underlying storage engine, and it provides a logical snapshot capability. That’s the reason we introduced this support. Alternatively, if a snapshot can be established from a copied file that was changing but contains logical snapshots, shipping a changing file as a snapshot could work, but we didn’t take that approach. Avoiding disruption of a leader by an unstable member The pre-vote mechanism is to avoid disrupting a leader by an unstable member. The original protocol will force a leader to step down when a follower sees a new term and agreeing to a leader vote request from a member disconnected from the leader. During pre-vote, the voter checks if it received heartbeat from the leader recently before its election timer expires, that means the leader is possibly alive. Then the node rejects the pre-vote request and the vote initiator will not move forward. With pre-vote, a legitimate leader will not step down just because a follower is temporarily out of touch with the leader while the majority of the members are working properly. Reading latest values from the leader requires non-overlapping leadership The leadership expiration is to avoid, in rare cases of a leader being isolated from the rest, more than one leader acting at the same time. It is important to avoid multiple simultaneous leaders when we use reads from the leader as the latest values. To tackle this problem, we introduce a parameter leadership_expiry_ to set the expiration time of the current leader who needs to receive responses from quorum nodes within this expiration time to maintain its leadership. Priority and learner-only membership Sometimes you would like to have leaders be in certain data centers, or have a learner-only member (with priority 0) for backup purposes. Priority is used to achieve that, although it’s not guaranteed to strictly follow the priority. A member with priority 0 will be guaranteed not to participate in a leader election. Custom quorum sizes for commit and leader election This helps to achieve the same functionality of Flexible Paxos . Overlap between the leader election quorum and replication commit quorum as in flexible Paxos is not enforced. It’s your responsibility to choose the right values for your scenarios. Asynchronous replication Since NuRaft is lightweight and efficient, we want to use it for replications with lower consistency requirements and lower latency. Asynchronous replication can be achieved using NuRaft by calling cluster_config :: set_async_replicatoin(true) . And we can use the same code to achieve tunable consistency levels. Note that asynchronous replication may cause data loss. If you are ready to explore NuRaft, go for a quick tutorial in the repo. Our goal is to make NuRaft the best standalone C++ Raft core open source implementation in the industry. Your contributions, in forms of bug reports, pull requests, feature requests, test cases or examples, or benchmarks, are all welcome.", "date": "2019-08-14"},
{"website": "Ebay-Engineering", "title": "SRE Case Study: Mysterious Traffic Imbalance", "author": ["Charles Li"], "link": "https://tech.ebayinc.com/engineering/sre-case-study-mysterious-traffic-imbalance/", "abstract": "As an architect of a large website, I spent over a decade of my life working on all kinds of troubleshooting cases. Many of those cases were quite challenging, similar to finding a suspect in a megacity, yet quite rewarding. I ended up with many Sherlock Holmes stories to tell. What I am sharing today is a troubleshooting case of mysterious traffic imbalance. Once upon a time there was a website. I am going to call it foo.com, but the name doesn't really matter. Feel free to replace it with any name that sounds better to you. Foo.com had two data centers, Miami and Denver, running in active-active mode for business continuity and disaster recovery. Web traffic was evenly distributed between the two data centers by round robin DNS. If you haven't heard about round robin DNS, the way it works is quite simple. As foo.com runs in two data centers, the foo.com name is registered to two IP addresses in Domain Name Server (DNS). The IP address in Miami is 100.100.100.100, and the IP address in Denver is 200.200.200.200. When clients browse foo.com, the first thing they do is to resolve the name into IP addresses. For each of the name resolution request, DNS server returns the two IP addresses in alternative order. For instance: The first client asks: what are the IP addresses of foo.com? DNS answers: The IP addresses of foo.com are 100.100.100.100 and 200.200.200.200. The second client asks: what are the IP addresses of foo.com? DNS answers: The IP addresses of foo.com are 200.200.200.200 and 100.100.100.100. Each client selects the first IP address in the response, so the first client talks to 100.100.100.100 in Miami, the second client talks to 200.200.200.200 in Denver, so on and so forth. When there are millions of clients, the end result is that both Miami and Denver would receive approximately the same amounts of traffic. It had been working like this for many years until mid-2007, when the Site Reliability Engineering (SRE) team noticed that Denver started getting slightly more traffic than Miami. The discrepancy was under 1%, which wasn't significant enough to cause any impact. It just seemed to be strange as it never happened before, so the SRE team opened a case and started to monitor the traffic distribution more closely. After several weeks of monitoring, the team clearly observed a trend that the Internet traffic from the users was shifting to Denver slowly and consistently, from 1% to 2% to 3%. At this point, the severity level of the case was raised and more engineers were grouped together to figure out the root cause. The team identified the related components in the data flow and checked all of them. They verified that the DNS systems did return the IP addresses in round robin fashion. They verified that all the major Internet service providers were not having any significant outage. They analyzed the traffic in Denver and Miami to see if the extra traffic came from a specific Internet service providers, or a specific country, or for a specific URL, but nothing stood out. They verified if the report generating system was working properly, and confirmed that the report was accurate and the system wasn't missing any data. While the troubleshooting activities were taking place, the discrepancy was still growing slowly and consistently, from 3% to 5% to 10% in several weeks. 10% of traffic imbalance wasn't a problem by itself. The website was designed to absorb much higher of discrepancy. The problem was that the reason of the discrepancy remained mysterious. Such a clear and growing pattern without a clear reason was very strange. The severity level was raised, the team was still in the dark, and everyone started to feel the pressure. The first thread of light arrived two months later, when one of the engineers noticed that most of the extra traffic in Denver came from IE7 (by User-Agent header of the HTTP requests, in case you are curious). This version of IE7 was only available in Windows Vista at that time, and Windows Vista was released right before the initial report of the traffic imbalance. So the question became: why does Windows Vista prefer Denver? The reason was still unknown, but the team felt relieved as they knew the rest the troubleshooting would be easy and straightforward. Why? SRE veterans know that the most challenging phase of troubleshooting is when there is no clue. When they are troubleshooting something and feel there is no clue, it means they haven't yet collected enough data. They must keep digging wider and wider, which would be time consuming and difficult to certain extent, especially when the troubleshooting effort is under time pressure. As soon as they find a clue pointing to a certain direction, digging 100 feet deep on that direction is much easier than turning an acre of land up side down. As a Sherlock Holmes story, the second half is the deciphered version. In 2003, Microsoft proposed RFC 3484 and decided to adopt it in Windows Vista. RFC 3484 defined a \"longest matching prefix\" method for a client machine to select the server IP address from round robin DNS. Taking foo.com as an example, let's say a client whose IP address is 150.150.150.150 talks to foo.com. It asks DNS server to resolve foo.com into IP addresses. DNS server returns two IP addresses, 100.100.100.100 and 200.200.200.200. Instead of selecting the first IP address, the client will use following procedure to decide which foo.com IP it should connect to: a) Convert the IP addresses from decimal to binary (e.g. 100 = 01100100, 150 = 10010110, 200 = 11001000) Client IP = 150.150.150.150 = 10010110 . 10010110 . 10010110 . 10010110 foo IP 1  = 100.100.100.100 = 01100100 . 01100100 . 01100100 . 01100100 foo IP 2  = 200.200.200.200 = 11001000 . 11001000 . 11001000 . 11001000 b) From left to right, compare the binary string of client IP with foo IP 1 and count the length of matching bits, until the first un-matching bit is reached. The first bit of client IP is \"1\", the first bit of foo IP 1 is \"0\", so the length of matching prefix is 0 (no matching bits at all). c) In the same way, compare the binary string of client IP with foo IP 2. The first bit matches (first bit is 1 in both client IP and foo IP 2), the second bit does not match (it's 0 in client IP, but 1 in foo IP 2), so the length of matching prefix is 1 (only the first bit matches). d) foo IP 2 is selected because it has a longer matching prefix than foo IP 1 (1 vs 0). Around the same time that RFC 3484 was proposed, there were two other technologies getting popular: Broadband Internet and 802.11 Wi-Fi. More and more households switched to cable or DSL, and set up a wireless router for their home Internet access. Most of the wireless routers (such as Linksys or D-Link) were designed to assign 192.168.0.0 to 192.168.255.255 private IP range to the home computers. Those events were unrelated, until January 2007 when Windows Vista was released. Let's see what happened when Windows Vista users connect to foo.com via their wireless routers at home: Client IP = 192.168.100.100 = 11000000 . 10101000 . 01100100 . 01100100 foo IP 1  = 100.100.100.100 = 01100100 . 01100100 . 01100100 . 01100100 foo IP 2  = 200.200.200.200 = 11001000 . 11001000 . 11001000 . 11001000 Comparing client IP with foo IP 1, the length of matching prefix is 0. Comparing client IP with foo IP 2, the length of matching prefix is 1. So Windows Vista selected foo IP 2, which was in Denver. With time going, more and more home wifi users upgraded to Windows Vista, so engineers at foo.com observed the increasing traffic imbalance between their Denver and Miami data centers. Technically speaking, the \"longest matching prefix\" method may be helpful only if both client and server are on public IP addresses. It doesn't make any sense when client is on private IP address because private IP addresses are not routable on the Internet, nor would they indicate the distance to any public IP address. After the root cause was identified, the next step was to find and implement a solution to rebalance the traffic. Microsoft could not force their users to patch Windows Vista, so the engineers at foo.com had to look for solutions on the server side. What they did was to change the Denver IP from 200.200.200.200 to 100.100.200.100, so that both Denver IP and Miami IP have the same length of matching prefix comparing with the 192.168 home Wi-Fi addresses. As the result, the longest matching prefix method in RFC 3484 was bypassed, the round robin behavior was restored, and the traffic finally became balanced again. To make the story complete: Two years later in March 2009, Microsoft accepted that the \"longest matching prefix\" method is inappropriate, and fixed it in Windows 2008 R2 and Windows 7. Five years later in September 2012, RFC 3484 was finally obsoleted and replaced by RFC 6724. Reviewing this troubleshooting case, there are several principles we learned and integrated in our decision making procedures ever since. These principles are becoming more and more valuable because the cloud is getting more and more complicated, which is why I was inspired to write this article. When proposing an RFC, its impact to the entire Internet community should be carefully evaluated. The Internet is a complicated system. A technology that benefits in a one area may result in unexpected impact in the other areas. When adopting an RFC, its feasibility in currently ecosystem should be advertently considered. The Internet is fast growing system. An RFC appropriate yesterday may become inappropriate today while the ecosystem evolves. Before releasing a product, its features should be thoroughly tested in real world. The Internet is a group of systems without central governance. If an inappropriate feature gets released, fixing it could take a lot of time and effort. Have you run into similar incidents? What are your guiding principles of selecting RFCs?", "date": "2018-11-08"},
{"website": "Ebay-Engineering", "title": "Interactive Visual Search", "author": ["M. Hadi Kiapour", "Shuai (Kyle) Zheng", "Robinson Piramuthu"], "link": "https://tech.ebayinc.com/research/interactive-visual-search/", "abstract": "Interactive visual search with user feedback helps buyers find the perfect item and while enjoying the exploratory journey. In our previous article, “ Seven Tips for Visual Search at Scale ,” we discussed visual search where a user query is an image, and the results are shown to the user based on visual similarity to that query image. One could consider this as a single iteration of the search process. This is great when the user has the picture of the exact product and finds the right match in the result set in terms of price, size, condition, shipping, etc. It is still possible that the exact product is not in the result set for reasons such as product out of stock. What if (i) the user knows the exact product but does not have the picture or (ii) the user has an image of a product but wants one with a few variations from the query or (iii) the user wants to explore the beautiful product landscape? An interactive approach is natural in such scenarios where the user gives feedback after each result set. Such feedback could be based on items in the result set that had a click-through from the search result page. Results on the next (unseen) page could be updated based on images that experienced a click-through. We present an approach for interactive visual search. Although the same could be true for text or speech or multimodal search, we limit our discourse to image queries. The aim of this material is to keep it simple. More technical details can be found in our paper “ Give me a hint! Navigating Image Databases using Human-in-the-loop Feedback ,” that was presented at WACV 2019 . Watch the 5-minute video at the end of this article as an alternative. The scope of this article does not include personalization, but includes only user feedback during the search session for a single quest. However, extension to personalization is straightforward, since our approach could be generalized easily. We represent each image by a compact signature in the form of a series of numbers represented as a vector a.k.a. embedding . Similarity is then measured by proximity based on this vector. This is illustrated in Figure 1. This vector could be generated by a deep neural network, i.e. the deep neural network maps the input image to a vector. One way to train such deep neural networks is to train it using triplets and predict if they have the desired order. The resulting embedding is called “triplet embedding.” Valid triplets are constructed such that they are made up of three images (A, P, N), where A is the anchor, P is a positive sample, and N is a negative sample. The definition of positive and negative samples could be based on whether they have a specific attribute in common or not. For example, in Figure 1, (A, B, C) is a valid triplet for brand and (A, C, B) is a valid triplet for fastening type. Figure 1. Each image is mapped into a vector using a deep neural network. These vectors are used to measure similarity of pairs of images. Closer pairs are more similar than those farther apart. In the above example, products A, B have the same brand, and products A, C have buckles. Pair (A, B) should be closer than (A, C) when we want to measure brand similarity. Similarly, pair (A, C) should be closer than (A, B) when we want to measure similarity based on presence on buckles. If our vector is of dimension 1, as shown here, there is no way to capture both of these two conditions (brand, buckles) of similarity simultaneously. However, “similarity” is subjective. For example, as in Figure 1, we may be interested in measuring the similarity of images A, B, and C based on brand or based on buckles. A single mapping function cannot be used to measure similarity universally. Thus, there is a need that the mapping function adapts to the desired conditions or constraints. One way to achieve this is to use “ Conditional Similarity Networks ” (CSN) which was published in CVPR 2017 . CSN learns a mask vector for each condition. The underlying vector is common for all conditions and is modulated by the mask vector by element-wise multiplication. This modulation operation is illustrated in Figure 2. Figure 2. Illustration of modulating an embedding by a conditional mask. Values in the mask vector lie between 0 and 1 with a total sum of 1. The original embedding is shown on the left. The mask is determined by the selected condition A or B as shown in the middle. The original embedding is modulated by element-wise multiplication with the selected mask, as shown on the right. Modulation operator is depicted by the “o” symbol. “ Study the past, if you would divine the future ” - Confucius Consider the scenario where the user does not have the exact product image, but knows what it looks like. Our goal is to show a collection of images to the user where the user picks the best image that shares the most aspects of the desired product. Once this feedback is received, a new set of images is shown to the user. This new set awaits user response. This process is repeated until the user finds the desired product. This is similar to the popular “ 20 Questions ” game. A single iteration is shown in Figure 3. Figure 3. During each iteration, the user picks the image that is closest to the desired image. Our algorithm constructs the best set of images shown to the user such that the desired image appears in as few iterations as possible. Our algorithm does this in two steps. The first step involves a form of visual search to get an initial set of candidates. The second step ranks these images based on how informative they are. We use reinforcement learning to train how to rank based on information content derived from user feedback in all past iterations. Figure 4. The quest starts with an initial query. Each search iteration results in showing the user a result set which could be 1, 2, or more images. As part of feedback, the user may choose to select an image or not. This feedback is used to generate the next result set, and the process continues until the user finds the desired product. This is similar to the popular “ 20 Questions ” game. For simplicity, assume that the inventory consists of only a single image per product. We can construct a graph as in Figure 5, where each node is a product. Nodes are connected if they have at least one shared attribute. For example, nodes corresponding to two products from the same brand may be connected. These connections have weights proportional to their similarity, which is based on the distance between their embeddings. Note that these weights depend on the condition of similarity (for example, brand, buckles). Figure 5. Our setup is also novel in the sense that the entire user feedback process can be simulated, and performance can be evaluated automatically. In fact, this helps us train our deep neural network for reinforcement learning. Given a graph of inventory where each node is an image of a product (for simplicity, assume one image per product), we can randomly sample two nodes, where one is the initial query and the other is the image of the desired product. Our goal is to go from one to the other in as few steps as possible. Typical approaches requiring such user feedback construct training and validation sets using actual user feedback by crowdsource. This could result in subjective and inconsistent labels. Also, this is an expensive procedure that results in a limitation in the size of the data set. Evaluation of the approach has further complexities related to the repeatability of the experiment. This is easily addressed by simulation. Our goal is to reach the desired product in a minimal number of steps. We train a CSN based on triplets generated from the available attributes in the data set. This creates rich embeddings for each attribute (as in Figure 1). It is important to note that we create triplets based on coarse attribute labels (for example, “are A and B both purple?”) which are easily available instead of the expensive relative attribute labels (for example, “is A more purple than B?”). Figure 6 shows that even coarse labels can produce rich embedding comparable to those achieved by expensive relative attribute labels. We achieve this by using CSN and making some modifications to it (i) restrict mask vector so that all elements add up to 1 (ii) discourage large values for the magnitude of embedding vector (iii) apply global consistency constraints by considering overlap of attributes between pairs. These modifications result in about 3% absolute improvement in accuracy. See our paper for technical details. Figure 6. The visualization of an embedding using t-SNE for the attribute “closure” on UT-Zappos50k data set. Note that this captures similarity based on shoe style very well even though it was trained to predict “closure.” We were able to learn high-quality embedding with just the binary attributes (for example, “do A and B have the same brand?”) instead of the expensive relative attributes (for example, “is A more purple than B?”). Our modifications to CSN improves absolute accuracy by about 3%. As shown in Figure 4, we use nearest neighbor search (think of visual search) to sample top candidates, and then use reinforcement learning (RL) to rank them and pick the top few (say, two) to be shown to the user. As mentioned in Figure 5, we can simulate the entire procedure for interactive visual search since we can randomly select initial query and final target. RL is very effective for such cases. We use Deep Q-Network (DQN) as in “ Playing atari with deep reinforcement learning ”, NeurIPS Deep Learning Workshop 2013. DQN learns to predict the Q-value for a given set of actions from a given state. The Q-value for a given state and action is the maximum expected long-term future reward for that action from that state. For instance, in the game of chess, Q-value could be the probability of winning the game when we make a specific move (the action) from the current configuration of the chess pieces on the board (the state). Note that the short-term reward could be to take out the opponent’s pawn even though this may not necessarily increase the long-term future reward of winning the game. Thus, Q-value is a measure of the quality of action at a given state. Note that it is dependent on both the state as well as the action. The same action may not be optimal at a different state. In our case, an “action” is selection of an image from the candidate set of images from the sampler. “State” consists of relative embeddings of images with regard to the embedding of the latest query that generated the candidates. As in Figure 7, the best image has the largest estimated Q-value (as indicated by the green checkbox). The DQN consists of three fully connected layers and ReLU as nonlinearity. This is illustrated in Figure 8. As discussed in the previous section, CSN learns a mask that adapts to the condition for similarity. Thus, the sampler can produce candidates per masked embedding. A separate DQN predicts Q-values for the candidate sets from each sampler, as in Figure 9. CSN was trained first, and then DQN was trained using Huber loss (a combination of piecewise linear and squared loss) based on the expected and observed rewards of picking the top images for user feedback. Note that user feedback is simulated. The best image will be the closest to the target image (known during simulation) among all the queries picked so far. Figure 7. Simplified view of Deep Q-Network showing inputs and outputs. The sampler gives a candidate set of images. The goal of DQN is to pick the top images from this candidate set based on the estimated Q-values. Here, “action” is selection of an image and “state” is defined by the candidate set of images from the sampler. We define the relative embeddings of images with regard to the embedding of the current query image to be the state variables. Also see Figure 8. Figure 8. Our Deep Q-Network contains three fully connected layers with ReLU as non-linearity. Also refer to Figure 7. Figure 9. Complete architecture to train the neural networks. This figure shows loss layers for both CSN and DQN. We use ResNet-18 as a backbone for the CSN. The architecture for DQN is shown in Figure 7. The sampler produces nearest neighbors based on the masked embedding. Huber loss is used to train the DQN based on the expected and observed rewards of picking the top images for user feedback. Figure 10. Sample navigation using our approach. Each row shows the traversal initiated by a query shown on the left. The desired products are highlighted in green boxes at the right. Although not immediately obvious, consecutive products share specific attributes. See Figure 10 for a qualitative illustration of navigation for various input queries. Every selected image has at least one common attribute with the previous one. The average agreement of human annotators with our simulated approach was about 79%. We observed a reduction in number of steps by 11.5-18.5% when we use our DQN approach, compared to competing hand-engineered rules to rank. The reduction is 17-27% when compared to nearest neighbor. See our paper for technical details. We presented a scalable approach for interactive visual search using a combination of CSN (for conditional masked embedding), DQN (no hand-engineered rules), and simulation (train without human-in-the-loop and at scale). Our approach can be easily extended to multimodal data as well as personalization. Improvements in Embedding No need for expensive relative attribute labels No need for expensive relative attribute labels Modified CSN Modified CSN ~3% absolute improvement in accuracy (UT Zappos 50K, OSR) ~3% absolute improvement in accuracy (UT Zappos 50K, OSR) Improvements in Navigation Learn to rank triplets and select candidates using DQN Learn to rank triplets and select candidates using DQN 11.5-18.5% reduction in number of steps when compared to competing hand engineered rules to rank 11.5-18.5% reduction in number of steps when compared to competing hand engineered rules to rank See our paper for technical details, or watch the short video below. Interactive Visual Search from eBay Newsroom on Vimeo . This is collaborative work with Bryan Plummer who was our summer intern in 2017 and the primary author of our paper .", "date": "2019-01-22"},
{"website": "Ebay-Engineering", "title": "Explainable Reasoning over Knowledge Graphs for Recommendation", "author": ["Dingxian Wang", "Canran Xu", "Hua Yang", "Xiaoyuan Wu"], "link": "https://tech.ebayinc.com/research/explainable-reasoning-over-knowledge-graphs-for-recommendation/", "abstract": "Incorporating knowledge graphs into recommender systems has attracted increasing attention in recent years. By exploring the interlinks within a knowledge graph, the connectivity between users and items can be discovered as paths, which provide rich and complementary information to user-item interactions. Such connectivity not only reveals the semantics of entities and relations, but also helps to comprehend a user’s interest. However, existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path. We have developed a new model named Knowledge-aware Path Recurrent Network (KPRN) to exploit knowledge graphs for recommendation. Our new model, Knowledge-aware Path Recurrent Network (KPRN), can generate path representations by composing the semantics of both entities and relations. By leveraging the sequential dependencies within a path, we allow effective reasoning on paths to infer the underlying rationale of a user-item interaction. Furthermore, we design a new weighted pooling operation to discriminate the strengths of different paths in connecting a user with an item, endowing our model with a certain level of explainability. We conduct extensive experiments on two datasets about movie and music, demonstrating significant improvements over state-of-the-art solutions, Collaborative Knowledge Base Embedding and Neural Factorization Machine. Incorporating knowledge graphs (KGs) into recommender systems has attracted increasing attention in recent years. By exploring the interlinks within a knowledge graph, the connectivity between users and items can be discovered as paths, which provide rich and complementary information to user-item interactions. Such connectivity not only reveals the semantics of entities and relations, but also helps to comprehend a user's interest. However, existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path. In this article, we summarize our paper “ Explainable Reasoning over Knowledge Graphs for Recommendation ,” accepted by the AAAI 2019 conference. We contribute KPRN to exploit knowledge graphs for recommendation. KPRN can generate path representations by composing the semantics of both entities and relations. By leveraging the sequential dependencies within a path, we allow effective reasoning on paths to infer the underlying rationale of a user-item interaction. Furthermore, we design a new weighted pooling operation to discriminate the strengths of different paths in connecting a user with an item, endowing our model with a certain level of explainability. We conduct extensive experiments on two datasets about movies and music, demonstrating significant improvements over state-of-the-art solutions, Collaborative Knowledge Base Embedding and Neural Factorization Machine. Figure 1. Illustration of a KG-aware recommendation in the music domain. Extra user-item connectivity information derived from a Knowledge Graph endows recommender systems the abilities to reason and explain. Taking music recommendations as an example (Figure.1), a user is connected to \"I See Fire,\" since she likes the song \"Shape of You\" sung by the same singer Ed Sheeran. Such connectivity helps to reason about unseen user-item interactions (i.e. a potential recommendation) by synthesizing information from paths. Running Example: {Alice, Interact, Shape of You}^{Shape of You, SungBy, Ed Sheeran}^{Ed Sheeran, IsSingerOf, I See Fire}=>{Alice, Interact, I See Fire}. Clearly, the reasoning unveils the possible user intents behind an interaction, offering explanations behind a recommendation. How to model such connectivity in KGs, hence, is of critical importance to inject knowledge into a recommender systems. Our new solution, KPRN, not only generates representations for paths by accounting for both entities and relations, but also performs reasoning based on paths to infer user preference. Specifically, we first extract qualified paths between a user-item pair from the KG, each of which consists of the related entities and relations. We then adopt a Long Short-Term Memory (LSTM) network to model the sequential dependencies of entities and relations. Thereafter, a pooling operation is performed to aggregate the representations of paths to obtain a prediction signal for the user-item pair. More importantly, the pooling operation is capable of discriminating the contributions of different paths for a prediction, which functions as the attention mechanism (Chen et al. 2017; Neelakantan, Roth, and McCallum 2015). Owing to such attentive effect, our model can offer path-wise explanations, such as why \"Castle on the Hill\" is recommended since you have listened to \"Shape of You,\" which is sung and written by Ed Sheeran. We conducted extensive experiments on two datasets to verify our method. In this section, we elaborate on our proposed method, as illustrated in Figure 2. Before introducing our proposed method, we first formally define the knowledge graph and the user-item data, and describe how to combine them in an enriched knowledge graph as the inputs of our model. Figure 2. Schematic overview of our model architecture. Background A Knowledge Graph (KG) is a directed graph whose nodes are entities ${E}$, and edges ${R}$ denote their relations. Formally, we define KG as ${KG}=\\{(h,r,t)| h,r\\in {E}, r\\in {R}\\}$, where each triplet $(h,r,t)$ indicates a fact that there is a relationship $r$ from the head entity $h$ to the tail entity $t$. The user-item interaction data is usually presented as a bipartite graph. In particular, we use ${U}=\\{u_{t}\\}_{t=1}^{M}$ and ${I}=\\{i_{t}\\}_{t=1}^{N}$ to separately denote the user set and the item set, where $M$ and $N$ are the number of users and items, respectively. Following that (Chaudhari, Azaria, and Mitchell 2016), we represent the interaction between a user and an item with a triplet $\\tau=$($u$, interact, $i$), if there is an observed interaction (e.g., rate, click, and view feedbacks), where the interaction is a pre-defined relation. We merge the item set and the entity set through string matching: ${I}\\subseteq{E}$, ${G}=\\{(h,r,t)|h,r\\in {E}',r\\in {R}'\\}$, where ${E}'={E}\\cup{U}$ and ${R}'={R}\\cup\\{\\text{interact}\\}$. For consistency, the KG in the rest of paper denotes the combined graph ${G}$, including both the original KG and user-item data, unless otherwise noted. The triplets in the KG clearly describe direct or indirect (i.e. multiple-step) relational properties of items, which shall constitute one or several paths between the given user and item pair. We explore these paths in order to achieve comprehensive reasoning and understanding for a recommendation. Within ${G}$, we formally define the path from the user $u$ to the item $i$ as a sequence of entities and relations: $p=[e_{1}\\xrightarrow{r_{1}}e_{2}\\xrightarrow{r_{2}}\\cdots\\xrightarrow{r_{L-1}}e_{L}]$, where $e_{1}=u$, $e_{L}=i$; $(e_{l},r_{l},e_{l+1})$ is the $l$-th triplet in $p$, and $L$ denotes the number of triplets in the path. We elaborate on the construction of paths in the Path Extraction section of the paper . Next, we will use a realistic example to show the sophisticated relationships (i.e. paths) between a user and an item behind their possible interactions, which inspires us to model the high-level semantics of a path compositionally by considering both entities and (multiple-step) relationships. Examples: Consider the music recommendation shown in Figure 1, where the “listen to 'Castle on the Hill'” behavior of user Alice can be referred by the following paths: $p_{1}=\\text{[Alice}\\xrightarrow{\\text{Interact}}\\text{Shape of You}\\xrightarrow{\\text{IsSongOf}}\\div\\xrightarrow{\\text{ContainSong}}\\text{Castle on the Hill]}$; $p_{2}=~\\text{[Alice}\\xrightarrow{\\text{Interact}}\\text{Shape of You}\\xrightarrow{\\text{SungBy}}\\text{Ed Sheeran}\\xrightarrow{\\text{IsSingerOf}}\\text{Castle on the Hill]}$. $p_{3}=~\\text{[Alice}\\xrightarrow{\\text{Interact}}\\text{Shape of You}\\xrightarrow{\\text{InteractedBy}}\\text{Tony}\\xrightarrow{\\text{Interact}}\\text{Castle on the Hill]}$; These paths from the same user Alice to the same item, \"Castle on the Hill,\" obviously express their different multiple-step relations, and implies various compositional semantics and possible explanations of the listen behavior. In particular, $p_{1}$ and $p_{2} $ infer that Alice may prefer songs that belonging to the album $\\div$ and the songs sung by Ed Sheeran , while $p_{3}$ reflects the collaborative filtering (CF) effect: similar users tend to have similar preferences. Therefore, from the view of reasoning, we consume the connectivity along all paths to learn compositional relation representations, and weighted pool them together for predicting the interact relationship between the user and the target item. Task Definition: Our task can be formulated as follows: given a user $u$, a target item $i$, and a set of paths ${P}(u,i)=\\{p_{1},p_{2},\\cdots,p_{K}\\}$ connecting $u$ and $i$, the holistic goal is to estimate the interaction by: $ \\hat{y}_{ui}=f_{\\Theta}(u,i|{P}(u,i)), % \\hat{y}_{ui}=\\Space{P}(\\tau|\\Set{P}), % \\hat{y}_{ui}=\\Space{P}((u,\\text{interact},i)|\\Set{P}), $ where $f$ denotes the underlying model with parameters $\\Theta$, and $\\hat{y}_{ui}$ presents the predicted score for the user-item interaction. Distinct from embedding-based methods, we can explain $\\hat{y}_{ui}$ as the plausibility score of the triplet $\\tau=(u,{interact},i)$ inferred by the connectivity ${P}(u,i)$. Modeling: KPRN takes a set of paths of each user-item pair as input and outputs a score indicating how possible the user will interact the target item. As illustrated in Figure 2, there are three key components: (1) the embedding layer to project three types of ID information: the entity, entity type, and the relation pointing to the next node into a latent space, (2) the LSTM layer that encodes the elements sequentially with the goal of capturing the compositional semantics of entities conditioned on relations, and (3) the pooling layer to combine multiple paths and output the final score of the given user interacting the target item. Embedding layer Given a path $p_{k}$, we project the type (\\eg person or movie) and specific value (\\eg Peter Jackson or The Hobbit II) of each entity into two separate embedding vectors, ${e}_{l}$$\\in$${R}^{d}$ and ${e}'_{l}\\in{R}^{d}$, where $d$ is the embedding size. In real-world scenarios, it is common that the same entity-entity pairs may have different semantics due to different relations connecting them. Such differences may reveal the diverse intents about why a user selected the item. As an example, let ({Ed Sheeran, IsSingerOf, Shape of You}) and ({Ed Sheeran, IsSongwriterOf, Shape of You}) be the triplets in two paths referring a user's preferences. Without specifying the relations, these paths will be represented as the same embeddings, regardless of the possibility that the user only prefers songs sung by Ed Sheeran, rather than that written by Ed Sheeran. We hence believe that it is important to explicitly incorporate the semantics of the relations into the path representation learning. Towards this end, each relation $r_{l}$ in $p_{k}$ is represented as an embedding vector ${r}_{l}\\in{R}^{d}$. As a result, we obtain a set of embeddings for path $p_{k}$, $[{e}_{1},{r}_{1},{e}_{2},\\cdots,{r}_{L-1},{e}_{L}]$, where each element denotes an entity or a relation. LSTM layer With the embedding sequence to describe a path, we employ LSTM models to explore the sequential information and generate a single representation for encoding its holistic semantics. Such a long-term sequential pattern is crucial to reason on paths connecting a user and item entities to estimate the confidence of the ''interact'' relation. At the path-step $l-1$, the LSTM layer outputs a hidden state vector ${h}_{l-1}$, consuming the subsequence $[e_{1},r_{1},\\cdots,e_{l-1},r_{1-1}]$. Simultaneously, we concatenate the embedding of current entity $e_{l-1}$ and relation $r_{l-1}$ as the input vector: ${x}_{l-1}={e}_{l-1}\\oplus{e}'_{l-1}\\oplus{r}_{l-1}$ where $\\oplus$ is the concatenation operation. Note that for the last entity $e_{L}$, a null relation $r_{L}$ is padded to the end of path. As such, the input vector contains not only the sequential information, but also the semantic information of the entity and its relation to the next entity. Consequently, ${h}_{l-1}$ and ${x}_{l-1}$ are used to learn the hidden state of the next path-step $l$, which could be found through LSTM . Having established the representation of path ${p}_{k}$, we aim to predict the plausibility of $\\tau=(u,\\text{interact},i)$. Towards this end, two fully-connected layers are adopted to project the final state into the predictive score for output, given by: \\begin{align}\\label{equ:path-pred} s(\\tau|{p}_{k})={{W}}_{2}\\text{ReLU}({{W}}_{1}{p}_{k}), \\end{align} where ${W}_{1}$ and ${W}_{2}$ are the coefficient weights of the first and second layers respectively. Bias vectors are omitted form simplicity, and the rectifier is adopted as the activation function. Weighted pooling layer A user-item entity pair usually has a set of paths connecting them in a KG. Let ${S}=\\{s_{1},s_{2},\\cdots,s_{K}\\}$ be the predictive scores for $K$ paths, ${P}(u,i)=\\{p_{1},p_{2},\\cdots,p_{K}\\}$, connecting a user-item pair $(u,i)$. The final prediction could be the average of the scores of all paths, which is formulated as, \\begin{align}\\label{equ:mean-pooling} \\hat{y}_{ui}=\\sigma(\\frac{1}{K}\\sum_{k=1}^{K}s_{k}). \\end{align} Nevertheless, prior studies suggest that different paths have varying contributions to model user preferences. Inspired by previous work~\\cite{reasonchain,ACF}, we design a weighted pooling operation to aggregate scores of all paths. Here the pooling function is defined as follows, \\begin{align}\\label{equ:per-path-score} g(s_{1},s_{2},\\cdots,s_{K})=\\log\\left[\\sum_{k=1}^{K}\\exp\\left(\\frac{s_{k}}{\\gamma}\\right)\\right], \\end{align} and the final prediction score is given by, \\begin{align} \\hat{y}_{ui}=\\sigma(g(s_{1},s_{2},\\cdots,s_{K})), \\end{align} where $\\gamma$ is the hyper-parameter to control each exponential weight. Such pooling is capable of distinguishing the path importance, which is attributed by the gradient: \\begin{align} \\frac{\\partial g}{\\partial s_{k}}=\\frac{\\exp(s_{k}/\\gamma)}{\\gamma \\sum_{k'}\\exp(s_{k'}/\\gamma)}, \\end{align} which is proportional to the score of each path during the back-propagation step. Moreover, the pooling function endows the final prediction more flexibility. In particular, when setting $\\gamma\\rightarrow 0$, the pooling function can degenerate to max-pooling; whereas, it can degrade to mean-pooling by setting $\\gamma\\rightarrow\\infty$. We conduct a case study on exploring the utility of the weighted pooling operation in the Case Studies section of the paper . Learning: we treat the recommender learning task as a binary classification problem, where an observed user-item interaction is assigned a target value $1$, otherwise $0$. We use the pointwise learning methods to learn the parameters of our model. In particular, the negative log-likelihood is adopted as the objective function, which is defined as follows, \\begin{align} l=-\\sum_{(u,i)\\in{O}^{+}}\\log\\hat{y}_{ui}+\\sum_{(u,j)\\in{O}^{-}}\\log(1-\\hat{y}_{uj}), \\end{align} where ${O}^{+}=\\{(u,i)|y_{ui}=1\\}$ and ${O}^{-}=\\{(u,j)|y_{uj}=0\\}$ are the positive and negative user-item interaction pairs, respectively. We conduct $L_{2}$ regularization on the trainable parameters $\\Theta$, which is omitted here for simplicity, to avoid overfitting. We performed experiments on two real-world datasets (movie item recommendation: MovieLens-1M and IMDb datasets, named MI, and music recommendation: KKBox) to evaluate our proposed method. We aimed to answer the following research questions: RQ1: Compared with the state-of-the-art KG-enhanced methods, how does our method perform? RQ1: Compared with the state-of-the-art KG-enhanced methods, how does our method perform? RQ2: How does the multi-step path modeling (\\eg the incorporation of both entity and relation types) affect KPRN? RQ2: How does the multi-step path modeling (\\eg the incorporation of both entity and relation types) affect KPRN? RQ3: Can our proposed method reason on paths to infer user preferences towards items? RQ3: Can our proposed method reason on paths to infer user preferences towards items? We process the datasets as: if a user rates a movie or has an interaction record with a song, we set the user-movie or user-song pair as the observed positive feedback with the target value of $1$, and $0$ otherwise. For each dataset, we hold out the $80\\%$ and $20\\%$ interaction history of each user randomly to construct the training and test sets. For each positive user-item interaction pair in the training set, we conducted the negative sampling strategy to pair it with four negative items that the user has not interacted with. During the test stage, the ratio between positive and negative interactions is set as $1:100$, namely, $100$ negative items are randomly sampled and pair with one positive item in the testing set. Path Extraction: In practice, it is labor intensive and infeasible to fully exploring all connected paths over the KG. Especially, the number of paths grows exponentially the length of path, where millions of interlinks will be generated. As suggested in prior efforts (Sun et al. 2011; Shu et al. 2018), truncating all paths at a certain length and disregarding remote connections are sufficient to model the connectivity between a user-item pair. Moreover, as pointed out by (Sun et al. 2011), paths with length greater than six will introduce noisy entities. Therefore, we extract all qualified paths, each with length up to six, that connect all user-item pairs. RQ1:  Our method KPRN substantially outperforms MF (Rendle et al. 2009), NFM (He and Chua 2017), CKE (Zhang et al. 2016) and FMG (Zhao et al. 2017) hit@$K$ and ndcg@$K$, achieving the best performance. RQ2: We set up another method KPRN-r without considering relations in paths. The performance of KPRN-r decreases on both datasets. This justifies our intuition that specifying different relations is of importance to capture the path semantics, especially when the same entities are involved. RQ3: The desirable property of KPRN is to reason on paths to infer the user preferences towards target items and generate reasonable explanations. This is because our model captures the higher-level semantics from these key factors: entity, entity type, and relation. To demonstrate this, we show an example drawn from KPRN on a movie recommendation task. We randomly select a user, whose ID is u4825 in MovieLens-1M, and select the movie \"Shakespeare in Love\" from her interaction record. We then extract all the qualified paths connecting the user-item pair and present the subgraph in Figure 3. We have several observations. Collaborative filtering effect plays a pivotal rule to recommend the movie \"Shakespeare in Love\" to the user, since the interaction behaviors from other users (e.g., u940 and u5448) are involved in two paths. In particular, the path containing u5448 offers the high contribution score of 0.356 to infer the user's interest. Collaborative filtering effect plays a pivotal rule to recommend the movie \"Shakespeare in Love\" to the user, since the interaction behaviors from other users (e.g., u940 and u5448) are involved in two paths. In particular, the path containing u5448 offers the high contribution score of 0.356 to infer the user's interest. The target item is connected to what u4825 has watched before {Rush Hour, Titanic, and Fantasia} by the shared knowledge entities, such as actor {Tom Wilkinson} and director{James Algar}. This shows that KPRN is capable of extending user interests along KG paths. The target item is connected to what u4825 has watched before {Rush Hour, Titanic, and Fantasia} by the shared knowledge entities, such as actor {Tom Wilkinson} and director{James Algar}. This shows that KPRN is capable of extending user interests along KG paths. Analyzing these three paths jointly, we find that different paths describe the user-item connectivity from dissimilar angles, which can be treated as the evidence why the item is suitable for the user. Specially, we can offer path-wise explanations such as {Shakespeare in Love is recommended since you have watched Rush Hour acted by the same actor Tom Wilkinson} or {since it is similar to Titanic that you watched before}. This case demonstrates KPRN's capacity of providing informative explanations. Analyzing these three paths jointly, we find that different paths describe the user-item connectivity from dissimilar angles, which can be treated as the evidence why the item is suitable for the user. Specially, we can offer path-wise explanations such as {Shakespeare in Love is recommended since you have watched Rush Hour acted by the same actor Tom Wilkinson} or {since it is similar to Titanic that you watched before}. This case demonstrates KPRN's capacity of providing informative explanations. Figure 3. Visualization of three paths with prediction scores for the user of u4825 in the MI dataset. The prediction scores are normalized for illustration. The details are discussed in our paper “ Explainable Reasoning over Knowledge Graphs for Recommendation .” In this work, we exploit knowledge graphs to construct paths as extra user-item connectivity, which is complementary to user-item interactions. We propose a knowledge-aware path recurrent network to generate representation for each path by composing semantics of entities and relations. By adopting LSTM on paths, we can capture the sequential dependencies of elements and reason on paths to infer user preference. Hopefully, this article gave you some insights into why these tips are important. Please read “ Explainable Reasoning over Knowledge Graphs for Recommendation ,” AAAI 2019 for more details. This work is supported by eBay, Search Science Shanghai Director Hua Yang, Manager Wu Xiaoyuan, and Intern Zhang Mohan. Chaudhari, S.; Azaria, A.; and Mitchell, T. M. 2016. An entity graph based recommender system. In RecSys. Chen, J.; Zhang, H.; He, X.; Nie, L.; Liu, W.; and Chua, T.-S. 2017. Attentive collaborative filtering: Multimedia recommendation with item- and component-level attention. In SIGIR, 335–344. He, X., and Chua, T. 2017. Neural factorization machines for sparse predictive analytics. In SIGIR, 355–364. McCallum, A.; Neelakantan, A.; Das, R.; and Belanger, D. 2017. Chains of reasoning over entities, relations, and text using recurrent neural networks. In EACL, 132–141. Neelakantan, A.; Roth, B.; and McCallum, A. 2015. Compositional vector space models for knowledge base completion. In ACL, 156–166. Rendle, S.; Freudenthaler, C.; Gantner, Z.; and SchmidtThieme, L. 2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI, 452–461. Shu, Z.; Yang, J.; Zhang, J.; Bozzon, A.; Huang, L.-K.; and Xu, C. 2018. Recurrent knowledge graph embedding for effective recommendation. In RecSys. Sun, Y., and Han, J. 2012. Mining heterogeneous information networks: a structural analysis approach. SIGKDD 14(2):20–28. Sun, Y.; Han, J.; Yan, X.; Yu, P. S.; and Wu, T. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. PVLDB 4(11):992– 1003. Yu, X.; Ren, X.; Sun, Y.; Gu, Q.; Sturt, B.; Khandelwal, U.; Norick, B.; and Han, J. 2014. Personalized entity recommendation: a heterogeneous information network approach. In WSDM, 283–292. Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W. 2016. Collaborative knowledge base embedding for recommender systems. In SIGKDD, 353–362. Zhao, H.; Yao, Q.; Li, J.; Song, Y.; and Lee, D. L. 2017. Meta-graph based recommendation fusion over heterogeneous information networks. In SIGKDD, 635–644.", "date": "2019-01-24"},
{"website": "Ebay-Engineering", "title": "Troubleshooting a Connection Timeout Issue with tcp_tw_recycle Enabled", "author": ["Edward Lin", "Huai  Jiang"], "link": "https://tech.ebayinc.com/engineering/a-vip-connection-timeout-issue-caused-by-snat-and-tcp-tw-recycle/", "abstract": "Availability and stability are very important for eBay's site, especially for those applications that take high traffic and are dependent on many other applications, such as CAL (our Centralized Application Logging framework). This blog shares an issue that happened recently that impacted the availability and stability of CAL, and how we found out the root cause using tcpdump and systemtap. CAL is eBay's Centralized Application Logging framework. The main purpose of CAL is to facilitate a centralized collection of application servers' local logs and to provide reporting on the collected data. These reports provide great insight into important areas of the eBay site—invaluable for making decisions on all aspects of business at eBay. In the CAL infrastructure, as the following figure shows, a CAL client will try to establish a long-term TCP connection to a CAL VIP (virtual IP address). There are multiple load balancers (LBs) and VIPs for CAL, because CAL's traffic is very large; a single pair of load balancers can't handle this large of a connection. A client will randomly pick one VIP to connect with based on DNS. If a connection can't be established, for instance, if the connection timed out or was refused, the CAL client will retry and pick another VIP, until it establishes a connection successfully. Recently, the CAL team and the operation team found CAL VIPs have some weird issues. VIPs sometimes return connection timeout errors intermittently, like the following nc (Netcat) command output, it returned connection timeouts frequently, which impacted CAL’s stability. Here’s how we handled troubleshooting. 1. TCPDUMP in LB side Why does a VIP return a connection timeout error sometimes? The VIP’s configuration looked fine, so we looked at the tcpdump to try to find a reason. The following Wireshark screenshots show what happened when the connection timed out. The LB sent a RST packet because of flow expiration (handshake timeout). The second screenshot shows how the SNAT (Source Network Address Translation) IP sent the SYN packets three times, but the server ignored all of them. After 15s, the LB SNAT IP sent an RST packet to close the connection. 2. Check SYN Queue and Accept Queue in Server side But why did the server ignore the LB's SYN packets? As there was CAL traffic migration going on, the most of traffic of CAL was migrated from the previous old application to the new application. The immediate suspect was targeting the new application. The new application not able to catch up, which then caused the SYNC Queue to fill up. The following illustration shows how the the SYN Queue and the Accept Queue work in the Linux Kernel. At first glance, the evidence seems support this assumption. However, running a probe ( Systemtap Scrpit ) to catch queue overflow didn't catch anything. We rechecked the OS network statics using netstat -s. The majority of SYN to LISTEN that dropped did not happen because of socket overflow. The passive connections were rejected because of the time stamp. 'Passive connections rejected because of time stamp' is dropped by LINUX_MIB_PAWSPASSIVEREJECTED . It only happened in one place in this Linux kernel code . When the function tcp_peer_is_proven(req, dst, true) returns false, the kernel will drop the SYN packets. The kernel code snippet is below. We then suspected that the LB SNAT plus the Linux Kernel parameter net.ipv4.tcp_tw_recycle together caused this issue. 3. How the LB SNAT works The following illustration shows how SNAT works on the LB side. There's a Connection Table maintained in the LB's memory, which records the mapping relationship between the Client-VIP connection and the SNAT-PoolMember connection. In layer 4 mode, the LB changes only src_ip, src_port, dst_ip, dst_port , then sends it to another connection side. Because TSval is based on the client CPU time, which is different across clients, different packets from different clients have a different TSval. 4. TCPDUMP and use systemtap script to confirm the issue A new systemtap script was written to probe into these functions. Running tcpdump and probing on the CAL server at the same time produced matching results. From the tcpdump side, the application running on CAL server port 1120 sends a (FIN, ACK) to LB SNAT IP. Then LB sends an ACK, its TSval is 517740536, and then the connection closed. After several seconds, the LB used the same SNAT IP trying to establish a new connection to same destination IP and port. Because the previous ACK was send at 18s and the new SYN is received at 60s, the interval is less than 60s, (u32)get_seconds() - tm->tcpm_ts_stamp < TCP_PAWS_MSL returns true (60−18<60). Then tm->tcpm_ts (517740536) and req->ts_recent (481284815) are compared. As the stap script's output, the delta is larger than TCP_PAWS_WINDOW, which is 1. So the function tcp_peer_is_proven returns false and the kernel drops this packet. Because TSval is based on the client CPU time, which is different across different clients, the kernel may drop some connections unexpectedly if the traffic was through a load balancer. So net.ipv4.tcp_tw_recycle should be disabled in SNAT network. This feature has been totally removed in the kernel since Linux 4.1. Dropping of connections with tcp_tw_recycle RFC 1323 [net-next,2/2] tcp: remove tcp_tw_recycle net.ipv4.tcp_tw_recycle has been removed from Linux 4.1 - kernel git Coping with the TCP TIME-WAIT state on busy Linux servers SYN packet handling in the wild", "date": "2018-11-28"},
{"website": "Ebay-Engineering", "title": "Sharing Modules Across Experience Services and Multi-Screen Applications", "author": ["Chuck Zheng"], "link": "https://tech.ebayinc.com/engineering/sharing-modules-across-experience-services-and-multi-screen-applications/", "abstract": "By now most eBay core business flows have gone through the journey of implementing Experience Service-based multi-screen application solutions, where web and mobile native app user interfaces are composed of one or more modules, and Experience Services directly returns these modules to clients with content synthesized from backend data sources, localized and formatted, ready for render and user interaction. This article outlines a plan to move more modules to a shared environment. This architecture offers benefits for each each business flow or domain by providing: Consistent user experience across web and mobile native apps Faster time-to-market for releasing business features across devices Reduce duplicate engineering work across devices During this journey, we have found that there are many modules that can or should be reused/shared across UIs of many business flows, including: Marketing modules Merchandising modules First-party and third-party advertising modules Product reviews modules Browse/search guidance Best selling/top-rated products module Deals modules Search result module Search refinement module Category navigation module Here are some examples how these modules (in red boxes) are reused across several hosting pages: But many pages hosting these modules have their own Experience Services directly integrating with those shared domain services (e.g. Merchandising) and producing their own flavor of module response schema (e.g. Homepage vs Search vs Product vs ViewItem). The native and web apps teams then have to repeatedly develop components for each flavor of the logically equivalent module. The following diagram illustrates this situation. The different shades of each color in the diagram represents different flavors of each component: Red represents Merchandising Blue represents Advertising implementation Yellow represents Finder implementation Green represents Product Review implementation As you can see, there are several problems: Duplicate design/development/QE effort across many teams Different flavor implementations give inconsistent user experiences to end users When a new functionality needs to be introduced to a module, the developers need to negotiate with many teams to implement it. And those implementations usually roll out to production at different times due to priority setting/resource planning in each team, which slows down business and further fractures the user experience. Instead of duplicating the effort across many teams, owners of the shared domain (e.g. Merchandising) should collaborate with other teams of hosting pages to: Have product management standardize the user experience of the modules across all hosting pages. If the business needs a few different variants of the module design across different hosting pages (Homepage vs Search vs Product vs ViewItem), then we should standardize a set of variants and let the hosting page choose which variant best suits them and avoid creating one-off flavor splinters again. Standardize the module response schema for these agreed-upon module designs. Develop one Module Provider service for these agreed-upon module designs to be invoked by all hosting page Experience Services. Allow different hosting pages' Experience Services call these module providers and pass through their response to clients. Integration of these modules to different pages are significantly simplified. Develop one set of web components for these modules with standardized schema and reuse them across all web applications. Develop one set of Mobile Native components per stack (iOS vs Android) for these modules with standardized schema and reuse them across all native apps. The following diagram illustrates such design. This approach solves many of the problems mentioned before: Minimizes duplicate design/development/QE effort across teams, which also translates to More engineering resources becoming available for other development Faster time-to-market for integrating these modules More engineering resources becoming available for other development Faster time-to-market for integrating these modules Creates consistent user experiences across business flows/pages (Homepage vs Search vs Product vs ViewItem), in addition to consistency across devices (web and mobile native apps) Speeds up delivery of business change Most minor changes (changing the verbiage and image content) do not even need any change on the Module Provider interface, therefore there is no change needed to the hosting page Experience Services and web/mobile apps Most algorithm/data source changes are managed by the Module Provider internally, and there is no change to the Module Provider interface, the hosting page Experience Services, or web/mobile apps. If there is a change to interface, changes will be localized to The module provider interface The shared web components to work with new response schema The shared mobile native components to work with new response schema The hosting page Experience Service interface, which is updated by up-revving the module’s Maven JAR dependency version and recompiling it, so integration effort is much lower compared to the previous coding change model Most minor changes (changing the verbiage and image content) do not even need any change on the Module Provider interface, therefore there is no change needed to the hosting page Experience Services and web/mobile apps Most algorithm/data source changes are managed by the Module Provider internally, and there is no change to the Module Provider interface, the hosting page Experience Services, or web/mobile apps. If there is a change to interface, changes will be localized to The module provider interface The shared web components to work with new response schema The shared mobile native components to work with new response schema The hosting page Experience Service interface, which is updated by up-revving the module’s Maven JAR dependency version and recompiling it, so integration effort is much lower compared to the previous coding change model The module provider interface The shared web components to work with new response schema The shared mobile native components to work with new response schema The hosting page Experience Service interface, which is updated by up-revving the module’s Maven JAR dependency version and recompiling it, so integration effort is much lower compared to the previous coding change model The Module Provider integration approach has been warmly embraced by many domains in eBay, including Merchandising, Advertising, Marketing, Vertical Finders, and Product Reviews. This approach has resolved a long-standing integration challenge they faced with each hosting page. In the past, they had to work in the hosting page code base to orchestrate integration with their domain, which usually created a fractured functionality and user experience due to constraints in that environment, resourcing, and planning. It has also been embraced by the mobile native app community, which saves large amounts of duplicated development by adopting reusable components built to support the standardized module response. There are some eBay specific challenges we are working through, including Web components—traditionally the eBay web app for each hosting page typically is not well aligned, so the developers use different versions of JavaScript libraries and utilities, which makes developing reusable web components difficult. But a solution is on the way with the eBay UI for the web effort in progress. This effort standardizes the web UI components and supported JavaScript library and utilities, which makes shared module web components possible. Product management alignment—traditionally eBay product management for each page is more siloed and not thinking about reusable modules across pages. With the adoption of Experience Services, where a module is now a first-class citizen, product management is changing its thinking and practice to adopt shared modules that are not tethered to any particular page and that can be reused across pages. Overall, with problems getting gradually resolved, more and more shared module provider-based solutions are created and used across eBay to provide: Consistent user experiences across domains and devices Faster time-to-market for adding/changing business features More efficient use of engineering resources Experience Services — eBay’s Solution to Multi-Screen Application Development Don’t Build Pages, Build Modules", "date": "2018-12-19"},
{"website": "Ebay-Engineering", "title": "Working on the Engines While the Plane is Flying", "author": ["Brian Davies", "Thilak Thankappan"], "link": "https://tech.ebayinc.com/engineering/planning-and-execution-of-a-high-risk-network-upgrade/", "abstract": "Operators of large scale networks will, from time to time, be required to perform major upgrades to the network while keeping the network available with no downtime. This type of work has been compared to working on the engines of an airliner while it is flying. At eBay, our Site Network Engineering team recently completed a migration of our data center aggregation layer from one platform to another under these conditions. By sharing our experience, we hope to help our peers in the industry plan for and successfully execute their own network transformations. eBay’s production network consists of large data center sites plus smaller point of presence sites, all connected by our global backbone. At the top of each data center network, there is a layer of four aggregation routers that we call AR routers. ARs are typically shared across two or more data center fabrics. The ARs are responsible for implementing routing policy and filtering between data centers and fabrics so that our network functions as designed. At a high level, one set of AR routers that serve a data center looks like this: Over the course of time through organic growth, we arrived at a point where we had various vendors and models across the data centers. We also had two different major versions of our Border Gateway Protocol (BGP) policies in use at the same time. We made the decision to refresh the older sites and bring all of the AR routers up to our newest standards for hardware, software, and configuration across the board. This initiative would make performance and policy behavior more consistent and predictable for this critical layer of our network as we continue to drive toward our goal of a completely automated network. We agreed on our definition of done early to ensure that all further actions would be in support of these goals. These goals included: No outage time for our business All data center ARs will be one vendor and code version All data center ARs will operate with our newest routing policies Physical design will be standard Number of links Connectivity pattern Speed of links Number of links Connectivity pattern Speed of links After establishing your definition of done, it is a good practice to spend some time surfacing challenges that you expect to face. Early identification of challenges gives you the most leverage on them. We discussed these items early to afford ourselves the greatest number of options to handle them: Some applications cannot be easily moved for a maintenance window Data center traffic would never be at zero during the work Some older data center environments still ran Open Shortest Path First (OSPF), so we needed to find a way to integrate with our current BGP only design Due to the number of configuration items, scripting would be required to keep things consistent and standard Due to the length of time the maintenances would require, we could only do one per night Our engineers could not do maintenances on back-to-back nights due to fatigue issues We would need to be able to run at peak load with any combination of four ARs from the set of eight total ARs (four old ones plus the four new ones) Some old and new configuration pieces could interact in unexpected ways when used together Armed with our definition of done and our list of expected challenges, our team spent considerable time to work out our execution plan. Each maintenance window would remove one old AR and turn up a new one in its place. Visually, it looks fairly easy, but the devil is in the details, as they say. Fortunately, we had some extra rack space so we racked and powered up the new devices ahead of time. Our Site Services team also pre-ran the hundreds of connections that would be required complete this migration. Early identification of challenges gives you the most leverage on them. In the best case, all traffic can be removed from a data center so that maintenance activities can be performed with less risk. At eBay, we perform regular maintenances, so we know what can be easily moved and what cannot. Executing our standard data center exit plan, we would be able to drain around 70% of the traffic moving through the ARs. In addition to the data center exit, we would start work in the evening at a time when site traffic is lower for us. Our plan was to perform the data center exit followed by BGP and OSPF metric changes for the AR that we wanted to migrate. This costing out step would reduce the remaining production traffic for one AR to zero to avoid causing any business impacts during the maintenance. We estimated that the work to move each AR router would take about six to eight hours, so doing all four ARs that serve a single data center at one time would simply not be possible. This constraint meant that to do each group of four AR routers would require four maintenance windows spread over two weeks. Because of the timeline, we had to be able to interoperate with a mixture of old and new ARs in parallel under full production load conditions indefinitely if needed. In order to make this work with both old and new BGP policies, smaller pieces of non-standard shim policy would be put in place to allow old and new to coexist until all four devices for that data center were moved. After all four were migrated, these shims could be removed later. BGP communities and the associated policy matches provided some challenges for us as well. We discovered that two different communities from the old and new designs would have unintended matches in policies once we had old and new devices running in parallel.  As an example, think about 65500:200 and 65500:2000. A match that is not specific enough in a policy somewhere could match both of these and take an action that was not expected. What was working in each version of our BGP design separately could not be counted on to work correctly when both policies were run at the same time. We resolved this by going through all policies with a fine-toothed comb looking for these type of issues and corrected them or worked around them. Having a lab environment in which to model your network and test the interaction of components is essential. OSPF is found in some of the oldest parts of our infrastructure, and the easiest way for us to solve this issue was to simply add a small OSPF configuration section to our newer design to support this feature until we can decommission that older environment or move the OSPF area border further “down” in the network. The best-laid plans of mice and men often go awry. - Robert Burns As already mentioned, some sites used one particular vendor and other sites used a different vendor. Over the multi-week course of the overall plan for each data center AR set, we knew that we would have to have these vendors operating in parallel for at least a few weeks without any unexpected issues with load balancing, policy actions, or best path selection. In addition, the configuration can look quite different between vendors, and you need to make absolutely sure that your intent is accurately implemented in the devices—especially when they are going to try to operate in parallel. Any subtle preference in selection of a route, a difference in tie breakers, or default behaviors could result in disastrous consequences. We built as much of our target configuration as possible using our normal build tools. This helped us generate boilerplate items such as the IP addresses (IPv4 and IPv6) needed and our standard set of BGP policies. The shim pieces were handcrafted and incorporated into the maintenance plan. Many portions of the maintenance plan itself were created using Jinja templates. With hundreds of links to move, this approach was worth the extra time to set it up. After a number of planning meetings spread out over more than a month, the plan had been documented to the last detail. Wiki pages, Git repos with generated configurations, and peer-reviewed maintenance plans were all set. At 7:00 p.m., our Site Engineering Center (SEC) initiated a data center exit for the site that we were about to work on. As expected, this took about 70% of the traffic off all of the links that we were working on. We allowed a few minutes to make sure everything was stable, and then we applied our cost-out metrics to the AR device that was going under the knife that night. We use this procedure on a regular basis, so it worked as expected and traffic was soon at nearly zero on the device. Again, we waited a few minutes to make sure that everything was stable before proceeding. Pausing for just a few minutes between major steps is an important best practice, because there is often some delay between an event on the network and issues being displayed in monitoring tools. If the plan is rushed, it becomes difficult to tell which step caused an impact. In addition, if there are steps that are slow to undo, such as powering down a router, you don’t want to execute those steps until you are sure that you are ready and the previous step didn’t have an unexpected effect. The next major step in our plan called for dozens of cables to be moved from the old AR to the new AR. Our standard practice for link commissioning consists of three layers: the physical layer is connected and tested, then routing protocols are brought up in a costed-out state, and finally, when we are ready, the data plane is set in motion with the final metrics and policies. After making the new connections, we used an internally developed tool to quickly check every link for correct physical operation. All links checked out except we found one link had been cabled to the wrong port, because we set up our auto build tools incorrectly. Our Site Services team also checked some links during the connection process with a light meter and got some strange results with very high power readings that did not make sense.  After some investigation, it was determined that the light meter was incorrectly reading the four lanes of the PSM4 optics that we were using. Sorting through these issues cost us about thirty minutes of extra time, bringing the total for this step to a little over two hours. Working on the engines while the plane is flying takes some planning. Next, we set about loading all of the required routing policies and pieces of shim configuration with everything still in a costed-out state. This would allow us to verify all policies and routing exchanges without risking any impact to the business. Our patient was effectively still under anesthesia while we patched him up and checked for issues - or so we thought. We turned up one out of four links to our backbone from the new AR router to test all of the routing in a costed-out state. Internal BGP (iBGP) did not establish with one of our older OSPF fabric environments downstream from the AR, because the loopback of the new AR was not reachable in OSPF from that device. This was unexpected. We had an export policy configured in the OSPF protocol on the new AR router with a specific match for the loopback address as well as a metric of 20 set in the policy that was designed to prevent the device from attracting any traffic at this stage. However, when we actually arrived at this step in the maintenance, the loopback was not in OSPF at all for reasons unknown to us at that time. Our team decided that adding an OSPF \"passive\" statement to the loopback interface on the new AR would fix this. The \"passive\" statement for the loopback interface was added, and iBGP came up. At this point, a large flow of traffic that was not expected saturated the upstream link from the AR to the backbone, and we were now impacting the business! Our SEC quickly notified us that something was not right, and we rolled back a step to stop the impact. Looking at the scrollback on the engineer’s terminals, it was discovered that the OSPF metric from the underlying fabric environment to the new AR was 1. We had expected a higher metric due to the overload feature being active as well as the metric of 20 being set in the OSPF export policy. The diagram below shows the network state when we had the impact. At the moment of impact, we had only one out of four links from the new AR to the backbone ready to take traffic. The numbers on the AR routers show the OSPF cost that was being sent downstream. The cost of zero being sent by the new AR1 was the best Interior Gateway Protocol (IGP) cost within the OSPF area, because it was the lowest. This affected the BGP best path selection algorithm and attracted all of the traffic in that part of the network to the new AR1. The red links show where the unexpected traffic flow occurred. In hindsight, we learned that on this particular vendor, when the OSPF overload feature is set, the export policy is not applied. That is the reason that the loopback was originally not visible—it wasn’t being exported. When we forced the loopback into OSPF with the \"passive\" statement, it did not receive our custom metric because it wasn’t getting into OSPF via the export policy. The default metric was used instead, resulting in a metric value of 1 from the other device. Pressing forward, we reversed the order of two steps so that all four of the AR to backbone links would be up before trying again with the step that caused the impact. We also hardcoded the OSPF metric for the loopback interface to a value of 1000 to ensure that this device would not become a preferred path again. The remainder of the work went according to plan from this point forward and allowed the maintenance to be completed, albeit very late into the night. Regrouping after the first device was migrated, our team evaluated what could have gone better. We identified several areas for improvement, including the order and technical content of the steps, testing procedures, and separation of roles during the work. The team made these changes after the first migration: We explicitly set an OSPF metric for the loopback outside of any export policies. We would keep the status of all four links to our backbone layer synchronized, so that we would have sufficient bandwidth even if the traffic started flowing unexpectedly. Cabling would not be tested with the light meter, and we would rely on our software tools to check cables once they were plugged into the devices instead. Crew Resource Management: Borrowing techniques originally developed to handle airliner incidents, one engineer focused on performing the actual maintenance procedure, one engineer worked with Site Services, and one engineer handled communication with other teams such as our SEC. With these improvements in place, we successfully executed several more of these migrations. The total time elapsed for each maintenance window dropped from over ten hours for the first one to about four hours as we polished our procedure. Best of all, there was no impact to the business in any of of the subsequent maintenance windows. With the site-wide upgrade of our AR layer complete, the overall performance and reliability of our site network has never been better. We completed many hours of high-risk work with nearly flawless execution. All team members pitched in to help in some way, and as a result, this trial by ordeal was overcome. We continue to refine our craft in the pursuit of quality. Most importantly, we learned from this experience and adapted so that we could ultimately be successful.", "date": "2018-11-13"},
{"website": "Ebay-Engineering", "title": "Providing Metadata Discovery on Large-Volume Data Sets ", "author": ["Sudeep Kumar", "Satbeer Lamba"], "link": "https://tech.ebayinc.com/engineering/an-approach-for-metadata-store-on-large-volume-data-sets/", "abstract": "Many big data systems collect petabytes of data on a daily basis. Such systems are often designed primarily to query raw data records for a given time range with multiple data filters. However, discovering or identifying unique attributes present in such large datasets can be difficult. Performing runtime aggregations on large data sets, for example the unique hosts that logged for an application for a particular time range, need high computational power and can be extremely slow. Performing sampling on the raw data is an option for attribute discovery. However, such an approach would also mean that we would miss sparse or rare attributes within large volumes of data. The metadata store is our internal implementation to provide guaranteed real-time discovery of all the unique attributes (or metadata) within truly massive volumes of different monitoring signals. It primarily relies on Elasticsearch and RocksDB in the backend. Elasticsearch enables aggregations to find unique attributes over a time range, while RocksDB enables us to perform de-duplication of the same hash within a time window to avoid redundant writes. We provide discovery on three types of monitoring signals: metrics, logs, and events. Metrics are periodic time-series data that contain a metric name, a source timestamp, dimensions in the form of a map (K, V), and a long numeric value, e.g. http.hits 123456789034877 host=A In the above example, http.hits is the metric name, 1234567890 is the EPOC UTC timestamp, 34877 is the long value, and host=A is the dimension { K, V } pair. We allow discovery of metric names and namespace along with their dimension map. They represent log lines from various applications or software/hardware infrastructure. We represent logs in the following sample structure format: Logs are always discoverable against a use-case (also known as namespace). Each log line can be a particular type, such as stdout or stderr . Discovery is also provided on the type (also known as name) of log signal. As shown in the above example, the dimension key and value maps are also discoverable. This monitoring signal resembles both a log and a metric signal. They can be considered as sparse metrics that are indicated as events within the system. They are aperiodic in nature. For example, a router switch becoming unavailable is logged as an event. Also, they can be verbose in nature, such as a log signal with a lot of text to indicate what happened during the event. A sample event looks like the following: Similar to log/metrics, they have a namespace and an event name, both of which are discoverable. In addition to the dimension key and its values, the value keys are also discoverable for events. Making the field key discoverable allows us to perform aggregation, such as MIN, MAX, and COUNT, on known fields. On our product console, the discover attributes are highlighted on the following illustration: All monitoring signals are received initially by our ingress service instances. These service nodes push different input monitoring signals (logs/metrics and events) onto a Kafka data bus topic using a custom key partitioning logic. Metadata store ingress daemons consume monitoring signals from different assigned partitions and then write data onto a backend Elasticsearch that is used exclusively for discovery purpose. The different monitoring signals that we collect are pushed onto a Kafka bus, which acts as our source data stream. An advantage of using Kafka is that it offers persistence even if the downstream pipeline is under maintenance or not available. We also use a custom Kafka key partitioner on the ingress service to ensure that all keys with same hash always go on the same Kafka partition destination. Different monitoring signals use different hashes internally. For example, we use hash on namespace + name for metric signals, while our log signals use a hash on \"namespace + dimension { K, V } map.\" This kind of grouping helps reduce the overall cardinality encountered on each downstream Kafka consumer, and thereby effectively reduces the total in-memory footprint. Similar to our metadata store ingress daemon, there are other consumers that write the raw monitoring signal onto some backend stores such as Hadoop, HBase, Druid, etc. Maintaining a separate discovery pipeline provides an easy drill down and subsequent egress of these raw monitoring signals without the need to perform expensive runtime aggregations. RocksDB is used as an embedded data cache in Metadata Store. This embedded cache avoids duplicate writes onto our backend Elasticsearch data sink. We chose RocksDB because of its impressive benchmark results and configuration flexibility. When a record is processed by the metadata store ingress daemon, its hash key is checked against the existing cache. If the record was not already loaded in the cache, then the entry gets written onto the search database (Elasticsearch), and its hash key is added to the cache. This is the only time when the record would be written to the database for that debounce period. No action is taken if the record is already present in the cache. The RocksDB cache is designed to read heavy, but we do see a burst of writes at the beginning of the debounce period (as the cache is reset). For the current load, we have seen reads go above 5 billion and writes in tens of millions into the cache, with most of the writes happening in the first few minutes after the start of the debounce window. For this reason, there could be a consumer lag on the onset of the debounce period. For lower read/write latencies, efforts were made to keep all the cache data in memory on the RocksDB to avoid secondary disk storage lookups. Write-ahead logs (WAL) and compression are disabled. During our benchmarking, we found that a single memtable of 16GB was sufficient to store the hashes. The above graph represents the number of unique documents that were written onto the backend Elasticsearch. Spikes correspond to the beginning of the debounce period when the cache was reset. For monitoring purpose, we report all the rocksDB statistics as metrics into our monitoring platform. The RocksDB Tuning Guide describes different database options. We use Elasticsearch version 6.x to power the backend aggregation to identify different attributes within the monitoring signal. We leverage a 30-node Elasticsearch cluster build on SSD and 64 GB RAM hosts that are managed on our internal could platform. 30 GB is allocated for the Elasticsearch JVM process, and the rest is kept aside for the operating system (OS). During the ingestion phase, documents are hashed based on different metadata on the monitoring signal to uniquely identify a document. For example, logs are hashed based on the namespace, name, and different dimension { K, V } pairs. The document is modeled in a parent-child format, and Elasticsearch indices are created per namespace and per month. The root or parent document _ids are hashed based on the entire { K, V } dimension map, while child documents are hashed on namespace, name, and debounce timestamp. Child documents are uniquely created for a time window, also referred to as a debounce period. The debounce timestamp is the beginning epoch time representation of debounce period. Each child document, if found within a debounce, would mean that a unique combination of the child document’s namespace and name occurred along with its parent documents topology (or distinct dimension pairs). The shorter the debounce period, the better is the time approximation for the discovery of a unique attribute. There is 1:N association between a parent and a child document within an Elasticsearch index. We have the following parent-child document dynamic template modeling on Elasticsearch: While the child document has the following template: We maintain two load balancers (LB) on Elasticsearch clusters. A READ LB virtual IP (VIP) is maintained on client nodes for all read operations while a WRITE LB VIP is maintained for data nodes. This helps us to perform aggregation-based compute operations onto separate client nodes without overwhelming data nodes. Elasticsearch is not an optimal data-sink if you have too many writes on the same document, because the segment merges are expensive. At high traffic volumes, such background segment merges greatly affect the indexing/search performance. We, therefore, designed our documents in a way that we treat documents as immutable. Every index is created on Elasticsearch clusters with the following nomenclature: For example, cConsider the following indices maintained on our backend Elasticsearch server: We maintain indices on a monthly basis and they are retained for a period of three months. Purging an index is as straightforward as deleting/dropping an index. Our Discovery service is a web application that is deployed as a docker image. This service exposes REST APIs to query on backend metadata store. Key REST APIs on discovery services are: Find namespaces (or use-cases) on different monitoring signals (logs/events/metrics) Find all names within a namespace for a given time range Discover dimension keys and their values for all monitoring signals for an input namespace, list of names, and for a given time range Discover value keys for an input event namespace and for a given time range Find all namespaces or names based on an input dimension {K, V} filter For a given namespace, name, and different dimension filters, we can also discover other associated dimensions found for that unique input combinations Our metadata store ingress daemon is deployed and managed on an internal Kubernetes platform (also referred to as Tess.io). The application lifecycle for the metadata store ingress daemon is managed on Kubernetes as a stateless application. Our managed Kubernetes platform allows custom metric annotations during deployment by which we can expose health metrics on a known port in Prometheus format. Monitoring dashboards and alerts are set up based on these health metrics. We also expose similar metrics on the Discovery service to capture the error/success rate and average search latency. We were able to handle 1.6 million metric signals per minute on 10 metadata ingress daemon hosts (downstream Kafka consumers) without any performance issues Discovery of any unique metadata attributes after ingestion from source is within a few seconds Our debounce window interval is set to 12 hours in our production environment, and within each debounce period, we have a unique cardinality of ~40 million (seen up to 60 million). Currently, we see an average latency of 100 ms for most of the discovery queries that are fired in our production environment today. As expected, we found that queries fired across namespaces are much slower than targeted namespace based queries. Separating discovery functions from the actual data pipeline has allowed us to quickly drill down on the raw monitoring data. The metadata store helps greatly limit the scope of data that needs to be queried and thereby improves overall search throughput significantly. The approach also additionally shields the raw data stores from discovery calls, and thereby saves significant compute cycles on dependent backend stores.", "date": "2018-12-06"},
{"website": "Ebay-Engineering", "title": "GUI Testing Powered by Deep Learning", "author": ["Yotam Sharan", "Honghao Wang", "Sovan Rath"], "link": "https://tech.ebayinc.com/research/gui-testing-powered-by-deep-learning/", "abstract": "Deep Learning (DL) is revolutionizing the face of many industries these days, such as computer vision, natural language processing, and machine translation, and it penetrates many science-driven products and technological companies, including eBay. These days, DL is taking its first strides in eBay’s Quality Engineering (QE) space, and it has already proven to outperform the best test veteran and industry-grade applications one could find. Current methods of Graphical User Interface (GUI) testing gravitate between Functional Testing (focusing on a system’s external behavior or its elements) to Structural Testing (focusing on internal implementation). These methods are susceptible to changes and usually involve extensive automation efforts. Cross-screen testing, like in the case of desktop Web and mobile Web or mobile App testing, accentuates these risks and costs. Testing across multiple operating systems, devices, screen resolutions, and browser versions quickly becomes a huge challenge that is difficult to execute and govern. Quality risk-control measures, such as coverage-based or usage-based testing, address some of these uncertainties, but only to a certain degree, as it comes at a cost to the overall quality. Testing methodologies of web interfaces are mostly browser-dependent, while mobile app interfaces are platform-dependent, where the GUI and its detailed implementation are validated with test applications that hold interactive capabilities with the GUI under test. These tools may be Selenium WebDriver for testing HTML-based web pages and Espresso Test Framework for testing View-based Android Applications. While product developers wrap-up GUI implementation, quality engineers begin breaking down the screen to its elements, identifying locators for each UI components and writing up large pieces of their code around asserting the elements’ aspects, such as dimension, position, and color, to make sure the GUI implementation matches the design. Even a slight design change or refactoring of product code could end up failing the regression suites and may involve significant re-work for QE to fix the automaton code. Some testing tools, like the ones mentioned above, call for a developer skill set and an intimate knowhow of the hosting platforms. Such prerequisites introduce a technical-proficiency dependency and compels the QE to master multiple test applications, frameworks ,and operating systems, such as TestNG, Selenium, Appium, IOS Driver, and Selandroid, etc. As a result, writing and maintaining test suites and scripts for multiple platforms take considerable time and effort and come at the risk of reducing the test scope. Contemporary developments in DL unleashes efficiencies in GUI testing and in the software lifecycle, potentially. A recent pilot, described below, proved this approach to be realistic and practical. DL simulates the human way of finding errors or anomalies. Humans are driven by past experience and conditioning to make decisions. Machines with the proper application of training or conditioning can detect errors that surpass human precision. We begin our understanding of DL as the subset of a broader class called as the supervised machine learning algorithm. The supervised learning algorithms take a set of training examples called as the training data. The learning algorithm is provided with the training data to learn a desired function. Further, we also validate our learning algorithm by a set of test data. This process of learning from training data and validating against test data is called modeling. Neural Nets (NN) A NN is a group of logically connected entities that transfer a signal from one end to another. Similar to brain cells or neurons that are connected to enable the human brain to achieve its cognitive powers, these logically connected entities are called perceptrons, which allow signals to move across the network to do formidable computations, for example, differentiating a lily from a lotus or understanding the different signals in the traffic. These operations become possible when we expose our NN to a significant amount of data. A deep neural net (DNN) is an addition of multiple layers arranged in the order shown in Fig 2. This mathematical lattice is the core structure that drives autonomous vehicles and other inventive applications, such as eBay’s image search. Fig. 2. NN example, 2 layers deep DL can be utilized to contribute to the efficiency of GUI testing and reduce the churn associated with this work. Process Fig. 3 Process outline The suggested methodology begins with capturing the entire webpage as an image (see Fig. 3). This image is then divided into multiple UX components. The division of UX components or groups of components helps in generating training and test data to feed our model. Once our model is ready, we can test any new UX component across browsers, resolutions, and additional test dimensions by feeding the image of the UX components for the desired specification to the model. Our model would classify whether our test UX component passes the desired quality criteria or not. This process of deciding the particular images into one of the classes (passing or failing the quality criteria) is called Classification. Training data and test data creation We create the training and test data by automated modification of UX components taken from the webpage wireframes. Based on the design guidelines and the test variations, we introduce potential flaws in direct correlation to the design input. These flawed design mockups are manifested as images. Proper labeling of these images ensure proper organization of test data. Once we have a minimal set of images in our arsenal, we are ready to train our model. Modeling Based on the training data and the complexity of our scenarios, different models such as Convolutional Neural Nets (CNNs), Support Vector machines (SVMs) or Random Forests (RFs) can be chosen. Once the model is decided, we can train our model to capture GUI defects. Pursuing the above-mentioned procedure and steps, we implemented our own process for one of the new home page modules called “Popular Destination.” Using the mockups created by the Design team, we generated 10,000 images that included different defects; we have introduced intentional design flaws by modifying images, texts, and layout to simulate the real world scenarios and issues. The following were some of the examples we used for emulating the defects. 1.  Missing images 2.  Layout issues The system provides a classification score between 0 to 1. A score closer to 0 should signify a model prediction of a potential test-case failure, which may imply a certain GUI imperfection was detected by our model. A score closer to 1 could signify a prediction of a test-case that meets its quality criteria. In such a case, we intend to establish a cutoff threshold. A cutoff threshold determines a value below which signifies that the module is having a potential GUI defect. This cutoff varies for different modules. Based on our model, we were able to capture defects with a 97% accuracy. During the process of testing, we were successful to find real production bugs. For example, we captured the UX component with an Internet Explorer 11 browser and found the production issue below, where thin lines appear across circular images in the Popular Destination module against this specific browser version. Automation testing would have never captured it and manual testers would probably need Steve Austin’s bionic eye and a whole lot of time and patience to even notice this artifact in the vast continuum of their test matrix. Fig. 6: Production bug for Popular Destination in IE 11 Key learnings and benefits Our learned lessons and insights came from testing our GUI in eBay’s top two domains: Homepage (HP) team and Advertising (Ads). Both team wanted to have a test tool and methodology that would enable them to conduct Ads testing using new approaches and tools that differ from their traditional validation and verification applications. Traditional approaches and tools come at a high cost to the individual engineer. Ramping up on some test applications can take more than a week and proficiency comes with much longer periods of time. ML calls for a different developer skill set, which deprecates the need to master a great deal of traditional validation and verification techniques and tools, such as Selenium WebDriver or iOS and Android drivers. Traditional approaches and tools come at a high cost to the individual engineer. Ramping up on some test applications can take more than a week and proficiency comes with much longer periods of time. ML calls for a different developer skill set, which deprecates the need to master a great deal of traditional validation and verification techniques and tools, such as Selenium WebDriver or iOS and Android drivers. The new approach eliminated the need for a deep and intimate domain knowledge. A new eBay intern was able to ramp up in a matter of a day or two and start generating test data when training a ML model. Previously, some QE teams would require a few weeks of daily work in order to become familiar with the domain’s specifics and the intricate knowledge of our webpages. The new approach eliminated the need for a deep and intimate domain knowledge. A new eBay intern was able to ramp up in a matter of a day or two and start generating test data when training a ML model. Previously, some QE teams would require a few weeks of daily work in order to become familiar with the domain’s specifics and the intricate knowledge of our webpages. The QE teams witnessed a quick set-up time for ML-based testing when a single engineer was able to prepare test automation to run against their main UX components in a matter of day or two. Usually, the teams invest multiple weeks to achieve such test coverage. The QE teams witnessed a quick set-up time for ML-based testing when a single engineer was able to prepare test automation to run against their main UX components in a matter of day or two. Usually, the teams invest multiple weeks to achieve such test coverage. Our experiment kicked-off the quality assurance process early-on, using design mockups. Training our model with these wireframes allowed us to begin the QE work potentially before substantial development phase even started. Now, implementation details are becoming irrelevant for such QE process. Our experiment kicked-off the quality assurance process early-on, using design mockups. Training our model with these wireframes allowed us to begin the QE work potentially before substantial development phase even started. Now, implementation details are becoming irrelevant for such QE process. Some findings were particularly prominent when it became clear that the defects detected by the model would have been practically impossible to capture by any other means of manual or automated testing. Some findings were particularly prominent when it became clear that the defects detected by the model would have been practically impossible to capture by any other means of manual or automated testing. The model produced a classification score per asserted output. Such results allowed the QE to focus their attention on GUI artifacts with the highest probability of having a fault. The model produced a classification score per asserted output. Such results allowed the QE to focus their attention on GUI artifacts with the highest probability of having a fault. Maintaining test suites and scripts for several platforms take considerable amount of time and effort. It comes at high risk of reducing test scope, when time is of essence. Even a slight design change or refactoring of product code could end up failing the regression suites and may involve significant re-work for QE to fix the automaton code. Our ML process became agnostic to implementation details and less sensitive to the platforms it runs on. Maintaining test suites and scripts for several platforms take considerable amount of time and effort. It comes at high risk of reducing test scope, when time is of essence. Even a slight design change or refactoring of product code could end up failing the regression suites and may involve significant re-work for QE to fix the automaton code. Our ML process became agnostic to implementation details and less sensitive to the platforms it runs on. Teams were excited about the use of innovative techniques and unleashing its potential. It inspired engineers to hone their skills and learn new tools and approaches. Teams were excited about the use of innovative techniques and unleashing its potential. It inspired engineers to hone their skills and learn new tools and approaches. In addition to the benefits listed above, we are also on the hunt for adopting other useful ML-based approaches, enhancements, and optimized processes. Some of the ideas described below are work-in-progress and some are exciting future concepts we are toying with. Attribute-based assertion ML can be applied to detect abstract components, such as images/shapes/text, and extract those components and their respective positions. When focusing on a narrower perspective of ROI (Region Of Interest) we can build an attribute-based assertion to compare with the wireframes. For example, layout-related assertion, such as image position extracted from our ML model, can be checked against predefined characteristics. Such approach would provide a more granular validation mechanism. This work is beneficial for scrutinizing specific elements or areas of elements, contrasted against certain predetermined anchored attributes. Feedback-adapting models As a next step to raise the accuracy of the system, one may train the model by integrating it with new or existing processes. Bug reports can be re-used to teach the model what is a real bug and what is not. Say an issue was closed with a fixed status, this may trigger a learning opportunity for our model and add this data set into its logic. In the case of an issues being closed as “not a bug,” then the system could automatically learn how a misleading issue may look like. When a classification score is assigned with in an inconclusive range (say, a range of 0.4 to 0.7, where it is not certain whether the quality criteria was passing or failing) then human judgment should be applied and fed back into the training process. Packaging ML modalities and creating an open-source service This testing methodology is browser, resolution, and even language agnostic if we use certain OCR libraries, therefore we need not train our models against different browsers, resolutions, or localization constraints. Further enhancements could be done by packing Accessibility and Application Security libraries. Captions and tabbing can be run against Accessibility requirements (by WCAG2.0, for example) as part of the ML modalities. Enhancing our current system into a full-scale service solution would let the service take our training images with other hyper parameters, such as the module name and module parameters, into consideration. In turn, this would allow us to create a full-fledged real-time solution for entire webpages, where each module would be a test case and a collection of modules would be called as a test suite. Test suites would be available for each webpage. The work above could be open-sourced for communities benefit by sharing ideas and libraries that enhance and extend the ML modalities. Such real-time service could become widely available for common use. Enhancements to Software Design Verification DL-powered GUI testing could be expanded into additional testing fields in software design verification techniques, where the machine develops an understanding about the relationships between businesses and people. Software design subjects, such as visual functionality, usability, accessibility, and others, could be captured by the same ML paradigm when enhanced with additional modalities. Some software functionality matters may manifest in a graphic manner, like in cases of slow-loading ads or poor implementation of user-control widgets. Usability has become an increasingly important factor, when Apple’s Human Interface Guidelines or the Android User Interface Guidelines may accept or reject certain an application’s availability in their stores. While User Experience Research (UER) is limited by time, resources, and quality of human feedback, DL can compute the highest score for effective Usable Designs, cutting down time and costs. Multimedia Accessibility Testing (especially for complying with Web Content Accessibility Guidelines 2.0) might be done by the use of recurrent neural networks (RNNs), which holds the power to understand natural language and extract a relationship between UX components and their descriptions provided by the developers. The current days of manual and automated GUI testing gradually become ineffective when contrasted with an ML-based solution. By following the recipe explained above, both stakeholders and engineering teams who deal with UX components would gain from one of the latest technology stacks the world has to offer. By applying this DL-fueled, human-like expertise on prevalent alternatives, we can now finally scale the existing labor-intensive and skill-expensive methodologies.", "date": "2018-06-28"},
{"website": "Ebay-Engineering", "title": "‘Gifts that Give Back’ Program Enhances Listing Options for Over 54k Charities", "author": ["Ethan Rubinson", "Marci Ross", "Scott McDowell"], "link": "https://tech.ebayinc.com/product/gifts-that-give-back-program-enhances-listing-options-for-over-54k-charities/", "abstract": "Revamped Gifts that Give Back Program introduces symbolic gift listings, helping charities raise funds at an exponential rate. From helping victims of the California wildfires to granting the wishes of critically ill children, eBay’s revamped Gifts That Give Back (GTGB) program provides over 54,000 charities the tools to create symbolic gift listings. These gifts may be purchased by millions of generous donors who wish to support their favorite non-profit organizations and the causes they stand for. eBay ensures that every symbolic gift sees 100% of the proceeds go directly to the sponsoring charity. Charities such as St Jude Children's Research Hospital can now create gift listings and raise funds to purchase books , art supplies , physical therapy, and so much more! Originally piloted in late 2016 by eBay for Charity, the GTGB pilot faced several challenges. Notably, charities were unable to create gift listings on their own. Rather, each participant in the GTGB program was required to work directly with the eBay for Charity team to design, create, and manage their gifts; a process which could take up to several weeks. This changed in early 2018 when Ethan Rubinson, a Software Engineer at eBay, secured funding through eBay’s Innovation Programs to revamp the GTGB program enabling it to scale for the global market. Ethan, with the support of Marci Ross, Scott McDowell, and the entire eBay for Charity team, successfully launched the revamped program early this year. The revamped GTGB program features a new gift creation experience for charities, allowing them to create gift listings in just a few minutes. Using the GTGB Gift Creator, charities who are currently registered as Direct Sellers on the eBay platform need only specify what the gift is for, its price, how many are available, and a short description of how raised funds will benefit the cause. Then, after uploading a picture, charities can submit their gifts and immediately start sharing their new gift with their donors. Not only can charities share their gifts with their current donor base, but every gift is also discoverable by each of eBay’s 177m active buyers. With the new GTGB program, charities can also choose to automate the delivery of tax receipts and donation certificates to individuals who purchase one of their symbolic gifts. The vision behind the new Gifts that Give Back program is to allow every charity registered on eBay to create unique and personalized listings to help fundraise for a cause. An example we’re extremely proud of, is a recent campaign by Make-A-Wish UK, in partnership with Disney and DanTDM. The campaign used the flexibility of GTGB to create an innovative way to raise additional funds through a virtual gift concept.  The campaign team created a £1 listing, with the proceeds benefiting Make-A-Wish UK and the buyer of the listing being entitled to receive video credit in one of DanTDM’s video. In less than a week, over 1,400 donations were made and the resulting credits can be viewed on DanTDM’s YouTube video . Help fill the world with color by purchasing a gift that gives back today!", "date": "2018-12-28"},
{"website": "Ebay-Engineering", "title": "SRE Case Study: Triaging a Non-Heap JVM Out of Memory Issue", "author": ["Eric Tian"], "link": "https://tech.ebayinc.com/engineering/sre-case-study-triage-a-non-heap-jvm-out-of-memory-issue/", "abstract": "Most Java virtual machine out of memory issues happen on the heap, but this time proved to be a little different. A Java virtual machine (JVM) has an auto memory management feature, so Java developers don’t need to care about object reclaiming. But they should still be concerned about memory, as it isn’t unlimited, and we do see out of memory errors sometimes. For out of memory issues, there are generally two possible reasons: 1) the memory settings for the JVM are too small, and 2) applications have memory leaks. For the first type, it is easy to fix with more memory; just change some JVM memory setting parameters. For the second type, we need to figure out where the leak is and fix it in code. Today I am going to share a JVM memory leak case that is a little different. At the beginning, we noticed garbage collection (GC) overhead exceeded and CPU usage alerts for some hosts. GC overhead was around 60%~70%, and the CPU was busy with GC. It appeared to be a memory issue. Figure 1. GC overhead alert Not all the servers for that application had this issue, just some, which meant it could take time to fill up the memory, anywhere from 1 or 2 hours to a few days. In order to mitigate this issue on site, first we took a heap dump and then nuked them for temporary recovery. For GC overhead issues, we analyze the verbose GC log, analyze the heap dump, and analyze the source code. 1.     Analyze the verbose GC log The app enables the verbose GC log, which is very useful to analyze memory issues. From the following screenshot, we can see there is a lot of free memory in both young and old generations, but GC is filling up more and more. This is a little strange, as most of time, we see the both young and old generations are used up, and JVM doesn’t have enough heap to allocate a new object. This issue is not caused by less memory in the young/old generation, so where is the issue? We all know that the JVM permanent generation full and explicit System.gc() call can also trigger a full GC. Next, we check these two possibilities: 1. If the full GC is triggered by an explicit System.gc() call, we will see the “system” keyword in the GC log, but we don't see it this time. 2. If it is triggered by permanent generation full, we can easily identify it in the GC raw log. From the following GC raw log, we can see that the permanent generation has enough free memory. Verbose GC log snippet: 2018-09-13T20:23:29.058-0700: 2518960.051: [GC2018-09-13T20:23:29.059-0700: 2518960.051: [ParNew Desired survivor size 41943040 bytes, new threshold 6 (max 6) - age   1: 3787848 bytes,    3787848 total - age   2: 2359600 bytes,6147448 total : 662280K->7096K(737280K), 0.0319710 secs] 1224670K->569486K(2170880K), 0.0324480 secs] [Times: user=0.08 sys=0.00, real=0.03 secs] 2018-09-13T20:23:44.824-0700: 2518975.816: [Full GC2018-09-13T20:23:44.824-0700: 2518975.817: [CMS: 562390K->563346K(1433600K), 2.9864680 secs] 795326K->563346K(2170880K), [CMS Perm : 271273K->271054K(524288K)], 2.9869590 secs] [Times: user=2.97 sys=0.00, real=2.99 secs] 2018-09-13T20:23:58.130-0700: 2518989.123: [Full GC2018-09-13T20:23:58.131-0700: 2518989.123: [CMS: 563346K->561519K(1433600K), 2.8341560 secs] 867721K->561519K(2170880K), [CMS Perm : 271080K->271054K(524288K)], 2.8345980 secs] [Times: user=2.84 sys=0.00, real=2.83 secs] 2018-09-13T20:24:01.902-0700: 2518992.894: [Full GC2018-09-13T20:24:01.902-0700: 2518992.895: [CMS: 561519K->560375K(1433600K), 2.6886910 secs] 589208K->560375K(2170880K), [CMS Perm : 271055K->271055K(524288K)], 2.6891280 secs] [Times: user=2.69 sys=0.00, real=2.69 secs] Therefore, these two possibilities have been ruled out. In the past, we encountered a complicated case whose symptoms were similar: Both young generation and old generation had 700M free space separately after full GC, and no issue in permanent generation or explicit System.gc() call, but the JVM continued doing full GC. The cause was a java.util.Vector on heap that used about 400M memory, and it tried to extend its size. As the JDK code wrote, each time it extended, it doubled its size, so it needed an extra 800M memory to expand. The JVM couldn't find such a large free space, so it resorted to continuous full GC. This time, we didn't see this kind of big collection instance. 2.     Check the application log, and find the issue We started to analyze the heap dump, but in the meantime, in the application log, we see a very useful error message: java.lang.OutOfMemoryError: Direct buffer memory. This error points out where the issue is. OOM error in the log: INFO   | jvm 1| 2018/09/15 03:43:13 | Caused by: java.lang.OutOfMemoryError: Direct buffer memory INFO   | jvm 1| 2018/09/15 03:43:13 |    at java.nio.Bits.reserveMemory(Bits.java:658) INFO   | jvm 1| 2018/09/15 03:43:13 |    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) INFO   | jvm 1| 2018/09/15 03:43:13 |    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306) The direct buffer memory is the OS’ native memory, which is used by the JVM process, not in the JVM heap. It is used by Java NIO to quickly write data to network or disk; no need to copy between JVM heap and native memory. Java application can set the JVM parameter –XX:MaxDirectMemorySize to limit the direct buffer memory size. If no such parameter is set, the JVM can use all the available OS’ native memory. In our case, we checked the JVM’s parameter; it was set to -XX:MaxDirectMemorySize=1024M, which means this application set the Direct Buffer limit as 1G. Based on the above log, this 1G native memory was used up, and then threw the OOM error. 3.     Find the direct memory issue in the heap dump Although the direct buffer memory is out of heap, the JVM still takes care of it. Each time the JVM requests a direct buffer memory, there will be a java.nio.DirectBuffer instance to represent it in the heap. This instance had the native memory address and the size of this memory block, etc. As the DirectBuffer instance’s life cycle was managed by the JVM, it could be collected by the GC thread when there was no reference to it. The associated native memory could also be released when the JVM GC thread collected the DirectBuffer instance. Why does this app needs more than 1G direct buffer memory? Why it doesn’t it release the memory during the full GC? Now that we have the heap dump, can we find any clue from it? As we just mentioned, the DirectBuffer objects in the heap have some information about the direct buffer memory. From the application error log, the JVM tries to create a new DirectByteBuffer instance. Let’s check the DirectByteBuffer first. With OQL, we see there are lots of DirectByteBuffer instances in the heap, and we don’t see other DirectBuffer instances, like DirectCharBuffers. We can confirm how much native memory these DirectByteBuffers are using with this OQL query: SELECT x, x.capacity FROM java.nio.DirectByteBuffer x WHERE ((x.capacity > 1024 * 1024) and (x.cleaner != null)) //here we only care objects whose capacity is bigger than 1M The capacity field in DirectByteBuffer means how many memory are requested in the DirectByteBuffer instance. And here we filter the object instances with: x.cleaner != null, which means we skip the sliced DirectByteBuffer instances that are just a view of other DirectByteBuffer instances. In this dump, there are many DirectByteBuffer objects whose capacity is less than 1M; we just skip them. This is the result: In this result, there are 25 instances that are holding more than 1M native memory. The biggest one is 179M (188124977/1024/1024), and second one is 124M (130804508/1024/1024). The summary of these top 25 instances is almost 1G. That’s why the total 1G direct buffer memory is used up. 4.     Why are these DirectByteBuffer not collected by GC? If these DirectByteBuffer instances are collected by GC, then direct buffer native memory can also be released. Why can't these DirectByteBuffer instances be collected by the GC thread? We further check the reference chain. From it, we can clearly see there are some thread local BufferCaches that are holding the references of DirectByteBuffer, and these thread local objects belong to some daemon threads, like the Tomcat daemon threads. That’s why they can’t be collected, as shown in the following reference chain screenshot: Who put these DirectByteBuffers in these thread local BufferCaches? And why not remove them? Following the reference chain, we looked into the source code of sun.nio.ch.Util.java class. In this class, you see the thread local BufferCache, and you see the method: getTemporaryDirectBuffer(int), which put the DirectByteBuffer objects in the BufferCache. This getTemporaryDirectBuffer is called by serval methods in JDK’s NIO classes. Also, the BufferCache reuses the DirectByteBuffer if the thread requests are not bigger direct buffer native memory. JDK NIO classes use these thread local DirectByteBuffer instances, but don’t release them if that thread is alive. From above analysis, the issue is in the JDK’s code. This was identified as a JDK issue . In the JDK 8u102 Update Release Notes , a new system property, jdk.nio.maxCachedBufferSize, was added  to fix this issue. But in this note, it also says, this parameter can only fix part of this issue and not all cases. Most of the time, your application won’t have this issue because your threads are short-life threads, where BufferCache and DirectByteBuffer are collected by the GC thread, and the direct buffer native memory is released to the OS, or because where each time you just need very little direct buffer memory, and the JVM will reuse them. When the only multiple threads are long-life threads, and these threads request a more and more direct buffer memory until reach the max direct buffer limit or all the memory is used up, you will see this issue. For our case, the app tries to allocate some direct buffer native memory for uploaded files, and Tomcat’s daemon threads handle these requests. There are some very big uploaded files, some more than 100M, and the app opens 40 daemon threads for Tomcat, then at last, it reaches the 1G direct buffer upper limit. In order to fix it, the app should split bytes to small ones before they operate with NIO utilities. This can be changed in application logic. Mostly we see out of memory issues on the heap, but it could happen on the direct buffer. When the direct buffer native memory is used up, even when it is not on the heap, we can still use a heap dump to help analyze the root cause.", "date": "2019-02-07"},
{"website": "Ebay-Engineering", "title": "How eBay Uses Microinteractions to Delight Customers", "author": ["Ryan Parker", "Prashant Desai"], "link": "https://tech.ebayinc.com/product/how-ebay-uses-microinteractions-to-delight-customers-in-china/", "abstract": "At eBay, we’re always looking for ways to truly localize for all of our international markets. Our design team has spent some time designing and experimenting with exciting new experiences and microinteractions for shoppers in China. Chinese shopping experiences are markedly different than those in the US. The most noticeable difference is in the design language. Chinese ecommerce is incredibly vibrant, fun, and visually engaging. These shopping experiences literally call for your attention at every turn. It’s almost impossible to not stay engaged. If US ecommerce is mac and cheese, Chinese e-commerce is truffle mac and cheese with lobster. To succeed in China, your design choices really need to stand out and connect with shoppers in a meaningful way. We’ve learned through user research that shoppers think highly of companies that continuously invest in their user experience. This is where microinteractions come into play. At the root of it, it’s about getting a shopper's attention when it counts. The question is, where do you direct that attention? How eBay approaches microinteractions Microinteractions are small moments that have the potential to make a big impact, especially in China where investing in the user experience delivers measurable gains in shopper engagement and conversion. At eBay, we believe that microinteractions have the power to transform critical moments in the shopping journey. Through design and research, we’ve identified important moments to add microinteractions and have discovered through experimentation that this can positively impact our conversion metrics. However, we’re not just focused on conversion. We know that microinteractions can positively impact our experience at an even larger scale. They can affect the perception of the brand or the speed of the application . They can impact the trust a user has in the product or help educate users on complex features. They can even simply be used to delight the users at important moments. Microinteractions are often used to draw the users attention to important conversion-based moments in the experience. Think “Hey, click here!” Price Strikethrough Percentage Off Badge Number of People Watching Incentivized Share Button Microinteractions can be used to delight users and celebrate important moments. Think “Woo-hoo! You did it!” Cart Registration Confirmation Microinteractions are often used educate shoppers on complex features in a simple and enjoyable way by showing in addition to telling. Think “Hey, I can help walk you through this.” Journey Map Microinteractions can be used to signal to the user that this platform can be trusted. In China, authentic items are difficult to come by and drawing attention to ecommerce trust signals like a 30-day money back guarantee are critical in creating loyal shoppers. eBay Money Back Guarantee It’s important to not only think about the individual microinteraction, but also think about them as a unit. They should work together in harmony to walk the user through the experience and draw their attention to the right things at the right time. Below you can see how we envision many of these microinteractions working together on an item page. At eBay, if we are unsure about something's impact, we test it. We ran a simple experiment where we rolled out the animated percentage off badge to 50% of our shoppers in the China market. Our hypothesis was that having this percentage off badge animate on the homepage would increase the click through rate (CTR) of an item. The purple percentage off badge is on the top left of the discounted items. After running the experiment for three weeks and exposing a large audience to this animated badge, we were able to get statistically significant results. Those revealed that users who saw the animated percentage off badge were more likely to click an item as compared to those who saw the static percentage off badge. This helps show that animations and microinteractions not only have numerous qualitative benefits, but also strong quantitative ones. This data has empowered us to spend time integrating microinteractions as part of our design system, and we continually look at how to bring these moments to other parts of the customer experience. We’d also like to thank the other members of the design team for their contributions to this project. Thank you Orlando Angel, Fred Zaw, Susie Liu, and Pree Kolari!", "date": "2019-03-06"},
{"website": "Ebay-Engineering", "title": "Complementary Item Recommendations at eBay Scale", "author": ["Yuri M. Brovman"], "link": "https://tech.ebayinc.com/engineering/complementary-item-recommendations-at-ebay-scale/", "abstract": "Generating relevant complementary item recommendations that drive conversion at eBay is a challenging problem. In this blog post, we describe some of these challenges, and how we incorporated several different signals, including behavior-based (co-purchase, co-view, co-search, popularity) and content-based (title text), to significantly enrich the number and quality of candidate recommendations. This can produce an improved user shopping experience, which can lead to increased transactions between eBay buyers and sellers, and an increase in the number of items bought, which is good for the eBay marketplace as a whole. Generating relevant complementary item recommendations that drive conversion at eBay is no easy task. eBay is an e-commerce marketplace with 1.2 billion items and 179 million buyers, where users can buy and sell virtually anything. In addition to the challenge of the large scale, there is limited structured data attributes, such as ISBN, available for these items, which makes it difficult to use traditional collaborative filtering approaches for generating recommendations. The inventory is also volatile; some items on eBay are listed for just a week and never appear again. Given all of these constraints, it is difficult to even generate similar item recommendations given an input seed item (example: seed = iPhone 7 32GB, recommendation = iPhone 7 64GB). Generating items that would complement the seed item, so that the seed and recommended items might be purchased together in a bundle for example, is even more challenging (example: seed = iPhone 7 32GB, recommendation = iPhone 7 case). Here, we describe the complementary items algorithm we developed to solve this task. The algorithm is used in modules on several pages on the eBay site, most notably on the item page in the “Frequently Bought Together” merchandising strip below the seed item description (see example below). The module is designed to provide the user with suggestions for add-on items to the seed item that the user might not have thought of buying, thereby enhancing the user’s overall shopping mission and experience. This can lead to increasing the number of items bought, which is good for the eBay marketplace as a whole. Using implicit user feedback (item purchases) with traditional collaborative filtering methods alone does not work at eBay due to the extreme sparsity of the user-item matrix . In layman’s terms, because of the very large number of often short-lived items, the available information about items purchased together by the same user is often insufficient to make recommendations with confidence. However, performing item-based collaborative filtering on aggregations of item-level implicit user data makes sense. How do you choose the appropriate level of aggregation of items? A natural choice would be to aggregate items at the category level. eBay has a category taxonomy and all items belong to a specific leaf category in this category tree. We can aggregate user purchases to form the user-category matrix , where the columns represent leaf categories and the entries in matrix are either 1 if the user has purchased from that category or 0 otherwise. We then use cosine similarity, with appropriate thresholds, to find the top-K nearest categories to the input seed category. Finding the nearest categories (related categories) constrains our search space of possible recommendation candidate items significantly and also reduces the possibility of irrelevant recommendations. All of the items that we recommend, in all the recall sets described below, will come from these related categories. Here are two examples of the top four related categories for the given input seed categories. Each category here shows the full breadcrumb in the category tree, with the last part ( italicized ) being the leaf category. Seed category = Cell Phones & Accessories: Cell Phones & Smartphones Cell Phones & Accessories:Cell Phone Accessories: Cases, Covers & Skins Cell Phones & Accessories:Cell Phone Accessories: Cases, Covers & Skins Cell Phones & Accessories:Cell Phone Accessories: Screen Protectors Cell Phones & Accessories:Cell Phone Accessories: Screen Protectors Cell Phones & Accessories:Phone Cards & SIM Cards: SIM Cards Cell Phones & Accessories:Phone Cards & SIM Cards: SIM Cards Cell Phones & Accessories: Cell Phone & Smartphone Parts Cell Phones & Accessories: Cell Phone & Smartphone Parts Seed category = Clothing, Shoes & Accessories:Men's Shoes: Athletic Clothing, Shoes & Accessories:Men's Accessories: Hats Clothing, Shoes & Accessories:Men's Accessories: Hats Clothing, Shoes & Accessories:Men's Clothing: Jeans Clothing, Shoes & Accessories:Men's Clothing: Jeans Clothing, Shoes & Accessories:Men's Shoes: Casual Clothing, Shoes & Accessories:Men's Shoes: Casual Clothing, Shoes & Accessories:Kids' Clothing, Shoes & Accs: Boys' Shoes Clothing, Shoes & Accessories:Kids' Clothing, Shoes & Accs: Boys' Shoes One interesting question here is how to decide whether to include the original seed category in the list of related categories. Most of the time when you are performing a K-nearest neighbor (KNN) search, you would not include the input entity in your search results. However, in the case of categories, it is possible that we want to recommend items from the same category as the category of the seed item. Think of Baseball Trading Cards or Video Games categories. It is reasonable to assume that a user who is looking at a baseball trading card will want to see more baseball trading cards (same category), as opposed to say basketball trading cards (different category). We capture this logic with the following heuristic: we calculate d, the mean # of purchases / user for each category. If the value of d for a specific seed category is above a threshold, we include the seed category as a related category, and exclude it otherwise. Now that we have a first-level relevance filter (related categories), we turn our attention to how to generate the actual candidate recommendation items. A set of such candidate items is referred to as a Recall Set . The input to generating the recall sets is the information about the seed item. This is a very strong piece of context, so it is imperative that the recommendations shown to the user have some relevance to the seed item. As we saw in the previous section, we use the seed category to generate a set of related categories. Here are some of the ways we generate candidate items for recommendations using a variety of signals: Related Products: This recall set uses the collaborative filtering approach seen in the previous section, but aggregated at the product level. “What is the difference between a product and an item?” you might ask. An item refers to any listing posted by a seller, while a product at eBay is defined as a concrete entity in the real world. For instance, for books, think of the ISBN number. Having product information for items allows many items to be aggregated to the same product entity. If the seed item can be mapped to a product, we generate a recall set of related products by taking the cosine similarity of vectors of implicit feedback in the form of product-level purchase data. The relevance quality of the recommended products depends highly on the minimum thresholds of the Ochiai coefficient. There is always a coverage/quality tradeoff here. Coverage here is defined as the percent of input seed items for which our algorithms produces recommendation results. Higher coverage typically means lower quality and vice-versa. Human judgment and business rules often guide the balance of this tradeoff. Since the final results that we want to show the user are items, we have a separate mapping from products to items that is stored in a cache. We generate this product-to-item mapping by aggregating the most-viewed items for a given product, which incorporates a popularity signal into the results. Co-views: While the last recall set utilized the purchase behavioral signal, this recall set utilizes the view behavioral signal. An item purchase is an ultimate sign of user intention. While a view signal carries less intention, for instance a user might simply be browsing, the benefit to using this signal is the sheer increase in volume/coverage of recommendations. We use this signal to generate recall sets at the product level and directly at the item level, since co-view data is dense enough. Recall sets that use the co-view signal are high quality in terms of conversion. Related Queries: Besides co-purchase and co-view signals, another source of behavioral data is co-searches. The related queries recall set is contextualized to a search session and incorporates the user search query into the recommendations. We developed a cache of related queries that uses user co-search signals (Ex: “digital camera” might be co-searched with “canon SLR”). To map the queries to items, we utilize another cache where we store the most popular items for a particular query. When a user arrives at an item page from a search page, popular items from related queries will be displayed. Compatibility: Issues with compatibility between the seed and recommended items can be a serious concern for quality in hard goods categories such as electronics. Recommending a Samsung cell phone case for an Apple iPhone cell phone is a bad user experience that will make the user lose trust in eBay’s recommendations. In general, there is an implicit user assumption that recommended items will fit well with the seed item. Therefore it is important to take compatibility into consideration when generating complementary recommendations. So far we have discussed sources of recall that use behavioral signals in some way. Often, behavioral signals are not available (cold start problem) so we look to content-based signals to generate recommendations. Some items have compatibility/fitment data associated with them. We curate pairs of aspect names in certain hard goods categories to validate compatibility. An aspect is a structure data attribute for an item. For example, we make sure recommended items with a “compatible model” aspect match the seed items’ “model” aspect. In addition to generating a recall set using this method, we also have a filter so that other recall sets benefit from this compatibility enforcement. Complementary of Similar: Often we encounter the situation when there will be complementary recommendations for a product (Ex: “Silver iPhone 7 32GB”), but a nearly identical product (Ex: “Gold iPhone 7 32GB”) will not have results, perhaps due to lack of behavioral data for instance. We address this problem by developing a “complementary of similar product” type algorithm. We generate product embeddings using textual information from the product title and aspects and use eBay’s GPU cluster to find similar products with a KNN search of the product embeddings. Therefore, when there is no direct complementary results from a seed product, this recall set will return complementary items from similar products (from product embeddings). DeepRecs: While the last recall set focused on finding product-based embeddings, the DeepRecs recall set explores item embeddings directly. This recall set uses a text-based deep learning model, incorporating the title, aspects, and category information, from both seed and recommendation candidate item pairs, trained with the implicit co-purchase signal as the target. The co-purchase probability between the seed and recommendation candidate items within the related categories is then calculated using the neural network architecture on a GPU cluster, and the top-K results are returned. Comparing item embeddings, which incorporates textual content information, instead of implicit item vectors directly, as in the case of collaborative filtering, helps address the sparsity issue endemic to eBay data. Details of this approach can be found here . Popular: When we run out of other behavioral or content-based signals, we fall back to the popular items in a related category recall set. Due its low relevance quality, this recall set is not used in all versions of the algorithm. This recall set has the lowest operation performance in terms of click-through rate (CTR) and purchase-through rate (PTR) metrics, and it is used as a baseline when developing new recall sets. In addition to storing popular items in a category in our cache, we also store popular items in a category with an aspect for all category-aspect combinations. An improved version of this recall set additionally matches an important aspect in each category (such as  brand for fashion), between the seed and recommended item. The important aspect for a given category is generated using another sub-model. For the most part, we have focused on the algorithms and modeling details up to this point. Here is a high-level overview of our engineering architecture. We typically perform most of our aggregation/offline computation of historical data in a Hadoop cluster using Twitter’s Scalding library or Spark. Training deep learning models as well as performing KNN search of embeddings is done in a GPU cluster. All of the pre-aggregated results are then stored in a Couchbase DB cache for access at run time. The backend Scala application for serving live production traffic uses data from eBay internal real-time services as well as the Couchbase DB to generate the results. We are leaving out many details here, but this is a simplified overview of the overall architecture. Our real-time application serves over 1 billion impressions daily. We presented numerous algorithmic features and models that together constitute the complementary recommender system at eBay. It is the combination of many smaller components that produces a high level of quality as well as coverage. It is important to have stable sub-components and sub-models as well as the overall engineering infrastructure to produce final recommendations that are robust. Incorporating several different signals, including those based on behavior (co-purchase, co-view, co-search, popularity) and content (title text), significantly enriches the coverage of complementary recommendations. In a typical information retrieval system, the retrieval process is divided into a recall and a ranking stage. In this blog post, we have focused mainly on the recommendation candidate generation (recall) stage. In another blog post, we will review our approach for the ranking stage, which is analogous to what was previously done for the similar items algorithm . An analogy comes to mind. Building a recommender system is like baking a cake: the recall sets are the cake, the ranking is the frosting on the cake, and personalization is the cherry on top. Here we described how to bake the cake, and we will leave applying the frosting for another time! We would like to acknowledge several people who have worked on components of this algorithm including Tommy Chen, Daniel Galron, Sourigna Phetsarath, Mike Firer, Natraj Srinivasan, Ved Deshpande, Aditi Nair, Katrina Evtimova, Paul Wang, Shuo Yang, and Barbara Duckworth.", "date": "2019-02-13"},
{"website": "Ebay-Engineering", "title": "eBay OAuth Client Library", "author": ["Sandeep Dhiman"], "link": "https://tech.ebayinc.com/engineering/ebay-oauth-client-library/", "abstract": "To make integrations with eBay RESTful APIs easier, eBay provides client libraries in C# and Java to make it simpler to set up authorization and reduce the amount of code application developers have to write to get OAuth Access Tokens. All eBay RESTful APIs use the OAuth 2.0 protocol for authorization. OAuth is the industry standard for assuring your online transactions are secure, and you must provide a valid access token for each request you make to the eBay RESTful interfaces. OAuth access tokens verify to eBay that a request is coming from a valid client and that the application has the user's authorization to carry out the requests. (Learn more about the OAuth Access Tokens .) eBay provides OAuth client libraries. The C# version can be downloaded from GitHub here , and the Java version can be downloaded from this GitHub location . The eBay OAuth Client Library supports different grant flows: Client credentials grant allows an application to get an access token for resources owned by the client. This grant type flow is for a client app and the authorization server. Authorization code grant is used by the application to get an access token after a user authorizes the application. The application acts on behalf of a registered end-user. The client sends a user to eBay Sign-In flow to authenticate and grant access to the application. Refresh token grant is used by clients to exchange refresh token for an access token when the access token has expired. Learn more about the Access Token Grant . The library caches the client credential grant type access token and refreshes the token if it’s already expired. eBay OAuth Client is a class library that targets the .NET Standard 2.0 . This library can be used by any .NET implementation that supports 2.0 version of the .NET Standard. For all of our RESTful APIs, we exposed contracts based on the OpenAPI specification . There are plenty of open source tools (Swagger Codegen is one of them), to generate client SDK for the API from OpenAPI specification. eBay OAuth C# Client Library is an open source project and use of this source code is governed by an Apache-2.0 license . If you're looking for the latest stable version (1.0.0), you can grab it directly from NuGet.org (.NET Standard 2.0). Prerequisites : Visual Studio 2017 with the Universal Windows Platform development workload, or Visual Studio 2017 with the Universal Windows Platform development workload, or Visual Studio 2015 Update 3 with Tools for Universal Windows Apps. Visual Studio 2015 Update 3 with Tools for Universal Windows Apps. You can add eBay.OAuth.Client version 1.0.0 as a NuGet Package in your application: In Solution Explorer, right-click NuGet in .csproj and choose Add Package. In Solution Explorer, right-click NuGet in .csproj and choose Add Package. Search for eBay.OAuth.Client, select that package in the list and click on Add Package Search for eBay.OAuth.Client, select that package in the list and click on Add Package Accept the License prompt Accept the License prompt Use the following command in your project directory, to install the eBay.OAuth.Client package: Install-Package eBay.OAuth.Client -Version 1.0.0 Use the following command in your project directory, to install the eBay.OAuth.Client package: After the command completes, open the .csproj file to see the added reference: <ItemGroup>\n   <PackageReference Include=\"eBay.OAuth.Client\" Version=\"1.0.0\" />\n</ItemGroup> After the command completes, open the .csproj file to see the added reference: Use the following command in your project directory, to install the eBay.OAuth.Client package: dotnet add package eBay.OAuth.Client --version 1.0.0 Use the following command in your project directory, to install the eBay.OAuth.Client package: After the command completes, open the .csproj file to see the added reference: <ItemGroup>\n   <PackageReference Include=\"eBay.OAuth.Client\" Version=\"1.0.0\" />\n</ItemGroup> After the command completes, open the .csproj file to see the added reference: Use the following command in your project directory, to install the eBay.OAuth.Client package: paket add eBay.OAuth.Client --version 1.0.0 Use the following command in your project directory, to install the eBay.OAuth.Client package: After the command completes, open the .csproj file to see the added reference: <ItemGroup>\n   <PackageReference Include=\"eBay.OAuth.Client\" Version=\"1.0.0\" />\n</ItemGroup> After the command completes, open the .csproj file to see the added reference: Clone the project from https://github.com/eBay/ebay-oauth-csharp-client : paket add eBay.OAuth.Client --version 1.0.0 Clone the project from https://github.com/eBay/ebay-oauth-csharp-client : After cloning the project, you can build the package from the source by executing the following script from the project directory : ./build.sh After cloning the project, you can build the package from the source by executing the following script from the project directory : ebay-oauth-csharp-client.dll will be created at ebay-oauth-csharp-client/bin/Debug/netstandard2.0/ ebay-oauth-csharp-client.dll will be created at ebay-oauth-csharp-client/bin/Debug/netstandard2.0/ Create a config YAML file in your application. The config file should contain your eBay applications keys: App Id, Cert Id & Dev Id. Learn more about creating application keys . name: ebay-config\napi.sandbox.ebay.com:\n    appid: <appid-from-developer-portal>\n    certid: <certid-from-developer-portal>\n    devid: <devid-from-developer-portal>\n    redirecturi: <redirect_uri-from-developer-portal>\n\nApi.ebay.com:\n    appid: <appid-from-developer-portal>\n    certid: <certid-from-developer-portal>\n    devid: <devid-from-developer-portal>\n    redirecturi: <redirect_uri-from-developer-portal> A sample config file is available at https://github.com/eBay/ebay-oauth-csharp-client/blob/master/Tests/ebay-config-sample.yaml Create a config YAML file in your application. The config file should contain your eBay applications keys: App Id, Cert Id & Dev Id. Learn more about creating application keys . A sample config file is available at https://github.com/eBay/ebay-oauth-csharp-client/blob/master/Tests/ebay-config-sample.yaml Once the config file is ready, use the following code to load it: CredentialUtil.Load(“YAML config file path”); or \nCredentialUtil.Load(System.IO.StreamReader); It is recommended to load the credentials during startup time (initialization) to prevent runtime delays. Sample: CredentialUtil.Load(“ebay-config-sample.yaml”); Once the config file is ready, use the following code to load it: It is recommended to load the credentials during startup time (initialization) to prevent runtime delays. Sample: Once the credentials are loaded, call any operation on OAuth2Api . GetApplicationToken : Use this operation when the application requests an access token to access their own resources, not on behalf of a user. OAuth2Api.GetApplicationToken(OAuthEnvironment environment, IList scopes) Learn more about Client Credentials grant flow . Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/buy.marketing\",\n     \"https://api.ebay.com/oauth/api_scope\"\n};\nOAuthResponse oAuthResponse = oAuth2Api.GetApplicationToken(OAuthEnvironment.PRODUCTION, scopes); GenerateUserAuthorizationUrl : Use this operation to get the Authorization URL to redirect the user to. Once the user authenticates and approves the consent, the callback needs to be captured by the redirect URL setup by the application. OAuth2Api.GenerateUserAuthorizationUrl(OAuthEnvironment environment, IList<String> scopes, String state) Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/commerce.catalog.readonly\",\n     \"https://api.ebay.com/oauth/api_scope/buy.shopping.cart\"\n };\nString state = \"current-page\";\nString authorizationUrl =oAuth2Api.GenerateUserAuthorizationUrl(OAuthEnvironment.PRODUCTION, scopes, state); ExchangeCodeForAccessToken : Use this operation when an application exchanges an authorization code for an access token. After the user authenticates, approves the consent and returns to the application via the redirectURL, the application will get the authorization code from the URL and use it to request an access token. OAuth2Api.ExchangeCodeForAccessToken(OAuthEnvironment environment, String code) Learn more about Authorization Code grant flow . Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/commerce.catalog.readonly\",\n     \"https://api.ebay.com/oauth/api_scope/buy.shopping.cart\"\n };\nString code = \"v^1.1************************Yw\";\t\nOAuthResponse oAuthResponse = oAuth2Api.ExchangeCodeForAccessToken(OAuthEnvironment.PRODUCTION, code); GetAccessToken : Usually access tokens are short lived. Use this operation to update the access token. OAuth2Api.GetAccessToken(OAuthEnvironment environment, String refreshToken, IList<String> scopes) Learn more about Using a refresh token to update the access token . . Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/commerce.catalog.readonly\",\n     \"https://api.ebay.com/oauth/api_scope/buy.shopping.cart\"\n };\nString refreshToken = \"v^1.1******************2MA==\";\nOAuthResponse oAuthResponse = oAuth2Api.GetAccessToken(environment, refreshToken, scopes); GetApplicationToken : Use this operation when the application requests an access token to access their own resources, not on behalf of a user. OAuth2Api.GetApplicationToken(OAuthEnvironment environment, IList scopes) Learn more about Client Credentials grant flow . Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/buy.marketing\",\n     \"https://api.ebay.com/oauth/api_scope\"\n};\nOAuthResponse oAuthResponse = oAuth2Api.GetApplicationToken(OAuthEnvironment.PRODUCTION, scopes); GenerateUserAuthorizationUrl : Use this operation to get the Authorization URL to redirect the user to. Once the user authenticates and approves the consent, the callback needs to be captured by the redirect URL setup by the application. OAuth2Api.GenerateUserAuthorizationUrl(OAuthEnvironment environment, IList<String> scopes, String state) Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/commerce.catalog.readonly\",\n     \"https://api.ebay.com/oauth/api_scope/buy.shopping.cart\"\n };\nString state = \"current-page\";\nString authorizationUrl =oAuth2Api.GenerateUserAuthorizationUrl(OAuthEnvironment.PRODUCTION, scopes, state); ExchangeCodeForAccessToken : Use this operation when an application exchanges an authorization code for an access token. After the user authenticates, approves the consent and returns to the application via the redirectURL, the application will get the authorization code from the URL and use it to request an access token. OAuth2Api.ExchangeCodeForAccessToken(OAuthEnvironment environment, String code) Learn more about Authorization Code grant flow . Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/commerce.catalog.readonly\",\n     \"https://api.ebay.com/oauth/api_scope/buy.shopping.cart\"\n };\nString code = \"v^1.1************************Yw\";\t\nOAuthResponse oAuthResponse = oAuth2Api.ExchangeCodeForAccessToken(OAuthEnvironment.PRODUCTION, code); GetAccessToken : Usually access tokens are short lived. Use this operation to update the access token. OAuth2Api.GetAccessToken(OAuthEnvironment environment, String refreshToken, IList<String> scopes) Learn more about Using a refresh token to update the access token . . Sample: IList<String> scopes = new List<String>()\n{        \n     \"https://api.ebay.com/oauth/api_scope/commerce.catalog.readonly\",\n     \"https://api.ebay.com/oauth/api_scope/buy.shopping.cart\"\n };\nString refreshToken = \"v^1.1******************2MA==\";\nOAuthResponse oAuthResponse = oAuth2Api.GetAccessToken(environment, refreshToken, scopes); eBay OAuth Client Library in Java : https://github.com/eBay/ebay-oauth-java-client If this article is helpful and the code is useful, please feel free to visit the C# or Java library and add a GitHub star or become a contributor!", "date": "2019-01-17"},
{"website": "Ebay-Engineering", "title": "How Developers Power eBay’s Product-Based Shopping Experience", "author": ["Curtis Gavin"], "link": "https://tech.ebayinc.com/product/powering-ebays-product-based-shopping-experience/", "abstract": "eBay is moving towards a Product-Based Shopping Experience (PBSE) in 2018. With the new experience, buyers will be able to easily find merchandise that meets their search criteria and quickly decide which products they want to purchase. eBay continues to refine our platform, applications, and tools to make our marketplace the best place to buy and sell. Likewise, we continue to expand and distinguish our developer offerings to enable buyers, sellers, and developers to integrate with eBay in ways that allow customization and scalability. eBay’s product-based shopping experience makes it easy for buyers to find and compare products, identify the best deals, and make purchase decisions with confidence. With the current listing-based experience, searches can yield thousands of different listings and offers that buyers must comb through to find the product, offer details, and seller that are right for them. With the new product-based shopping experience, similar listings are grouped by product and product aspects. With the new experience, buyers get a short list of products with pricing and relevant rating information to help narrow the search. Once buyers select specific products, they can select product aspects, such as color. Then, they simply choose the buy box corresponding to the relevant listing details, such as item condition or listing type, to get the best deal. As the online retail industry moves to a product-based commerce model, buyers have come to expect the simplicity and control provided by a product-based shopping experience. With over one billion active listings, this model is critical to providing our buyers the best retail experience. This will, in turn, enable continued success of eBay and the sellers who use our platform. eBay is taking steps to ensure we provide the best retail standard shopping experience in the industry with the following advancements: eBay’s comprehensive product catalog with high-quality structured product data eBay’s comprehensive product catalog with high-quality structured product data API capabilities that enable sellers to easily retrieve the information they need to fuel a product-based shopping experience API capabilities that enable sellers to easily retrieve the information they need to fuel a product-based shopping experience Capabilities that help sellers and manufacturers continuously grow and refine the contents of eBay’s product catalog Capabilities that help sellers and manufacturers continuously grow and refine the contents of eBay’s product catalog Seller integration options that are simple and scalable Seller integration options that are simple and scalable eBay refers to a seller’s act of associating their inventory with specific products from the eBay catalog as product adoption. Product adoption is required for listings and offers to qualify for inclusion in the product-based shopping experience. While seller participation is not mandatory, by adopting products in the catalog, sellers’ items will be part of a retail-standard experience that will enable buyers to easily find what they want and need. Throughout 2018 and 2019, we will extend this approach to other categories across the eBay catalog in phases—and provide sellers and developers with sufficient advance notice and tools to help them list with the catalog efficiently and effectively. With each phase, we invite contributions from our expert sellers to improve the catalog and help ensure that products in the catalog are accurate and up-to-date. In listing-based commerce, sifting through thousands  of listings creates a convoluted search experience for buyers. However, when listings and offers are associated with products in the eBay catalog, similar listings and offers are grouped, filtered, and easily compared. This is why a product-based shopping experience is optimal. As eBay expands and refines the product catalog and creates relationships between products, buyers will have a much easier experience finding exactly what they need on eBay. For example, buyers might be able to find parts and accessories for their product or they may find competing products for detailed comparisons. Just as the product-based shopping experience simplifies the way buyers shop and make buying decisions on eBay, the product-based selling experience greatly simplifies the process for creating listings/offers on eBay. Once the seller has found the correct product in the eBay product catalog, they can simply attach a listing to a product, leaving them more time to concentrate on more important aspects to the selling process like pricing, shipping service, fulfillment, and returns, to ensure their products are competitive and attractive to buyers in that marketplace. The following sections outline the added or updated API capabilities that help sellers participate in the product-based shopping experience. We’ve updated our metadata APIs to allow users to programmatically determine the listing requirements for a given category and brand with respect to product adoption. Currently, this capability is provided by the GetCategorySpecifics and GetCategoryFeatures calls in the Trading API . Soon, we will add this capability to the Taxonomy API . The Taxonomy API belongs to the new modern family of eBay RESTful APIs, which offer many benefits, including consistent modeling, support for OAuth 2.0, JSON format payloads, OpenAPI specifications, performance benefits, and more. The new Compliance API , also one of eBay’s RESTful APIs, helps identify a seller’s listings/offers that do not meet the product adoption requirements for exposure in the product-based shopping experience. The new Catalog API provides capabilities to find and retrieve product data from the eBay catalog, as well to suggest edits to the catalog product data. With the Catalog API, sellers can find a product that matches various search criteria, including category IDs, keywords, product identifiers, and aspects. It also supports search refinements by applying additional aspect-based filters. Both eBay’s traditional APIs, such as the Trading API, and eBay’s RESTful APIs, such as the Inventory API, make it easy to associate seller items or inventory with matching products in the eBay catalog. Use the createOrReplaceInventoryItem call from the Inventory API to create a new Inventory Item or revise the existing one to associate your inventory with catalog products. Developers integrated with the Trading API can use AddFixedPriceItem and AddItem to associate new listings with catalog products for enabled categories and brands and the corresponding revise calls can be used similarly to associate existing listings with catalog products. Soon, when eBay can identify a “high confidence” match in the eBay product catalog for listings that do not meet product adoption requirements, the Compliance API will provide recommended products. This will significantly speed the process of product adoption. Three newly released methods in the Catalog API allow sellers to suggest edits to the catalog by submitting catalog change requests to create or revise products in eBay catalog. These new methods are now available in the Sandbox environment for early testing and easy integration. The Taxonomy API provides a way to retrieve required and available aspects for a product or category. These aspects serve as a sort of template for product catalog change request submissions. With product-based commerce, it is critical for sellers to understand the competitive forces at play for their inventory in a given marketplace. The Marketplace Insights API, which is currently in a private beta release, provides sellers with data about demand, availability, and trending pricing for a specific product on a given eBay marketplace. This helps sellers make decisions about where and how to sell their inventory on eBay to maximize exposure on product pages and to, ultimately, drive better conversion rates. The Marketplace Insights API is in Beta and is available to a limited group of eBay partners at this time. We’ll provide updates on broader availability in the future. Stay tuned. The RESTful Buy APIs allow eBay partners to create their own shopping experiences, tailored to the needs of their buyers. All of the data provided to and by sellers to help drive eBay’s product-based shopping experience is available to support buy-side applications, as well. Each new API that eBay releases includes a downloadable API contract based on the OpenAPI specification (YAML or JSON). These contracts describe the details of the API interface comprehensively. The OpenAPI specification is the industry standard for describing APIs. An OpenAPI specification is both machine- and human-readable. The OpenAPI specification is community driven and is supported with a large set of mature tools, including Swagger, which provides the ability to generate client code in over 40 programming languages. The OpenAPI Initiative (OAI) controls and administers the OpenAPI specification. OAI is a Linux Foundation project, of which eBay is a member with a seat on the Business Governing Board. eBay has also created a user experience for sellers to submit product change requests for the eBay catalog. That is, eBay has done the integration to save third-party developers time and effort. The URLs to access this experience can be retrieved via the GeteBayDetails call in the Trading API. Whitelisted applications can send sellers to this web flow to create or update products in the catalog and to check the status of their submitted change requests. Third-party applications provide access tokens for their sellers to retrieve a session token needed to initiate the session. As eBay evolves and adapts to stay ahead of commerce trends and user preferences, eBay’s public API portfolio continues to expand and improve to enable third-party developers the means to easily integrate with eBay and to create innovative and valuable experiences for their customers and ours. Product-Based Shopping Experience landing page: https://developer.ebay.com/pbse Spring 2018 Seller Update - Inventory Optimization: https://pages.ebay.com/seller-center/seller-updates/2018-spring/inventory-optimization.html", "date": "2018-06-14"},
{"website": "Ebay-Engineering", "title": "Your Own Spring Test Context", "author": ["Tony Da Silva Bernardino"], "link": "https://tech.ebayinc.com/engineering/your-own-spring-test-context/", "abstract": "When applications become big and complex, we are presented with a whole new set of challenges. As engineers, we have to find ways to overcome them. Read how we addressed one of those challenges: Spring integration tests performance. Spring Framework 1 is widely used in Java applications due to the powerful set of features it provides. 2 One important feature is the implementation of Inversion of Control (IoC) principle, 3 also known as dependency injection (DI): the object itself does not instantiate dependencies, it defines them. Spring is responsible for creating the object and passing in its dependencies. Objects created by Spring are called beans and are managed by Spring application context. 4 The startup time for Spring applications increases proportionately with the number of beans. Build time also increases. Execution of tests requiring a Spring application context, commonly called Integration Tests, contribute considerably to that build time: there are more test to execute and it takes increasingly longer to run each one of them. Here at eBay, to meet requirements such as security, accountability and reliability, Spring applications can increase to a point where integration tests are quite slow. Integration tests have an important role; 5 not writing them is out of the question. If they take too long to execute, running them often during development is also out of the question. In extremes cases, integration tests are implemented at the end and are executed only by the CI server. What if there was a way of speeding up Spring-based integration tests? Build time would much shorter, we could run tests more frequently, and we wouldn't have to wait on the CI server to get feedback. Good news, there is: creating a custom Spring context for integration tests. A Spring context can be configured using an XML-based 6 or an annotation-based approach. 7 In an annotation-based approach, classes annotated with @Component or objects created inside classes annotated with @Configuration are considered beans and are Spring aware. The scope of beans picked up by Spring can be narrowed down with @ContextConfiguration 8 using configurable values (modifiers): classes define classes to include individually classes define classes to include individually location or locations indicate XML configuration file(s) to use location or locations indicate XML configuration file(s) to use @ComponentScan used with @Configuration can include all beans contained in a entire package structure and is a good alternative to defining classes individually in @ContextConfiguration when a considerable number of beans is involved. The XML tag context:component-scan achieves the same purpose in a XML configuration approach. 9 Scanning packages for beans is easier to apply if the code is modular and coupling between packages is low. Using the mechanisms described above, Spring context can be reduced by creating only the beans required for the integration tests, improving its performance. Imagine we want to write a test for a REST 10 endpoint of a Spring application. HTTP requests to that specific endpoint should be forwarded by Spring MVC 11 to SomeController bean and cause a chain of invocations all the way down to bean Dependency . Dependency bean should replaced with a mock object in this test. We want to be able to send an HTTP request and assert its response. Figure 1: Invocation hierarchy to test The starting point to write this test is to create a @Configuration class where a Dependency mock object is initialized: Snippet 1: Configure a mock in Spring context Next, the SomeController and SomeService and SomeConfig are added to Spring context using @ContextConfiguration and specifying them individually. Notice the mock Dependency created at SomeConfig is being injected in into the test class. Snippet 2: Integration test with Spring context configuration Using Spring MockMvc 12 test util, SomeController can be made accessible through HTTP on the test, resembling production execution. Snippet 3: Registering Spring REST controller on MockMvc The test example below sends an HTTP request to the application and asserts if the response contains the expected values. The expected values can be derived from the values Dependency mock was configured to return. Snippet 4: Test example This test will have a Spring context of its own, with only the beans we specified. The beans are initialized by Spring and added to its application context. How long does it takes the test to execute? Well under a second, but you can test it for yourself. The whole source code can be found at this Github repository . The same source code contains an equivalent example for JAX-RS. 13,14 1 Spring Homepage 2 Spring Core Technologies 3 Inversion of Control - IoC 4 Understanding Application Context 5 Integration Testing 6 Spring XML-based configuration 7 Spring Context Configuration example 8 Spring Context Configuration JavaDoc 9 Using Spring XML component-scan Tag example 10 Representation state transfer (REST) 11 Spring Web MVC 12 Spring MVC Test Integration 13 JAX-RS Java EE 6 tutorial 14 Integration tests using JAX-RS example", "date": "2018-07-05"},
{"website": "Ebay-Engineering", "title": "Optimization Study on Processing Order of NetScaler Load Balancer Layer 7 Policies", "author": ["Charles Li", "Leona Zhang", "John Yang"], "link": "https://tech.ebayinc.com/engineering/optimization-study-on-processing-order-of-netscaler-load-balancer-layer-7-policies/", "abstract": "Traffic on ebay.com is processed by thousands of layer 7 policies on the load balancers. Clearly understanding the processing order ensures availability (by avoiding misconfigurations) and performance (by prioritizing the policies efficiently). NetScaler is one of the most popular load balancer devices in modern cloud computing data centers. It supports four types of layer 7 policies: Responder: Generates HTTP response upon certain HTTP request. Rewrite: Modifies the HTTP request or response. Content Switching: Routes request to downstream applications. Filter: Accept, Deny or Drop certain requests. A virtual server can have multiple types of layer 7 policies applied simultaneously. In this case, the final output would depend on the order that the policies are processed. For instance, assuming a virtual server, http://www.site1.com, has the following layer 7 policies: Responder: IF request URL = http://www.site1.com/ THEN HTTP 301 redirect to https://www.site1.com/ Rewrite: IF request = https://www.site1.com/a/ THEN rewrite it to https://www.site1.com/b/ Content Switching: IF request URL Path = /a/ THEN route to application_A ELSEIF request URL Path = /b/ THEN route to application_B Filter: IF source IP = 1.2.3.4 THEN reset TCP connection When clients access http://www.site1.com/a/: If Responder is processed first, clients are redirected to https://www.site1.com/a/. If Content Switching is processed first, the request is routed to application_A. If Rewrite is processed first, the request is modified to /b/ and then routed to application_B. If Filter is processed first, the client connection is reset. The example above shows that the processing order is critical, as the output would be totally different if the processing order varies. However many documents and online resources contain conflicting information of the processing order. For instance: This document indicates the processing order is Responder -> Content Switching -> Filter -> Rewrite. Another document indicates the processing order is Content Switching -> Responder -> Rewrite -> Filter. So we decided to perform several tests to clarify what the processing order really is. Below are the test cases, followed by our conclusion. Configure a CSVserver with two Content Switching policies: IF request URL Path = /a/ THEN route to LBVserver-A ELSEIF request URL Path = /b/ THEN route to LBVserver-B. Configure a Rewrite policy on the same CSVserver, to change the request URL Path from /a/ to /b/. Configure a Filter policy on the same CSVserver to RESET the TCP connection if source IP = 1.2.3.4. Configure a Responder policy on the same CSVserver, to HTTP 301 redirect the request URL from HTTP to HTTPS. $ curl -I http://IP-address-of-CSV/ HTTP/1.1 301 Moved Permanently Location: https://IP-address-of-CSV/ Output matches the action defined in the Responder policy. This indicates that Responder is the first place in the processing order. $ curl -I http://IP-address-of-CSV/ curl: (56) Recv failure: Connection reset by peer Output matches the action defined in the Filter policy. This indicates that Filter policy is the second place in the processing order. $ curl -I http://IP-address-of-CSV/a/ Check host in LBVserver_A, the log shows 10.20.30.40 - - [11/Jun/2018:01:02:03 +0000] \"HEAD /b/ HTTP/1.1\" 200 0 \"-\" \"curl/7.35.0\" \"-\" Output shows the request is routed to application_A. This indicates that Content Switching policy is the third place in the processing order, and accordingly, Request_Rewrite is the forth place in the processing order. Based on the test results our conclusion is that on NetScaler CSVserver, the layer 7 policies are processed in the order of Responder -> Filter -> Content Switching -> Rewrite. All the tests are executed on NetScaler MPX v11.5. Knowing the order helps eBay manage the load balancer policies precisely and efficiently, avoid misconfigurations, and ensure the availability, security, and performance of our site.", "date": "2018-07-03"},
{"website": "Ebay-Engineering", "title": "Altus Care: Applying a Chatbot to eBay Platform Engineering", "author": ["Alison Shu"], "link": "https://tech.ebayinc.com/engineering/altus-care-apply-chatbot-to-ebay-platform-engineering/", "abstract": "eBay developers recently combined several commonly used technologies, including Lucene search and the Stanford Natural Language Processor, to create Altus Care, a chatbot that provides an instant one-stop support solution for internal eBay Altus users. Altus is application lifecycle management system that sits on top of eBay's Platform as a Service (PaaS). A core component of the Altus UX, Altus Care uses a chatbot technology to provide instant help and resolve support requests without having to engage support personnel. Eventually, we want to evangelize Altus Care to the rest of eBay development as a general purpose support solution. Altus Care provides three key features: context-based dynamic FAQ, behavior flow analysis, and interactive Q&A. Context-based dynamic FAQ Altus has dozens of pages, and each page focuses on different functions. Configured in advance, each page maps to one or multiple keywords. Altus Care listens to the user's current page and displays the recommended knowledge article. This feature pushes relevant content to a user in an easy, accessible way. Behavior flow analysis Altus Care listens to the user's current page and captures error codes and messages thrown from back end. If an error is captured, Altus Care will highlight the help document related to the error in the banner. This feature improves user troubleshooting capability. Interactive Q&A If the user still needs help after viewing all the recommended FAQs, they can enter a question and view the result. Two knowledge articles will be displayed with snippets and highlights. When the user clicks the link, they will be redirected to the full article. Altus Care has allowed us to achieve the following: Increased self service: In the last 30 days, we saw 302 clicks on \"Report Issue,\" and 122 were resolved by Altus Care in real time, i.e. 40% of the support requests were resolved by Altus Care without engaging a support contact. Allowing users to help themselves can offer them quicker resolution to their issues. Unified support entrance: Altus Care is a one-stop support solution. If it can’t satisfy the user, it will redirect the user to the right support contact. In the last 30 days, 42% of the support workload came from Altus Care. By regulating the support entrance, we offer a consistent user experience and get insights into the areas blocking the users. The Altus Care back end is a chatbot platform that uses search algorithms and Natural Language Understanding (NLU) in its core technology. It’s modular and runs in Raptor production pools, which ensures high availability and reliability. The rich REST APIs streamline on-boarding and easily expand to new domains other than Altus. Altus Care includes four modules: Admin UI: the web console lets administrators easily manage the chatbot lifecycle, including onboard, configure, off-board, and statistics insights. Crawler: the service that crawls the knowledge base (KB). The KB source is configurable; it could be JIRA, wiki, GitHub, or a data file. It’s schedulable; it can crawl data at a specified time and date, such as 7 p.m. daily. Database: a Redis cluster used as a public storage solution. The KB and domain profile are stored here. Back end Service: the most important module. It contains the open API, the search service, and the NLU service. Altus Care has three data flows: Configuration data flow: the domain configuration data from the Admin UI syncs to the crawler and back-end service, respectively API calling data flow: API requests from the client (i.e. Altus Care front end) to the back-end service mostly query search Data loading/sync flow: the data flow from the crawler to the database to the back-end service is all about the KB and index We use Lucene as the index and search base library and enrich it. The chatbot handles: Synonyms: e.g. prod = production, app = application Keyword boosting: e.g. CNAME^4 Stemming and lemmatization: e.g. failed = fail, pools = pool We use the Stanford NLP and refine it to identify user intention. In general, the chatbot works out the grammatical structure of a user query and boosts it accordingly. Here is an example of how the chatbot handles a query: A user queries “How to create prod pools?” The NLU Handler produces a grammar tree. The chatbot implements boosting based on the grammar tree and pre-defined rules. The chatbot removes stopwords, performs stemming and lemmatization, and implements domain keywords boosting. The chatbot handles synonyms. To understand how the grammar tree works together with pre-defined rule, we can refer to the following chart. The chatbot supports Altus Care very well and is capable of expanding to more domains. Plus, rich REST APIs enable the following features: Domain lifecycle management Onboard: domain profile, NLU and KB configuration Data manipulation: Insert, update, retrieve, delete operations on the domain KB and index, domain configuration update Search: multiple options (Top N, snippets and highlights); feedback Statistics and analysis: search history, top queries, normalized query and resolving rate Offboard: retire the domain Onboard: domain profile, NLU and KB configuration Data manipulation: Insert, update, retrieve, delete operations on the domain KB and index, domain configuration update Search: multiple options (Top N, snippets and highlights); feedback Statistics and analysis: search history, top queries, normalized query and resolving rate Offboard: retire the domain Flexible user interface A client can call the chatbot API and integrate it with a website or application A client can call the chatbot API and integrate it with a website or application Easy onboard A pure rule-based or trained chatbot works well for one domain, but needs its rules redefined or retrained to work well for a new domain. Altus Care’s chatbot solution provides a hybrid solution that enables the global optimum, based on a tuned search algorithm and NLU, and also allows domain-configurable attributes taking account of domain specifics. The solution balances domain commonality and specialty and makes on-boarding plug and play. A pure rule-based or trained chatbot works well for one domain, but needs its rules redefined or retrained to work well for a new domain. Altus Care’s chatbot solution provides a hybrid solution that enables the global optimum, based on a tuned search algorithm and NLU, and also allows domain-configurable attributes taking account of domain specifics. The solution balances domain commonality and specialty and makes on-boarding plug and play. A pure rule-based or trained chatbot works well for one domain, but needs its rules redefined or retrained to work well for a new domain. Altus Care’s chatbot solution provides a hybrid solution that enables the global optimum, based on a tuned search algorithm and NLU, and also allows domain-configurable attributes taking account of domain specifics. The solution balances domain commonality and specialty and makes on-boarding plug and play. Altus Care successfully applies chatbot technology to the eBay Platform engineering. It improves the user experience by providing a one-stop support solution and also improves the engineering efficiency by reducing human contact. We recommend Altus Care as a general purpose support solution within eBay and believe it will contribute to internal customer focus in an inventive way. Apache Lucene https://lucene.apache.org/ Stanford NLP https://nlp.stanford.edu/", "date": "2018-05-23"},
{"website": "Ebay-Engineering", "title": "eBay’s Platform is Powered by AI and Fueled by Customer Input", "author": ["Sanjeev Katariya"], "link": "https://tech.ebayinc.com/engineering/ebays-platform-is-powered-by-ai-and-fueled-by-customer-input/", "abstract": "Artificial intelligence is woven into all aspects of eBay’s platform, inspiring economic empowerment and enhanced experiences. Artificial intelligence touches every experience within eBay. It is woven into all aspects of the eBay marketplace, anticipating the needs and wants of buyers and sellers, inspiring shoppers on the hunt for something special, empowering entrepreneurs looking to grow their business, and making the platform more accessible to everyone. Being one of the most trusted, democratized, global marketplaces means navigating additional layers of complexity. eBay does not have its own storefronts, warehouses or delivery channels. We don’t own inventory. We use AI to provide structure in a non-structured world made up of millions of buyers and sellers across 190 markets, interacting with 1.2 billion listings. Within that challenge lies our advantage. We have over two decades of data and customer insights that we use to train our algorithms and make our AI smarter. Every time a user interacts with the marketplace, the AI learns and provides feedback so we can create a better experience.The advances have been accelerated by developments in deep learning that allows us — and others — to make longer strides in how we process data. eBay develops and deploys AI at scale to evolve the experience on our platform through computer vision, recommendation systems, machine translation, natural language processing, search, personalization, insights and discovery. Creating Highly Personalized and Inspiring Shopping Experiences eBay isn’t the same site it used to be. Over the last few years, we’ve worked to make our shopping experience more relevant to every shopper’s needs, whether customers are looking to discover, be inspired, find a specific item or just browse. AI has become the key to understanding buyer behavior and removing friction to ensure we’re serving up the best experiences. We want all shoppers to easily interact with our platform without having to sign up, taking advantage of the same parts of eBay that are the most fun for our power shoppers. We use technology to personalize the guest experience along the way, sometimes even before the guest decides to become an eBay registered user. By dramatically simplifying the guest experience, we halved the number of steps it takes to purchase an item. That made many things — from receiving guest recommendations to checking out as a guest — much easier. There is no logging in. Just browse and effortlessly purchase. Customers can sign up for eBay after they get exactly what they want. We want all shoppers to easily interact with our platform without having to sign up, taking advantage of the same parts of eBay that are the most fun for our power shoppers. We use technology to personalize the guest experience along the way, sometimes even before the guest decides to become an eBay registered user. By dramatically simplifying the guest experience, we halved the number of steps it takes to purchase an item. That made many things — from receiving guest recommendations to checking out as a guest — much easier. There is no logging in. Just browse and effortlessly purchase. Customers can sign up for eBay after they get exactly what they want. We use our AI to improve how we understand user intent in natural language processing and how we measure units. That understanding is applied to not only our diverse sets of buyers and sellers but also to the state of items, price points and shipping conditions. We use our AI to improve how we understand user intent in natural language processing and how we measure units. That understanding is applied to not only our diverse sets of buyers and sellers but also to the state of items, price points and shipping conditions. Improved search diversity uses AI to understand context, showing a full spectrum of relevant products. For example, a search for “smart home” surfaces various relevant aspects such as smart home compatibility (Alexa, Google Assistant, Apple HomeKit, Nest, Philips Hue) or smart home protocol (Wi-Fi, Bluetooth), brand, condition and price so the buyer can pick what is right for them. Improved search diversity uses AI to understand context, showing a full spectrum of relevant products. For example, a search for “smart home” surfaces various relevant aspects such as smart home compatibility (Alexa, Google Assistant, Apple HomeKit, Nest, Philips Hue) or smart home protocol (Wi-Fi, Bluetooth), brand, condition and price so the buyer can pick what is right for them. In 2018, we evolved the eBay homepage and added Interests, the ability for our customers to instantly personalize their shopping experience based on passions, hobbies and styles.Interests uses a combination of machine learning and human curation to make shopping more inspiring. We saw 2.6 million users onboard across five countries since its launch. Shoppers who love it really love it, with power users averaging 18 Interests on our native app and 10 Interests on desktop. In 2018, we evolved the eBay homepage and added Interests, the ability for our customers to instantly personalize their shopping experience based on passions, hobbies and styles.Interests uses a combination of machine learning and human curation to make shopping more inspiring. We saw 2.6 million users onboard across five countries since its launch. Shoppers who love it really love it, with power users averaging 18 Interests on our native app and 10 Interests on desktop. Shopping is a visual experience, and through computer vision, we are turning camera phones into ecommerce devices. In 2017, eBay launched Image Search, which allows buyers to use images as a search query, whether it’s a photo they take or a picture of an item they are inspired by on eBay. Image Search makes it easier to find exactly what they are looking for. Shopping is a visual experience, and through computer vision, we are turning camera phones into ecommerce devices. In 2017, eBay launched Image Search, which allows buyers to use images as a search query, whether it’s a photo they take or a picture of an item they are inspired by on eBay. Image Search makes it easier to find exactly what they are looking for. All of these improvements use AI to power a sharper experience for our buyers. We have a better understanding of who our shoppers are, what they might be looking for and how they want to shop. Delivering Our Platform as a Service to Enable Sellers Success eBay is building a stronger relationship with our sellers through AI and investing in technology so they don’t have to. We level the playing field for sellers who may not have access to cutting-edge AI-powered tools. That way, sellers can focus on sourcing, fulfilling sales and growing their businesses. Our platform manages data, metrics and analytics that we then surface to our sellers to help them succeed. Investing in technologies, tools and our platform to help sellers thrive is the heart of eBay's mission. Using natural language processing, we can accelerate productlisting time by refining the search process to find the best matched item. It leads to sellers saving time while getting better descriptions and details about products. Using natural language processing, we can accelerate productlisting time by refining the search process to find the best matched item. It leads to sellers saving time while getting better descriptions and details about products. We push the boundaries for our first-party ad experiences by providing advanced promoted listings on eBay, and we better tailor our advertisements off eBay through ad science, improving how we engage users. Through that, we continue to improve the purchase journey and inspire our buyers to discover more of our global inventory. We push the boundaries for our first-party ad experiences by providing advanced promoted listings on eBay, and we better tailor our advertisements off eBay through ad science, improving how we engage users. Through that, we continue to improve the purchase journey and inspire our buyers to discover more of our global inventory. eBay is in 190 markets and enables cross border trade by supporting ecommerce transactions across the globe. eBay is pushing boundaries in machine translation. The idea is to create a unified AI system that processes multiple languages and creates faster processing when buyers shop internationally. A study focused on English to Spanish machine translation on the eBay platform found that the system increased exports by 17.5 percent, in terms of the effect on international trade based on U.S. exports. eBay is in 190 markets and enables cross border trade by supporting ecommerce transactions across the globe. eBay is pushing boundaries in machine translation. The idea is to create a unified AI system that processes multiple languages and creates faster processing when buyers shop internationally. A study focused on English to Spanish machine translation on the eBay platform found that the system increased exports by 17.5 percent, in terms of the effect on international trade based on U.S. exports. We want to give sellers better insights not just with shipping but also with pricing and sourcing. We’re committed to using machine learning to help sellers identify the best prices to help an item sell, know when they should be looking at or listing a specific product to get the highest market value, and market themselves better to attract buyers. We want to give sellers better insights not just with shipping but also with pricing and sourcing. We’re committed to using machine learning to help sellers identify the best prices to help an item sell, know when they should be looking at or listing a specific product to get the highest market value, and market themselves better to attract buyers. Bringing Together the Experience, the Platform and the Science Making billions of real-time decisions to optimize for business outcomes is difficult in any ecommerce environment. But making enhanced experiences for buyers and sellers in a dynamic global marketplace creates additional layers of complexity. It’s because of our modern AI architecture that eBay is able to provide insights and personalized experiences at scale. We take a four-pronged approach: improve the customer experience, build and deploy the right infrastructure for the unique needs of eBay, apply core AI and domain AI science to all that we do, and ensure everything is done in service of the customer experience. That approach coupled with our AI platform allows us to be nimble and scalable in our development. We use recent technological advancements to solve more difficult problems. While we’ve elevated the experience for our customers with AI for more than a decade, the recent advances in deep learning have enabled us and others to solve problems we were not able to in the past. That’s enabled us to scale to larger and more complex data sets and helps build our structured data, recommendation system , visual search and machine translation services . Through deep learning models, we process billions of data points and are reaching human-level competency. Our models are being developed to learn from and adapt to feedback. We use recent technological advancements to solve more difficult problems. While we’ve elevated the experience for our customers with AI for more than a decade, the recent advances in deep learning have enabled us and others to solve problems we were not able to in the past. That’s enabled us to scale to larger and more complex data sets and helps build our structured data, recommendation system , visual search and machine translation services . Through deep learning models, we process billions of data points and are reaching human-level competency. Our models are being developed to learn from and adapt to feedback. We used open source technology to build an in-house AI platform that reaches across eBay, to enable collaboration and training of our AI models at scale. It allows our data scientists and engineers to experiment, build products and experiences for customers, and leverage AI at scale. We used open source technology to build an in-house AI platform that reaches across eBay, to enable collaboration and training of our AI models at scale. It allows our data scientists and engineers to experiment, build products and experiences for customers, and leverage AI at scale. We leverage AI to accelerate ecommerce within our developer ecosystem. APIs help accelerate selling and buying from eBay by allowing developers to rapidly create and integrate. APIs for Image Search, Machine Translation and Marketplace Feed are all available for developers to implement in their business. AI helps us display the right items from eBay’s buying experience on partner sites. We leverage AI to accelerate ecommerce within our developer ecosystem. APIs help accelerate selling and buying from eBay by allowing developers to rapidly create and integrate. APIs for Image Search, Machine Translation and Marketplace Feed are all available for developers to implement in their business. AI helps us display the right items from eBay’s buying experience on partner sites. In 2018, we introduced product-based shopping, which helps buyers find items quickly and easily.Organizing the world’s largest catalog is a massive undertaking, but new AI capabilities will allow us to opt-in millions of listings in the torso and tail of our inventory that we believe will drive SEO and social traffic moving forward. In 2018, we introduced product-based shopping, which helps buyers find items quickly and easily.Organizing the world’s largest catalog is a massive undertaking, but new AI capabilities will allow us to opt-in millions of listings in the torso and tail of our inventory that we believe will drive SEO and social traffic moving forward. We apply specialized domain science to further our marketplace intelligence around ecommerce-specific issues like shipping times, risk and trust. We apply specialized domain science to further our marketplace intelligence around ecommerce-specific issues like shipping times, risk and trust. Recent advances in AI are so powerful that AI researchers need to make it explainable in order to understand why our systems make certain decisions. At eBay, we improved our models and reduced bias by studying how our systems understand visual aspects of brands. Recent advances in AI are so powerful that AI researchers need to make it explainable in order to understand why our systems make certain decisions. At eBay, we improved our models and reduced bias by studying how our systems understand visual aspects of brands. Our approach allows us to determine what methods work and at what time so that we can create these meaningful experiences for our buyers and sellers. eBay is a robust marketplace and AI is at the center of that journey, positioning the platform as a leading intelligent marketplace, and we’ll never stop aiming for the best customer experience. Looking ahead, we will continue down the path of innovation through eBay’s AI-managed marketplace, knowing that the scope of what’s possible with AI expands every day. We’ll continue to share what we’re discovering and how we’re incorporating AI on our platform to create the most personalized experiences.", "date": "2019-03-13"},
{"website": "Ebay-Engineering", "title": "Integration Testing with React and Enzyme", "author": ["Matthew Wood"], "link": "https://tech.ebayinc.com/engineering/integration-testing-with-react-and-enzyme/", "abstract": "As a React application grows, it is important to ensure that all the components continue to work together. Using Enzyme, we created an integration testing framework that tests a React application in its entirety, without relying on a server. The result was a lightning fast integration test suite that gives us high confidence that the critical paths of our application are always working. When coding an application, it is essential to have tests as part of the code base. Martin Fowler proposed a metaphor that tells us the different types of tests and how many of each we should have in a project. It’s called the testing pyramid : We were recently tasked with creating an integration testing framework for a React application we maintain. We had a few requirements for our integration tests. The tests should: Be quick to run. Be quick to run. Be easy to implement. Be easy to implement. Have the ability to cover the entire React app without relying on a server. Have the ability to cover the entire React app without relying on a server. Our first action was a short spike into the best tools to use, and inspired by a great blog post titled React integration testing with Enzyme , we settled on Enzyme . Enzyme is a Javascript testing utility for React that can render React components into a virtual DOM. We can then interact with the DOM and assert that our rendered components are working as expected. Typically, Enzyme is used for unit testing by shallow rendering a single component in isolation, but it also allows us to fully render components, including all nested components, with the mount function. Since everything in React is a component, there is nothing stopping us from mounting the top level component and rendering the entire application in memory. We can then use the Enzyme API to interact with our app in any way we wish. Let’s imagine a simplified version of the shipping label printing flow on eBay. Our app will have two pages. On page 1, we will load two possible carrier services from the server and display them to the user. The user will select the carrier they want their package to be delivered with, enter the destination address, and click submit. This will fire off a request to the server containing the user’s input details. We will then return a link to a shipping label and display it in a new page (page 2). The user can then open the shipping label, print it, and stick it on their package. Mocking the Server In order to isolate the React application, we need to mock the requests and responses from the server. Using the fetch-mock library, we were able to set our mocks as follows. Before each test in our integration test, we are setting up our mock calls to the server. Our app can do two possible requests. One to get the carriers when the app loads, and one to submit the user’s details and get a label link. The beforeEach block should be responsible for every call that is required to get the app into the first usable state. In our case, that’s the get carriers request. Any other mock calls that might be needed from there (our submit details request) should go into the individual test. The reason for this being that not all tests need the submit details request (integration tests on just the details page for example), so the call should not be set up if it is not required. After each test, we want a clean environment. This means cleaning up any mocked server requests. Setup For context, here is what the final integration test will look like: In order to make the test understandable at a quick glance, we abstracted the test logic into three layers: Layer 1 is the integration test shown above, layer 2 is the main setup for the test, and layer 3 is the direct interaction with the Enzyme API. Let’s first look at layer 2, integrationTest.js. There is a lot going on here so let’s step through it. The entry point of this file is the exported integrationTest function on line 33 above. This will create a TestHelper Object. On creation of this object, our app is mounted with Enzyme and associated with the object instance. One thing we noticed is that the Enzyme DOM was not always representing the most recent DOM state. This is due to updates that are not a direct result of an Enzyme interaction, such as a Redux store update from a server response. As a solution, whenever we need to access the DOM, we always return an updated version as shown on line 17: This is not a very expensive operation and it provides a guarantee that we are always testing the most up-to-date DOM state. In addition to mounting the app, our TestHelper class also provides access to our page test helpers. I will touch on these in a bit. An important hurdle we encountered was waiting for our app to get into a state that can be tested. In the case of our simple label app, we need to wait for the carriers to be loaded and rendered before we can begin our test. Assuming our server request to get the carriers is asynchronous, the way we solved this problem was by introducing a successFrom function shown on line 3: The asyncFlush function: The premise is that we can load the app, fire off the initial requests, and wait for a certain condition on our first page to be fulfilled. The condition for the details page is that the carriers list contains more than 0 carriers, shown on line 22, and the condition for the label page is that the label link is present, shown on line 28: The successFrom function works by continually adding and resolving functions on the Javascript message queue . When this returns, we know that the event loop has been run in its entirety a single time. This will hopefully mean that we have resolved the promise from the mocked fetch request to the server. If not, we will continue to flush until the callback passed in returns true or we reach our max flush count and fail. Let's now look at layer 3. So far, I have shown how our integration test mounts our application and waits for it to get into a state that is ready to be tested. From here we can start performing actions on our application using the Enzyme API. We set this up using page helpers. The page helper simply translates a high-level action to Enzyme API interactions , keeping our tests describing behavior and free from the implementation details of the app under test. Below shows our DetailsPageHelper.js. Integration Test We now know all the required information to run through the integration test: We first set up any extra mock calls required for the test. In this case, it is mocking the submit details call. We then wait for the page to load, shown on line 4. Interactions are then carried out on line 6, 7, and 8, and we then wait for the next page (the labels page) to be ready. The reason we need to wait here again is because our submit action on the details page is sending an asynchronous request to the server and waiting for a response. If we did not wait here, the test would immediately attempt to retrieve the label link before the server has responded. If there was no server request between the two pages, we could continue the test without waiting for a success condition. We then make a final assertion on line 11 to ensure we have a label link. These are the basics needed to build integration tests that cover an entire React application using Enzyme. There are limitations that need to be kept in mind with the proposed format, one being the way the mock API is set up. All the responses are in a single file. If we added a second test, the server responses would be the same as the first test. If we required a different response for each test, we would need to think about a scalable way to manage all the response data, and how to connect the appropriate responses to the appropriate tests. Writing out the responses in a complex application can also be quite tedious. Another limitation is the page helper structure. Most of the React apps we work on are 2-4 pages. For an app with a significant number of pages, the structure presented above might not be feasible. After some time using this approach to integration testing, we are happy with the way it has turned out. Overall, the benefit of getting a high level of confidence that our components work together in such a short period of time far outweigh any drawbacks in this approach. We plan on rolling it out to more of our React apps to see how it works under different scenarios.", "date": "2018-06-26"},
{"website": "Ebay-Engineering", "title": "Event Sourcing: Connecting the Dots for a Better Future (Part 1)", "author": ["Nataraj Sundar", "John Long"], "link": "https://tech.ebayinc.com/engineering/event-sourcing-connecting-the-dots-for-a-better-future/", "abstract": "Using an Event-centric approach has enabled our team at eBay to scale to handle millions of events with the resiliency to recover from failures as quickly and reliably as possible.  Though similar approaches have been widely adopted to augment large-scale data applications, for eBay's Continuous Delivery team, Event Sourcing is at the heart of decision-making and application development. To that end, we've built a system that continuously scales and tests our ability to handle an increasing volume of events and an ever growing list of external data sources and partner integrations. In the 2000 film Memento , the protagonist suffers from a condition where he can no longer create new long-term memories. Every few minutes or so, he forgets everything that has happened to him. He has no clue why he is, where he is, why he feels, how he feels, or even who the people around him are. In software development, this is fairly close to the situation when we use CRUD. In the conventional approach to data, all we know is what the current state is. We don’t have any reliable record of what came before the current state, and we have no context to understand why the current values are what they are. Some code, somewhere, changed the values of our data, and we can often find ourselves asking “how could we possibly be in this state?” In Memento, the hero tries to scribble notes to himself, take pictures, and even get tattoos to let himself understand what is happening. Yet he finds he cannot trust even these, as they can be forged or made under duress or even made to manipulate his future self. This is the best we can hope for from our application logs: a disorganized collection that may or may not represent how we arrived at our current state. Compare all of this with the natural state of a human being. We remember the events that happened, we know how we got there, and we know why our current state is the way it is. In fact, our current state is inextricably linked to our past. Reality itself is a series of changes to an initial state, resulting in the world as we know it today. An alternative to this state without context is something referred to as Event Sourcing. In this model, instead of having a single data model that we modify as various events happen, we store all of the events that can change our model as immutable objects. When we need to know our current state, we take all of our past events and calculate what the current state should be. Our present is literally a combination of our past. Let’s take a real world scenario, such as the NBA finals. If we were to track the score of a basketball game using CRUD, we would accurately determine who won a game, but not much else. Every time someone scored, we would update that teams score appropriately. It’s mostly functional, but rather boring and provides no insight for how the game is going. Even a simple form of event sourcing changes our perspective completely. Imagine instead of updating our single table with new scores, we registered an event every time someone made a basket: Instead of simply getting the current score from our database, we get all of the events in the game and add up the points for each team. We would do this every time we need to get the score. At first glance we are no better off, but we can now reproduce the state of the game at every point: This already tells us a far more interesting story. Did one team dominate throughout? Did the underdog rally from behind yet not hold the lead? Has it been a neck-in-neck battle with each team claiming and losing the lead? This also gives us one functional advantage: if the refs review a play and reverse a basket, you have a record of what kind of shot it was and can adjust the score correctly. If there is any disagreement over the score, you have the exact record of how it was calculated. If we add just a little more data, we get an explosion of information: Now we can get the traditional “box score” by breaking out the points for each player: Now imagine if we had to create that box score using CRUD. We would need to update several records every time a basket was scored: the overall score, the players points total, the assisting players assists, the players 3 point or 2 point total, etc. The more data we want to collect, the more tables and relationships we need, and the system rapidly becomes more complicated and brittle as we track more and more information. And if we didn't think of something we wanted to track ahead of time? Too bad. Contrast this with our event-sourced approach. We don’t need to decide exactly what data to collect in advance, we can simply process our existing events in new ways. Want to know how often Durant goes on a rally during the third quarter when he had a slump during the second? The data is all there, you just need to ask your events. This is not a new model, of course. Accountants have used Event Sourcing for millennia. When an accountant tracks a financial account, you will not see a pencil or an eraser. They do not simply write the current balance and then replace it with a new balance after subtracting the latest entry. Instead, they carefully write down each event as it happens, and then use those records to calculate the current balance. And they are fanatical about this. Even when a mistake is made, no one goes back and modifies the entry that was incorrect. Take a look: this is an actual entry from one of the authors' bank account from several years ago. He was charged a bank fee of over 200 million dollars! Now look at how they fixed this: not by erasing the error, but by creating another entry that tried to offset the first one: You may notice that this too is not entirely correct, so the next day yet another correcting entry was added: If you have watched any reasonable amount of science fiction, you should know exactly why we don’t go and try to change the past. Name one time that actually went well! No, like all good protagonists, we have to acknowledge that the past cannot be changed, and that all we can hope for is to do better in the future. It makes no more sense to modify the past in our software than it does in real life. If we go and change the past, the next thing you know Biff has married your mom and you no longer exist. More practically, it’s how things like Enron happen. There are many other practical benefits to Event Sourcing. Since writing is cheap, we can easily scale the processing of incoming data. Since we can process each event sequentially, concurrency issues are also much easier to address. Finally, separating our code into parts that record information coming in, parts that calculate our final model and parts that act when our model changes makes testing, designing and debugging this code much simpler. In our follow-up article, we go into the details of how we have used Event Sourcing here at eBay in the implementation of our continuous delivery system. The progress of code through a development pipeline is a natural match for Event Sourcing, and we would love to share our experience and challenges in making it work.", "date": "2018-07-10"},
{"website": "Ebay-Engineering", "title": "Event Sourcing in Action with eBay's Continuous Delivery Team (Part 2)", "author": ["Nataraj Sundar", "John Long"], "link": "https://tech.ebayinc.com/engineering/event-sourcing-in-action-with-ebays-continuous-delivery-team/", "abstract": "In our first article, we introduced the concept and some of the benefits of event sourcing. For this article, we are going to get very specific about how we implemented event sourcing for the Enterprise Continuous Delivery (ECD) project here at eBay. Our project is a Continuous Delivery Orchestrator. It coordinates, defines, and observes pipelines that take code through pull requests, builds, tests, and deployments across many different internal eBay systems. There are three major components involved, and we have picked appropriate technologies for each: UI—Our UI uses AngularJS (moving to React with Redux). UI—Our UI uses AngularJS (moving to React with Redux). Pipeline Definition Service (PDS)—This service defines and sets up the pipelines that need to be run. It is written with Spring Boot mostly in Java, though we are moving it over to Kotlin as we go. Pipeline Definition Service (PDS)—This service defines and sets up the pipelines that need to be run. It is written with Spring Boot mostly in Java, though we are moving it over to Kotlin as we go. Pipeline Execution Service (PES)—This is the service that is responsible for making the pipelines run, tracking their progress and reporting their state to GitHub via email and through our UI. It is written in Scala using the Akka actor framework and the Akka-Http web framework using MongoDB for persistence (via the ReactiveMongo library). This is where we are using Event Sourcing. Pipeline Execution Service (PES)—This is the service that is responsible for making the pipelines run, tracking their progress and reporting their state to GitHub via email and through our UI. It is written in Scala using the Akka actor framework and the Akka-Http web framework using MongoDB for persistence (via the ReactiveMongo library). This is where we are using Event Sourcing. While we alluded to many reasons for using Event Sourcing in our first article, there are three primary reasons we decided to use it for PES: Concurrency—Before Event Sourcing, we had several race conditions where different parts of a pipeline would send us events at nearly exactly the same time. Concurrency—Before Event Sourcing, we had several race conditions where different parts of a pipeline would send us events at nearly exactly the same time. Debugging/Traceability—We are a small team supporting a large number of pipelines and being able to quickly figure out what has happened when a pipeline doesn’t behave as expected is extremely important. Debugging/Traceability—We are a small team supporting a large number of pipelines and being able to quickly figure out what has happened when a pipeline doesn’t behave as expected is extremely important. Clarity and Correctness—This kind of orchestrator can get complicated very quickly, and because it is a critical tool for the company and without dedicated QA, Ops, or support teams, we need to have the highest quality and the easiest to understand code. Clarity and Correctness—This kind of orchestrator can get complicated very quickly, and because it is a critical tool for the company and without dedicated QA, Ops, or support teams, we need to have the highest quality and the easiest to understand code. We would like to expand a bit on this last point, as we consider it to be our strongest reason for using Event Sourcing. One of the authors of this article has four kids. No, seriously, four of them. While of course they are amazing and beautiful and all that, they are also, lovingly, little demons who will take everything you think you need. Private time, personal space and, of course, sleep are all things of the past. What does this have to do with programming? It has made one thing very clear: you will need to come and work on your code when you are sleepy or distracted or distraught or hungover. And when that inevitably happens (such as the entire first year of a child’s life), if your code is hard to think about, reason through, and test, you and your company are in trouble. The availability of your website, the security of your customer data, and the stability of your app should NOT hinge on whether or not your 4 year old wet the bed last night. This is why, for any reasonably complex system that involves time and state, event sourcing is the way to go. When you try to think about how to process an event, how it changes your state model, how other bits of code might also be changing that state, and what needs to happen because of that change all in the same block of code, you are going to screw it up. And that’s on a good day. The day after the baby cried all night and your 4 year old had nightmares? All bets are off. In a well-designed event-sourced model, each of those things are dealt with separately, or not at all. There is code that processes things coming from outside your service, makes sense of these facts, and then records them accurately in your event stream. There is the code that processes those recorded events and decides how each (in the order they were inserted) changes your eventual read model. And then there is the code that decides what to do if those events change the state of your read model in particular ways. Each of these components is easy to imagine, easy to talk about, easy to change, and easy to test. Now let’s dig in a bit on each of these components and the patterns we have used to implement them. ECD works with systems like Jenkins and GitHub to trigger pipelines. For instance, we use GitHub Webhooks to notify us when a team opens up a Pull Request or pushes changes to a branch. When these events show up, PES needs to figure what, if anything, they mean. For example, when we get an event from Github when someone pushes to a repository, PES will need to query PDS to see if this specific repository and branch has a pipeline specified. If there is indeed a pipeline for this Push, we create our first event: From a testing and “thinking about it” perspective, this makes writing code to process incoming events quite easy. The code must simply take certain inputs and use them to produce the correct events. In essence, we are asking the question “Given that this external event happens, what does it MEAN to our system?” The event is our answer. Let’s break this event down a bit. Our RunEvent class looks like this: runId is a GUID that we create for this new run. Every subsequent event for this run will need to have the same ID. data is the specific data for a RunEvent. All of our different events have different implementations of RunEventData. timestamp is usually the time that the event was created. processed is the flag that the event has been used to trigger actions. We will cover this later. description is for informational purposes only to make it easy to read through our events and understand them. eventId is the trickiest here. It is a sequential ID for each event for this run. It starts at 0 and goes up by one for each event added. However, since we are using mongoDB, this is not a great fit. MongoDB does not have an auto-increment field built in. If we were greatly concerned with write performance, we might consider a database that was better optimized for this, such as Cassandra. However, while there are thousands of projects going through ECD every day, the scale is still relatively small, so we picked MongoDB for the flexibility of our event definitions. To create something like auto-increment for mongoDB, we settled on using the “ optimistic loop ” solution. In our Scala code, that logic looks something like this: As a summary, we query for this highest existing eventId already in our DB, then add 1. Since our DB has keys on runId and eventId, if we somehow insert multiple events with the same eventId, we will get a duplicateKeyError from MongoDB, at which point we add 1 again and retry the insert. You may have noticed that the “processed” flag defaults to false. This allows us to use a FindAndModify query against our DB to pull off events that have not been processed, flipping the processed field as we do so: We are running this query every second across our instances. This way each event will only be processed exactly once, and we try to process them from oldest to newest. Note that we are not guaranteed to act on each event in the order it was created, but we will show you that it does not matter if we process events out of order. When we get a new unprocessed event, we query for all the events before it and serialize everything into actual classes in memory. For instance, the event we created when our run started is defined in Scala like this: Notice the process method. It takes in the PipelineRun that exists before this event is processed (the initial state is defined as PipelineRun.empty), and creates a modified copy based on the data for the event. For this event, that means calculating an initial state based on the Pipeline definition and then adding information about the GitHub Push that kicked us off. So we take this initial state and then pass it through the process call of each event in our list, then take that result and pass it on to the NEXT event, and so on. In non-functional programming, the code would look like this: Scala, however, is a functional language, and it would be far better to use foldLeft to produce the same result: Here again we have a contained problem. Each event impacts our view model in a specific way, and when we write the code we simply need to write tests asserting that the view is modified correctly, given various values we feed to the event. Easy to test; easy to think about. When we get the calculated PipelineRun for a specific RunEvent, we do two things: Save a copy of the view in our runView collection Save a copy of the view in our runView collection Act on the state changes that this latest event caused Act on the state changes that this latest event caused We are using here a pattern called CQRS, or Command Query Responsibility Segregation. Since we have to calculate the view from our events, it becomes very hard and expensive to try to query against views without loading all the events and processing every one of them whenever we run a query. Instead, we save the result of calculating the runView into our database. This does mean that our read data (runViews) has eventual consistency with our Write database (our events). In practice, there is a delay of less than a second. We mentioned that we may process our events out of order. Obviously this would mean that an older version of our view (calculated from an older event) might write over a newer version in our runView database. Since that is unacceptable, we actually have a field in our runView called “eventVersion,” which is the eventId of the event we used to create the view. Our code for inserting a runView will only insert if the runView has a higher or equal eventId to the one already in the database: At the same time that we persist our new view, we also execute code based on the how the latest event changed the view. We do this by calculating the view for the event right before the current one and saving that view as “previous,” then calculating including the current event and packaging them up as a RunViewChange. We pass this RunViewChange onto our RunEventRouter, which takes charge checking for any change subscriptions that are looking for this change and then launching a new Actor to receive and act on the change: This filters the entire list of change subscriptions to ones that care about this specific change, then creates an actor for the matching class and sends it the change itself to act on. We define our change subscriptions like this: The “runPaused” value is a function that takes a RunViewChange and returns a Boolean, true if the run view has changed to a Paused status. This will create a PauseNotifier actor that will then send out emails to the team to let them know their Pipeline is paused and awaiting their action, something like this: The email is sent by yet another actor (important to do since sending email is synchronous and must be carefully managed) that the PauseNotifier creates and waits for it to finish. Once again, this is easy to think about. We define a Subscription that describes what changes in the runView we care about. We create an actor that should act on that change, either by creating more actors, calling external apis or creating more events. At no point during any of this do we need to rack our overburdened brains thinking about concurrency or interactions or scale. Event Sourcing and our implementation takes care of those details. We haven’t touched a lot on how we use the Akka Actor framework, which is an excellent library, and we have used it to great effect in this project. Perhaps another article is in order. Creating a system that can scale up and out and enable parallel execution in a loosely coupled, non-blocking way has enabled us to handle over 2.2 million Run Events, resulting in almost 200,000 Run Views. This architecture was crucial for building a solution that is both performant and intuitive. If you’re using Event Sourcing, CQRS, or the Akka Actor framework in your architecture we would love to hear from you. If you intend to use this as a guide to building out your own Event Sourced system, we’re excited to see what you build!", "date": "2018-07-12"},
{"website": "Ebay-Engineering", "title": "Large-Scale Product Image Recognition with Cloud TPUs", "author": ["Shuai (Kyle) Zheng"], "link": "https://tech.ebayinc.com/engineering/large-scale-product-image-recognition-with-cloud-tpus/", "abstract": "Customers all over the world use eBay to buy and sell all kinds of products. With more than one billion product listings on eBay, it is essential for eBay to use cutting-edge AI to help everyone find the products that they want. And visual search is increasingly important—with cameras on every smartphone, eBay customers want to be able to snap a photo to find exactly what they see. Figure 1. Visual search on eBay mobile app Training a large-scale visual search model is an extremely challenging task. Using our on-premise hardware, eBay engineers and researchers spent months training a single model to recognize more than 10,000 product categories. We want to iterate much faster, even when we are working with datasets of tens or hundreds of millions of product images. Machine learning hardware is evolving very rapidly, and we faced an important choice when planning our next-generation visual search effort: should we purchase and deploy new ML hardware in-house, or should we move to the cloud? Building cutting-edge shared distributed computing infrastructure in-house requires us to wait months for certain components and is complicated and expensive. And each generation of hardware is soon surpassed by the next. We decided to evaluate Google Cloud Platform, which makes a wide range of powerful ML hardware accelerators available as scalable infrastructure. Because even our smallest datasets contain tens of millions of images, we were especially interested in Cloud TPU Pods, which can deliver up to 11.5 petaflops while providing the experience of programming a single machine. Our results are very promising: an important ML task that took more than 40 days to run on our in-house systems completed in just four days on a fraction of a TPUv2 Pod, a 10X reduction in training time. This is a game changer—the dramatic increase in training speed not only allows us to iterate faster but also allows us to avoid large up-front capital expenditures. Figure 2. Visual search in eBay Shopbot on Facebook Messenger We believe ML hardware accelerators such as Cloud TPUs and TPU Pods will become the norm for business AI workloads. With the availability of such resources at public cloud scale, many enterprises large and small will have the capability to innovate with AI. By adopting GCP’s Cloud TPUs as one of our strategic assets, eBay can ensure that our customers see the freshest possible product listings and find what they want every time. We would like to acknowledge help from Google Brain and Google Cloud Platform teams.", "date": "2018-07-24"},
{"website": "Ebay-Engineering", "title": "Managing HTTP Header Size on NetScaler Load Balancers", "author": ["Charles Li"], "link": "https://tech.ebayinc.com/engineering/manage-http-header-size-on-netscaler-load-balancers/", "abstract": "The way that the NetScaler load balancer handles oversized HTTP header is not quite straightforward when combined with layer 7 policies and may result in unexpected consequences and bad user experiences if overlooked. This article explains how the header limit works and offers our recommendations on how to manage it properly. HTTP specification ( RFC 7230 ) requires that an HTTP request must begin with an HTTP header. The size of the HTTP header however is not restricted in the RFC, allowing each web server implementation to define its own limit, as well as its own behavior when the limit is exceeded. On the NetScaler load balancer, the HTTP header is limited to 24KB by default. If the load balancer receives an oversized HTTP header, it immediately stops processing all the layer 7 policies on that TCP connection and begins treating the traffic as TCP payload ( CTX120674 ). This behavior is not quite straightforward when combined with content switching policies and may cause unexpected consequence and a bad user experience if overlooked. In this article, we are going to use an example to explain the behavior, followed by recommendations on how to customize this behavior. Assume there is an HTTP content switching virtual server (CSV) bound to multiple layer 7 content switching policies: Policy A: if request = http://www.foo.com/url-a , then send traffic to application-a. Policy B: if request = http://www.foo.com/url-b , then send traffic to application-b. Policy C: if request = http://www.foo.com/url-c , then send traffic to application-c. Default Policy: if request = http://www.foo.com/any-other-url , then send traffic to application-x (default application). A client establishes a TCP connection with the CSV and conducts multiple HTTP transactions sequentially by reusing this TCP connection. The header size is less than 24KB for all the requests, except request 3 which header size is greater than 24KB. Request 1 = http://www.foo.com/url-a (Header < 24 B) Request 2 = http://www.foo.com/url-b (Header < 24 B) Request 3 = http://www.foo.com/url-c (Header > 24KB) Request 4 = http://www.foo.com/url-a (Header < 24 B) Request 5 = http://www.foo.com/url-b (Header < 24 B) Request 6 = http://www.foo.com/url-c (Header < 24 B) Some engineers may expect request 3 to be rejected and all the other requests to be sent to the desired application, but this is not true. The actual outcome is illustrated as below: Request 1 is sent to application-a by Policy A, as expected. Request 2 is sent to application-b by Policy B, as expected. Request 3 has header size > 24KB, so load balancer stops processing layer 7 policies on this TCP connection. Request 3 is considered as TCP payload and sent to application-x (instead of application-c) because application-x is the default \"catch all\" application. Furthermore, all the subsequent requests on the same TCP connection, such as request 4, 5, and 6, are also considered as TCP payload and sent to the default application (application-x), regardless of whether their header size is over the limit. In summary, an oversized request can poison all the subsequent legitimate requests, resulting in bad user experience. The header size limit and over-limit behavior need to be managed properly to avoid the unexpected outcomes. Our recommendations are: 1. Manage the header size limit. Define a standard header size limit for all the applications. For reference, legacy applications usually have a lower limit such as 8KB or 16KB, while modern platforms often have a high limit such as 32KB or more. On load balancer, set the header size limit to a number that is equal or higher than the standard limit on the applications, so that the legitimate traffic will not trigger the bypass layer 7 behavior. 2. Manage the over-limit behavior. On the default application, prepare to return a proper landing page instead of the HTTP 400 error code for unknown requests or requests with oversized headers. On load balancer, update the HTTP profile to reset the TCP connection when an oversized header is received, so that an invalid request would not poison the subsequent legitimate requests.", "date": "2018-07-17"},
{"website": "Ebay-Engineering", "title": "The Web Push Checklist", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/the-web-push-checklist/", "abstract": "eCommerce sites are ideal candidates for push notifications and eBay is no exception. Timely notifications can help users track the status of items that they have purchased or watching or bidding, without effectively being on the site. But we are also aware that push notifications can end up being a double-edged sword. At eBay, we recently launched web push notifications for a subset of desktop users and want to share that experience. The web platform has been evolving at a very rapid pace. One particular feature that has been in limelight in recent years is the Web Push API . Web push notifications allow users to opt-in to timely updates from their favorite sites and effectively re-engage with relevant content. A capability that was exclusive to native apps was opened up to the web platform. Many sites started embracing the notification capability and the adoption steadily grew. Over a period of time, the sites that prompt the browser permission dialog became overwhelming. It has, in fact, reached a point that Firefox added a preference that will block sites from even asking for permission to users. Instant notification permission dialogs are now considered as an anti-pattern . We used a checklist-based approach to enable Web Push. This checklist encapsulates our learnings and research to provide the best user experience when dealing with notifications. In this post, we will outline that checklist. Our most important goal was to not annoy users with the permission pop-ups. The entire checklist was created with this goal in mind. Avoid instant permission prompt for new users: The most common mistake is to instantly show the browser permission dialog as soon as users land on a site. This has been the biggest source of annoyance. Users have no context on why the browser is prompting for permission and in some cases, they don’t even know what the website is for. To avoid this frustration, at eBay we decided to build a custom UI module that provides the necessary context. Only when a user interacts with it, we will prompt the browser permission dialog. The content of the custom module is algorithmically configurable, which allows us to experiment with various context injections. This enables us to customize the module based on the customer’s purchase journey. Figure 1. Custom UI banner to enable browser notifications Figure 1. Custom UI banner to enable browser notifications Make Web Push a progressive enhancement: We do all the required browser API checks ( navigator.serviceWorker and window.Notification ) before enabling this feature. Enable the feature only to users who have context: We show the custom UI module only for signed-in users. Registered and signed-in users obviously have more context about eBay and are more likely to engage with the module. We have not yet enabled this feature for guest users. Context varies between guest and signed-in users, and the message we show for signed-in users may not be appealing to guests. They may also be first time users who lack sufficient background on why this module is showing up. The customizable aspect of the module has allowed us to experiment with various content that may be relevant for guests. Once we identify a range of use cases, we will enable it. Enable the feature only on pages that benefit users: The module is currently shown only on the homepage. Displaying the same module on every page of the user journey would be highly redundant and also annoying to some extent. Homepage acts as a hub and a good place to experiment. In the future, we plan to expand the feature to other pages where users may benefit. For example, the checkout success page, where users would be interested to track the status of the item they just purchased or the listing success page, were sellers might want to know on how their item is performing. For these pages even the instant permission dialog will be beneficial, as users already have a thorough context. Respond to implicit user feedback: We also apply some heuristics on the module display frequency, based on user behavior. For instance, if the user does not interact with the module for 2 consecutive sessions, we do not show the module for 2 weeks. It is pretty clear that the user is not interested in it. If the same thing happens after the 2 week snooze period, we again hide the module for an even longer duration, and the pattern continues. We also configure a max cap for non-interactiveness, after which the module is never shown. A similar logic applies when the user clicks on the “Maybe later” button. These heuristics are not set in stone. We keep tweaking them as we see fit. Honor the user’s existing preference: Before displaying the module we always check the current permission status using the Notification.permission JavaScript API. We do not show the module if the permission status is denied, even if our backend system qualifies the user to be asked for permission. This can happen if the user has explicitly blocked permission access through browser preferences. In that case, we honor the user’s choice and do not display the module. (It would anyway not work as the permission is already denied.) There are also scenarios where a user can “Allow” or “Deny” notification access when the browser prompts, but can later change their decision through browser preference. We handle this by first checking if the user’s prior decision and the Notification.permission JavaScript API are in sync. If they are not, we update our system to reflect the JavaScript API status and sync the UI treatment accordingly. Handle failure gracefully: Subscribing a user to push notifications involves a sequence of steps. Each step is prone to failure. Efficient error handling in each of these steps is key to a pleasant user experience. We do not want to be in a scenario where a user opts in, but due to unhandled errors, the grant is not recorded and later displays the same permission module again. To minimize this scenario, we make sure every possible error is handled in a distinctive way that doesn't disrupt the user flow. Firebase Cloud Messaging (FCM) is used for managing our notifications, and the whole subscription code can be found here . But at a high level, the following 5 steps are required for subscription, and each step has to handle failure gracefully. Figure 2. Subscription steps Figure 2. Subscription steps Don’t overdo it: And finally, just because users granted permission for notification, it doesn't mean we can bombard them with notifications. Too many push messages can also lead to annoyance. Our backend systems are designed to be courteous about this and batch the notifications accordingly. The entire checklist evolves around this one aspect — avoiding user friction. Along those lines, the whole implementation is designed in a way that when the checklist is not met, the feature is not enabled. As mentioned earlier, we enabled the push feature for a subset of desktop web users for A/B testing. Our first metric to consider was the bounce rate. Adding a new module usually causes variation in bounce rates. We did not want the homepage bounce rate to increase due to this feature. To our surprise, there was no difference in bounce rate between test and control. This sort of validates our checklist — users were not annoyed by web push. There are two interactions a user needs to perform to enable push — first on the custom permission module and next on the browser permission dialog. We wanted to get some numbers around this. We found that 21% of the users who interacted with the custom module clicked on “Yes please.” And of this 21%, close to 95% chose “Allow” in the browser permission dialog. Again a validation to our checklist — when the module is shown, it is contextual and appropriate. We are ramping up the feature to more users, targeting 100% in the next couple of weeks. Web push notifications, when used wisely, can be an incredible feature for customers. There is already a lot of good literature around web push best practices. We hope this checklist will be another add-on to that list. Huge thanks to my colleagues Jonathan Calvin , Sathish Shanmugam , Vivek Muraka , Deepak Kumar , and Nishant Banore for partnering on this effort and launching to site.", "date": "2018-08-02"},
{"website": "Ebay-Engineering", "title": "From ‘Viable’ to ‘Awesome’: The Thinking Behind eBay’s Product Development", "author": ["Claire Xiao", "Parin Jogani"], "link": "https://tech.ebayinc.com/product/from-viable-to-awesome-the-thinking-behind-ebays-product-development/", "abstract": "Hear from two technologists on how we’re evolving our approach to building new products. Each product we build is sort of like starting a small business. It starts with an idea and a hypothesis of consumer need or desire. Then, we keep building upon this, testing and learning along the way. At eBay, our technology teams work in an Agile model where we focus on building what’s called a minimum viable product (MVP) to test ideas for new products and features. MVPs are essentially prototypes—or the most basic versions of a product that allows us to collect consumer learnings before launching at scale. During this stage, the main focus is on functionality, knowing the experience can be optimized in the future as long as the MVP helps us validate consumer demand. In the product development process, this can lead to less-than-ideal (and sometimes, outright confusing) user experiences. Through an MVP approach, what might actually be the result of a poor user experience—for example, where users cannot discover or navigate through—might be interpreted as a lack of customer need for the feature or product we are testing. Given this possible outcome, we’ve recently shifted our approach to building products. Shifting from 'Viable' to 'Awesome' We don’t just want to build a product that is viable at minimum—we want to build a product that is awesome. When it comes to building products in the MVP model, we seldom talk about creating minimum awesome experiences, but this is precisely what we need to do in order to truly create the best experiences for our customers. An awesome experience can mean different things at various stages of the product life cycle. For a mature product, it means a fluent experience and robust features that foster great user engagement and retention. For a fast growing product that strives to penetrate deeper into the user base, it means a series of moments that delight customers and create viral effects. For an MVP, however, it means a product that only allows us to precisely test product market fit. Let’s consider an example. If you were opening a restaurant with an exotic menu, you might want to test the waters and start a food truck as an MVP for a few months before deciding to invest further in the restaurant. But a food truck with fantastic food in the wrong location won’t get many customers, while a food truck in a great location with a confusing menu won’t pull through many orders. MVPs can fail sometimes simply because they are confusing, not because they aren’t valuable. Putting the New Model to Work: Promoted Listings Lite Our team employed the MAP approach with Promoted Listings , an advertising product for business sellers that allows them to boost the visibility of their items with premium placements on eBay and to help enhance their sales velocity. To really push ourselves in how we approached product development, we asked: By optimizing for the user experience, could we quickly validate user demand for a new promoted listings feature? This led us to Promoted Listings Lite. We had a hypothesis that the full Promoted Listings offering had too many options and was too robust for the average consumer seller. Contrary to business sellers who want comprehensive levers to fully control their advertising strategy and grow their business, consumer sellers are less interested in advertising strategy and more focused on getting things sold without hassle. So, we chose to build an MAP that was as simple as possible for our consumer sellers: Promoted Listings Lite. How We Built an MAP 1) MAP = MVP (functionality) + user experience While building our MAP, we made sure to not only focus on functionality but also the user experience. The original Promoted Listings experience for our small business users has full functionality, including manual bidding and campaign creation for advertising, which suits these users’ preferences for more control. Through early user research, however, we found consumer sellers sit on the other extreme of the spectrum: they need a simple, clear and hassle-free experience. To satisfy these needs, we simplified the experience via a one-click campaign creation. To deliver this “advertising easy button,” we recommend a bid percentage and removed the concept of a campaign setting for this segment. In this “lite” version of Promoted Listings, users only need one click to complete the journey. The simplicity of the solution drove wide adoption. 2) Stay relentlessly focused Great companies define their mission and principles to give employees a direction. When it comes to developing a new product, we also aimed to clearly state the questions we sought answers to. This guided our team to optimize for the core functionality and experience while making tradeoffs wisely. As a result, we were able to move faster without sacrificing quality. The key to success for our Promoted Listings pilot was having clear objectives and a relentless focus. To give you one example, there were a lot of debates around adding opt-out and reporting features to our MAP. It was really tempting to slide to the extreme—perfectionism and trying to solve all problems in the pilot. However, in a test-and-learn environment, perfect is the enemy of good. We reminded ourselves that our goal for building the MAP was to validate whether consumer sellers wanted to use a product like this. This enabled us to quickly re-focus our energy on solving only for the critical questions we were seeking answers to. 3) Form a cross-functional tiger team from the beginning MAPs benefit from cross-functional expertise, so it’s a good practice to form a tiger team comprised of engineers, designers, user researchers, marketing experts and data analysts from the beginning of your project planning, and sync on regular basis. Each expert will help your MAP progress in different ways: Engineering partners will quicken product discovery since they can help design solutions that balance agility and delivery time. Engineering partners can also help nail down lean and modular solutions while keeping long-term scalability in mind. Engineering partners will quicken product discovery since they can help design solutions that balance agility and delivery time. Engineering partners can also help nail down lean and modular solutions while keeping long-term scalability in mind. User researchers and marketers can help build the value proposition for the feature and carry them over to go-to-market activities. User researchers and marketers can help build the value proposition for the feature and carry them over to go-to-market activities. A member from the legal team can also contribute to the successful messaging of the product positioning to end customers. Working with this tiger team from the beginning not only helped us launch the new product, but also champion it, making it easier to scale in the future. Shifting our product development approach allowed us to build Promoted Listings Lite quickly and created new opportunities for us to further improve this product line. Promoted Listings Lite is currently available in the U.S., U.K., Germany and Australia.", "date": "2018-08-01"},
{"website": "Ebay-Engineering", "title": "Love It? Find More Like It with eBay’s New Image Search Feature", "author": ["Seema Jethani", "Venkat Medapati"], "link": "https://tech.ebayinc.com/product/love-it-find-more-like-it-with-ebays-new-image-search-feature/", "abstract": "Our latest visual shopping feature helps you discover more items that look like your favorites with only the swipe of a finger. There’s an easier way to find more of what you love on eBay. It’s as simple as dragging and dropping images into the search bar as you shop on our mobile app. When we introduced Image Search last year, we used computer vision to make it easier to find like items from the world around you in our global marketplace. Now, you can explore our marketplace further with just a swipe of your finger. As we continue to make the entire internet shoppable using computer vision, we’re evolving the search experience on eBay to help you more easily find what you want. Starting in August, our new visual shopping feature allows you to drag an image to the search bar to find more items that look like the image that you love. Let’s say you’re looking for a new chair, so you type “egg chair” into the eBay search bar and you see a list of results for your search. If one chair in particular stands out, now you can search for more products that look like it—just drag and drop the image of the chair you like into the search bar to see visually similar results. Our new visual shopping feature is powered by artificial intelligence. We use deep learning networks known as convolutional neural networks to process your images. Behind the scenes, when you submit your image in the search field, the neural network converts your image into a vector representation. Then, the vector representation of the image that you submit is compared against more than 1.1 billion live listings in eBay’s marketplace, using nearest neighbor search. Finally, we surface the best-matched items, ranked by visual similarity. Building this type of feature is particularly challenging when you think of all the different types and styles of images on eBay’s marketplace—there are more than 2.1 billion images—as opposed to clean and uniform images. We had to train our models to cancel out the noise to better understand what you might be looking for and show you items that are visually similar. Over the past several months, we have also invested in training models specifically for visually rich categories such as Fashion and Home & Garden. We focus on images and text labels specific to those categories to return better quality results. As we improve the experience, we are heavily focused on continuing to enhance its quality and integrate more into eBay’s shopping experience. The visual shopping feature will be available starting in August on Android and iOS for the U.S., U.K., Germany and Australia. We’ll continue to evolve it and add more ways to search with images soon.", "date": "2018-07-25"},
{"website": "Ebay-Engineering", "title": "Two Years Later: APIs are the Destination", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/two-years-later-apis-are-destination/", "abstract": "eBay speaks API! Two years ago, we started a journey to deliver a new, modern family of APIs to expose marketplace capabilities to sellers and buyers. The APIs evolved and grew with the business and enabled expansion to new experiences. They are built by developers for developers and treated as first-class products at eBay. We delivered a simple, consistent, and well-documented API portfolio rather than an API maze where it is challenging to discover capabilities. Looking back over the past two years, there are so many lessons that we have learned about building reliable, secure, and scalable APIs. In the API world, the goal is to create a vision for the APIs that also includes technical vision. Standards and specifications play an important role here, but they are nothing without proper process. The governance part is crucial to ensure that the APIs fit the technical vision. Agreeing on standards is challenging. Keeping the process up to date, enforcing the specifications, and making sure the engineering teams are aligned is difficult. So the governance is all about finding a balance: define standards but be flexible on the how part. We continued in the direction of relying on existing patterns (somebody has already solved your problem) and defining our own: reuse rather than reinvent the wheel on every new API. This approach brings agility, speed, and consistency to the API development process. One of our latest patterns is an asynchronous request used for capabilities that might take too long to complete. A client initiates a long-running job and retrieves the current state of the process. If the process is successfully initialized, the resource representation, which APIs send back, contains an expected completion time and other relevant data. If the process is successful, the client retrieves the newly created or updated resource. We followed this pattern in the Marketing API that enables sellers to manage their campaigns and get comprehensive performance data. As part of the new product-based shopping experience, sellers are invited to suggest edits to the eBay product catalog. Because the process to approve catalog change requests is asynchronous, the recently released Catalog API leverages the same pattern. For large binary resources, we introduced a partial response pattern. This pattern helps to deal with unreliable and intermittent connections and to improve response time. Our Feed API , which exposes eBay inventory, leverages the partial response pattern. This approach allows downloading large feed files in chunks. (We have more than 1B listings on eBay!) A client specifies the byte range in the Range standard HTTP header. On the response side, we also leverage standard HTTP headers. The Content-Length header has the size of the chunk in the response. The Content-Range header indicates the byte range contained in the response and the total size of the resource content. “An API without a developer portal is like a hackathon without pizza; like a room of developers without awkward conversations. Or a developer’s laptop without stickers.” - Apigee A developer portal is a front door to the API program. It provides everything external developers need to integrate with our services. Delivering APIs is only half of the work done. Developer experience is the heart and soul of a successful API program. For most of our modern RESTful APIs, we exposed contracts based on the OpenAPI specification. This specification is a standardized, community-driven, and both human- and machine-readable way to describe the API contract. There are plenty of mature open source tools, one of them being Swagger, which has SDK support for more than 40 technology stacks. In 2017, we officially joined the OpenAPI Initiative (OAI) and later became one of the first in the industry to publish API contracts based on the OpenAPI 3.0 specification. Partner integrations with eBay APIs are streamlined and simplified, with significantly reduced time to market. Our ultimate goal for eBay APIs is to be intuitive and easy to discover, understand, and consume. (And, difficult to misuse!) Names matter: we truly believe that each endpoint is a sentence where resources are nouns and HTTP methods verbs. OpenAPI-based contracts are for sure a jump-start to integrating with an API. It literally takes minutes to invoke a read-only capability. This approach puts a smile on any developer’s face. Still, contracts are not documentation and are insufficient for a good developer experience. Providing samples, quick start guides, and documentation that tells a story is what really makes the face of the API program. APIs are forever—there is one chance to get it right. Once there are live integrations, it becomes impossible to change APIs without impacting partners’ business. In the API world, there is no such thing as one-size-fits-all. APIs should do one thing and do it well, without offering surprises. Gather requirements with a healthy degree of skepticism. Understand the problem statement and find a solution. (And iterate!) APIs represent the consumer’s view of the capabilities. They are the front door to the portfolio of internal microservices (producer’s view) and have requirements tailored to external clients. Typically, the APIs are complex orchestrators that construct responses by leveraging fine-grained capabilities of backend services. Internal APIs or legacy backend functionalities should not constrain API design. Also, the APIs are not supposed to reflect organizational charts. Plan and design expandable interfaces that will evolve and grow with the business. APIs express the value of a business, so they must be designed with a long-term vision in mind. Customer satisfaction is one of the most important metrics to measure the success of the API program. Have a good reason to introduce a new (major) version of your API. New APIs are not supposed to be slimmed-down legacy services. Developers invested significant effort over the years to integrate with old APIs, so migration to the new modern family of services is a long-term process. Evangelize API adoption, act on customer feedback, and support partners and new integrations. The art of crafting and shipping APIs that developers love continues. Our vision is to have the latest eBay marketplace features, including the emerging field of AI, exposed through public APIs. These APIs will attract partners across the globe to leverage the new capabilities and create successful integrations and fantastic experiences for our buyers and sellers. The ultimate goal is to keep the marketplace open and have public APIs acting as lingua franca for our partners. We are announcing improvements and additions to our API portfolio next week at eBay Connect. Stay tuned!", "date": "2018-08-08"},
{"website": "Ebay-Engineering", "title": "New eBay Motors Feature Makes Car Schematic Diagrams Shoppable", "author": ["Anny Jeung"], "link": "https://tech.ebayinc.com/product/new-ebay-motors-feature-makes-car-schematic-diagrams-shoppable/", "abstract": "Our Product Managers explain how the new tool lets you shop by diagrams, making it easier to find all the parts you need for your next auto repair job. Have you ever started a repair job on your vehicle only to find you didn’t know the name of the part you needed? You could stop by the local car parts store next time you are out and hope someone there can help you find it. Or you could spend hours researching to find that part ID or description. You might not even know where to start! Our latest motors tool—Shop by Diagram—does all the hard work for you, letting you search and shop for parts visually using interactive diagrams. As we transform ecommerce on our platform, we’ve created a number of different ways to shop and search on eBay, including image search , Find It on eBay and Interests . Now, we’ve built a whole new shopping experience so you can find that part for your next repair job. Shop by Diagram makes it easy to find any auto part by making your car’s schematic diagrams interactive and shoppable. This new feature supplements our current browse and search options by introducing a new shopping experience to mimic what happens in an auto parts store. After you tell us what car you’re shopping for, you select a vehicle system such as brakes or suspension, and then we’ll show you all of the original equipment and aftermarket parts available for purchase on eBay that specifically fit your vehicle. This is a new way of activating our structured data by pairing it with diagrams. To bring this new shopping experience to life, we’ve taken schematics for thousands of North American vehicles from 1985 to today and created an interactive experience for both desktop and mobile web users. All you have to do is select the part on a schematic diagram to identify it. You can get a better look at each part by zooming in and out. We even support pinch and zoom for mobile users. This experience works seamlessly with our existing listings, surfacing only the part you selected from the diagram that will fit your car. To make these diagrams interactive, we’re using scalable vector-based graphics. eBay automotive data specialists map the relationship of each diagram to the appropriate vehicle for that diagram. Then, each part on a diagram is mapped to its correct part description, eBay category, industry standard part type IDm, as well as OEM and aftermarket part numbers. Due to the complex data relationship and the size of the vector files, we are using Elasticsearch to search the diagrams quickly to maintain a fast user experience. Finally, we created a brand new scalable widget platform that will allow us to drop our diagram experience on multiple entry points throughout the site to route you to the appropriate diagram to find your part. You can try this out by: 1. Starting at eBay Motors Home Page . From here, you’ll see a banner that says “Find parts on a diagram.” Next, click “View Diagrams,” enter your vehicle details and select a vehicle system or area of the vehicle. Then browse through all of the available diagrams for that system and select the part you need on the diagram.  Click the “Shop Now” button to go to a search results page where we’ll only show you inventory that fits your particular vehicle. Choose the part you want and continue through the regular check out process. 2. You can also get started directly from your eBay Garage if you have stored your vehicle information in eBay Motors Garage. From here, you’ll see a banner that says “Find parts on a diagram.” Then continue with the same steps above. Shop by Diagram is now live on eBay Motors for all desktop and mobile web users in North America.", "date": "2018-08-16"},
{"website": "Ebay-Engineering", "title": "Experience Services — eBay’s Solution to Multi-Screen Application Development", "author": ["Chuck Zheng", "Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/experience-services-ebays-solution-to-multi-screen-application-development/", "abstract": "Many companies want to offer their products and services as both web and native experiences -- so-called multi-screen application development. Application development on various devices is historically siloed. The eBay Experience Service interface abstracts most business logic/knowledge away from clients to provide new functionality to native and web apps with minimum or no client code change. Many companies want to offer their products and services as both web and native experiences — so-called multi-screen application development. Application development on various devices is historically siloed. Every device-specific team performs the same tasks: Orchestration of calls to underlying domain services for data Orchestration of calls to underlying domain services for data Massaging the data from various services to fit into UI’s need Massaging the data from various services to fit into UI’s need Performing L10N translation and formatting on data Performing L10N translation and formatting on data Implementing Experimentation of features Implementing Experimentation of features Handling user behavior tracking Handling user behavior tracking Presenting UI to end user and managing user interactions Presenting UI to end user and managing user interactions Presentation work is truly device specific and must be done for each device type. Other works are almost the same, but duplicately developed on all devices. This approach has caused many problems: Duplicate development wastes engineering resources. It is not a scalable process. Duplicate development wastes engineering resources. It is not a scalable process. So much duplicate work by each team slows down business feature time-to-market. So much duplicate work by each team slows down business feature time-to-market. Users see different experiences on different devices. Features are either not available or implemented differently. Users see different experiences on different devices. Features are either not available or implemented differently. Delivering new business functionalities to all native app users with a fast time-to-market is particularly challenging. Studies show that 70-80% of users upgrade to the new app version within a couple of weeks of its release. The remaining 20-30% users wait up to 2 years before getting latest business features. Therefore, we need a different approach that can deliver business functionality to most mobile users with minimal or no change to the mobile native app. An Experience Service removes duplicate work by centralizing those duplicate tasks into a new server-side layer to speed up client app development, as illustrated in the diagram below. A UI page/view is comprised of a collection of UI modules. The Experience Service interface is driven by the UI module, as opposed to a regular data service driven by a data entity. The experience service can Identify between web and native clients Identify between web and native clients Select an appropriate set of modules for the client type, taking experimentation into account Select an appropriate set of modules for the client type, taking experimentation into account Orchestrate calls to the underlying domain process and entity services Orchestrate calls to the underlying domain process and entity services Select/transform/compute domain service responses into groups of what’s needed for each UI module Select/transform/compute domain service responses into groups of what’s needed for each UI module Format/convert/localize the data to the client’s need, with presentation hints when needed Format/convert/localize the data to the client’s need, with presentation hints when needed Attach user behavior tracking metadata to user actions Attach user behavior tracking metadata to user actions Return module data in a standardized format to clients Return module data in a standardized format to clients Client apps, upon receiving the Experience Service response, render the module data with matching UI components and deal with user interaction. This approach greatly speeds up client app development and, at the same time, increases information and user experience consistency. Let’s take a look at one of eBay’s first Experience Services -- Checkout Experience Service. In the diagram below, on the left is the Experience Service JSON response. To its right, we have screen dumps of the checkout page for desktop browser, mobile web, and iOS app. Unlike the domain service interface consisting of data entities -- orders, items, users, etc., the Experience Service response JSON contains a collection of modules made of synthesized/transformed entity data, which provides concise and relevant information within each module. For easy matching between JSON responses and UI modules in the screen dumps, modules are color-coded, e.g: Summary module - pink, Shipping Address module - green, Payment Method module - red, Cart Details module - blue. Let’s zoom in to the Summary Module: Across the desktop Web, mobile web (mWeb), and iOS apps, the Summary module needs following info: An Order total with label and amount An Order total with label and amount A list of line items, each with a label and an amount for Items purchased and shipping cost A list of line items, each with a label and an amount for Items purchased and shipping cost These are directly supported by the JSON module’s “total” attribute and “summaryItemList” attribute. Each element is consistently modeled with A “label” attribute with localized text A “label” attribute with localized text A “type” attribute to provide a client hint to present “TOTAL” differently from others A “type” attribute to provide a client hint to present “TOTAL” differently from others An “amount” attribute, which has An “amount” attribute, which has “textSpans” holding multiple segments of localized/formatted/styled version of amount “textSpans” holding multiple segments of localized/formatted/styled version of amount An optional “value” of raw data for the client to store and process when needed An optional “value” of raw data for the client to store and process when needed With this approach, all client apps are receiving consistent and relevant data ready to present to the user, without concern about which underlying domain services to call and what business rules to implement. New line items and business functionalities can be added (e.g. coupons) into the list and delivered to eBay users on all devices with very little or without client app change. For the modules where different clients do need different experiences, the Experience Service can identify the client type and return different modules or data for each. It has proven in eBay production that a carefully designed Experience Service interface that abstracts most business logic/knowledge away from the clients can provide new functionality to native and web apps without a client code change by altering business logic inside the service. If new UI components need to be built, a client app change is required. However, many new business functionalities can be introduced by composing the previously built client UI components with agreed protocols from the Experience Service. This drastically reduces client app development and release timelines. Since the introduction of Experience a couple of years ago, most eBay major user-facing apps have developed Experience Services. Some of them are already in production, and most eBay’s major user-facing pages will be served by Experience Service by the end of 2018 on all devices. The Experience Service pattern has proved valuable to eBay’s customers, business, and engineering organization. Teams also have found many areas for Experience Service patterns and processes to evolve, so it can further improve the multi-screen application development.", "date": "2017-11-30"},
{"website": "Ebay-Engineering", "title": "Faster E-commerce Search", "author": ["Roberto Konow"], "link": "https://tech.ebayinc.com/research/making-e-commerce-search-faster/", "abstract": "The search engine plays an essential role in e-Commerce: it connects the user's need with a set of relevant items based on a query. This is not a simple task; millions of queries per second need to be processed over possibly billions of items, and it is expected that every query will be executed in just a few hundred milliseconds using limited resources. In this article, we show how we improved eBay's search engine efficiency by over 25%, inspired by a technique coming from web search. This article is an adaptation of our original publication at the SIGIR e-commerce workshop. This work was done by eBay's Search Backend Engineering team members Vishnusaran Ramaswamy, Roberto Konow, Andrew Trotman, Jon Degenhardt,  and Nick Whyte. Search engines are implemented as large distributed systems where there is a limited budget in CPU and memory that every machine can use. Any improvement in efficiency could potentially be translated into a reduction in hardware, networking, and operating costs. Tremendous research and engineering efforts have gone into addressing performance challenges: reduction of memory requirements by improving data compression, reduction of CPU usage by implementing early termination techniques, and the use of massively parallel execution engines are just a few of the techniques that have been extensively studied in the past. In this article, we focus on one technique originally designed to improve data compression and reduce the size of the data that is loaded into main memory: document id reordering . Before we go into more details, let us explain some basic concepts about the search engine’s main data structure: the Inverted Index . The inverted index is an old and simple, yet very efficient data structure that is at the heart of most search engines and is used to support various search tasks. From a collection of documents, an inverted index stores for each unique term $t$ (or word) a list of postings . Each posting stores pairs $\\langle d,w(t,d) \\rangle$, where $d$ is a unique document identifier (doc_id) and $w(t,d)$ is a relevance measure of the term $t$ with respect to document $d$ (often the number of times term $t$ occurs in document $d$). These posting lists can be extended to store additional information such as the positions of the term within the document. Posting lists are usually stored sorted by doc_id and processed one document at a time. To help with compression, doc_ids are often stored difference encoded—the value stored in the list is the difference (or d-gap ) between this id and the preceding id. The list of doc_ids $\\langle d_1, d_2, d_3,\\dots d_n \\rangle$, is a strictly monotonically increasing sequence. These integers can be decreased in size by calculating the differences between each document id and the preceding document id, resulting in a list of d-gaps  $\\langle d_1, d_2 - d_ 1, d_3 - d_2, \\dots , d_n - d_{n-1}\\rangle$. These d-gaps are further compressed using variable-width integer codes such as variable byte encoding , PForDelta , QMX , or similar. In practice, search engines can assign doc_ids in a number of different ways: at random, based on document similarity, in the order documents are indexed (collection order), based on a global measure of quality such as page rank, or for web documents, URL order. Others have noted that document reordering not only reduces index size, but can also decrease query processing time. This has motivated several authors in the past to study the doc_id assignment process in such a way as to optimize compression and query latency. Query processing involves a number of steps such as query parsing, query rewriting, and the computation of complex machine-learned ranking functions that may include hundreds or thousands of features derived from the documents, the query, and the user. To rank efficiently, it is common to separate the query processing into multiple stages. The first stage is responsible for identifying which documents must be ranked, and the subsequent stages rank those documents. In the first phase, a simple and fast retrieval filtering such as Boolean operators are often used. A conjunctive Boolean query of the form \"Last AND Jedi'' requires an intersection calculation, whereas a disjunctive query of the form \"Episode OR VIII'' requires a union calculation. Union queries are solved by linearly traversing all postings lists for all terms in the expression and returning all documents containing either term. Efficiently resolving intersection queries requires complicated traversal of the postings lists and has been examined for decades . It has been proven that the optimal intersection algorithm for two sets of length $m$ and $n$ with $m \\leq n$ is $O(m\\log\\frac{n}{m})$. The most popular algorithm for solving intersections is Set Versus Set (and also known as Small Versus Small). A popular e-commerce search technique to improve precision is to constrain a query to a particular set of categories in a category tree. This can be done automatically by a trained classifier, or manually by the user. For example, the results of the query iphone case can be constrained so that all the resulting items belong to the category \"Cell Phones & Accessories Cases, Covers & Skins.\" These categories also form natural partitions, clustering items according to popular search dimensions. Document reordering is a well-studied technique in web search. Most prior work has focused on reordering documents to achieve better compression. The approach is to perform text clustering on the collection to find similar documents and then assign doc_ids to minimize the d-gaps in the posting list. Fabrizio Silvestri explored a much simpler idea that takes advantage of an implicit organization of the documents in the Web. In his work, he proposes to assign doc_ids sequentially according to the document URL and showed that this simple technique achieved competitive results when compared to text clustering techniques. In essence, he roughly clustered on topic, because different sites and different parts of the same sites are usually topically related. Our approach is motivated by the work of Silvestri—we present a simple but non-optimal document id reordering technique. It takes advantage of the structured nature of documents in e-commerce, specifically that items that are for sale are usually classified and categorized before being listed. E-commerce search is a much more structured and constrained scenario than web search. In e-commerce, much of the document content is given by the item properties, such as price, brand, model, category, color, and so on. It is natural to consider these features as filtering components of a search, and it is consequently common practice to generate posting lists for those kind of features. For example, by generating posting lists for each instance of the feature \"brand\" (i.e brand:apple, brand:samsung, etc) the user can easily constrain their search to just the items made by a given manufacturer—and indeed they expect to be able to do so. Category is a particularly interesting property of e-commerce items (and queries), because it is not only used to divide the inventory into distinct types of products, but it is also commonly used to improve the precision of the results. Given a user query, the search engine can constrain the results to just those from the most relevant category. This can be done in an automatic way by training query to category classifiers, or by allowing the user to manually select a category. Figure 1 shows an example user query \"star wars\" being constrained to the category \"art\" on ebay.com. Figure 1: User category constrained query on ebay.com If the search engine creates posting lists for each category, the first stage of query processing can be improved significantly, since it can perform a direct Boolean intersection between the (possibly term expanded) user query and the posting list for the given category. Since this cuts down the size of the recall base, it increases the efficiency of the search engine, but since it restricts to the most likely category, it also removes noise from the results list, increasing precision. And this is the motivation for document id reordering based on item category. We re-order the collection so that the doc_ids are assigned in such a way that items belonging to the same category are given contiguous doc_ids. This reduces the size of the d-gaps in posting lists, which leads to better compression. However, this is not the only benefit. Because posting lists are sorted by doc_id, it creates implicit category \"divisions\" within each posting list. Figure 2 illustrates this. On the top left, the collection of documents is shown in \"collection order,\" where the distinct shades of gray represent different categories. The top right gives example posting lists for words ($w_1,w_2,w_3$). The bottom left of the figure shows the collection category reordered where, for example, collection ordered $d_2$ becomes category ordered $d_3$. The bottom right shows the effect of document reordering on the posting lists; they are now implicitly divided by category. Figure 2 : Document reordering diagram. Different grat levels represent different categories. On the left, we show a sketch of how documents get new doc_id assignment. On the right, the effect on posting lists. This document reordering scheme not only helps compression, but also decreases latency: as the d-gaps are smaller, the decompression of the integers is faster because, in general, a smaller number of operations is required to decode a smaller integer. Equally, since similar documents are stored together, fewer skips are needed. It is obvious that when a query is category constrained, the results must lie within a consecutive part of the postings list. We implemented this idea and conducted our experiments using eBay's search engine. We selected 12 million random items from our dataset and constructed two indexes: Random Index: documents were assigned doc_ids at random. Reordered Index: documents were sorted by category and then assigned doc_ids. To evaluate the performance of our document reordering technique, we use two sets of queries: General Queries: approximately 3 million queries from production logs of one eBay data center. These queries included user-issued queries as well as system-issued queries (such as those from the eBay public APIs). No country-specific filtering was performed, so queries are in many languages. User Queries: a random sample of 57,058 queries from ebay.com exactly as entered into the search box by ebay.com users. 3.2% Smaller For the purpose of this work, we constructed a special index that uses variable byte encoding to compress doc_ids. We see a reduction of 6.1% in the average number of bytes required to represent a doc_id using this encoding scheme. This represents a 3.2% space reduction of the complete index. Table 1 presents a summary of the results. It shows that the number of d-gaps equal to 1 has increased by 70%, that the average d-gap has decreased by 67.5%, and that the average number of bits required to represent d-gaps is reduced by 28%. In practice, the actual size reduction in the index will depend on the integer encoding scheme. Random Reordered Change d-gaps = 1 $890 \\times 10^6$ $1,510 \\times 10^6$ +70% Avg. $\\log_2(d-gaps)$ 5.73 4.11 -28% Avg. d-gaps 1966 639 -67.5% Avg. Bytes/d-gap(vByte) 1.30 1.22 -6.1% Index Size 29.67 28.72 -3.2% Table 1 : Space savings and bits per d-gap obtained before and after applying document reordering. 26.9% Faster! The first experiment considered throughput using the General Queries—it's a mirror of the production environment. We computed the average number of queries per second (QPS) that could be resolved when the CPU was held at 80% utilization (the other 20% is used in eBay for processes such as index maintenance). We found that on average, the Reordered Index could process about about 30% more queries per second than the Random Index. Table 2 shows the average search time per query (in milliseconds) at the mean, median, 95th percentile, and 99th percentile. In all cases, we see a substantial improvement; the mean latency improved by 26.9% while 95th percentile latency is almost halved when compared to the random document Reordering. Random Reordered Latency Reduction Mean 22.43 16.4 26.9% Median 4.35 3.21 28.3% 95th Percentile 57 30.2 47% 99th Percentile 375 224 40% Table 2: Comparison of random doc_id assignment versus category doc_id reordering. Mean, median, 95th, and 99th percentiles of query processing times in milliseconds for general queries. We also evaluated the impact of applying category constrains to the queries. The results are shown in Table 3. The left side shows the latency (in milliseconds) when category constraint is not used. In this case, the Reordered index improved mean query latency by 47% and the 95th percentile by 41%. The right shows the effect of enabling category constrain on the queries. There the mean query latency has reduced by 55% when the Reordered Index is used, and a similar effect is observed for the 95th percentile. Clearly both unconstrained and category constrained queries are materially improved. Without Category Constraint With Category Constraint Random Reordered Latency Reduction Random Reordered Latency Reduction Mean 26.8 14.3 47% 18.9 8.4 55% Median 5.9 3.5 42% 3.2 1.7 48% 95th Pct 85.8 50.8 41% 61.6 27.6 55% Table 3: Execution time in milliseconds for user queries with and without category constraint enforcement. Categories naturally cluster both document terms and query terms. Items satisfying a multi-term AND query will tend to come from a small number of categories. Ordering posting lists by category will put documents satisfying these queries near each other both in posting lists and in the forward index . This should improve CPU cache hit rates and even improve the inherent efficiency of the posting list processing algorithms. The latter effect would result from having larger clusters of posting list entries that are either matched or skipped than in a random assignment of the documents. Query expansion is likely to compound these effects, especially in an e-commerce environment. As in most search engines, we make extensive use of query expansion to improve recall and precision. Rewritten queries often form a complex Boolean expression involving many posting lists and nested AND/OR constructs. Expansions involve not only text keywords, but also the structured meta-data associated with products. For example, the term \"black\" may expand to \"color:black\" and \"samsung\" to \"brand:samsung.\" To examine these effects, we used the CPU usage profiling tool perf , while processing a portion of a General Queries collection, to analyze and identify the exact locations where the improvement was more noticeable. We observed the functions performing the task of iterating (and skipping) through posting lists was consuming about 5% of the total CPU time, and we observed a noticeable improvement especially in these parts. We also saw improvements in the doc_id variable byte decoding section. Finally, we analyzed the effect of how cache misses were affected by the document reordered index. We observed a 7% reduction in overall cache misses and a 13% reduction in last-level cache misses (LLC). These numbers show that that document ordering by category yielded a significant improvement in overall CPU cache hit rates. These numbers are consistent with our hypothesis for the improvements in latency. Additional analysis is still needed to quantify the effects on posting listing processing. The probability that any given cluster of posting list entries will be referenced by a query is far from uniform in the General Queries collection, and more likely following something like a zipfian curve. This should reduce the number of CPU cache entries filled with posting list data during processing of a query, and thus reducing the CPU cache working set size for the portion of query processing dedicated to processing posting lists. The reduction in CPU cache working set size for posting lists allows a larger amount of CPU cache to be used for other functions performing query processing work, which improves the CPU cache hit rate for those other functions. The above discussion focuses on the recall component of query processing. As mentioned earlier, document reordering also better co-locates forward index data for items satisfying the recall expression for a query. Forward index data is used extensively in the ranking component of query processing. As such, this also has potential to improve CPU cache hit rates. Our results show that document id ordering on category reduces mean latency per query by 27% and 99th percentile latency by 45%. Latency improvements are seen both with and without category constraints applied. It also reduces the index size by 3.2%. For more details you can find our SIGIR e-commerce workshop paper here . We thank Prof. Alistair Moffat and Prof. Gonzalo Navarro for their help and comments during the development of this project.", "date": "2018-01-18"},
{"website": "Ebay-Engineering", "title": "Android Accessibility Automation with Espresso", "author": ["KP Bhat"], "link": "https://tech.ebayinc.com/engineering/android-accessibility-automation-with-espresso/", "abstract": "As one of the accessibility champions in our company and a digital inclusion proponent, I have always tried to explore new avenues to improve accessibility across the different platforms that we support. According to the World Bank, 15% of the world’s population has some type of disability. Adding accessibility features makes using mobile apps easier for all users, not just those with disabilities. The Android platform offers multiple assistive technologies available, including TalkBack, which we use in our eBay Android app. TalkBack helps vision-impaired users interact with their devices by adding spoken, audible, and vibration feedback, and comes pre-installed on most Android devices. To ensure TalkBack works as expected with our application, traditionally we have focused predominantly on manual testing to ensure the critical flows in our app works for our TalkBack users, and that they are able to complete any action, such as searching for items, checking for prices, or completing a transaction, just like our sighted users do. With advent of automation, it makes sense to adopt automated testing for accessibility also. Espresso is a popular testing framework that helps us to synchronize asynchronous tasks while automating in-app interactions, such as: Performing actions on View objects Locating and activating items within RecyclerView and AdapterView objects Validating the state of outgoing intents Assessing how users with accessibility needs can use the app It makes sense for us to add support for accessibility in UI tests using Espresso. To extend the existing accessibility test framework provided by Android and enable Accessibility checks for our application, we add the following line in the @BeforeClass annotation in the Espresso test: This line will cause accessibility checks to run on a given view every time you use a ViewAction from the ViewActions class. As long as we have sufficient coverage in our tests, and we can ensure all of our UI components are reachable, these accessibility rules will validate all the pertinent UI fields in the current screen for which we are testing. Based on the current version available at the time of writing (VERSION_2_0_CHECKS), there are eight available checks that can be executed from our Espresso tests. Refer to the Accessibility rules coverage section later in this article for the details of these rules and a summary of what the rule asserts. We can next specify which particular views to skip from these checks. This is especially helpful if we are aware of certain accessibility violations with a given view and are not able to fix them right away. We can achieve this by specifying a simple matcher: Finally, we also have the provision to skip any particular rule from being executed. This is quite handy if we know that a certain rule is going to fail if we have not addressed it yet, or whether it’s okay for our app to not handle this particular rule. Skipping rules should be an exception and not a norm. This is also achieved by specifying a matcher, as follows: All of this comes together in a simple util class defined for accessibility testing. Turning on accessibility in our existing Espresso test is then as simple as invoking one line of code, as illustrated in the following code snippet. AccessibilityTestUtil is the util class, as shown in the previous example. Here is a sample Espresso test, with accessibility checks enabled: The following rules are invoked when we enable tests for accessibility checks: TouchTargetSizeViewCheck Target height or target width less than 48 dp is flagged, unless there is a touchdelegate detected. TextContrastViewCheck Checks text color and background and factors in large text, and calculates the contrast ratio: - 4.5 for regular text, 3 for large text. DuplicateSpeakableTextViewHierarchyCheck If two Views in a hierarchy have the same speakable text, that could be confusing for users if at least one of them is clickable. SpeakableTextPresentViewCheck If the view is focusable, this checks whether valid speakable text exists, and errors if the view is missing speakable text needed for a screen reader. EditableContentDescViewCheck Throws an error if Editable TextView has a contentDescription. ClickableSpanViewCheck Checks if ClickableSpan is inaccessible. Individual spans cannot be selected independently in a single TextView, and accessibility services are unable to call ClickableSpan#onClick. RedundantContentDescViewCheck Accessibility services are aware of the view's type and can use that information as needed. For example, it throws a warning if the content description has a redundant word, such as “button.” DuplicateClickableBoundsViewCheck Throws an error if Clickable view has the same bounds as another clickable view (likely a descendent). Sometimes there are containers marked clickable, and they don't process any click events. Start enabling Accessibility Checks to your existing tests, observe if there are any failures, and address them as required. As a best practice, keep Accessibility Checks in mind while adding new tests. Once considerable progress is made, enable Accessibility Checks for the entire test suite, say in Base TestCase, so that all tests run these accessibility validations by default. You can always add specific rules to be skipped if you believe that a particular rule does not apply for your app. In summary, using Accessibility checks in Espresso tests has two key advantages—more focus on accessibility issues and better automation coverage at scale for UI tests. If you are as passionate as I am about accessibility and digital inclusion, I believe this is the right thing to do.", "date": "2017-12-04"},
{"website": "Ebay-Engineering", "title": "Big Data Governance: Hive Metastore Listener for Apache Atlas Use Cases", "author": ["Aroop Maliakkal Padmanabhan", "Tiffany Nguyen"], "link": "https://tech.ebayinc.com/engineering/bigdata-governance-hive-metastore-listener-for-apache-atlas-use-cases/", "abstract": "At eBay, we are obsessed with data quality and governance. Because eBay's Hadoop platform hosts 500 PB of data running over 15,000 nodes, the focus on governance is of utmost importance. This article discusses our experiences handling data governance at scale. Data governance helps ensure that high data quality exists throughout the lifecycle of the data. One big difference between traditional data governance and Hadoop big data governance is the sources of the data that are out of the platform team's control. In conventional data warehouses, we had everything under check, whether it was how much data came in or from where the data came. However, in big data governance, we have challenges on the volume and diversity aspect of the data. Another major problem is that we are dealing with unstructured, semi-structured, and various other types of data. The technologies are relatively new in big data, and the systems around big-data governance are still very naive. Currently, in the eBay Hadoop landscape, organizations have their own data sets, which are managed by local data architects working inside their organization, where the governance is mainly on the local level, restricted to the department or only to their organization. We want to converge these local data governances into one single platform and provide a holistic view of the entire platform. We want to unite these silos or the local data governance initiatives into one unique place to provide a unified and consolidated view of the data. Big data governance is more like traditional data governance, except for the scale and the lack of definition associated with the data. Also, when we move data from traditional data warehouses to the Hadoop world, a lot of metadata associated with the data sets gets dropped, making it hard for the data steward to manage all the data in the big data ecosystem. We need to have some kind of self-service capability, where we can socialize some of the governance to end users. To ensure security and privacy of the data and access control. To capture the metadata of datasets for security and end-user data consumption purposes. To help to ensure the quality of the data. To identify the owner of the data set. As the eBay analytics data platform team, we want to have the following capabilities on the platform level for all data existing on our Hadoop and Teradata clusters. We started this project with the following primary objectives: Establish a data tagging approach for the enterprise where the metadata that will govern the use of information will be embedded with the data as it passes through various systems in the enterprise. Provide a centralized platform for all Hadoop and Teradata customers to generate and consume the technical metadata. We started this project as an initial prototype to evaluate the technical feasibility of tagging metadata in the HDFS (Hadoop Distributed File System). We wanted to create a solution that is technically performant, scalable, pluggable, and that doesn't interact with the natural Hadoop workflow. Data governance is a vast topic, and in this prototype, we are concentrating only on how to set/view tags on the file system. We did a small prototype with HDFS extended attributes, and we found out that we can leverage these kinds of solutions just for small clusters. Some of our cluster sizes have more than 4500 nodes and use more than 300 PB of storage, which means we need a more robust solution. We then evaluated Apache Atlas and found that we can leverage it for building the data tagging capabilities and as a metadata store. Also, we can integrate Apache Ranger with Apache Atlas to roll out role-based access control on the Hadoop platform. We split the projects into four major phases: Phase 1: Technical feasibility and onboard hive/sparkSQL/Teradata datasets to Atlas Phase 2: Model HDFS datasets on Atlas Phase 3: Build tools on top of Atlas for creating/consuming the metadata Phase 4: Enable Role-Based Access control on the platform In this blog, we are going to discuss the details for Phase 1, where we will be mainly focusing on onboarding primarily hive/sparkSQL/Teradata datasets to Atlas. We are massive hive and Spark-SQL users and have around 200k+ tables on some of our clusters. The two main problems were doing the initial and incremental loads to Atlas. The initial load was challenging because of the vast amount of the databases/tables that we need to load to Atlas. We optimized the code a lot to make this process efficient. The next challenge was how we should handle the incremental loads. We have 1000s of tables being created on a daily basis, and we want to ensure the metadata repository always presents the most accurate data for governing and security purposes. Atlas already provides hive hooks for capturing the data definition language (DDL). You can deploy these hooks on the gateways nodes (a.k.a. CLI/edge nodes) or in the HiveServer2 server. These Atlas hooks can help us capture the table metadata updates real-time on the Atlas side. Even with this approach, we faced two significant challenges: Currently, Atlas doesn't have any hooks for the hive metastore server. The majority of our customers are still using hive/sparkSQL by connecting to hive metastore servers. We have some dedicated clusters primarily running only sparkSQL workloads by connecting to hive metastore servers. One practical solution is to help customers migrate from using hive metastore service to HiveServer2 service. HiveServer2 has metastore hooks, which we can leverage for capturing the table metadata changes. But, this migration needs a lot of code changes on the customer side as well and will take a few months to complete this migration due to the customer's priorities and other milestones. Deploying client-side hive hook on hundreds of CLIs/edge nodes is not a flexible solution for us. If a CLI or edge node misses the hook, this will cause inconsistency in the table metadata on the cluster and the Atlas side. We don't want these kinds of differences in our governance tool. Also, deploying these kinds of client-side hooks would create a lot of operational nightmares in the future. Given these challenges, we decided to deploy a listener on hive metastore server, so that we can capture any DDL changes on the server side. Synchronize hive metadata and Atlas repo with hive metastore event listener: Environment Hive data and Atlas reside in separate clusters in which Atlas functions as a repo for several Hive data clusters. Entities in Atlas is uniquely identified by having the cluster name as part of its qualified name. Multiple data clusters (HDP 2.4.2, Hive 1.2, Spark 2.1.0) → Atlas cluster (HDP 2.6, Atlas 1.0.0 alpha) Workflow Methodology The metastore listener listens for table create/change/drop events and sends this change to Atlas via message bus (Kafka). To set up on a hive server box in the data cluster, register the metastore listener with the hive by specifying the name of the customized metastore listener, which in this case is AtlasMhook in the hive config file (hive-site.xml). Set up the metastore listener to be aware of the messaging bus (Kafka) by adding Kafka info in the atlas-application Properties file in the same config directory where hive-site.xml resides: The metastore listener code consists of a class called AtlasMhook that extends the MetaStoreEventListener and classes for each event. On each DDL event (create/alter/drop...), retrieve the current Table object and instantiate the corresponding event class accordingly: Use a similar framework for the alter and drop table events. Here is Kafka producer thread as seen in Hive metastore process: Kafka producer takes the metastore listener's message payload and sends it to the Kafka consumer process in Atlas cluster. Then sample message as received by Kafka consumer process in Atlas cluster is, as follows: The Kafka notification message is then sent to Atlas, and the entity is created/changed in Atlas accordingly. The result is stored in the Janus Graph database with hbase as the storage backend. At eBay, the hive metastore listener is helping us in two ways: It helps us keep the metadata in sync with Atlas almost real-time. We are a hive powerhouse, and each of our clusters has more than 200,000 tables, which means there are a lot of DDL changes happening on these systems at any point in time. We wanted to make sure our data governance solution is always consistent with what is available on the cluster. After we deployed the hive metastore listener, we were able to keep the DDL changes in sync between Hadoop clusters and Atlas. This way, the customers can do tagging, and we can enforce role-based access controls on these table without any delays. Another major use case is capturing these tables on Atlas in real-time provides the real-time insights into the technical metadata through Atlas to our customers. In our environment, we have a requirement to keep some of the tables and databases in sync between some clusters. Initially, we were scanning the tables and databases on the source clusters, identifying the missing tables/databases, and then recreating the tables on the destination cluster. With this implementation, we can quickly determine the DDL changes happening on the source clusters, and we were able to recreate these table on the destination clusters. This way, we were able to move from batch processing with automation to almost real-time streaming of the DDL changes. Near real-time metadata sync between the source and destination through the metastore listener and clusters enhanced our developer productivity a lot, since they don’t need to wait for the batch sync-up to happen between these clusters.", "date": "2018-09-13"},
{"website": "Ebay-Engineering", "title": "eBay Makes It Easier to Find All the Parts You Need for That Repair", "author": ["Miwa Takaki"], "link": "https://tech.ebayinc.com/product/ebay-makes-it-easier-to-find-all-the-parts-you-need-for-that-repair/", "abstract": "Hear from the product manager who built our latest feature that gives personalized guidance on finding the exact parts needed for your repair job or upgrade. So you’re a motors enthusiast and just found out your fuel injection is starting to cause you problems. You know the pressure regulator on your 2010 Honda Civic is the root cause, but did you also know you need a fuel filter and pressure regulator seal to finish the job? We are pairing machine learning with our strong product catalog to help you find exactly the parts you need to finish a repair or conduct an upgrade. It bundles listings together in a way that allows you to find everything you need to complete a project in a single listing page. When you find the regulator that you’re looking for, you’ll now also see listings for related parts to help complete your restoration, upgrade or repair project. The recommendations are personalized to YOU. This new personalized bundling experience makes it easier for you to navigate the millions of parts and accessories listings in our marketplace and lets you shop for your car in the same way as if you went into the car manufacturer or an auto repair shop. While other online auto retailers may show you related parts, there’s rarely a guarantee that those parts will fit your vehicle or even fit the same repair or project. But this new eBay experience is tailored to show only recommendations for parts that fit your vehicle. Now you can get the best of the in-store experience on eBay, as we use years of industry expertise and domain-specific data through our collaboration with WHI. Using advanced machine learning and personalization methods, we recommend the parts that fit the vehicle you are searching for and couple that with data of what others working on similar projects are searching for. With over 500 million replacement and performance part data from WHI’s catalog applied to eBay’s vast inventory, we’re able to bring the most relevant and personalized recommendations to you. So, when you start your search for that 2010 Honda Civic project, we use information that other users have provided about parts they needed in their Honda projects to suggest the most relevant parts you’ll need for your project. The personalized bundling experience is live on desktop and mobile listing pages in the U.S. BIO Miwa Takaki is a Senior Product Manager at eBay, where she focuses on Algorithmic Merchandising and Research. She lives in Manhattan and is an an avid runner. Miwa recently completed the NYC Marathon with many of her colleagues, which she’s run every year since moving to the city.", "date": "2017-12-06"},
{"website": "Ebay-Engineering", "title": "Finding Desirable Items in eBay Search by a Deep Dive into Skipped Items", "author": ["Ishita Khan"], "link": "https://tech.ebayinc.com/research/titledemand/", "abstract": "When you search on eBay and there are many matching listings, how does eBay figure out which ones to rank at the top? One key ingredient is to determine how well the listing matches the intent of the query. If a user is looking for a “kitchenaid mixer,” results titled “kitchenaid artisan series stand mixer” would be closer to the query intent than those for “kitchenaid mixer dough hook.” However, for the query “kitchenaid mixer attachment,” the latter result would be more desirable. Just as the example illustrates, effectively ranking relevant items in the search result page is one of the core challenges in e-commerce. At eBay, where people can list and describe their items almost any way they want, we have an even more interesting problem to solve. eBay uses machine learning to incorporate many signals into its core ranking algorithm. One of the most important signals is the intent of the item title. In this article, we present a novel approach to predict the intent of item titles in the context of an e-commerce query by looking at the skipped items for a query, which is subsequently used as a feature for the item ranking. Given a user-issued query and an item title, the problem we address in this article is to assign a score to the title that reflects how well the intent of the query matches the title of the item in the result set. Historically, this was done using two features in eBay ranking. The first is a well-known probabilistic relevance model used as a ranking function called BM25, which applies to any query and is based purely on how many tokens (words) are in common between the query and the title. In more detail, it uses TF-IDF, so the BM25 formula uses the frequency of tokens in our inventory. The second feature is a Naïve Bayes method called \"titleRel,\" which is based on behavioral data (clicks). In this work, our goal is to develop a model to replace the Naïve titleRel method, addressing its intrinsic challenges. Contrary to the Naïve approach, we developed a custom parameterized model of user behavior to learn why users skip a result. We leverage behavioral data to find the parameters of this Item Skip Likelihood Estimation (ISLE) model, which is consequently used to learn title intent in the context of a query. Compared to the Naïve titleRel baseline, the new ISLE model shows significantly better accuracy in offline evaluation metrics and lift in A/B test, and is currently in use in eBay in all markets. We have also published this work as an industry track full paper in the 2017 IEEE International Conference on Big Data: What is skipped: Finding desirable items in e-commerce search by discovering the worst title tokens . Since the reasons item titles are skipped vary widely across queries, we build a separate ISLE model for each query. The baseline titleRel had two steps, one offline and the other online. Offline, it examined clicks made on a set of search result pages (SRPs) for the query, and used the clicks to compute the probability that a token is clicked when it appears in an item title. Each token is assigned a probability. At run-time, it takes a title and sums the log of the probabilities of the tokens in the title to get a score for title. The offline computation is done separately for each query, so the token probabilities are query specific. The final title score is the feature. There are some refinements, but that is the basic idea. In summary, the score of a title is the sum of the scores of the tokens, and the score of a token is the log of its probability. The new ISLE model also computes the score of a title using an aggregation function to combine the scores of the tokens, but it mainly differs from titleRel in the way it computes the score of a token. It doesn't use log of the probability of a click, where the probability is directly observed from training data. Instead, it posits that the probability that a user skips over a title on a SRP depends only on the worst token in the title. When searching for a KitchenAid Mixer, it assumes that a title containing the token \"attachment\" is just as bad as one that contains \"attachment\" and \"broken.\" The ISLE score of a token is computed using maximum likelihood. Each token has a probability of being skipped. These probabilities are the parameters to be determined. This model is based on a simple yet strong assumption that the probability of skipping an item title is the same as the probability of skipping its worst token. Again using an example, consider the query \"iphone 7,\" and titles \"apple iphone 7 16gb white \" and \"iphone 7 verizon 16gb screen cracked.\" Although both the results are iPhone 7 phones, the token \"cracked\" in the second title makes it less desirable for the query. In other words, \"cracked\" uniquely stands out as the deciding factor for the relatively higher skip likelihood of the item. Concretely, if a title consists of tokens w 1 , w 2 , .., w M then the probability of skipping the title is: \\begin{equation*} max_{j=1}^{M}Pr(w_j) \\end{equation*} where Pr(w j ) is the parameter to be learned, the probability of skipping the token w j . Then a training set of titles is assembled from historical behavioral data, containing titles that were skipped and not skipped, and the expression for the probability of seeing this training set becomes: \\begin{equation*} { \\prod_{i=1}^{N} (max_j\\lambda_j)^{1-\\delta_i}(1-max_j\\lambda_j)^{\\delta_i} } \\end{equation*} If the total set of tokens used in titles for this query is { w 1 , w 2 , .., w K }, then $\\lambda$ j = Pr(w j ) are the parameters to be learned. For a given query $q$, say $(T_i, \\delta_i)$ is the training set for the $i^{th}$ title, $\\delta_i$ is 1 if the title was clicked and 0 if it was skipped. $N$ is the number of titles for a query. In this data set accumulated over weeks of behavioral data, the same titles are impressed multiple times for a given query. Consequently, the $(T_i, \\delta_i)$ signals appearing multiple times for a query is captured by numSkips and numClicks parameters in the equations below: \\begin{equation}\\label{eq:3} { L = \\prod_{i=1}^{N} (max_j\\lambda_j)^{numSkips_i}(1-max_j\\lambda_j)^{numClicks_i} } \\end{equation} This computation is done using maximum likelihood , where the token skip probabilities are the parameters of the likelihood function that are learned in the optimization process. These token skip probabilities are then converted to click probabilities (1 - Pr(w j ) ) which are stored in a fast lookup table as token weights to be used in the online phase, saving significant online computational time. When a user issues a query, the ISLE model looks up the relevant token weights for that query from the lookup table and uses an aggregation function to compute a score for each item title in the recall set, which is structurally similar to titleRel model's online step where titleRel aggregates by summing. This item desirability score is further normalized using model score distribution from a second lookup table, and the normalized score is used as a feature for the item ranking algorithm. The overall architecture of the ISLE model is shown in the figure above. The Token Weight Computation block shows the offline query-specific token weight computation. Here the ISLE model computes the token weights and also computes the summary statistics to be used for normalization. These are stored in the ISLE Token Scores and ISLE Score Distribution lookup tables. The Title Score Computation block depicts online title score computation leveraging data in the look-up tables. The offline token weight generation itself happens in three phases: the first phase is sampling of user behavioral signals for queries issued over a 6-week time frame. Next, in the modeling phase, token weights are learned by looking at the probabilities of users skipping titles containing the tokens. The third phase is score distribution computation, where distribution statistics of the title scores leveraging this model are computed. This information is used for normalizing title scores when they are used as a feature in the item ranker. In the online title scores computation, the token weights learned during offline modeling are looked up and an aggregation function is applied on these weights to obtain a score for each item for the query. This is followed by a normalization step using the query-specific score distribution statistics computed offline. This score for each item is used as a feature in the item ranking model. The ISLE model described so far uses clicks and skips from user behavioral data to learn token weights. In e-commerce, sale and sale-related behavioral signals are readily available and can be used to augment the click-only model. These signals, while sparser than clicks, are cleaner and better correlate with intent. While we use several user actions that indicate purchase intent at eBay, for illustration purposes we'll consider the add-to-cart and buy events along with clicks to model the token skip probabilities. The augmented click-sale model, modifies the likelihood equation as follows: \\begin{equation}\\label{eq:5} { L' = \\prod_{i=1}^{N} (max_j\\lambda_j)^{numSkips_i}(1-max_j\\lambda_j)^{numAugClicks_i} } \\end{equation} where $numAugClicks^i$ is the weighted counts of clicks and the sale signals in the augmented model (here, $numAugClicks_i$ = $numBuy_i$ X $w_{buy}$ + $numAddToCart_i$ X $w_{AddToCart}$ + $numClicks_i$ X $w_{click}$ ). These weights for each signal are added as additional parameters along with the token skip probabilities for the ISLE model to optimize. Note that since the ISLE model is query-specific, these signal weights are also learned per query. The gradient of log likelihood with respect to $\\lambda$ now becomes: \\begin{multline}\\label{eq:6} \\frac{d(\\log L')}{d\\lambda_j} = \\\\ ~~~~~~~~\\begin{cases} \\sum_{i=1}^{N} \\frac{numSkips^i}{\\lambda_j} - \\frac{numAugClicks^i}{1-\\lambda_j},& \\text{if $\\lambda_j$ is max_j$\\lambda_j$ } \\\\ 0, & \\text{otherwise} \\end{cases} \\end{multline} And the gradient of log likelihood with regard to the sale weights is computed as the following (for buy ): \\begin{equation}\\label{eq:7} \\frac{d(\\log L')}{dw_{buy}} = \\sum_{i=1}^{N} numBuy_i * log(1 - max_j\\lambda_j) \\end{equation} Similar to the click-only ISLE model, the augmented click-sale model described here maximizes the augmented log likelihood function ( L' ) and upon convergence, learns token skip probabilities, where the weights for each of the sale signals are also optimized. The ISLE model is a feature used in our overall ranking algorithm for item search. Accordingly, we evaluated our model against the baseline Naïve Bayes model in three stages: offline evaluation in a synthetic ranking task where both the baseline titleRel and ISLE were used as stand-alone rankers (metrics used here were AUC, NDCG, Average Precision and F-Measure), both titleRel and ISLE as a feature for the overall item ranking algorithm (metric used here was validation error of the overall ranking algorithm), and finally both crowd-sourced human judgment evaluation and A/B testing with live traffic. From offline evaluation of the ISLE model as a standalone ranker, we compare the ISLE model against the titleRel baseline on a ranking task with a data set of 125K randomly selected queries and between 4-10 items for each query of which at least 1 item is relevant. The query item pairs used in this task are human judged with binary relevance labels (1 for relevant and 0 for irrelevant). The relevance labels of the ranked items are compared to evaluate our new model using average-precision@k, f-measure@k, ndcg@k, and auc@k, where the rank k is 5. We observed statistically significant improvement along AUC, NDCG, Average Precision and F-Measure as shown in the figures below (bootstrap confidence intervals shown for each method): Here we compare a click-only model (C-ISLE), clicks augmented with sale signals (CS-ISLE) using a weighted (weights empirically determined) combination of clicks and other sale signals, and a weighted combination of clicks and sales where the weights are also learned in the model (OCS-ISLE). It is evident from the plots that all ISLE models outperform the baseline. While not statistically significant, augmenting the model with an optimized weighted combination of sale based engagement signals (OCS-ISLE) is observed to have higher model performance. This was also observed in the next experiment using ISLE as a feature for the overall item ranking model. The sale-based signals, though sparser than clicks, are much cleaner and stronger for predicting relevance and demand. All three variants of our model are significantly better than the baseline feature. We also achieved 1.73% reduction in validation error in the overall ranker with ISLE as one of the top three features of the ranking algorithm, as shown in the figure below: Finally, we observed significant search relevance improvements in US (0.31%), UK (0.27%) and DE (0.35%) through human judgment-based evaluation. Evaluating the enhanced ranker (with ISLE) through A/B tests, we saw neutral GMB in US and UK, and a significant GMB improvement in DE (0.75%). We built our ISLE model offline on Hadoop using Scala and Spark, which generates models for ~8M queries every week in under 1.5 hours. The reported performance is based on eBay’s commercial search engine serving millions of queries each day. Based on these improvements of the ISLE model over the Naïve Bayes baseline used previously, the ISLE model is currently in use in eBay search for all markets.", "date": "2018-03-23"},
{"website": "Ebay-Engineering", "title": "Is It Perfect for You? eBay Tells You ‘Why to Buy’", "author": ["Sasi Somasundarum"], "link": "https://tech.ebayinc.com/product/is-it-perfect-for-you-ebay-tells-you-why-to-buy/", "abstract": "Hear from the product manager who introduced the new feature that uses machine learning to help you shop with confidence, buying the items you really want on the eBay mobile app. When I open up the eBay app to shop for Chanel sunglasses, there are a few things that matter to me: the brand, the style and that I’m buying them from a seller who has experience. So how can I tell which sunglasses are right for me? We have added ‘Why to Buy’ signals that will quickly highlight the reasons you should buy the item you're looking at. The ‘Why to Buy’ signal intelligence uses data and machine learning to help you confidently make your purchase. Machine learning is the “brain” behind these signals and constantly optimizes along dimensions such as price, popularity and rarity. We want to simplify the decision-making process with these signals so it’s easier for you to find the things you really want. The signals range in focus from free and fast shipping to urgency, or how many items are in stock. Since we launched these signals on desktop in 2015, we’ve learned which signals give buyers more confidence to make their purchases. Now, we’re expanding these signals to eBay’s mobile apps and honing in on what’s most important to buyers. Overall, we’ve seen signals such as ‘Free in-store pickup,’ ‘Experienced seller’ and ‘Limited time remaining’ perform the best across all of our inventory. For fashion listings, signals that indicate popularity, rarity and return options matter. For electronics listings, savings, urgency, good shipping and return signals are critical. For rare inventory such as collectible items, trusted seller, savings and lower charges for customs matter, especially while buying across regions. By highlighting relevant, accurate signals, we can make it simpler for you to decide on a purchase. For example, you can see that the Chanel sunglasses above are sold by an experienced seller and that they are selling fast with the “Limited quantity available” signal – showing that they are a popular item and that, if you really want them, you should jump on the deal because they might not be available much longer. As the ‘Why to Buy’ signal intelligence expands to additional platforms, we continue to address the most challenging questions such as what is the ideal signal criteria, number of signals, order and interaction effect, and we continue to test different machine learning models. The ‘Why To Buy’ signals are currently live on desktop, iOS and Android in the U.S., U.K., Germany and Australia and will be rolling out to mWeb soon. BIO Sasi Somasundaram is Group Product Manager at eBay, where she and her team focus on providing a compelling, personalized and engaging item experience and buying flows for the 1.1 billion listings in our marketplace as well as catering to the needs of eBay’s diverse global users.", "date": "2018-01-11"},
{"website": "Ebay-Engineering", "title": "Beyond HTTPS", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/beyond-https/", "abstract": "HTTPS is not just about security. There are many benefits that come along with it. One such benefit is access to modern technologies. Check out how eBay leverages some of these new technologies that HTTPS opens up. Recent years have seen a drastic increase in websites migrating to HTTPS. There are many benefits that come along with this migration. One such benefit is access to modern technologies that improve the performance and experience of end users. At eBay, when we started migrating pages to HTTPS, we also started looking into how we can leverage some of these new technologies that HTTPS opens up. Our focus was performance. With that in mind, we added a bunch of features to our infrastructure that can have a positive impact on customer latency and experience, including the following: This was our top priority. As soon as we migrated a page to HTTPS, we also enabled HTTP/2 on them. The groundwork for this was already done on our CDN, which made the switch pretty easy. To get the full benefit of HTTP/2, we had to remove domain sharding for static resources (CSS/JS/images) and consolidate them under one or two domains. Domain sharding, once considered an HTTP/1.1 best practice, can adversely impact HTTP/2. HTTP/2, as expected, helped us to offset the performance overhead that comes from HTTPS. An important thing to consider when switching to HTTP/2 is connection coalescing . Most websites use a CDN provider to host static assets and that domain would be different from the main page domain. This means you may not be able to share the same HTTP/TCP connection (a major advantage of HTTP/2) between the main page and static assets. This is where connection coalescing comes into play and to use it, even the main page should be frontended by the same CDN provider. We are still working on this. But for now, we implemented connection coalescing for static assets, even if they are split between few domains. At eBay we use content-aware URLs for all static assets, meaning a URL is a hash representation of its content. Even a small change in the content of a static resource (CSS, JS, images, etc.) results in a new URL. In our case, browsers making a conditional validation request on refresh, and servers responding with a 304 is totally unnecessary. To avoid this performance overhead, we added a cache-control: immutable header extension to all our static assets. Critical assets such as CSS are now rendered without any network requests for repeat visits. The browser support for immutable is also increasing . For now, browsers support it only for HTTPS requests. We have also started experimenting with Brotli compression in some of our pages. The benefits of Brotli was very evident, and we were excited to look into it. Even the recent comparison metrics look impressive. Brotli will be used for our text-based static resources. We have not enabled it 100% in production yet, as we are changing our asset pipeline to get the max out of Brotli (doing a pre-compression at level 11). Our current experiments are with lower Brotli compression levels. Once we do a full-fledged launch, we will publish a detailed blog post with comparison numbers. Now that HTTPS is enabled, it was about time to a put a Service Worker to our site. We now have our own implementation in place. To begin with, we are trying out an offline experience for product pages, with the eventual goal of building a fully featured PWA . Our offline experience was featured at the recent Chrome Dev Summit. There is a common notion that in order to use Service Workers, you need to rebuild the site from scratch as a PWA. That is not necessarily true. Service Workers can as well be used in your existing websites to quickly enhance your current caching, prefetching and navigational behaviors. Above all, this exercise will be a great learning experience to understand all the nuances associated with Service Worker lifecyle, especially when dealing with production deployments. If a website is served over HTTPS, this should be a good list of features to try out and improve end-user performance. Many major websites have already leveraged these technologies and have seen great results. Based on your existing infrastructure, the level of complexity for implementing each of these features may vary. But the general awareness is more important and will help us plan accordingly for the future. At eBay, we are very excited about the possibilities that HTTPS has opened up. With the adoption of modern technologies that come with HTTPS, our web platform is now ready to build the next wave of compelling user experiences. Cheers to a secure and innovative future.", "date": "2017-12-13"},
{"website": "Ebay-Engineering", "title": "Peer-to-Peer Fundraising on eBay", "author": ["Scott McDowell"], "link": "https://tech.ebayinc.com/product/peer-to-peer-fundraising-on-ebay/", "abstract": "For many years, eBay has provided the ability to sell items to benefit a given charity or nonprofit organization through our eBay for Charity program. Now eBay sellers can list an item and have the funds raised credited to their favorite Event, Team, and/or Participant. Peer-to-Peer or P2P fundraising is a strategy that leverages an individual’s network of friends, family, coworkers, and peers to raise funds on their behalf. P2P is a form of crowdfunding, but differs in that it’s multi-tiered vs. the single-tier model of traditional crowdfunding. To understand the market size of P2P fundraising, nearly one-third 1 of all online donations are now made through P2P. In 2016, charitable giving in the U.S. topped $390 billion, which was up nearly 3 percent from 2015. 2 The following illustration compares the differences between traditional Crowdfunding and P2P. Click the image for a larger version. Many charities and nonprofit organizations create fundraising campaigns structured on a P2P model. One of the more popular campaign types is known as the Event/Team Fundraising campaign, in which supporters participate in a run, walk, or ride event with the goal of raising a certain dollar amount to support their cause. For many years, eBay has provided the ability to sell items to benefit a given charity or nonprofit organization through our eBay for Charity program. Now eBay sellers can list an item and have the funds raised credited to their favorite Event, Team, and/or Participant. Our new P2P offering aligns perfectly with the gamified and pseudo-competitive nature of Event Fundraising campaigns. We achieve this by creating a link between an eBay Seller and the Event, Team, and Participant they choose to support. During our research phase, we’ve identified the majority of charities and nonprofits use the following hierarchy: Event Teams Team Captain Team Participants Teams Team Captain Team Participants Team Captain Team Participants Team Participants As a Team Participant, I can define a personal goal of raising $500, which supports my team’s goal of raising $5,000, all of which rolls up to the event’s goal of $50,000. The diagram below illustrates how committed supporters become new eBay Sellers, who list items on our platform, while soft supporters become new eBay Buyers. Our P2P model creates an opportunity for organizations to raise more funds while giving supporters a new way to donate besides the traditional cash only models. In order to make all of this happen, the eBay for Charity team of engineers have developed two new web services. The first is known as the Account Affiliation Service and the second is our Transaction Reconciliation Service. The following illustration shows how these two services work in conjunction to link accounts, affiliate items for sale to a given P2P Event, and close the loop by sending all this information back to the partner organization raising funds. Click the image for a larger version of the workflow. Now that we’ve covered the high-level product concept, let’s dive into our P2P Affiliation Methods, and then we’ll explore Transaction Reconciliation. In order to understand our charitable partner’s needs, we have spoken with several stakeholders to identify a primary and secondary way of supporting an event participant. Method A - Primary Affiliation Workflow From the Charity’s Event/Team/Participant Page(s) Charities sign up for an eBay Developer Account, and we create a token to ensure the entire process of passing data between the Charity and eBay has been secured and is trusted. Charities place a Call to Action on their Event, Team, and Participant pages. Logged-in users select a Call to Action and the Charity sends the user to eBay. After logging into eBay we affiliate their account. Users then sell items to support their favorite Event, Team and/or Participant. Method B - Secondary Affiliation Workflow From eBay Charity Profile Page (CPP) In some cases, there will be donors who are not interested in creating an account on the Charity’s website, but would still like to sell items that benefit their favorite charity. In this scenario, we’re providing support by allowing the user/donor to find Events, Teams, and Participants from eBay’s website by using the Charity’s open API. Logged in eBay users select the Affiliation Call to Action from an eBay Charity Profile Page. Users then search for a Charity Event, Team, or Participant. User then selects the Event, Team, or Participant they would like to affiliate. Users then sell items to support their favorite Event, Team and/or Participant. After an affiliated supporter has sold an item, we then need to ensure the funds raised are properly reconciled with our P2P data. This allows organizations to update their online fundraising meters and financial systems. We accomplished this by writing a new Reconciliation Service that takes the key-value pair from our Affiliation Service and combines it with the rest of the transaction data for a clean response. Below is an example payload from our Reconciliation Service. Response Sample As the Product Manager for eBay’s P2P Fundraising, I’m happy to announce we recently kicked off a pilot with the American Cancer Society (ACS). As of January 2018, supporters of ACS can now list items and have the profits raised directly applied to the Event, Team, or Participant they wish to support. What’s equally exciting is that shortly after the seller has been paid, ACS will update fundraising goal meters on their website to reflect the amount of the proceeds raised through eBay sales. Below is an example of ACS’ Charity Profile Page on eBay with the Affiliation Method B. Lastly, I’d like to thank our incredibly smart and talented eBay for Charity team, without your efforts this product wouldn’t exist. And of course to our partners at the American Cancer Society, we’re looking forward providing a new way to raise funds and beat our biggest rival. 1 Donor Engagement Study; Aligning Aligning Nonprofit Strategy With Donor Preferences, Abila Inc. 2015 2 Giving USA 2017: Total Charitable Donations Rise to New High of $390.05 Billion, Giving USA. 2017", "date": "2018-02-14"},
{"website": "Ebay-Engineering", "title": "OpenAPI–An eBay Perspective", "author": ["Shekhar Banerjee"], "link": "https://tech.ebayinc.com/engineering/openapi-an-ebay-perspective/", "abstract": "Searching for a new specification for our API contracts: the WHYs, WHATs, HOWs on OpenAPI adoption and our journey so far, including feature highlights and a discussion on the exciting possibilities. Implementation architecture has undergone a radical shift in the last two decades, leading us to a landscape that is more services oriented than ever before and certainly more diverse in terms of technology choices. The transition from a legacy monolithic centralized architectural paradigm to the distributed lightweight micro-services-driven world has proved liberating in the choice of technology stacks, allowed greater consistency of experience, and improved efficiency and reduced TTMs. However, it has also surfaced the need to evolve the way service contracts are explored, tested, published, and integrated. The realization that API contracts would need to meet the needs of seamless exploration and integration across a diverse technology stack, be industry standard, and be feature rich to complement our Technical Standards and governance models necessitated the exploration for a new specification. The primary criteria was a specification that was both human and machine readable, language agnostic, vendor neutral, and open source. The OpenAPI specification was the unanimous choice. “The OpenAPI Specification (OAS) defines a standard, language-agnostic interface to RESTful APIs that allows both humans and computers to discover and understand the capabilities of the service without access to source code, documentation, or through network traffic inspection. When properly defined, a consumer can understand and interact with the remote service with a minimal amount of implementation logic.” The OpenApi Specification is driven by the OpenAPI Initiative , created by a consortium of forward-looking industry experts who recognize the immense value of standardizing on how REST APIs are described. As an open governance structure under the Linux Foundation, the OAI is focused on creating, evolving, and promoting a vendor neutral description format. The open source ecosystem that supports OpenAPI specification is one of the most advanced and feature rich compared to any other contemporary specification. Some of the open source tooling complementing OpenAPI includes interactive UIs that allow API exploration. The code generation support, incidentally, currently has the widest SDK generation capability across 40+ languages to boot strap both server side and client side integration. The OpenAPI tooling ecosystem (a.k.a. swagger ) is powered by one of the largest active open source community in its category. While mature as a specification with strong open source tooling support, it is also an evolving standard led by industry leaders. OpenAPI scores over our legacy and, indeed, many other contemporary specifications in multiple ways. Before discussing the possibilities that OpenAPI brings, this section highlights a few features of interest: Structural integrity and richness: To call out a few examples, the specification requires response definitions within the context of http status codes, has fairly rich support for data types and formats (for example: RFC3339 for date and date-time), and constraints (for eg: RFC3986 (reserved character support), ECMA 262 regex support), etc. The specification also supports extensibility at almost every level. Also, quite importantly, it allows for fragmented contracts using remote references. This opens up the possibility of storing a set of governed common definitions and referencing those in a language-agnostic manner across different APIs. With OpenAPI 3.0 GA Release, we now have support for multiple base paths and environments. Futuristic and evolving: OpenAPI 3.0 introduced the ‘links’ concept that allows for call associations with semantics that allow programmatic references back to a previous request or response structure (full Augumented BNF support). The support for webhooks was recently introduced as well. Tooling support: From open source interactive UIs for API exploration to code generation across 40+ technology stacks for client and server side to testing and mocking capabilities, OpenAPI ecosystem is one of the most feature rich in the industry. OpenAPI tooling opens up both code first and contract first approaches to API design and development. Finally, OpenAPI offers a fully open source and therefore a fully customizable stack. We, however, need to acknowledge the risks associated with too much non-generic customization, which can make any implementation eventually resemble more of a homegrown solution. A moderated approach that includes a loop contributing back to the open source community any generic patterns or tooling of relevance to the Industry is the recommended path forward. This section discusses a few high-level opportunities that increased OpenAPI adoption brings. First, OpenAPI decouples contracts from the implementation stack and allows contracts to exist and be shared, published, explored, and integrated in a stack neutral way. OpenAPI-based tooling enables contract-first development and boot strapping development through customizable code generation templates and tooling. To call out a few other opportunities across different aspects: Standards and Compliance: The rich data types and format support complements the new emerging eBay Technical Standards, and opens up possibilities for very detailed automated governance through compliance checks and reviews. Bootstrapping development across technology stacks: The only truly polyglot machine-readable specification, this makes service development and integration trivial across multiple languages. Downstream Service Integration: OpenAPI contracts make downstream service integration so much easier through interactive UIs for exploring the contracts and code generation of client artifacts. Continuous Delivery: Opens up possibilities for language agnostic test code authoring. Pipelines leverage OpenAPI code generation capabilities to execute automation suites. Application Security: Contracts with constraints defined for every inbound field for example could potentially be used to scan and filter out malicious payloads by upstream tiers way before it reaches the resource servers. External Development Community: OpenAPI provides seamless tooling and fast integrations support for our external developer community. The opportunities discussed here are by no means exhaustive, but just a glimpse of opportunities that could make a huge difference in the days ahead. A true micro-services-driven world would be hard to sustain without leveraging the machine-readable and language-agnostic nature of a contract specification. We moved swiftly once the decision was made to adopt OpenAPI in April 2017, leading to our first publication of OpenAPI contracts to our external community in July 2017. The challenge in those early days was introducing a new specification in an ecosystem that was heavily reliant on our legacy contracts without any disruption or rework on any of the existing tooling or processes. The tactical solution was therefore to create an OpenAPI contract generator service that takes in a legacy contract and transforms it to an OpenAPI contract. Any aspirational OpenAPI-based tooling, therefore, would just need a simple pass through the transformation layer, as depicted in the diagram below: Incidentally, because the idea was not limited to OpenAPI contracts for internal use, but also to the external world via our published Public API contracts, the generator needed capabilities to sanitize the contract for external releases. Some of the key features required for external releases included defaulting serialization risks as well as having the generator strip out any other internal usage markers and clean up any resulting orphaned definitions. For validating the contracts, we leveraged the swagger code-gen to redo our testing pipelines. The approach is detailed in the diagram below: From a conversation in April 2017 to the first CD pipeline powered by OpenAPI in June 2017 to our first ever external Public API contract published in July 2017, we have made steady progress. The infographic below highlights some of our milestones from 2017: eBay joined the OpenAPI organization in August 2017. With this move, we are happy to partner with the OpenAPI Initiative and contribute towards its evolution. In terms of fully harnessing OpenAPI capabilities and contributing back, we do have some way to go, but the path is exciting, with many tangible benefits to us and the community at large. https://www.openapis.org/ https://swagger.io/ https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md", "date": "2018-02-27"},
{"website": "Ebay-Engineering", "title": "Elasticsearch Performance Tuning Practice at eBay", "author": ["Pei Wang"], "link": "https://tech.ebayinc.com/engineering/elasticsearch-performance-tuning-practice-at-ebay/", "abstract": "Elasticsearch is an open source search and analytic engine based on Apache Lucene that allows users to store, search, analyze data in near real time. While Elasticsearch is designed for fast queries, the performance depends largely on the scenarios that apply to your application, the volume of data you are indexing, and the rate at which applications and users query your data. This document summarizes the challenges as well as the process and tools that the Pronto team builds to address the challenges in a strategic way. It also shows certain results of benchmarking various configurations for illustration. Elasticsearch is an open source search and analytic engine based on Apache Lucene that allows users to store, search, analyze data in near real time. Pronto, the platform that hosts Elasticsearch clusters at eBay, makes it easy for eBay internal customers to deploy, operate, and scale Elasticsearch for full-text search, real-time analysis, and log/event monitoring. Today there are 60+ Elasticsearch clusters and 2000+ nodes managed by Pronto. The daily ingestion reaches 18 billion documents, and daily search requests reach 3.5 billion. The platform offers a full spectrum of value from provision, remediation, and security to monitoring, alerting, and diagnostics. While Elasticsearch is designed for fast queries, the performance depends largely on the scenarios that apply to your application, the volume of data you are indexing, and the rate at which applications and users query your data. This document summarizes the challenges as well as the process and tools that the Pronto team builds to address the challenges in a strategic way. It also shows certain results of benchmarking various configurations for illustration. The challenges for the Pronto/Elasticsearch use cases observed so far include: High throughput: Some clusters have up to 5TB data ingested per day, and some clusters take more than 400 million search requests per day. Requests would accumulate at upstream if Elasticsearch could not handle them in time. Low search latency: For performance-critical clusters, especially for site-facing systems, a low search latency is mandatory, otherwise user experience would be impacted. Optimal settings always change since the data or query is mutable. There is no optimal setting for all scenarios. For example, splitting an index into more shards would be good for time-consuming queries, but it may hurt other query performance. To help our customer address these challenges, the Pronto team builds strategic ways for performance testing, tuning, and monitoring, starting from user case onboarding and continuing throughout the cluster life cycle. Sizing : Before a new use case comes onboard, collect customer-provided information like throughput, document size, document count, and search type to estimate the Elasticsearch cluster initial size. Optimize index design : Review the index design with the customer. Tune index performance: Tune indexing performance and search performance based on the user scenario. Tune search performance : Run performance tests with user real data/query, compare and analyze test results with combinations of Elasticsearch configuration parameters. Run performance tests : After the case is onboard, the cluster is monitored, and users are free to re-run performance test whenever data changes, query changes, or traffic increases. The Pronto team runs benchmarks for every type of machine and every supported Elasticsearch version to gather performance data, and then uses it with customer-provided information to estimate cluster initial size, including: Indexing throughput Document size Search throughput Query type Hot index document count Retention policy Response time requirement SLA level Let’s think twice before starting to ingest data and run queries. What does the index stand for? The Elastic official answer is “a collection of documents that have somewhat similar characteristics.” So, the next question is “Which characteristics should I use to group my data? Should I put all documents into one index or multiple indices?” The answer is that it depends on the query you used. Below are some suggestions about how to organize indices according to the query that's used most frequently. Split your data into multiple indices if your query has a filter field and its value is enumerable. For example, you have a lot of global products information ingested to Elasticsearch, most of your queries have a filter clause “region,” and there are few chances to run cross-region queries. A query body can be optimized: Under this scenario, we can get better performance if the index is split into several smaller indices based on region, like US, Euro, and others. Then the filter clause can be removed from the query. If we need to run a cross-region query, we can just pass multiple indices or wildcards to Elasticsearch. Use routing if your query has a filter field and its value is not enumerable. We can split the index into multiple shards by using the filter field value as a routing key and removing the filter. For example, there are millions of orders ingested to Elasticsearch, and most queries need to query orders by buyer ID. It’s impossible to create an index for every buyer, so we cannot split data into multiple indices by buyer ID. A proper solution is to use routing to put all orders with the same buyer ID into the same shard. Then nearly all of your queries can be completed within the shard matching the routing key. Organize data by date if your query has a date range filter. This works for most logging or monitoring scenarios. We can organize indices by daily, weekly, or monthly, and then we can get an index list by a specified date range. Elasticsearch only needs to query on a smaller data set instead of the whole data set. Plus, it would be easy to shrink/delete old indices when data has expired. Set mapping explicitly. Elasticsearch can create mapping dynamically, but it might be not suitable for all scenarios. For example, the default string field mappings in Elasticsearch 5.x are both \"keyword\" and \"text\" types. It's unnecessary in a lot of scenarios. Avoid imbalanced sharding if documents are indexed with user-defined ID or routing. Elasticsearch uses a random ID generator and hash algorithm to make sure documents are allocated to shards evenly. When you use a user-defined ID or routing, the ID or routing key might not be random enough, and some shards may be obviously bigger than others. In this scenario, a read/write operation on this shard would be much slower than others. We can optimize the ID/routing key or use index.routing_partition_size (available in 5.3 and later). Make shards distributed evenly across nodes. If one node has more shards than other nodes, it will take more load than other nodes and may become the bottle neck of whole system. For indexing heavy scenarios like logging and monitoring, indexing performance is the key metric. Here are some suggestions. Use bulk requests . Use multiple threads/works to send requests. Increase the refresh interval. Every time a refresh event happens, Elasticsearch creates a new Lucene segment and merges them later. Increasing the refresh interval would reduce the cost of creating/merging. Please note, documents only can be available for search after a refresh happens. Relationship between performance and refresh interval From the above diagram, we can see that the throughput increased and response time decreased as the refresh interval increased. We can use the request below to check how many segments we have and how much time is spent on refresh and merge. Reduce replica number. Elasticsearch needs to write documents to the primary and all replica shards for every indexing request. Obviously, a big replica number would slow down indexing speed, but on the other side, it would improve search performance. We will talk about it later in this article. Relationship between performance and replica number From above diagram, we can see that throughput decreased and response time increased as the replica number increased. Use auto generated IDs if possible. An Elasticsearch auto generated ID is guaranteed to be unique to avoid version lookup. If a customer really needs to use a self-defined ID, our suggestion is to pick an ID that is friendly to Lucene, such as zero-padded sequential IDs, UUID-1, or Nano time. These IDs have consistent, sequential patterns that compress well. In contrast, IDs such as UUID-4 are essentially random, which offers poor compression and slows Lucene down. A primary reason for using Elasticsearch is to support searches through data. Users should be able to quickly locate the information they are looking for. Search performance depends on quite a few factors. Use filter context instead of query context if possible. A query clause is used to answer “How well does this document match this clause?” A filter clause is used to answer “Does this document match this clause?” Elasticsearch only needs to answer “Yes” or “No.” It does not need to calculate a relevancy score for a filter clause, and the filter results can be cached. See Query and filter context for details. Compare between query and filter Increase refresh interval. As we mentioned in the tune indexing performance section, Elasticsearch creates new segment every time a refresh happens. Increasing the refresh interval would help reduce the segment count and reduce the IO cost for search. And, the cache would be invalid once a refresh happens and data is changed. Increasing the refresh interval can make Elasticsearch utilize cache more efficiently. Increase replica number. Elasticsearch can perform a search on either a primary or replica shard. The more replicas you have, the more nodes can be involved in your search. Relationship between performance and replica number From above diagram, you can see the search throughput is nearly linear to the replica number. Note in this test, the test cluster has enough data nodes to ensure every shard has an exclusive node. If this condition cannot be satisfied, search throughput would not be as good. Try different shard numbers. \"How many shards should I set for my index?\" This may be the most frequently question we've seen. Unfortunately, there is no correct number for all scenarios. It fully depends on your case. Too small a shard number would make the search unable to scale out. For example, if the shard number is set to 1, all documents in your index would be stored in one shard. For every search, only one node can be involved. It’s time consuming if you have a lot of documents. From another side, creating an index with too many shards is also harmful to performance, because Elasticsearch needs to run queries on all shards, unless a routing key is specified in the request, then fetch and merge all returned results together. From our experience, if the index is smaller than 1G, it’s fine to set the shard number to 1. For most scenarios, we can leave the shard number as the default value 5, but if shard size exceeds 30GB, we should increase the shard number to split the index into more shards. The shard number cannot be changed once an index is created, but we can create a new index and use the reindex API to move data. We tested an index that has 100 million documents and is about 150GB. We used 100 threads to send search requests. Relationship between performance and shard number From above diagram, we can see the optimized shard number is 11. Search throughput increased (Response time decrease) at the beginning, but decreased (Response time increase) as the shard number keeps increasing. Note that in this test, just like in the replica number test, every shard has an exclusive node. If this condition cannot be satisfied, search throughput would not be as good as this diagram. In this case, we would like to recommend you try a shard number less than the optimized value, since it would need a lot of nodes if you use big shard number, and make every shard have an exclusive data node. Node query cache. Node query cache only caches queries that are being used in a filter context. Unlike a query clause, a filter clause is a \"Yes\" or \"No\" question. Elasticsearch used a bit set mechanism to cache filter results, so that later queries with the same filter will be accelerated. Note that only segments that hold more than 10,000 documents (or 3% of the total documents, whichever is larger) will enable a query cache. For more details, see All about caching . We can use the following request to check whether a node query cache is having an effect. Shard query cache. If most of the queries are aggregate queries, we should look at the shard query cache , which can cache the aggregate results so that Elasticsearch will serve the request directly with little cost. There are several things to take care with: Set \"size\":0. A shard query cache only caches aggregate results and suggestion. It doesn't cache hits, so that if you set size to non-zero, you cannot benefit from caching. Payload JSON must be the same. A shard query cache uses JSON body as the cache key, so you need to ensure the JSON body doesn't change and make sure the keys in the JSON body are in the same order. Round your date time. Do not use variables like Date.now in your query directly. Round it. Otherwise you will have a different payload body for every request, which makes the cache always invalid. We suggest you round your date time to hour or day to utilize a cache more efficiently. Set \"size\":0. A shard query cache only caches aggregate results and suggestion. It doesn't cache hits, so that if you set size to non-zero, you cannot benefit from caching. Payload JSON must be the same. A shard query cache uses JSON body as the cache key, so you need to ensure the JSON body doesn't change and make sure the keys in the JSON body are in the same order. Round your date time. Do not use variables like Date.now in your query directly. Round it. Otherwise you will have a different payload body for every request, which makes the cache always invalid. We suggest you round your date time to hour or day to utilize a cache more efficiently. We can use the request below to check whether the shard query cache has an effect. Retrieve only necessary fields. If your documents are large, and you need only a few fields, use stored_fields to retrieve fields you need instead of all fields. Avoid searching stop words. Stop words like “a\" and \"the” may cause the query hit results count to explode. Image you have a million documents. Searching for “fox” may return tens of hits, but searching “the fox” may return all documents in your index since “the” appeared in nearly all documents. Elasticsearch needs to score and sort all hit results, so that a query like “the fox” slows down whole system. You can use the stop token filter to remove stop words, or use the “and” operator to change the query from “the fox” to “the AND fox” to get more a precise hit result. If some words are frequently used in your index but not in the default stop words list, you can use cutoff-frequency to handle them dynamically. Sort by _doc if you don’t care about the order in which documents are returned. Elasticsearch uses the “_score” field to sort by score as default. If you don’t care about the order, you can use “sort”: “_doc” to let Elasticsearch return hits by index order. Avoid using a script query to calculate hits in flight. Store the calculated fields when indexing. For example, we have an index with a lot of user information, and we need to query all users whose number start with \"1234.\" You might want to run a script query like \"source\": \"doc['num'].value.startsWith('1234').\" This query is really resource-consuming and slows down the whole system. Consider adding a field named \"num_prefix\" when indexing. Then we can just query \"name_prefix\": \"1234.\" Avoid wildcard queries. For every change, it’s necessary to run performance tests to verify whether the change is applicable. Because Elasticsearch is a restful service, you can use tools like Rally, Apache Jmeter, and Gatling to run performance tests. Because the Pronto team needs to run a lot of benchmark tests on every type of machines and Elasticsearch versions, and we need to run performance tests for combinations of Elasticsearch configuration parameters on many Elasticsearch clusters, these tools cannot satisfy our requirements. The Pronto team built an online performance analysis service based on Gatling to help customers and us run performance tests and do regression. The features provided by the service allows us to: Easily add/edit tests. Users can generate tests according to user input query or document structure, without Gatling or Scala knowledge. Run multiple tests in sequence without human involvement. It can check status and change Elasticsearch settings before/after every test. Help users compare and analyze test result analysis. Test results and cluster statistics during testing are persisted and can be analyzed by predefined Kibana visualizations. Run tests from command line or web UI. The Rest API is also provided for each integration with other systems. Here is the architecture. Performance test service architecture (click to enlarge diagram) Users can view Gatling reports for every test and view Kibana predefined visualizations for further analysis and comparison, as shown below. Gatling report Gatling report Gatling report This article summarizes the index/shard/replica design as well as some other configurations that you should consider when designing an Elasticsearch cluster to meet the high expectation of ingestion and search performance. It also illustrates how Pronto strategically helps customers to do initial sizing, index design and tuning, and performance testing. As of today, the Pronto team has helped a number of customers, including Order Management System (OMS) and Search Engine Optimization (SEO), to achieve their demanding performance goals and thus contribute to eBay's key business. Elasticsearch performance depends on a lot of factors, including document structure, document size, index settings/mappings, request rate, dataset size, query hit count, and so on. A recommendation for one scenario does not necessarily work for another one. It’s important to test performance thoroughly, gather telemetry, tune the configuration based on your workloads, and optimize the clusters to meet your performance requirement. Editor's note: The Chinese version of this article is posted on InfoQ.", "date": "2018-01-08"},
{"website": "Ebay-Engineering", "title": "Contextual Advertising for eBay Affiliate Marketing", "author": ["Mukund Thiyagarajan"], "link": "https://tech.ebayinc.com/engineering/contextual-advertising-for-ebay-affiliate-marketing/", "abstract": "eBay uses various marketing channels to funnel new and existing customers to the site, and one of them is the eBay affiliate program. This article talks about a way to contextually advertise on our affiliate sites based on the content on the page. Algorithmically figuring out how to identify top/relevant keywords on the page based on the content of the page Eliminating messy HTML and filtering only relevant and important keywords Scaling with increasing numbers of URLs without losing the relevance of the recommended keywords Using the tested and effective eBay search algorithm to provide relevant eBay items to be rendered on the publisher page Triggering the item rendering algorithm only when the page returns a positive sentiment. We have built a sentiment prediction algorithm for every HTML page, based on the content and context By showing content that the user has interacted with eBay in the past (also called retargeting advertising) and we target them with the same, similar or complementary items. (There is already an interest generated, and we try to convert the customers by a purchase.) By providing eBay items from publisher-provided keywords. (Use eBay search service in providing items based on publisher-provided keywords) Based on the content that the user is viewing, thereby inspiring them to make an engagement or purchase decision on eBay Crawl the publisher website. Scrape the publisher content. Identify if the content reflects positive or neutral sentiment. Use natural language processing techniques to identify relevant keywords for the page of interest. Based on the keywords, call search API to get the top item for that word and share it on the publisher's webpage. Crawl the publisher website: Publisher sites are crawled using a home-built crawler. The crawler crawls and returns HTML files for publisher URLs. (Publishers explicitly opt-in for this way of targeting and therefore understand and give us permission to crawl their site.) Scrape the publisher content: Scrape the HTML file for the content tag described by <p>. Also, scrape the contents under the tag <div> so we can have more details about the page. The algorithm will take care of unwanted content on the page. Identify the sentiment of a page: Once the page has been scraped, we clean the content of the site by removing stop words, special characters, punctuations, spaces, etc., to get the document term matrix (DTM). The DTM contains a list of words on the page and their respected frequency counts (commonly also called as term frequencies). We then run three separate general purpose sentiment lexicons available as AFINN, bing, and nrc (by Finn Årup Nielsen, Bing Liu and collaborators and Saif Mohammad and Peter Turney). All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words, and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. The bing lexicon categorizes words in a binary fashion into positive and negative categories. The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment. The page has to have 2 of 3 lexicons to returns positive scores in order to deem the URL as positive. Based on the content: We run the LDA topic modeling algorithm with Gibbs sampling and get 2 topics for the page with 3 keywords per topic. The terms in the topic with the highest probability are taken as the keywords for the page. Words with less than 1 occurrence are dropped from the set. Based on title of the page: We pull the title of the web page, parse it and filter only the nouns (singular and plural), proper nouns (singular and plural), foreign words, and cardinal numbers, and pull only the top 3 keywords from the title based on their frequency of occurrence on the webpage. Based on the content: We run the LDA topic modeling algorithm with Gibbs sampling and get 2 topics for the page with 3 keywords per topic. The terms in the topic with the highest probability are taken as the keywords for the page. Words with less than 1 occurrence are dropped from the set. Based on title of the page: We pull the title of the web page, parse it and filter only the nouns (singular and plural), proper nouns (singular and plural), foreign words, and cardinal numbers, and pull only the top 3 keywords from the title based on their frequency of occurrence on the webpage. Topic modeling is an unsupervised method that automatically identifies topics present in a text and derives hidden patterns exhibited by a text corpus. Loosely speaking, a topic is a probability distribution over a set of terms in a vocabulary and can be thought of as “a repeated pattern of co-occurring terms in a corpus.” Topic models are different than rule-based approaches that use regular expressions or dictionary-based keyword searching techniques. We are using LDA (Latent Dirichlet Allocation) for Topic Modeling. It is a matrix factorization technique, a probabilistic version to Latent Semantic Indexing (LSI) that tries to extract latent factors, i.e. “topics,” in the data. Once the site has been crawled (we use the content in the HTML tags <div> and <p> when scraping the crawled content), we clean the content of the site by removing stop words, special characters, punctuations, spaces, etc., to get the document term matrix that has a list of words in the document and their related frequencies. We then scrape just the headings and subheadings of the page (<h1 to h6>) and filter only the nouns (singular and plural), proper nouns (singular and plural), foreign words, and cardinal numbers on the headings and subheadings. We then get the top 3 most frequently occurring headings/subheadings on the actual page content and pass those 3 keywords to the search service. The sentiments algorithm that triggers the keyword generation algorithm was able to predict the correct sentiment of the page ~84% of the time. Algorithm 2 for keyword and thereafter eBay item generation gathered a higher feedback score compared to Algorithm 1 (greater than 3.5 on a scale of 5 on the human judgment tool). A new eBay category prediction algorithm is in the works for every URL, and this will be used if there are no keywords that are returned as a result of the keyword generation algorithm. Also, the marketing data science team is working on generating similar eBay item listings based on the images shown on the page. In addition, the team is building a reasonable recall set of eBay items for the image on the publisher page. (If there are mountains on the publisher page, the algorithm should not include those images as seed image when rendering relevant eBay items for the image.) Once these different ways of targeting are generated, the marketing data science team plans to build a machine-learned model in identifying on a user level how they respond to different ways of targeting (contextual relevant keyword based, publisher-provided keyword based, image based, retargeting based) and tune it according to different customers visiting the same page.", "date": "2018-02-07"},
{"website": "Ebay-Engineering", "title": "Augmented Reality? Now It’s a Reality with eBay’s New Shipping Tool", "author": ["Thai Hai Pham"], "link": "https://tech.ebayinc.com/product/augmented-reality-now-its-a-reality-with-ebays-new-shipping-tool/", "abstract": "Hear from the product manager who created a new augmented reality feature that helps find the right box to ship items sold on eBay. Congratulations, you just sold your item on eBay! As you bask in the satisfaction of a successful sale—and some extra cash—you come to the realization that now this item needs to be shipped. You don’t have a ruler, and you don’t have a box yet, so you’re left staring at your item as the countdown to your 'ship by' date looms. But what if you could try on virtual boxes and find just the right one to ship your item? eBay’s new shipping tool makes finding that perfect box as simple as holding up your phone. Today, we’re launching a tool that uses augmented reality (AR) to fix real-world problems—letting you hold your phone over a real item and try on virtual boxes. To try this, enter the Selling section of the mobile app and look for ‘Which box?’. You’ll be guided through a few steps to begin, like finding a non-reflective surface (think a wood tabletop). Then, move your phone around while pointing where you’ll eventually place the item. Once you’ve identified the right surface, place your item on it and begin trying on virtual boxes to see which one fits before making your shipping selection. Tap where you want to place the box, use two fingers to rotate and try on different sizes. You can even move around and inspect your item to find the right fit (don’t forget room for padding). Try looking from the top, the front, the sides, and anything in between. You can immediately see if your item sticks out the top or the side and pick another box. Now, you’re ready to buy the right box to ship your item. And that feeling of completing a sale is all-around sweet again. The idea for this feature sparked at our annual companywide Hack Week last July. To create this AR feature, we’re using a few key technologies, built on Google's ARCore platform. To understand your phone’s position in the real world, we use concurrent odometry and mapping, as well as sensors for movement and orientation to determine and synchronize your real and virtual world perspectives. We use feature points to be aware of the environment and create planes and surfaces in AR to represent the real world. When you place a box, we anchor it to the world, and this allows you to move around your item while the virtual box maintains its size and position. This is just one of the many new AR experiences we are creating this year to make shopping and selling on eBay easier. Our new AR shipping tool is currently available in the U.S. on Android devices that support ARCore. BIO Thai Hai Pham is a Product Manager who focuses on Spatial Computing at eBay. He’s also a self-proclaimed DIY extraordinaire and a tinkerer, chasing elegant and simple solutions to complex problems.", "date": "2018-03-19"},
{"website": "Ebay-Engineering", "title": "Thinking Inside the Box", "author": ["Dan Slightam"], "link": "https://tech.ebayinc.com/engineering/thinking-inside-the-box/", "abstract": "The use of Augmented Reality in mobile app development is gaining momentum very quickly. eBay just introduced its first AR features into the Android mobile app. The use of Augmented Reality (AR) in mobile app development is gaining momentum very quickly. Some apps enable placement of a 3D object (i.e. a piece of furniture) in order to inspect how it might look in a room through the live view of the phone camera. For shopping, that is a great use case that other teams are exploring at eBay. The eBay Mobile team decided to introduce a utility-focused feature for a different real-world, augmented-reality use case. We wanted to provide a fun and innovative way to expedite and simplify selling and shipping on eBay. Many teams' collaborative ideas, prototypes, and hackathon projects inspired this initial beta feature. Rather than looking at how a 3D object looks in a room through augmented reality, how about flipping that concept around? It might be useful for our customers to see whether a real object in a room fits inside a transparent 3D box sized to real-world, real-shipping box dimensions. Finding the correct size box is an essential step when preparing a product to sell. Our previous research clearly indicated an opportunity for mobile app innovation to reduce the barriers to selling online, while hopefully adding both fun and utility to the selling experience. “Which box?” (WB) is an Augmented Reality feature that allows users to quickly determine whether a USPS shipping box will fit around an item in order to ship it safely and cost-effectively. This is the first new Augmented Reality (AR) feature integrated into the eBay Core mobile app on the Android platform. It utilizes the device’s camera and the pre-installed ARCore APK toolkit. The UI employs basic gestures and interactions to ensure a simple and successful user experience. Simply place the box on the item, rotate it if needed, and see if it fits. Rapid prototyping, iterative design and development sprints, as well as research with our users was incorporated directly into developing the initial version of this feature. Simple “on the glass” user-guidance and box-picker widgets were developed to guide the user through simple placement of the box in the space. Those interactions will evolve as more standards are formed for UX in AR. We learned from showing this feature to eBay users who were interacting with AR for the first time. When development began, we used developer preview versions of the ARCore APK, prior to its official 1.0 release. Engineering research spikes were completed to investigate the potential of utilizing the ARCore Unity plugin and its development environment. Unity enables developing highly polished 3D and game experiences using a separate, mature IDE, as well as excellent 3D asset creation, manipulation, and integration capabilities. The other alternative considered was native 3D graphics development done using OpenGL ES . Unity-based development of AR will almost certainly be used for other features, but we decided to implement the WB feature as a standard Java-based Android Activity and GLSurfaceView.Renderer. OpenGL ES on Android requires a little more heavy lifting with regard to assets and interactions, but a native Java implementation enabled expedient integration with the existing Android eBay Core codebase. One of the primary goals for the team was to iterate quickly, delivering incremental value to the customer via small experiences integrated within the eBay Core app. This enabled the team to adapt and adjust as needed to better ensure user success. Native Java and OpenGL also allowed utilization of 2D UI widgets that had been developed for reuse within the eBay Core codebase. Much of the OpenGL code is also reusable for other features within eBay. The architecture of this feature also follows the Model-View-ViewModel (MVVM) pattern, including the use of Android Data-Binding . The box picker carousel is a UI component seen on multiple views within eBay Core and is initialized with Box ViewModel objects. Each box item, and the widgets that control the experience, are updated at runtime through data-binding. For example, when the box orientation toggle is clicked, it’s bound to a handler in the activity presenter, which changes the bound orientation field of each box in the picker carousel. The bound Boolean state of that field helps manage the state of the toggle as well as the orientation of the boxes in the picker carousel. Vector assets were used for all of the UI elements. android:onCheckedChanged=\"@{(view, isChecked) -> presenter.onOrientationCheckChanged(isChecked)}\" android:src=\"@{ContextCompat.getDrawable(context, uxContent.imageSource)}\" Design of the 3D assets is critical to the implementation of most AR applications. WB utilizes the 3D asset format recommended in the ARCore documentation. Predefined Wavefront .obj and image-based texture/material assets were used for the translucent box assets with dimensions and eBay packing tape materials built-in. One critical step during asset design was making sure they were the correct size and scale in real-world space. Also, when working with transparent assets, one key step in OpenGL ES is to utilize blending correctly as below: The new feature was enabled in eBay Core v5.18.1 simultaneously with Google’s release of ARCore 1.0. It is available on these supported devices: https://developers.google.com/ar/discover/ . The eBay Core app is an “AR Optional” application, because obviously it does not require ARCore for the majority of functionality. ARCore is specified in the AndroidManifest: <meta-dataandroid:name=\"com.google.ar.core\"android:value=\"optional\"/> Ultimately, augmented reality will be able to combine its power with other technologies such as artificial intelligence and object-detection. Experiences like Which box? will be able to do much more than simply allow the viewing of 3D objects in the room. They will be able to automatically detect, make recommendations, and guide the user through the shipping process, as well as other fun and innovative eBay buying and selling use cases.", "date": "2018-04-05"},
{"website": "Ebay-Engineering", "title": "Building eBay's Simpler Listing Flow", "author": ["Megan Scurich"], "link": "https://tech.ebayinc.com/product/ebays-new-feature-lets-you-list-items-in-seconds/", "abstract": "On the heels of our latest eBay app update, hear from our product manager who worked on a super fast way to list items onto our Marketplace. How many things would you start selling if it only took seconds to list them? We've updated our listing flow to make it even simpler and faster to start selling your items on eBay. It’s the latest release in our journey to improve the seller experience. The new tool leverages our catalog–or all the information that we have about the billions of items for sale on eBay. Now, you can get your new condition items up for sale in about 10 seconds–and primed for the millions of global buyers on our site. Our goal with this new tool is to help you quickly list those items you want to sell, giving you guidance along the way. We’re using barcode recognition technology that identifies the UPC code and maps the same product you’re holding in your hand to a match in our catalog. From there, you’re taken straight to the listing page with all the information auto-populated. In addition to finding the item for you, eBay provides you with pricing and shipping recommendations derived from more than 20 years of buying and selling data. I think it’s an important differentiator to highlight because no other platform can offer that kind of valuable guidance to sellers. It streamlines the decision-making process and eliminates the need for you to do independent research for similar items on the site. Our new tool is an evolution of the ability to search for a product by barcode to actually being able to list and start selling something simply by scanning a barcode. The update takes advantage of our structured data, providing insight into more than 1.1 billion listings on our site. We then pair that with historical data so that you can quickly list your items at the optimal price point. Once your item is matched to a product in our robust catalog, we’ll provide you with a stock image of the product, and surface a recommended selling price and estimated shipping costs. You can get these recommendations on the summary page and list your item immediately. Suddenly, that daunting pile of ‘sellable items’ is a little less scary. Our new tool is available now on eBay's native apps for iOS and Android.", "date": "2018-04-19"},
{"website": "Ebay-Engineering", "title": "eBay Lets You See It for Yourself in Reviews", "author": ["Andy Chan"], "link": "https://tech.ebayinc.com/product/ebay-lets-you-see-it-for-yourself-in-reviews/", "abstract": "eBay engages its community through product review images. They say a picture is worth a thousand words. Now, imagine that multiplied by 169 million active buyers and the potential that new flexibility with images can bring to the eBay community. Tap into the power of eBay’s community to get insights on a product you’re considering with our new feature that lets you add images to product reviews. We all know the research and time that goes into trying to find the right product. Part of the research experience includes scouring product reviews to find out what people who bought a particular product are saying about it. But what about actually being able to see how other people are using that product? Images bring product reviews to life on eBay, providing a richer shopping experience and activating eBay’s powerful and passionate community. These images in reviews are just one of the ways eBay is fostering a community of trust among its members and helping buyers make well-informed purchase decisions. I saw how powerful images can be when I was shopping for a bike for my 4-year-old son. While researching bikes, reviews gave me helpful insights on details, but the reviews with images actually helped me decide exactly which bike to buy. One reviewer provided images of her son (similar in age and size to my son) on the bike, making me feel confident enough to buy that model. If you aren’t sure which product to purchase, eBay’s product reviews with images can help you make the right decision, and could even save you time and money.  In addition, product images in reviews can also provide insights to sellers on how buyers are using their products and what features matters most to them. We make sure images in eBay reviews are appropriate by working with various teams and data scientists to automatically detect spam, profanity, and non-product related reviews or images. A trained team of moderators provide a final check on every image. Images in product reviews are currently supported in the United States, the United Kingdom, Germany, Australia, Canada—including French Canada—France, Italy, and Spain, on both desktop web and mobile web, and are coming soon to our native apps on iOS and Android. eBay was founded on the value of community and connecting people through commerce—and we’re carrying that idea forward to activate the eBay community through our product reviews platform. BIO Andy Chan is a Lead Product Manager at eBay where he focuses on shopping experiences specifically around product reviews and ratings. He is passionate about building a great experience that helps users feel confident about buying the right products on eBay. Andy is a huge hockey fan and wishes he could lift the Stanley Cup.", "date": "2017-07-06"},
{"website": "Ebay-Engineering", "title": "Optimizing CAL Report Hadoop MapReduce Jobs", "author": ["Wanxue Li"], "link": "https://tech.ebayinc.com/engineering/optimization-of-cal-report-hadoop-mapreduce-job/", "abstract": "eBay's Central Application Logging system (CAL) collects log data from all kinds of applications. The summary reports for log data are created using Hadoop MapReduce jobs. This article discusses our experiences optimizing these jobs. eBay's Central Application Logging (CAL) collects log data from all kind of applications to provide a log viewer and summary report service. Application owners can use logging summary reports to find out the following: Percentile of time spent on each transaction Service transaction calls DB operations for an application CAL uses Hadoop MapReduce jobs to generate reports for each kind of application. This article discusses about our experiences optimizing Hadoop jobs. We hope to provide developers some insight to the identification and solutions/options to optimize and resolve Hadoop MapReduce (MR) problems, including the reduced consumption of resources. There are three issues that need to be addressed: Data Set—The volume of CAL logs is increasing 70% YoY. Also, the CAL upstream applications log patterns are different. Some are database operation intensive and some contain complex nested transactions, and the log volume of each application varies widely. Computing Resource—CAL uses a shared Hadoop Cluster that can only use 19% capacity to run jobs during the hours of 23:00-8:00 GMT-7. Additionally, jobs running more than 1 hour are killed. A current job takes ~50% resources of all clusters to complete. Success Rate—The success rate is 92.5% for current MapReduce jobs. Also, some jobs take up to 6 hours to complete. Before we go into the solution, let’s briefly visit the Hadoop MapReduce phases. There are five phases in the CAL Hadoop MR job: CombineFileInputFormat, Mapper, Combiner, Partitioner, and Reducer. Figure 1: Hadoop MapReduce Job Phases In CombineFileInputFormat, small files are combined into a split, and the split is assigned to a mapper to be processed. The split is a logical representation of the data stored in file blocks on Hadoop. Mapper transforms input records into intermediate records, which are then processed later in the reducer. The intermediate records are transferred from the mapper node to the reducer node through the network. If the intermediate record set is too large, the effort would be expensive. The combiner is a semi-reducer in MR that preprocesses intermediate results and delivers them to the reducer. The partition assigns intermediate results to different reducers, according to the key of the intermediate results. Bad practices can result in data skew in the reducer. Reducer reduces a set of intermediate values that share a key to values you would expect. The duration of a Hadoop job depends on the slowest map task and the slowest reduce task. Given: $T_{Map}$ stand for Map task duration $Count_{Map}$ stand for Map task number $T_{Reduce}$ stand for Reduce task duration $Count_{Reduce}$ stand for Map task number Then $T$, which is the MR job duration, could be expressed as: $$T = \\max_{Count_{Map}}T_{Map} + \\max_{Count_{Reduce}}T_{Reduce}$$ Map time $T_{Map}$ or reduce time $T_{Map}$ is proportional to the input record number $N_{input}$ and its output record number $N_{output}$. $$T_{Map}  = { \\theta * (\\alpha_{0} * N_{input}+  (\\alpha_{1} * N_{output}) + T_{GC}}$$ $$T_{Reduce}  = { \\theta * (\\alpha_{0} * N_{input}+  (\\alpha_{1} * N_{output}) + T_{GC}}$$ Also, the computation complexity $\\theta$ has an impact on the Hadoop job duration. For some abnormal jobs, garbage collection (GC) time $T_{GC}$ should be take into consideration. In order to control the Hadoop job duration, we need to work on three aspects: GC, which can reduce $T_{GC}$ Data skew, aimed at reducing max $T_{Map}$ and max $T_{Reduce}$ Computation, which is about reducing $\\theta$ in the expression Given: $Size_{Map}$ stands for the Mapper container memory size in an MR job $Size_{Reduce}$ is the container memory size for reducer $Size_{AM}$ stands for the application master container memory size in an MR job $Count_{MAP}$ stands for the number of mappers in each MR job $Count_{Reduce}$ is for the reducer Then Total memory resource usage as R should be related to container size and container holding time, which can be expressed as: $$R = \\sum_{Count_{Map}}Size_{Map}* T_{Map} + \\sum_{Count_{Reduce}}Size_{Reduce}* T_{Reduce} + T_{job} * Size_{AM}$$ Therefore, to reduce resource usage, we need to do the following: Reduce Map or Reduce task number: $Count_{Map}$ Decrease Map or Reduce task container size: $Size_{Map}$, $Size_{Reduce}$ Optimize time cost: $T_{Map} $, $T_{Reduce} $, $T_{job}$ Garbage collection is the standout problem in CAL. The reason for job failure usually is “GC overhead” or “Out of Memory.” Even for successful jobs, GC times are high. At the same time, jobs are using the CPU of the Hadoop node during GC. In CAL, the logging event is represented as a CAL record. A CAL transaction is a logical concept that is composed of multiple records. A transaction logs events related to a user request. Each transaction might involve other transactions in response to its user request. That is, transactions are organized in a tree-like hierarchical structure. Each transaction only knows its parent transaction, but the child transaction wants to know its root transaction. However, a transaction is not always processed before its children. Hence, all transaction information needs to be kept for traceability. And for any transaction, we have no idea when its children's transactions have been totally received. We need keep all those transactions in order to give the upcoming transaction information about its parent transaction or its children's transactions. To summarize, Figure 2 shows a typical tree-like hierarchy of transactions. Transaction F is the root transaction. It will involve transaction B and G to handle a user request. If we already have transaction F, then transaction B, and finally transaction C, we can’t help C find its root transaction F until the transaction D comes in. Transaction B should always be kept in memory so that its child transaction A can be reached, because B has no idea how many children it has, same as other transactions. Figure 2: Transaction Hierarchy Unfortunately, this situation will cause out of memory (OOM). A transaction should have a time window property. If the time window is t, and the start timestamp of transaction is ts, all child transactions should happen before ts + t. In my experiment, the time window was set at 5 min. We verified this assumption with logging data from 12 applications with the largest log volume. By discarding transactions that are out of this time window, 10 of 12 applications could promise almost 100% accuracy. At the same time, we set a whitelist for the remaining two applications, disabling this function for them. The time window reduces the Hadoop job duration effectively and increases success rate from 93% to 97%. Figure 3: Transaction and Metrics Previously, mapper would read all records for that transaction in memory, then extract useful metrics once we got entire transaction, as shown in Figure 3. However, we can get metrics while reading records; whole transaction information is not required. The combiner could reduce data transfers between the mapper and reducer tasks. But, when the output of mapper is even larger, GC time would be high because of sorting. So, we pre-aggregate in mapper to decrease output intermediate record numbers. Reducer was facing a GC issue similar to the Mapper's issue. In MR, metrics are extracted from raw logs with its timestamp. Time granularity for a CAL report is 15 minutes, so the timestamp could round up to 4 timestamp values in an hour. The reducer of a CAL report job outputs two kinds of files—a base file that is used to store metrics in a 15-minute time granularity and an aggregate file that is used to store metrics in 1 hour time granularity. In Reducer, the input record is sorted according to its key. The previous key format is on the left in Figure 4. Aggregation keeps records until records for last timestamp comes in. We changed the key of input of reducer to the format on the right in Figure 4. Memory usage in reducer was reduced effectively. Optimization shouldn’t change the output of a job, even data order. In the base file, a record is sorted by timestamp. So we write records for 15-minutes granularity into different temporary files according to its timestamp, and merge them together when reducer has completed. Figure 4: Aggregation in Reducer This solution solves failure in Reducer and makes Reducer scalable for increasing input. Another obvious problem is data skew. While checking the map task and reduce task in a Hadoop Job, we learned that a map task executed time differences from 3s to more than 1 hour. The reduce task experiences a similar problem. According to the formula, assigning even input records to mapper or reducer, max $T_{Map}$and max $T_{Reduce}$can be minimized. For data skew in Mapper, CombineFileInputFormat can help. In previous CAL MR jobs, CombineFileInputFormat was not enabled. But now, combining log files from same application and setting the max input file size of each mapper to 256MB in CombineFileInputFormat of MR job decreases the mapper task number to around half of what was before. For data skew in Reducer, Partition takes over. In a CAL report, there are two concepts, report name and metrics name. For each kind of report, there are multiple metrics. Previously, the partition strategy used a hash of the report name. Now, it’s changed to a hash of the report name and metrics name. Data skew is much better. In the expression in the Hadoop Job duration section, the job duration should be proportional to the input record number. In my experiment, there are two data sets. Data set A is 20MB and data set B is 100MB. An MR job with input set A takes 90 minutes to complete. A job with B as input takes only 8 minutes. To figure out what is going on, we need to look into the log data. In a CAL log, there are two types of logs: SQL and general logs. The general log is an event log. The SQL log is related to DB operations like SQL statements. The general log might reference the SQL log. Parsing the SQL log is more time consuming. Accordingly, we counted the SQL logs in data sets A and B. They are almost equal. Counting the general log that referenced SQL log provides some idea; the number for B is much larger than A. The implementation suffers some repetitive computation on the reference SQL log part. Each SQL would be parsed every time, if referenced. To solve this issue, we cached the parsed result of the SQL log and reused it once referenced. The job for A can now complete in 4 minutes. After the optimization of memory for mapper, reducer, the data skew problem, and the repetitive computation issue, we also need to also look at resource usage. The action item to reduce resource usage includes: Combining small files to reduce mapper number using CombineFileInputFormat. Reducing memory usage. Optimizing time cost. Item 1 was addressed in the Data skew section. The solution for item 2 was addressed in the GC problem section. Item 3 was addressed in the repetitive computation chapter. With all these items addressed, the usage of Hadoop cluster decreased from about 50% to less than 19%. Optimization work should always care about data quality. The result for a user shouldn't be changed. Before we do this work, the verification plan should come first. We can use a metric with the idempotent property for evaluation. Another important issue is monitoring—bringing the KPI we care about, the success rate, resource usage, and job duration into a dashboard to observe trends during optimization. Before optimization, a CAL report job required 50% resources in the Hadoop cluster to complete. Now, it can be completed with 19% capacity. Figure 5 shows the memory resource usage on Hadoop Cluster. The left side shows the situation before optimization, the right is the current situation. Figure 5: Queue Resource Usage in Hadoop Eagle We can save more than 50% relative computing resources, which equates to nearly ~200 Hadoop nodes in the Hadoop Cluster. Figure 6: Compute Usage in Hadoop Eagle The success rate has increased from 92.5% to about 99.9%. A failure rate of 00.1% is due to jobs running more than 1 hour. These jobs are killed because they need more complex computing. 95% of the Hadoop jobs now have a running time of about 40 minutes, down from 90 minutes before optimization. Figure 7: CAL Report MR Job Running Time Trend Because the CAL log volume increases 70% YoY, the volume will continue to increase 70% each year. We need to use CombineFileInputFormat and Partition to make MR scale. Data volumes increase with each passing day. Hadoop MR is a straightforward solution for big data analysis. Coupled with increasing computing resources to handle data volume increases, optimization can be lead to major cost savings. These optimizations for MR jobs have helped save more than 50% resources than before and have increased success rates to 99.9%.", "date": "2018-04-17"},
{"website": "Ebay-Engineering", "title": "Find It On eBay: Using Pictures Instead of Words", "author": ["Steve Neola", "Ben Klein", "Max Manco"], "link": "https://tech.ebayinc.com/product/find-it-on-ebay-using-pictures-instead-of-words/", "abstract": "Learn how new eBay features make the entire internet shoppable with just the snap of a photo. Have you ever seen someone wearing something you wanted to buy but you didn’t know where to get it? Or maybe you saw something you liked while reading your favorite blog or flipping through Pinterest but you didn't know where you could buy it, let alone how to describe it in words? There is a saying that a picture is worth a thousand words, but who really wants to write that much when you are trying to find something quickly. Today, eBay announced Find It On eBay and Image Search , two features that will make the entire internet shoppable. These new features bring you a step closer to getting that item you really want, whether it’s brand new or nearly new—it’ll be totally you. Find It On eBay is a new feature in our eBay app and mobile platform that lets you share images from any social platform or web browser. All you have to do is “share” the image with eBay and our mobile app will find listings of the item in that image or others like it. With Image Search , you can take a photo of something you want to buy—or use an existing photo from your camera roll—and put it into the eBay Search bar on our native apps. Then, we’ll show you listings that match the item you are looking for. These features sift through the more than 1.1 billion listings on eBay, creating a seamless shopping experience and helping you find your version of perfect. They also open up new ways to discover unique and fun items that wouldn’t be possible with just using words. Leveraging the latest advances in two core parts of artificial intelligence -- computer vision and deep learning -- these new features make it easier to buy the things that inspire you. When you upload images to run Find It On eBay and Image Search, we use a deep learning model called a convolutional neural network to process the images. The output of the model gives us a representation of your image that we can use to compare to the images of the live listings on eBay. Then, we rank the items based on visual similarity and use our open-source Kubernetes platform to quickly bring these results to you, wherever you are in the world. We developed the idea for Image Search with a small team during eBay Hack Week, an annual company-wide competition challenging our technologists to innovate and reimagine the ecommerce experience. Our project won the competition in 2015, and from then on, our team has grown substantially while we’ve been building these features. We are continuing to work on additional features and expect to launch more computer vision products in the coming months. Find It On eBay and Image Search will be rolling out this Fall. At the time of launch, Image Search will be supported on both Android and iOS and Find It On eBay on Android. So, the next time you come across something you love when you are browsing online, snap a picture and find it on eBay. ### BIOS Steve Neola is a Product Lead for Recommendations and Image Search at ebay. He is captivated by enabling everyone to use images to shop the world’s most diverse inventory no matter where they are or what inspires them. Steve is a huge music fan and can be found at jazz clubs in the West Village when not working at eBay NYC. Ben Klein is an Applied Researcher at eBay, where he works on computer vision, machine learning, and deep learning. Ben leads eBay’s efforts on applying computer vision for search and recommendation systems applications. Prior to joining eBay in 2014, Ben worked at Microsoft Research as a machine learning researcher and developed algorithms that are used by Microsoft Xbox. Ben holds a Master’s degree in Computer Science from Tel-Aviv University and his work has been published in CVPR and ECCV. Max Manco is a Lead Engineer for Image Search and is responsible for its design, performance and scalability. Max joined the eBay Israeli Office in 2011 and has been involved in several key projects in the Structured Data group. In 2015, shortly after relocating to NYC, he started working on Image Search. Max is very passionate about Java, the Spring Framework, and building efficient large scale systems that delight our users.", "date": "2017-07-26"},
{"website": "Ebay-Engineering", "title": "Cube Planner - Build an Apache Kylin OLAP Cube Efficiently and Intelligently", "author": ["Qiaoneng Qian"], "link": "https://tech.ebayinc.com/engineering/cube-planner-build-an-apache-kylin-olap-cube-efficiently-and-intelligently/", "abstract": "Life is about carefully calculating daily necessities and the same is true for technology. Frugal people spend money on things that are needed most, while programmers are always seeking to reduce the resources cost of their code. Cube Planner is a new feature created by eBay’s programmers that helps you spend resources on building cost-effective dimension combinations. Apache Kylin is an open source Distributed Analytics Engine designed to provide multi-dimensional analysis (OLAP) on Hadoop, originally contributed from eBay to the Apache Software Foundation in 2014. Kylin became a Apache top-level project in 2015. With the rapid development of the Apache Kylin community since 2015, Apche Kylin has been deployed by many companies worldwide. There has also been a crescendo of data analysis applications deployed on the Apache Kylin platform within eBay. There are currently 144 cubes in the production environment of eBay, and this number continues to grow on an average of 10 per month. The largest cube is about 100T. The average query amount is between 40,000 and 50,000 per week day. During last month, the average query latency is 0.49 seconds. In 2016, eBay Data Services and Solutions (DSS) department began to introduce self-service reports. Data analysts are able to drag and drop dimensions and measures to create their own reports. But the self-service report generally has many dimensions, measures, and it is difficult to define the query pattern, which requires Kylin to pre-calculate a large amount of dimension combinations. In addition, in companies that reach a certain scale, data providers and data users don’t belong to the same department, and data providers may not be particularly familiar to the query pattern, which leads to a gap between the capability of cube design and actual use. Some combinations of dimensions are pre-calculated, but are not queried, while some are often queried, but are not pre-calculated. These gaps have effect on the resource utilization and query performance of a cube. We decided to make a cube build planner, Cube Planner, with the goal of efficiently using computing and storage resources to build cost-effective dimension combinations intelligently. Cube Planner checks the costs and benefits of each dimension combination, and selects cost-effective dimension combination sets to improve cube build efficiency and query performance. The Cube Planner recommendation algorithm can be run multiple times throughout the life cycle of the cube. For example, after the cube is created for the first time, or after a certain amount of query statistics is collected, the cube is periodically optimized. Here are some definitions and formulas used in the Cube Planner algorithm. Dimensions and Measures are the basic concepts of data warehousing. A dimension combination (Cuboid) refers to any combination of multiple dimensions of the data. Suppose the data has three dimensions (time, location, category), then it has eight kinds of dimension combinations: [ ], [time], [location], [category], [time, location], [time, category], [location, category], [time, location, category]. The basic dimension combination (Basic Cuboid) is a combination of all dimensions, with the most detailed aggregation information, such as the combination of the dimensions above [time, place, category]. The symbol≼ represents a parent-child relationship between dimension combinations, w ≼ i, indicates that a query involving dimension combination w can be aggregated from the data of the dimension combination i, the combination w is a child combination, and the combination i is the parent combination. For example, dimension combination w [time, category] and dimension combination i [time, location, category]. Each parent combination may have multiple child combinations, and the basic dimension combination is the parent of all other dimension combinations. Costs of a dimension combination The costs of a dimension combination is divided into two aspects. One is the build cost, which is defined by the number of rows of data combined by itself, and the other is the query cost, which is defined by the number of rows scanned by the query. Since the build is often one-time while the query is repetitive, we ignore the build cost and only use the query cost to calculate the cost of a dimension combination. C(i) = Cost of dimension combination(i). Benefits of dimension combination The benefits of a dimension combination can be understood as reduced costs, that is, the cost of all queries on this cube that can be reduced by pre-calculating this dimension combination.B(i, S) refers to the benefit of creating a dimension combination (i) in the dimension combination set S, calculated by summing up the benefits of all the subsets of i: B(i,S) = ∑ w≼i B w Among the set, the benefit of each subset B w =max{ C(w) – C(i) ,0} Benefit ratio of dimension combination Dimension combination benefit ratio = (cost of the dimension combination/benefit of dimension combination) * 100%: Br(i, S) = (B(i, S) / C(i)) * 100% We use greedy algorithms to choose a cost-effective dimension combination, and the process of selection is iterative. For each iteration, the system will go through the candidate set, select a dimension combination with the highest benefit ratios at the current state, and add it to the combination result set that needs to be built. Let’s now look at a simple example to understand the process of the iterative selection. Let’s assume there is a cube that has the following dimension combination. The parent-child relationship between each dimension combination is shown in the following figure. Each dimension combination has a set of numbers (i, j) in addition to ID. i: the number of rows in this dimension combination j: the cost of querying the dimension combination In the following figure, dimension combination a is the basic dimension combination (Basic Cuboid), and the data has 100 rows. In the Kylin cube, the basic dimension combination will be pre-calculated by default. Initially, the result set starts with only the basic dimension combination. Therefore, queries on other dimension combinations need to answered by scanning the basic dimension combination data then doing the aggregation, so the cost of these queries is the number of rows, 100. The green dimension combinations are the combinations that have been selected into the result set during the previous rounds, initially with only the basic dimension combination. The red ones are the combinations that may be selected into the result set during the this round. The blue ones indicate that these combinations are selected and will be added to the result set during this round. What is the benefit of selecting dimension combination b, that is, the reduced cost of the entire cube? If b is selected, the cost of b and its children (d, e, g, h) will be reduced from 100 to 50, so that the benefit ratio of the dimension combination b is (100 – 50) * 5 / 50 = 5. If you choose the dimension combination C, the benefit ratio is 1.67. If other dimension combinations are selected, the benefit ratio is as follows. It is obvious that g is a combination with the highest benefit ratio, so dimension combination g is added to the result set during this round. At this point, the result set has become (a, g). At this state, if b is selected, although the children of b are d, e, g and h, only query over b, d and e will be effected because g has been pre-calculated and g has fewer row counts than b. When the query against dimension combination g arrives, the system will still choose g to answer the query. This is exactly why the benefit of each child combinations mentioned above is B w =max{ QC(w) – QC(i) ,0 }. So, the benefit ratio of combination b is 4. If you pick the combination c, the benefit ratio is 1.34. The benefit of other combinations is as follows. Combination h is selected eventually and is added to the result set during this round. When any one of the following three conditions is fulfilled, the system will stop the selection: The expansion rate of combination dimensions in the result set reaches the established expansion rate of the cube. The efficiency ratio of the selected dimension combination this round is lower than the established minimum value. This indicates that the newly added dimension is not cost-effective, so there is no need to select more dimension combinations. The run time of the selection algorithm reaches the established ceiling. In the previous algorithm, the benefit weights of other dimension combinations are the same when calculating the benefit that one dimension combination will bring for the entire cube. But in the actual application, the probability of querying dimension combination varies. Therefore, when calculating the benefit, we use the probability of querying as the weight of each dimension combination. Weighted dimension combination benefit WB(i,S) = ∑ w≼i （B w * P w ) P w is the probability of querying this dimension combination. As for those dimension combinations that are not queried, we set the probability to a small non-zero value to avoid overfitting. As for the newly created cube, the probability is set to the same because there are no query statistics in the system. There is room for improvement. We want to use the platform-level statistics data and some data-mining algorithms to estimate the probability of the querying dimension combinations of the new cube in the future. For the cubes that are already online, we collect the query statistics, and it is easy to calculate the probability of querying each dimension combination. In fact, the real algorithm based on weighted query statistics is more complex than this. Many practical problems need to be solved, for example, how to estimate the cost of a dimension combination that is not pre-calculated, because Kylin only stores statistics of the pre-calculated dimension combinations. Greedy algorithms have some limitations when there are too many dimensions. Because the candidate dimension combination set is very large, the algorithm needs to go through each dimension combination, so it takes some time to run. Thus, we implemented a genetic algorithm as an alternative. Using the iterative and evolutionary model of the gene to simulate the selection of the dimension combination set, the optimal combination set of dimensions is finally evolved (selected) through a number of iterations. There is no refinement. If anyone is interested, we can write a detailed description of the genetic algorithm used in Cube Planner. For the cube that is already on the production site, Cube Planner can use the collected query statistics, and intelligently recommend a dimension combination set for pre-calculation based on cost and benefit algorithm. Users can make a one-click optimization based on this recommendation. The following figure is the one-click optimization process. We use two jobs, Optimizejob and Checkpointjob, not only to ensure the optimization process concurrency, but also to ensure the atomicity of the data switching after optimizing. In order to save the resource of optimizing the cube, we will not recalculate all the recommended dimension combinations in Optimizejob, but only compute those new combinations in the recommended dimension combinations set. If some recommended dimension combinations are pre-calculated in the cube, we reuse them. Cube Planner’s development was basically completed at the end of 2016. After a variety of testing, it was formally launched in the production environment in February 2017. After the launch, it was highly praised and unanimously recognized by the users. Here is an example of the use of Cube Planner on eBay’s production line. The customer optimized the cube on April 19th. The optimized cube not only had a significant boost in building performance, but also improved significantly in query performance. In addition to enhancing resource utilization, Cube Planner has another goal of reducing the cube design complexity. At present, the design of a Kylin cube depends on the understanding of data. Cube Planner has made Kylin able to recommend and optimize based on data distribution and data usages, but this is not enough. We want to make the Kylin cube design a smart recommendation system based on query statistics across cubes within same domain.", "date": "2017-08-02"},
{"website": "Ebay-Engineering", "title": "eBay’s New Homepage", "author": ["Yotam Sharan", "Shivkumar Chandrashekhar"], "link": "https://tech.ebayinc.com/engineering/ebays-new-homepage/", "abstract": "Building and releasing a modern homepage for eBay’s hundreds of millions of annual and tens of millions daily users was the team’s greatest challenge to date. We were given the mission to develop a personalized buying experience by mashing merchandised best offers with tailored ads, topping it off with the hottest trends and relevant curated content. Any such virtual front door had to be robust to changes, be easily configurable for any site, and had to load really, really fast! There’s no second chance to make a first impression, so we had to make it count. But how quickly can you go from 0 to 100 (100% deployed for 100% of traffic to the homepage)? The team commenced this project in late 2016, and we rushed to get an early flavor available for an internal beta in mid-February. Then, traffic was slowly ramped until reaching a 100% mark for the Big 4 sites (US, UK, DE, AU), delivering it less than three months after the beta, while simultaneously adding more scope and fresh content to the sites. Our new homepage consists of three main components, spread across three large pools: Node Application – The homepage front-end web server handles incoming user requests and acts as the presentation layer for the application, boasting responsive design. Experience Service – The experience service tier is the primary application server that interacts with the page template and customization service and orchestrates the request handling to various data providers. It performs response post-processing that includes tracking. This layer returns a render-ready format that is consumable by all screen types. The experience layer is invoked by Node for desktop and mobile web traffic and directly by the native applications. Data Provider Service – The data provider service is facade that massages responses from datastores into a semantic format. Providers, such as Merchandise, Ads, Marketing, Collections, and Trends, are interfacing with this service. JSON transformation, done using Jolt, produces the response datagrams. Additional auxiliary components help keep the system configurable and simple to reorganize or to publish new content to site within seconds. The Experience service was built on a shared platform, called Answers, developed jointly with the Browse and Search domains. Due to the hierarchical nature of the system, its complex compositeness (16 unique modules), and its multiple dependencies (9 different services, to be exact), special consideration has to be given to the rollout and deployment process. We adopted a bottom-up strategy by deploying in the reverse order of dependencies. That is, we start with datastores, data providers, experience services and lastly deploy the node application server. This approach helps us bubble up issues early on in the process and tackle misalignment in an effective manner. All service-to-service calls are made via clearly defined, internally reviewed APIs. Interface changes are only allowed to be additive to maintain backwards compatibility. This is essential for the incremental bottom-up rollout scheme. When developing and deploying a new build, as you would delicately change a layer in a house of cards, one must act with great care and much thought about the inherited risks. Test strategies like “mock-first” were used as a working proof-of-concept and allowed testing to begin early on, where caching techniques quickly took the place of the very same mocks in the final product. Unit tests, hand-in-hand with functional and nonfunctional tests, had to be done with rigor, to make regression easy, fast, and resilient to change. The team always deployed to Staging and Pre-production before conducting any smoke-testing on three boxes in Production, one VM instance per Data Center. Specific networking issues were exposed and quickly remediated, as a result. Some engineering-related challenges that the team was facing had to do with Experimentation Platform complexities, where a selective pool channel redirect appeared to have unexpected behavior with traffic rerouting, while gradually diverting from the current Feed home page to the new carousel-based one. Internal Beta testing, done by Buyer Experience teams, and external in-the-wild testing conducted by a crowd-testing company, took place with a zero-traffic experiment. This process was followed by a slow 1% traffic ramp, to help control the increments, as gradual 5% and 25% steps were introduced to the pools, spanning days to weeks between the phases. Each ramp involved multiple daily Load & Performance cycles, while fine-tuning timeout and retry variables. To summarize our key learnings from the homepage experience: Relying on clearly-defined APIs was vital in achieving an iterative, incremental development and rollout strategy. Co-evolving our logging and monitoring infrastructure and the team’s debugging expertise was essential to achieving an aggressive rollout approach on a distributed stack. Tracking and Experimentation setups, as well as baking in Accessibility, proved to be major variables that should get well-scoped into a product’s development lifecycle and launch timelines. Introducing gradual functional increments along with a design for testability in mind proved to be a valuable quality control measure and allowed absorbing change while moving forward. A meticulous development and release process maintained the associated risks and mitigation actions, allowing the team to meet their milestones, as defined and on time.", "date": "2017-07-26"},
{"website": "Ebay-Engineering", "title": "Announcing the Accelerator", "author": ["Anders Berkeman", "Carl Drougge", "Sofia Hörberg"], "link": "https://tech.ebayinc.com/engineering/announcing-the-accelerator-processing-1-000-000-000-lines-per-second-on-a-single-computer/", "abstract": "The Expertmaker Accelerator is a well-proven data processing framework that provides fast data access, parallel execution, and automatic organization of source code, input data, and results. It can be used for daily data analysis tasks as well as operating as a live recommendation system with hundreds of thousands of large data files and many CPUs. The Accelerator is now released as open source by eBay. The Accelerator runs on computers ranging from laptops to rack servers, handles datasets of billions of rows with ease, and keeps order in hundreds of thousands of input files, computations, and results. Data throughput is typically in the range of millions of rows per second. On a fast computer, tens or hundreds of millions of rows per second is a reality. For example, adding all values in a column may run faster than 1000 million rows per second, all in the Python programming language. The Accelerator was developed by the Swedish AI company Expertmaker, first deployed in 2012, and since then it has been the core tool in a number of studies and live production recommendation systems. In 2016, Expertmaker was acquired by eBay. eBay is now making the Expertmaker Accelerator available to the open source community under the Apache License, Version 2. The Accelerator is designed bottom up for high performance with the following main design goals: It should be easy to process data in parallel on multiple CPUs. After all, computers have shipped with multiple CPU cores for many years. Data throughput should be as fast as possible. Even a small laptop should handle millions of rows of data with ease. Old results should be reused, if available, and not recomputed. Similarly, sharing results between multiple users should be effortless. A data science project may have lots (hundreds of thousands) of input files and lots of source code and intermediate results. The Accelerator should remove the need for manual administration and bookkeeping of data files, computations, results, and how they are related. The Accelerator is intended to work on log files . Log files are a powerful way of storing data and includes transaction logs, event logs, database dumps, and more. The atomic operation of the Accelerator is building jobs . Building a job is the process of executing some program with input data and parameters and storing the result, i.e. the output, together with all information required to do the computation to a directory on disk. The resulting so called job directory will contain both the computed result and all information that was required to compute this result. As we will see later, jobs can be anything from simple or complex calculations to containers of large datasets. Jobs can also link to each other, so that a new job can depend on one or more older jobs. Two key features stand out, result reuse and data streaming . Before building a new job, the Accelerator checks whether the job has been built before. If it already exists, it will not be built again. Instead, the Accelerator will return a link to the existing job. This saves execution time and helps sharing results between users. More importantly, it provides visibility and ensures determinism. On top of that, there is a mechanism that stores job information in sessions into a database, which helps administrating jobs and the relations to each other. Streaming continuous chunks of data from disk to CPU is much more efficient than performing a random query in a database. Streaming is the optimum way to achieve high bandwidth from disk to CPU. It is cache oblivious and may make good use of the operating system’s RAM-based disk buffers. The Accelerator is built on the idea that in many data science and machine learning projects, all or a large part of the data is involved in the computations. Let’s take an overview of the Accelerator. For a more detailed overview and technical details, please see the Accelerator User’s Reference Manual . The Accelerator is a client-server based application, and figure 1 shows what it looks like from a high level. Figure 1. High level view of the Accelerator framework On the left side there is a runner client. To the right, there are two servers, called daemon and urd , where urd is optional. The runner program runs scripts, called build scripts , that execute jobs on the daemon server. This server will load and store information and results for all jobs executed using the workdirs file system-based database. In parallel, information about all jobs covered by a build script will be stored by the urd server into the job logs file system database. urd is responsible for job bookkeeping. This includes storing and retrieving sessions, or lists, of related previously executed jobs. Jobs are created by the execution of small programs called methods . Methods are written in Python 2 or Python 3, and sometimes in the C programming language. In a method, some reserved function names are used to execute code sequentially or in parallel and to pass parameters and results. A method that is under execution or has completed execution is called a job . A simple “hello world” program is used to illustrate job building. We create a program (method) with the following contents: This program does not take any input parameters. It just returns a string and exits. In order to execute it, we create a build script that calls the method, like this: When execution of the method is completed, a single link, called a jobid, is the only thing that is returned to the user. (In the example above, it is stored in the jid variable.) The jobid points to a directory where the result from the execution is stored, together with all information that was needed to run the job plus some profiling information. See figure 2. If we try to run the job again it will not execute, because the Accelerator remembers that a job exactly like this has been run in the past. Instead of running the job again, it immediately returns the jobid pointing to the previous run. From a user’s perspective, this means that there is no difference between job running and job result re-use, except for the execution time. In order to make the job execute again, we have to change either the source code or the job’s input parameters. Figure 2. The method hello world is used to build the job test-0 Assume that the hello_world job that we just built was computationally expensive, and that it returned a result that we’d like to use as input to further processing. To keep things simple, we demonstrate the principle by creating a method named print_result that just reads and prints the result from the previous job to stdout . Here it is: This method expects the hello_world_job input parameter to be provided at execution time, and we will see next how to do this. The method then reads the result from the provided jobid and assigns it to the variable x , which is then printed to stdout . In this example, the method is not very useful, because it does not store anything in the job directory, so there is no result to be recalled later. However, to build this job, we extend the build script like this: When we run the build script, only the print_result job will be built, since the hello_world job was built previously. Figure 3 illustrates the situation. Note the direction of the arrow. The second job, test-1 has test-0 as input parameter, but test-0 does not know of any jobs run in the future. Figure 3. Job test-0 is used as input to the print result job So far we’ve seen how to create, connect, and run simple jobs. Now we turn our focus to the methods. There are three functions in a method that are called from the Accelerator when a method is being executed, and they are prepare() , analysis() , and synthesis() . All three may exist in the same method, and at least one is required. When the method executes, they are called sequentially. Figure 4 shows the execution order from top to bottom. Colored branches represent data passed between functions. prepare() is executed first, and its return value is available to both the analysis() and synthesis() functions. There are slices (a configurable parameter) number of parallel analysis() processes, and their outputs are available to the synthesis() function, which is executed last. Return values from any of the three functions may be stored in the job’s directory, making them available to other jobs. Figure 4. Execution flow and result propagation in a method We’ve already seen how jobids from completed jobs can be used as input to new jobs. The jobid parameter is one of three kinds of input parameters that a job can take. The others are a flexible option dictionary parameter and a set of pointers to datasets. The dataset is the Accelerator’s default storage type for small or large quantities of data, designed for parallel processing and high performance. Datasets are built on top of jobs, so datasets are created by methods and stored in job directories, just like any job result. A single job may contain any number of datasets, making it possible to write example jobs that split an input dataset into several new datasets. Internally, data in a dataset is stored in a row-column format. All columns are stored, entropy coded, and accessed independently to avoid the overhead of reading data that is not necessary for each actual processing task. Data is also sliced into a fixed number of slices to allow efficient parallel access. Datasets may be hashed , so that slicing is based on the hash value of a given column. Hashing will group all dataset rows with the same value in the hashed column into one slice. In many practical applications, hashing separates data so that parallel processes may execute independently, minimizing the need for complicated merging operations. This is explained further in the Parallel dataset access and hashing section. Let’s have a look at the common operation of importing a file, i.e. creating a dataset of the file’s contents. There is a standard method bundled with the Accelerator designed for this. The csvimport method has been used on many different file types and can parse a plethora of comma separated value (CSV) file formats and store the data as a dataset. The created dataset is stored in the resulting job, and the name of the dataset will, by default, be the jobid plus the string default , but a custom string may be used instead. See figure 5. Figure 5. Importing file0.txt Just like jobs can be linked, datasets can link to each other, too. Since datasets are built on top of jobs, this is straightforward. For example, assume that we’ve just imported file0.txt into imp-0 and that there is more data stored in file1.txt . We can import the latter file and supply a link to the previous dataset, see figure 6. Since the datasets are linked, the imp-1 (or imp-1/default ) dataset reference can now be used to access all data imported from both files. Linking datasets containing related content is called chaining , and this is particularly convenient when dealing with data that grows over time, such as log data. Using chaining, we can extend datasets with more rows just by linking, which is a very lightweight operation. Figure 6. Chaining the import of file1.txt to the previous import of file0.txt . In the previous section we saw how easy it is to add more lines of data to a dataset using chaining. Chaining is implemented by simply assigning a link. Now we'll see that is it equally simple to add new columns to an existing dataset. Adding columns is a common operation and the Accelerator handles it efficiently using links. The idea is very simple. Assume that we have a \"source'' dataset to which we want to add a new column. We create a new dataset containing only the new column, and while creating it we instruct the Accelerator to link all the source dataset's columns to the new dataset. See Figure 7. Note the difference between adding columns and chaining. In chaining, whole datasets are linked to each other. When adding columns, we link columns from other datasets into the new dataset. Accessing the new one-column dataset will transparently access all the data in the source dataset too, making it indistinguishable from a single dataset. Figure 7. Adding a new column to an existing dataset Typically, a method creates a single dataset in the job directory, but there is no limit on how many datasets that could be created and stored in a single job directory. This leads to some interesting applications. One application where it is convenient to create multiple datasets is when splitting data into subsets based on some condition. For example, assume that we want to separate a dataset into two disjoint datasets based on a column storing a Boolean value. Figure 8. job-1 separates the dataset job-0/default into two new datasets, named job-1/train and job-1/test Figure 8 shows how job-1 has created two datasets, job-1/train and job-1/test , based on the input dataset job-0/default . A third job, job-2 , is then accessing the job-1/train dataset. Let us take a short detour and consider an example of when such dataset splitting makes sense, and how it relates to the design methodology that the Accelerator is based upon. Assume that we have a (perhaps large) dataset that we want to split into, say, a training set and a test set. When splitting, we “physically” separate the data into two sets, while still keeping all the data in the same place. This is good for transparency reasons, and any method following the split may still iterate over both subsets to read the complete data. Furthermore, it is very likely that we’ll read the training and validation datasets many times, so splitting them initially will probably save execution time in the end, even if one of the sets is small. As mentioned earlier, data in datasets is stored using multiple files, allowing for fast parallel reads. The slices parameter determines how many slices that the dataset should be partitioned into. This parameter also sets the number of parallel analysis () processes, so that each analysis() process operates on a unique slice of the dataset. Datasets can be partitioned, or sliced , in different ways. One obvious way is to use round robin, where each consecutive data row is written to the next slice, modulo the number of slices. This leads to datasets with an approximately equal number of rows per slice. Another alternative is to slice based on the hash value of a particular column’s values. Using this method, all rows with the same value in the hash column end up in the same slice. This is efficient for some parallel processing tasks. It turns out that hashing a dataset is a relatively fast operation, and in many applications this allows for efficient parallel processing without the need of merging the (independent) results. If we make an analogy to map and reduce , using the Accelerator we replace the post-reduce by a pre-hashing. This is particularly efficient if we run the computations many times, since we only do the hashing once. There are a number of useful types available for dataset columns. They include floating and integer point numbers, Booleans, timestamps, several string types, and json types. Several of these types are designed to make importing data from text files straightforward, without parse errors, overflows, etc. Furthermore, the dataset has a number of attributes associated with it, such as shape, number of rows, column names and types, and more. An attribute is accessed like this: Data in a dataset is typically accessed using an iterator that reads and streams one dataset slice to a CPU core. In this section, we’ll have a look at iterators for reading data, how to take advantage of slicing to have parallel processing, and how to efficiently create datasets. The first iterator example will be a sequential, i.e. non-parallel, solution to show the basic iterator concepts. A corresponding parallel solution will be presented thereafter. Assume that we have a dataset with a column named movie containing movie titles, and we want to know the ten most frequent movies. Consider the following example of a complete method: This will compute and store the ten most common movie titles and their corresponding counts in the source dataset. The code will run on a single CPU core, because of the synthesis() function, which is called only once. The iterate method therefore has to read through all slices, one at a time, in a serial fashion, and this is reflected by the first argument to the iterator being None . We rely on the Counter class to do the counting and sorting. The Accelerator is designed for parallel processing, which is mainly provided by the combination of sliced datasets and parallel analysis() calls. The following modification implements a parallel version of the movie example: Here, iterate is run inside the analysis () function. This function is forked once for each slice, and the argument sliceno will contain an integer between zero and the number of slices minus one. The returned value from the analysis () functions will be available as input to the synthesis() function in the analysis_res Python iterable. It is possible to merge the results explicitly, but analysis_res comes with a rather magic method, merge_auto() , which merges the results from all slices into one based on the data type. It can for example merge Counters , sets , and composed types like dicts of Counters , and so on. Since each column is stored independently in a dataset, there is no overhead in reading more than one of the dataset’s columns in parallel. Iterating over several columns is straightforward by feeding a list of column names to iterate , like in this example: This example creates a lookup dictionary from users to sets of movies, and stores it to disk for future use. A special case is iterating over all columns, which is done by specifying an empty list of columns or by using the value None . This example will print the first row for each slice of a dataset and then exit. The iterate function iterates over a single dataset. There is a corresponding function, iterate_chain , that is used for iterating over chains of datasets. This function takes a number of arguments, such as length , i.e. the number of datasets to iterate over. By default, it will iterate over all datasets in the chain. callbacks , functions that can be called before and/or after each dataset in a chain. Very useful for aggregating data between datasets. stop_id which stops iterating at a certain dataset id. This jobid could be from another job’s parameters, so we can, for example, iterate exactly over all new datasets not covered by a previous job. This makes is simple to do delta updates, i.e. read previous result, append anything that is new, and store. range , which allows for iterating over a range of data based on a column’s values. The range option is based on the max/min values that are automatically stored for each column in a dataset at creation time. Assuming that a chain is sorted, one can for example set in order to get rows within the specified range only. range is quite costly, since it requires each row in the dataset chain to be checked against the range criterion. Therefore, there is a sloppy version that iterates over complete datasets in the chain that contains at least one row with a value within the range. This particularly efficient if the datasets are sorted on the column that is used for range checking. Depending on how the parallel processing is implemented in a method, some methods will only work if the input datasets are hashed on a certain column. To make sure this is the case, there is an optional hashlabel parameter to the iterators that will cause a failure if the supplied column name does not correspond to the dataset’s hashlabel. The iterator may perform data translation and filtering on-the-fly using the translators and filters options. Specifying translators and filters as options makes it easier to write methods that are less application specific and more generic. Here is an example of how a dictionary can be fed into the iterator to translate a column’s values: This will iterate over the NUM_LEGS column, and map numbers to strings according to the mapper dict. The translator is not limited to dictionaries, it could also be a callable, i.e. a function. Filters are used to discard rows from the iterator on-the-fly. They work similarly to translators. We’ve already seen how the Accelerator keeps track of all jobs that have been built, keeping them ready to be reused when possible. While this saves time and ties linked computations together, there is another layer on top that pushes visibility and job reuse even further. This is what the Urd 1 server is about. Urd stores lists of jobs together with their dependencies in a log-file based database. Everything that takes place in a build script may be recorded to Urd. In order to do that, we need a name for the list to store the information in, and we also need a key , in most situations a date, to be able to look it up again. We’ll use a simple example to explain the concepts. Consider the following build script that is used to import log files into a chain of datasets: Everything that goes on between the begin and finish calls is recorded to the import list in Urd, and can be recalled by querying Urd with that particular date. Or, as in the case shown, we can ask Urd to fetch the latest entry in the list. In this example, we use this to create a chain of datasets. We look up the latest entry in the import list, fetch the csvimport jobid, and feed it as previous dataset to the current csvimport . We can then use this import list for data processing: The example above writes to the processing Urd list. It fetches the latest entry in the import list, from which it extracts the jobid to the csvimport job and the timestamp. This timestamp is then used as key when this session is written to the processing list. Now we have everything bookkept nicely. We can query the processing list to find the latest (or any specific) processing session, and from there we can see which import session it corresponds to. From this entry, in turn, we can immediately find the csvimport and thus the filename of the file being imported. All connections between jobs and inputs are tracked with full visibility! The Accelerator itself makes sure no jobs are rebuilt unless data or code has changed. Taken together, this provides a powerful way of having the Accelerator organize all jobs and related files (including source code) for its users, removing the need for error prone manual administration. Consider the example where we are doing data processing in, say, ten different languages. For each language, we have tens of thousands of files. By importing the files in dataset chains, and storing each import session to Urd with one list per language, we can easily look up any file in any language, and know exactly which data we are processing. The startup time for a new job is a fraction of a second. Below is an example of processing times for some different job types. The example data file is 1.1TB (gz-compressed 280GB), and has 6.3 × 10 9 rows and 14 columns. The Accelerator is running on a large machine with 72 cores and a fast disk. The above values are measured when operating on the full data, six billion lines times 14 columns. The import job (A.) is importing a gz-compressed file. Interestingly, the import runs almost 30% faster than a plain (GNU-) zcat file.gz > /dev/null . On FreeBSD, zcat is faster. The typing job (B.) is typing to: 5 json-lists, 5 numbers, 2 dates, and 2 unicode columns. Each line is on average 172 bytes. The job is reading more than a half gigabyte per second, and simultaneously storing approximately the same amount, so disk bandwidth is above one gigabyte per second. Since hashing speed depends on the column that is hashed, the shown value (row C.) is an average of four hashing jobs, working on different columns. We compute the operation ∑ (a × b × c), by having one method reading three columns, multiplying their values, and writing the result to a new column that is appended to the dataset. A second job then adds all values in this new column together. As we can see, multiplying three float64 together and writing back to disk is really fast—77 million rows per second. Summing the values together is even faster—above one billion values per second. It takes six seconds to sum all values in the Python language. The Accelerator is a well-proven tool for fast data processing. On a single computer, processing speed of millions of rows per seconds is possible, and simple tasks may execute at 1000 million rows per second. In addition to fast processing, the Accelerator minimizes manual administration of source and data files, computations, and related results. It has been used successfully in several projects and is now available as open source. ExpertMaker Accelerator Repository Installer Repository Accelerator User’s Reference Manual 1 Urd is one of the three Norns in Norse mythology. She represents the past.", "date": "2018-04-26"},
{"website": "Ebay-Engineering", "title": "Dissect Helps Engineers Visualize and Debug Distributed Applications", "author": ["Ramesh Mahadevan"], "link": "https://tech.ebayinc.com/engineering/dissect-helps-engineers-visualize-and-debug-distributed-applications/", "abstract": "In a natural evolution from a services architecture, we at eBay have adopted microservices to help us drive faster and more productive development cycles. eBay’s sophisticated application log infrastructure helped us for the most part, but with the use of microservices, we need to build the next-level infrastructure to visualize and debug microservices and to achieve these goals: Make debugging more efficient by reducing time to troubleshoot and debug an API without understanding its upstreams and downstreams. Provide clarity and transparency for API developers to understand the caller and callee. Treat instrumentation as a primary data point. Logs are great, but technically, instrumentation is a special kind of log. Last but not least, the ability to query and visualize the service chain in near real time. To help achieve these goals, we built Dissect . Dissect is an eBay distributed tracing solution, based on Google’s Dapper , that helps engineers visualize and debug distributed Java and Node.js applications. It identifies how to instrument services and which services to instrument. Dissect Instrumentation library intercepts the incoming request and instruments the Request. Dissect records the Instrumented Span Data based on Sampling. Dissect provides a simple waterfall view to visualize the Traces. The recorded Traces can be analyzed for understanding release-over-release performance enhancements, etc. Dissect provides the following benefits: Bottleneck Detection . Dissect helps us understand the depth of calls. On most occasions, one single microservice does not cause a bottleneck, and multiple issues can be either up in the chain or downstream calls. Developer Productivity. Dissect helps API developers produce a complete call graph and increases the room for optimization of the API. Highly Scaleable. Dissect supports eBay’s large volumes of transactions and scales up to billions of requests for eBay’s use cases. Polyglot . Dissect includes SDKs for Java and Node.js libraries. The root of the transaction generates CorrelationId and every request generates RequestId . This figure illustrates the sample Request Flow of a distributed service chaining based on the user’s activity with eBay. CorrelationId – A unique 64-bit identifier that identifies the transaction and propagates across the whole service life chain RequestId – A unique 64-bit identifier generated and propagated for every Request part. Response – Standard HTTP Response with Status, Duration . Dissect borrows the ideas and inspiration from OpenTracing and follows the terminologies: A trace is a unit of work performed in a server-bound transaction. An HTTP Request is a trace. A span is a u nit of work performed in the context of a Trace. Executing an HTTP Client Call is a span. Another example is an o utbound HTTP client request in the context of a trace. Trace and span are defined by this data construct: The following example demonstrates how the TraceId, ParentId, and Id are propagated across different systems. The following figure illustrates the Dissect path. The figure identifies various distributed Service Nodes. Circle nodes are Server Nodes, square nodes are identified as Clients. Along with the standard values described above, Dissect captures custom attributes for quicker debuggability. These custom attributes provide enough information to quickly identify the source of the transaction.  The following table lists the custom key information captured. Even though these examples are tailored for HTTP protocols, the implementation is generic enough to adapt to RPCs. After all it is a simple Java and Node.js API. Dissect, by default, allows several modes of sampling strategies: Sampling is implemented to collect Sampled Traces across the applications. Also down-stream applications can set up higher sampling rates to collect Traces based on conditions like Failures, Status Codes, and Latency in ms. Dissect also allows a sliding window to dissect the requests for every X minutes in X hours. This allows applications to automatically dissect the requests. This approach helps in getting a good sampling rather than a % of sampling. Finally, you can use a custom sampling strategy. At eBay we have developed extensible A/B strategies. Applications using an A/B strategy can leverage the same as a sampling strategy for Dissect, also. Note: The sampling and experimentation platforms have been a part of the eBay platform for some years, and applications can sample based on any of the above strategies and use Dissect to trace the requests. Dissect provides default set of SDKs for developers to quickly boot-strap integration with the Request Stack Trace. The following SDKs support instrumenting your code with Trace. All the SDKs listed below support Java and Node.js implementations. Interceptors/Filters HTTP request Interceptors (or) filter to intercept all the incomming HTTP request and create a Trace. HTTP request Interceptors (or) filter to intercept all the incomming HTTP request and create a Trace. Client Handler Client Handler wraps the outbound HTTP client calls and creates a Span transaction for the Trace context. Client Handler also maintains lifecycle of the Span. Client Handler wraps the outbound HTTP client calls and creates a Span transaction for the Trace context. Client Handler also maintains lifecycle of the Span. JMX Interfaces Request Stack Trace provides support for on-demand sampling using JMX interfaces. Provides configurations to setup or update Reporters. Request Stack Trace provides support for on-demand sampling using JMX interfaces. Provides configurations to setup or update Reporters. The collector, aka the Reporter SDK, is responsible for transporting the Dissected requests and reporting to the back-end infrastructure. Dissect Reporters Reporter is an interface that helps to ship the collected Traces out of the system to a central storage. The default reporter is a simple log line reporter. Dissect out of the box supports Kafka Messaging as default reporter. Dissect uses the Avro entity as the data format to transport data. ElasticSearch is used as the warm datastore to ingest warm data consumed from Kafka streams. The implementation at Ebay uses Rheos as the datastore and Pronto scaleable ES as the warm storage to query and aggregate results. Reporter is an interface that helps to ship the collected Traces out of the system to a central storage. The default reporter is a simple log line reporter. Dissect out of the box supports Kafka Messaging as default reporter. Dissect uses the Avro entity as the data format to transport data. ElasticSearch is used as the warm datastore to ingest warm data consumed from Kafka streams. The implementation at Ebay uses Rheos as the datastore and Pronto scaleable ES as the warm storage to query and aggregate results. We use Kibana dashboards to surface all the collected traces, and a simple waterfall view to visualize a selected Trace. Visualizing every single Trace is important, but it is not practical to drill through every single Trace. We are evaluating Druid to provide richer and faster aggregation across the collected Traces. When we evaluated the Open Source alternatives to instrument the libraries, we found it compelling to use the Open Source libraries and the polyglot support they offer: After evaluating the Open Source alternatives, we decided to go with a standard implementation that suits the standard spec that is already defined and flowing all the sub-systems. A few reasons we made the instrumentation libraries separate implementations include: Instrumentation needed to confirm to [wwww.ebay.com][eBay] instrumentation spec: Didn’t want to adapt a library and try to retrofit to the standard. Rather, we will come up with a standard instrumentation library that adapts to the existing spec and the data flow. Ability for the instrumentation modules to have eBay internal monitoring and operatibility built in. These are plugins that can be injected and operationalized to the eBay infrastructure. The ability to instrument based on a custom sampling strategy eBay uses sophisticated and robust A/B sampling strategies, and we wanted to leverage this strategy and provide the ability to supply it as a custom sampling strategy. Reporters Reporters need to emit enough information about monitoring and operatibility in the eBay infrastructure. Dissect internal reporters are built with possible hooks and are integrated with the eBay infrastructure. We wanted to take ideas from OpenTracing on semantics and other conventions. OpenTracing was too new for us to pilot a few production-level applications, and we didn’t want external dependencies to derail our timelines. Lastly, building a Instrumentation library is easy and quick as long as the standards are defined clearly. Our goal for designing Dissect is to fill the a Service Tracing need. eBay has sophisticated log, monitoring, and event systems. Dissect complements the missing piece in the puzzle, Service Tracing. Dissect has been widely accepted as a need of the hour inside eBay. Currently, checkout microservices have onboarded with Dissect, and they are extremely happy to see the results. Checkout is a super critical flow for eBay , which explains how important the product is. Large projects and initiatives aren’t successful without the help of many people. Thanks to: Sanjeev Kataria Gail Frederick Venkatesh Ramaswamy Mahesh Somani Abhishek Pandey Venkatesh Palani Jancy Vincent Dapper", "date": "2017-09-19"},
{"website": "Ebay-Engineering", "title": "Want to Help Your Item Sell? Increase the Chances with eBay’s New Listing Feature", "author": ["Claire Xiao"], "link": "https://tech.ebayinc.com/product/want-to-help-your-item-sell-increase-the-chances-with-ebays-new-listing-feature/", "abstract": "Learn how eBay’s new listing feature makes it easier for business sellers to bump up the visibility of item listings on eBay to have a better chance at selling. When you come to eBay to list that modern-style floor lamp or that rustic farmhouse dining table, how can you give the items you’re selling a chance to stand out from the other millions of home decor listings in our marketplace? How can you give your listings that extra edge? eBay’s new listing feature—Promoted Listings—makes it easier for business sellers to do just that, by giving sellers the option to bump up their item listing to places on eBay with more visibility, so it has a better chance of standing out and selling—faster. All it takes is a few simple clicks while listing that item. We’ll even recommend how much sellers should spend to improve the chances of sales by boosting the visibility of their listing. When we launched Promoted Listings for multi-quantity listings, business sellers who used it had more buyers who saw their listings and their inventory sold faster, boosting visibility by about 30% on average.* With this new feature, business sellers can use Promoted Listings for single quantity and multi-quantity listings. And promoting a listing is effortless. To start, sellers can go to the Marketing Tab on the dashboard and pick from the drop down Promoted Listings, then select the inventory they want to promote and the percentage rate that will be taken out from the final sales price once that item has sold (starting at just 1%). Now this promoted item will be boosted so it’s more visible in search results, on the eBay homepage and on the product pages for similar items. Sellers will also get information for real-time impressions, clicks and sales coming through the Marketing tab on the dashboard, allowing them to make powerful changes to their listings to increase their sales. We’re using machine learning to determine exposure for each listing based on both the predicted likelihood of an item selling and the amount that a seller is willing to spend. As we see more sales go through Promoted Listings, we will be able to better predict the chances of single item sales and make sure that we in turn give that information back to the sellers to better inform their listings through trending rates. This tool is currently live on desktop in the U.S., U.K., Germany, and Australia and will be coming to mobile and other regions soon. We’re continuing to integrate the Promoted Listings tool into the selling journey and make it even easier to use. So, want to increase the chances of selling that item? Try Promoted Listings and ignite those eBay sales! *Based on data from May to June 2016. Measured on 40,000 listings that had sales before they were promoted and had promoted listings sales after they were promoted. Author Bio Claire Xiao is a Product Manager at eBay where she focuses on seller experience. She is also very passionate about artificial intelligence and its potential application to make our everyday life easier and more efficient. She loves reading and spends a lot of time catching up on the newest tech trends. She also enjoys taking deep learning courses.", "date": "2017-09-20"},
{"website": "Ebay-Engineering", "title": "Introducing Regressr - An Open Source Command Line Tool to Regression Test HTTP Services", "author": ["Ashwanth Fernando"], "link": "https://tech.ebayinc.com/engineering/introducing-regressr-an-open-source-command-line-tool-to-regression-test-http-services/", "abstract": "In the Artificial Intelligence-Human Language Technologies team at eBay, we work on software that powers eBay’s conversational bot, ShopBot. We ship software daily to production that makes our bot intelligent, smarter, and more human. As a crucial part of this effort, we have to make sure any regressions are caught quickly and fixed to help keep our customers doing what they love – making purchases on ShopBot. ShopBot’s backend is built on a polyglot suite of Scala, Java, and Python-based microservices that work in unison to provide ShopBot’s functionality. Hence, many of the crucial services need to be regression tested before we can release a new version to production. To help with that effort, we built and are open sourcing our regression testing tool, Regressr . We looked at the common approaches that are widely used in the industry today to build an automated regression testing suite. In no particular order, they are listed below. Comprehensive JUnit suite that calls two versions (old and new) of the service and compares the minutiae of the responses – JSON elements, their values and the like. Using SOAP UI’s Test Runner to run functional tests and catch regressions as a result of catching functionality failures. No regression tests. Wait for the front-end to fail as a result of front-end regression tests in dev or test, and trace the failure to the backend. We also looked at Diffy and were inspired by how simple it was to use for catching regressions. We had some very unique requirements for testing eBay ShopBot and found out that none of these tools provided the features we wanted: Super-low ceremony: Must quickly be able to productionize and gain significant value without too much coding or process. Low conceptual surface area: An engineer should be able to grok what the tool does and use it quickly without going through bricks of manuals and frameworks. Configurability of comparisons: We want to able to specify how the response should be compared. Do we want to ignore JSON values? Do we want to ignore certain elements? What about comparing floating point numbers, precision, etc.? Compare at high levels of abstraction: We want to capture high-level metrics of the responses and then perform regression testing on them. For example, we would like to be able to say the number of search results in this response were 5 and then use that number to compare against future deployments. Low maintenance overhead: We want maintenance of the regression suite to have low or negligible coding effort. Once every deployment is approved for release, we just want the suite to automatically capture the current state of the deployment and use that as a reference for future deployments. CI/CD Integration: Finally, we wanted this suite to be hooked into our CI/CD build. We built Regressr specifically to solve these requirements, so that the team can focus on the important stuff, which is serving great experiences and value to our customers who use ShopBot. Regressr is a Scala-based command line tool that tests HTTP services, plain and simple. We built it to be really good at what it does. With Regressr, you can use the out-of-the-box components to get a basic regression test for your service up and running quickly and gain instantaneous value, while coding regression tests that will cover close to 100% of the functionality in a more delayed fashion as time permits. Finally, Regressr doesn’t even need the two services to be up and running at the same time, as it uses a datastore to capture the detail of the baseline. Regressr works in two modes: Record – Use Record when you want to capture the current state of a deployment to be compared as the baseline for later deployments. A strategy file is specified that contains the specifics of what needs to be recorded. Compare/Replay – Compares the current state of a deployment with a baseline and generates a comparison report. The image below captures what is done in these two flows. The strategy file is the configuration that drives what happens during a record and a compareWith execution. An example strategy file that posts two requests and performs regression testing is specified below: The important parts of the strategy file are the different components, RequestBuilder, Recorder, and Comparator. RequestBuilder is used to specify how the request should be built in case of a POST or a PUT request. The interface for RequestBuilder accepts a Map of Strings and outputs the payload that will be sent in the request. Recorder is used to specify what parts of the response should be recorded for future comparison. Regressr injects all parts of the response to the Recorder during this time. The interface for Recorder accepts a List of HTTPResponses (most of the time this will be one) and return a RequestRecordingEntry. The RequestRecordingEntry is a holder for a value that will be recorded in Regressr’s datastore. The response code can be stored in a RequestRecordingEntry. Similarly a JSON response can be stored in a RequestRecordingEntry. You can also do some computation on the JSON and store a number (like the number of search results). The interface for Recorder looks like the below. Finally, the Comparator is used to specify the details of comparison during the compareWith mode. How do you want to compare JSON’s? What about strings? The interface for Comparator looks like the below. It accepts both the recorded RequestRecordingEntry and the current one and returns a List of CompareMessages which will be included in the comparison report. Regressr comes with out-of-the-box components that can be plugged in to provide significant value instantaneously for many common types of services. However, you can write your own components implementing these interfaces and include them into Regressr (Use ./regressr.sh -r to build everything) The comparison report is generated at the end of the compareWith lifecycle and looks like this: HATEOAS (Hypermedia As The Engine Of Application State) is where some classes of RESTful services tend to go to, especially when there are lightweight GUIs in front of them that mimic the conversation which happens to the service. Regressr also supports simple and efficient breadth first traversal of HATEOAS resources for regression testing. We support this through the use of a new component class called as Continuations. Let’s imagine you have a shopping cart service exposed at a URL such as /shoppingCart/items. When issued a GET request on this URL, if the services is modeled on HATEOAS principles the results will be similar to: As you can imagine, these new requests are non-deterministic and cannot be modeled with the help of Regressr’s configuration files, because the data may change over time. That is where Continuations come in. With continuations, the tester can specify how many new requests should be created programmatically based on the response of a previous service call. This allows the tester to write a continuation generically that creates new requests based on how many items were present in the response of the /items call. An example of continuations is here . Maven plugin that attaches to Regressr that can be used in a CI/CD build. Jenkins plugin for Regressr report. Global comparators that can be used to capture global metrics across requests and compare them. We have found Regressr to be a very useful regression testing tool for lean and low ceremony engineering teams that wish to minimize effort when it comes to regression testing of their services. There were many people involved in the design, build and testing of Regressr without which this could not have been possible. Recognizing them: Ashwanth Fernando , Alex Zhang , Robert Enyedi , Ajinkya Kale and our director Amit Srivastava. Comments and PRs are welcome with open hands at https://github.com/eBay/regressr .", "date": "2017-08-10"},
{"website": "Ebay-Engineering", "title": "Automating the Creation of Standard Change Requests at eBay", "author": ["Brian Davies"], "link": "https://tech.ebayinc.com/engineering/automating-the-creation-of-standard-change-requests-at-ebay/", "abstract": "eBay’s Network Engineering team operates a large-scale network infrastructure with a presence across the globe. Our mission is to provide a seamless experience connecting buyers and sellers wherever they may be. The network we created to support that goal is comprised of different vendors and designs that have evolved over time. Networks require care and feeding on a regular basis in order to ensure that performance targets are met. How can we make the numerous weekly changes required while minimizing the risk of an impact? One way in which we accomplish our goal is by making all change management procedures as standard and reproducible as possible. Common tasks such as line card installations, BGP changes, or the turn up of new ports are formalized into Standard Operating Procedures (SOPs). A SOP lays out all of the needed pre-checks, change steps, and post-checks for a successful change to be executed. Our SOPs are put through an engineering review process where we review and hone these steps so that the combined experience of all team members can be brought to bear on the problem. As we went through this process of creating SOPs for most of our workload, we realized that we were doing many of the same things each time. Examples include things such as backing up the configuration, verifying that the console works, and executing commands that let us verify status before and after a change is executed. All of these steps, taken together, began to sound very much like a broken record to us as we created SOP after SOP. We determined that fully automating the creation of SOP-based change requests would be a worthwhile investment of our time. Now that we had the most common tasks well-documented in SOPs, we could actually run through most steps programmatically with some work invested. Because many steps were identical (such as collecting ‘show ip ospf neighbor’) from one type of change to another, bits of code would be reusable. Some challenges, such as how to detect different vendors, code versions, or design standards, would present themselves, but the important part for us was to get started and validate that the concept was workable before expanding it. Our project outline for automating standard change requests was as follows: Preparation and Planning Design the System Develop Proof of Concept Document the System Execute Pilot Evaluation We decided to focus on a few common and relatively easier tasks with already defined SOPs. The tasks selected were: Costing links in or out for maintenance Enabling or disabling ports Decommissioning switches VLAN add/change Code upgrades (various vendors) A smaller set of tasks like this kept the scope contained to a reasonable size while still allowing the opportunity to bump into a few challenges and solve problems that might be encountered when the project is expanded to cover all of our SOPs. Dividing the work among several people allowed us to build components in parallel. All coding was stored in a Git repository to facilitate group participation. The system is built out of various building blocks. The foundation is a Python script named ‘Auto About.’ This script contains functions that lay out the high-level outline of the pending change request. It defines specific devices, interfaces, or neighbors that are involved in the pending change. It gathers the most basic information, “What is this maintenance about?”, hence the name. A few examples of functions within Auto About are ‘get_routing_instances,’ ‘collect_vlan_info,’ and ‘collect_power_supply_data.’  Feeding Auto About the arguments of a device name and the type of maintenance is all that is required to gather information. The output of Auto About is a small YAML file that contains the information collected at this step. This small file is fed into the Collector script. This component gathers information from the network devices. Collector is written in Python and is a stateless system. There is no database or long-term storage of information at this point. Collector’s output is a YAML file, much longer than Auto About’s file, with everything we need to know about the change we’re about to execute. At this point, we have all of the information we need, but reading a YAML is not very friendly for humans. We still track changes in a ticketing system, and we want to be able to review them. A separate Python script, sop.py, combined with a Jinja2 template that matches the specific type of maintenance desired, takes that long YAML file and generates a few plain text files for us. Each step or check from our original SOP is broken down in the same way within the script, and the output lists each step and sub-step in the proper order. Any device-collected information is added where it is required. Output files created include an Action plan (your “forward” steps), a Verification plan where changes are tested, and a Rollback plan (your “backwards” steps in case you need to undo your changes). These plain text, human-readable files are used to create the Change Request (CR) in our in-house ticketing system. They represent a step-by-step and line-by-line plan to execute the work. A final Python script called cr.py (‘cr’ indicating change request) handles the task of pushing the information files created by sop.py into Trace, our internally developed ticketing system that tracks changes. This saves engineer time by automating another piece of the CR puzzle for them. cr.py handles aspects of the change ticket process, such as filling in names of the people who submit or check tickets, setting the time and date of the proposed change, and requesting the creation of a new CR ticket. The proof of concept (PoC) involved creating the first versions of the components highlighted above and testing for functionality as well as interoperation of the individual pieces. A number of different people worked on this project, and the correct operation of all of the parts together was tested in the PoC phase. The PoC was a success, and we decided to press forward to a pilot phase. Documentation was created primarily within our Git repository. This was done so that everything a contributor would need was in one place and could be easily updated by anybody working on the project.  A simple ‘readme’ file uploaded into the appropriate directory in Git provided a place to put higher level information about how a piece code was supposed to function. This was done in addition to good commenting within each file, of course! Some project tracking items were also hosted on a wiki page, where they were more easily accessed by stakeholders who were not directly involved in the coding aspects. During the pilot phase, we saw a rapid expansion of the PBR program as we started onboarding more use cases and actually using this system in our live change management workflow. Exposing the output from PBR to the wider group of engineers during our pilot phase was a great way to get additional feedback on how we could collect the right information that would be valuable for the change type being executed. During the pilot period of about six months, numerous small issues with the various CR templates were corrected. Many of these issues were uncovered in our regular change management meetings as we discussed pending CR tickets. Where it was possible, we aimed to make the CR tickets have the same look and feel.  For example, standardizing the sequence numbers for prechecks, change steps, and post checks is one way we found to make the CRs more readable and faster to evaluate at change management meetings. As a result of this feedback loop, our templates and methods quickly evolved to be more comprehensive and polished. Project Broken Record took us approximately one year to complete from the initial meetings to a working product that had been successfully piloted.  We found that all of the pieces of this product require updating and fine tuning from time to time as we strive to execute the perfect change.  This type of regular time investment is a good tradeoff for eBay, because we are confident that this system has helped to avoid outages while streamlining the change process. Our change management meetings were able to be run more efficiently, because we became familiar with the standard layout of change tickets. It was easier to review and approve very standard SOP-based things vs. the previous system of a queue of tickets all written differently by different engineers. Increasing the throughput of the review process directly benefited our internal customers waiting on change work to be completed. We track all impacts to business availability and analyze what we could have done to avoid impact. One way that we sort this data is by root cause. Causes could be things such as hardware failure, vendor software bugs, change tickets gone wrong, etc. In 2017, impact time due to change tickets was very nearly zero. There were several parallel initiatives that contributed to this, but Project Broken Record was a part of that success story to be sure. Doing the same change the same way each time reduces the chance of unexpected consequences and builds our confidence in our procedures. We are happy with the progress we have made so far, but there are still a number of things we would like to improve upon. We want to become more disciplined in our coding by creating development and master branches of our code. Currently, most portions of this system are in a development type state, but are also being used daily. We are also testing systems built up from this that will perform standard maintenances completely automatically by following the SOPs using the large YAML file information. The larger goal we are pursuing here is minimal human interaction with the production network. Now that we have seen a return on our initial investment, we want to take this to a higher level of engineered solution. A ground-up rewrite of many of the pieces described above is already underway to consolidate functions and improve the way in which we gather information from the network. We are committed to this program, and we expect it to continue to evolve and grow. Our team exists to help eBay’s business to be successful. As we explore this new automation-focused landscape, we are looking for the best ways to achieve that goal through solid uptime, delivery of projects, and a great user experience for everyone on the platform. The thought processes on our team have shifted from one in which we directly care for an ever-increasing number of network devices directly to one in which we create tools that can do that for us. This new way of approaching operations at eBay is much more scalable and is where we are placing a heavy emphasis as we march toward 2018 and beyond.", "date": "2017-10-03"},
{"website": "Ebay-Engineering", "title": "eBay’s Font Loading Strategy", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/ebays-font-loading-strategy/", "abstract": "The usage of custom fonts in web pages have steadily increased in recent years. As of this writing, 68% of sites in the HTTP Archive use at least one custom font. At eBay, we have been discussing custom web fonts for typography for quite some time, but never really pursued it. The main reason was due to uncertainty in end user experiences from a performance standpoint. The usage of custom fonts in web pages have steadily increased in recent years. As of this writing, 68% of sites in the HTTP Archive use at least one custom font. At eBay, we have been discussing custom web fonts for typography for quite some time, but never really pursued it. The main reason was due to uncertainty in end user experiences from a performance standpoint. But this changed recently. Our design team made a strong case for a custom font to complement our new branding and, after multiple reviews, we all agreed it makes sense. Now it was on the engineering team to come up with an optimized implementation that not only uses the new custom font, but also tackles the performance overhead. This post gives a quick overview of the strategy we use at eBay to load custom web fonts. Our new custom font is rightly named “Market Sans” to denote affiliation with an online marketplace. As observed in the below image, it adds a subtle difference to typography, but in its entirety makes the whole page elegant and creates a unique eBay branded experience. Check out our desktop homepage , where “Market Sans” has been deployed. It is well known and documented that custom web fonts come with a cost, and that is performance. They often delay rendering of text (critical to any web page) until the font is downloaded. A recent post from Akamai gives an excellent overview of the problems associated with custom fonts. To summarize there are two major issues, and it varies among browsers: FOUT — Flash of Unstyled Text FOIT — Flash of Invisible Text As expected, the design and product teams were not happy with the compromise. Yes, custom fonts create a unique branded experience, but not at the cost of delaying the same experience. Additionally, from an e-commerce perspective, custom fonts are a good enhancement and not an absolute necessity. System fonts can still provide a compelling typography. So it was up to the engineering team to come up with an efficient font loading strategy with minimal tradeoffs. Our strategy was pretty simple — avoid FOUT and FOIT: Use the custom font if it is already available (meaning downloaded and cached), else use default system fonts. Fortunately, there is a CSS Font-Rendering proposal that adds a new @font-face descriptor named font-display . Using font-display, developers can specify how a font is displayed, based on whether and when it’s downloaded and ready to use. There are many values for font-display (checkout this quick video to understand them), and the one that maps to our strategy would be ‘font-display: optional’. Unfortunately, the adoption of font-display among browsers is not widespread, as it is relatively new. So for now, until the adoption becomes mainstream, we came up with a solution that leverages the localStorage , FontFaceSet APIs and the Font Face Observer utility (as a backup if the FontFaceSet API is not present). The below illustration gives an overview of how it works: To summarize, When users visit an eBay web page, we add a tiny inline CSS and JavaScript snippet in the response HTML <head> tag. We also include a small JavaScript snippet in the footer HTML that incorporates the font loader logic. The JavaScript in the <head> checks the localStorage if a font flag is set. If the flag is set, it immediately adds the CSS class to document root to enable the custom font. The page renders with the “Market Sans” custom font. This is the happy path. The JavaScript in the footer again checks the localStorage for a font flag. If it is NOT set, it calls the font loader function on the document load event. The font loader function loads (downloads) the custom fonts either using the built-in FontFaceSet API (if present) or through the Font Face Observer utility. The Font Face Observer is asynchronously downloaded on demand. Once the font download is complete, a font flag is set on the localStorage. One thing to note — even though the font flag is set, we do not update the current view with the custom font. It is done on the next page visit with Step 2 above kicking in. We have open sourced this module as ebay-font . It is a small utility that works along with other eBay open source modules Skin , Marko , and Lasso , as well as in standalone mode. We hope others can benefit from it. There are a couple of tradeoffs with this strategy: First time users: A new user visiting eBay for the first time will get the system font. On navigation or subsequent visits they will get our custom font. This is acceptable, as we have to start the custom font at some point, and we will start it from the second visit of a new user. Private or incognito mode: When a user starts a new browsing session in private or incognito mode, they get the system font initially. But subsequent browsing in the same session will render the custom font (Safari is an exception , but it is getting fixed ). We do not have metrics on how many users fall under this category, but this is something we have to live with. Cache eviction: In certain rare scenarios we observed that the custom font entity in the browser cache is evicted, but the localStorage entry is still present. Probably browsers clean up cache more frequently than localStorage. In these scenarios, users will experience a FOIT or FOUT based on the browser. This is more of an edge case and hence less concerning. As a team we agreed that these tradeoffs are acceptable, considering the unpredictable behavior that comes with default font loading. Custom web fonts do add value to the overall user experience, but it should not be at the cost of delaying the critical content. Each organization should have a font loading strategy based on their application needs. The new built-in CSS property ‘font-display’ makes it very easy to choose one. We should start using it right away, even if the support is minimal and even if there is already an in-house implementation. Huge thanks to my colleague Raja Ramu for partnering on this effort and help in open sourcing the module ebay-font . — Senthil Padmanabhan", "date": "2017-09-21"},
{"website": "Ebay-Engineering", "title": "Beats @ eBay - Collectbeat - A Journey where Company and Community Come Together", "author": ["Vijay Samuel"], "link": "https://tech.ebayinc.com/engineering/beats-ebay-collectbeat-a-journey-where-company-and-community-come-together/", "abstract": "In early 2016, the Monitoring Special Interest Group (SIG) ventured into solving the problem of logs and metrics shipping from Tess.io (eBay’s Kubernetes ecosystem). Kubernetes , as one may be aware of, is a container management system. Users have the flexibility to drop in Docker containers and let Kubernetes manage them. These Kubernetes clusters, or Tess clusters inside of eBay, are multi-tenanted. We have many customers running their workloads at any given time. The multi-tenanted aspect of our Tess clusters brings about some interesting problems. Some of them are: All logs and metrics need to be logically grouped (namespaced) based on customer/application type. Workloads are ephemeral and tend to move across Nodes. Additional metadata is required to search/query logs and metrics from Pods. Metrics and logs need to be exposed/collected in a cloud native fashion. Ability to self onboard logs and metrics to the centralized system. With these problems in mind, we wanted to offer a solution that allowed users to drop their Pods into Kubernetes and obtain their logs and metrics in the simplest possible way. If a user’s Pod logs to stdout / stderr , we should be able to collect the logs, append all the Pod metadata to each log line. If a user exposes metrics in a well-known HTTP endpoint, we should be able to collect it. Knowing these challenges and the goal in mind, we embarked on the problem, attempting to solve one issue at a time. Let us take logs as the first example and see how we attempted to solve it. Docker allows users to write to logs stdout / stderr , which is taken and placed in a well-known file path of: If we were to listen to /var/lib/docker/containers/*/*-log.json, we would be able to collect all the logs that are being generated by all Pods in a given Node. Configuring Filebeat to listen to that path is simple and is exactly what we did. When we collected all these logs, we needed a way for users to be able to query based on Pod name, Namespace name, etc. Kubelet also started exposing these files in a symlink of: It would be easy to write a processor on Filebeat to split the source value in the payload and extract pod, namespace, and container name. But, we realized that pod labels also carry significance in querying an entire deployment’s worth of logs and that information was not present. To solve this, we wrote our own custom Beat called Annotatebeat which can: Listen on lumberjack protocol for Beat events Look for the source field and extract the container ID Look up the Kube API server for metadata of all pods in a given node Use the container ID to append all the remaining metadata onto the event Send it to a prescribed destination As long as a user writes an application that can write to stdout / stderr , Docker would pick up the log and place it in a well-known log file. Filebeat tails the logs, sends it to Annotatebeat, which annotates the log message with pod metadata and ships the logs out. At this time, the Beats community wasn’t fully invested in Kubernetes, so we built some of these features internal to eBay. Seeing how simple it was to write logs and have them shipped, we wanted a simple experience for metrics as well. At Elastic{ON} 2016, the Elastic folks announced Metricbeat as a new offering they were coming up with. Metricbeat has the concept of “modules” where a module is a procedural mechanism by which metrics can be collected from a given application. If Metricbeat is configured to listen to localhost:3306 for module type “mysql”, the MySQL module knows that it should connect to the host:port and run a ` SHOW GLOBAL STATISTICS ` query to extract metrics and ship them out to the configured backend. This concept appealed to us because it allows “drop in” installations like MySQL, Nginx, etc. to be monitored out of the box. However, we needed users, who write their own code and deploy applications into Kubernetes, to also be able to monitor their applications. We hence came up with Prometheus/Dropwizard modules for users to expose their metrics via the above formats as HTTP endpoints, so that we could collect metrics from them and ship them. However at the time of Metricbeat creation, it was designed to be tailored for specific applications like MySQL, Apache, and Nginx and not for generic frameworks like Prometheus or Dropwizard. Hence our PR was not initially accepted by the community, and we managed the module internally. The discovery is something that is not supported by Beats out of the box. We had to come up with a mechanism that says “given a node on Kubernetes, find out all the pods that are exposing metrics and start polling for metrics.” How do we find the pods that are poll worthy? We look for following metadata found as annotations: As long as these three mandatory annotations are present, we should be able to start polling for metrics and write them into the configured backend. This discovery module uses Kubernetes’ controller mechanism to keep watching for updates within the node and start polling configured endpoints. This discovery module resided in a custom Beat that we lovingly call Collectbeat. To sum up, we used Collectbeat for collecting metrics from pods and Filebeat for collecting logs. Both sent their data to Annotatebeat, which appended pod metadata and shipped it to the configured backend. We ran this setup internally for about a year on version 1.x. Then Beats came out with 5.x. When we were ready to upgrade to Beats 5.x, most of the interfaces had changed, and all of our custom code had to be upgraded to the newer interfaces. By this time, the Beats community had evolved Metricbeat to support generic collectors like Prometheus and several other changes for which we had written changes in our internal fork were available upstream. The effort to upgrade to 5.x would be substantial. We had two options in front of us. One was to keep going down this path of managing our internal fork and invest a month every major release to pull in all the new features and make necessary changes to our internally owned features. The second option was to open source anything that was generic enough to be accepted by community. On taking stock of all the features that we had written, 90% of them were features applicable to any Kubernetes cluster. The remaining 10% was required to ship data to our custom backend. Hence, we took a decision to upstream that 90% so that we don’t have to manage it any longer. In Elastic{ON} 2016 we met with the Beats community and came to an agreement to open source as much as we can with regards to the Kubernetes use-case, since we already have expertise monitoring Kubernetes internal to eBay, in return for faster PR reviews. The first thing that we decided to get rid of internally was Annotatebeat, which did the metadata enrichment. Today in libbeat there is a processor called add_kubernetes_metadata , which was a result of that decision. We took all the logic present in Annotatebeat and converted it into a processor with the help of Carlos Pérez-Aradros, a member of the Beats community. We also took our internal Prometheus implementation and used it as a reference to update the community-available version to cover a few missing use cases. Dropwizard , Kubernetes Metricbeat modules, were something we used internally that we also open sourced. Eventually we got to a point where we could run both Filebeat and Metricbeat as available upstream without any necessary changes. With go1.8 out, there was also support for plugins and we offloaded all our custom code internal to eBay. It is managed independent of stock Beats. We realized the hard way that it is impossible to keep up with the rapid pace of an open source community if we have custom code residing in our internal fork. Not having a custom fork internally has helped us to be on the most recent version of Beats all the time and has reduced the burden of pulling in new changes. It is always easier to make progress when we work with the community on features that not only benefit us today, but may also benefit someone else tomorrow. More thoughts and ideas on the code can always make it better. A good working relationship with the Beats community has helped us not only with code management, but also with features that were required internally that ended up getting built by the community. Today, eBay contributes the most amount of code outside of Elastic itself to the Beats product. This has not only benefited the product, but also eBay as well. With the combined effort of eBay and the Elastic, Beats will have native Kubernetes support in 6.0 . Removing all of our custom code improved our agility to think of newer use cases. We wanted to increase coverage for the number of applications from which metrics can be collected. We realized that writing Metricbeat modules for every application is an impossible task and that going after protocols is a more scalable option. One protocols that has tremendous coverage is the plain text protocol understood by Graphite. Tools like CollectD and StatsD can write to destinations that understand the Graphite protocol . We then implemented “ Graphite server ” as a Metricbeat module and contributed it back to Beats. This module inside of Collectbeat’s Kubernetes discovery helped us support use cases where customers can annotate their Pods with a parsing rule, and Collectbeat would receive metrics and parse them to split the metric name and tags before ingesting them to the desired backend. Another similar protocol that we went after was vanilla HTTP , where users can send metrics as JSON payloads to Metricbeat, and it would be shipped to the desired backend. Being able to discover metrics inside of a Kubernetes environment is a big win in itself. The benefits were quite huge, and we saw the need to do the same for logs as well to support two use-cases: Being able to stitch stack trace-like log patterns Being able to read logs that are not being written into stdout Because Kubernetes clusters inside of eBay are multi-tenanted, it becomes impossible to configure a single multiline pattern on Filebeat for all Pods inside of the cluster. We applied our learnings from metrics to log collection and decided to expose annotations that users can use to define multi-line patterns based on how Filebeat expects multiline to be configured . A user can, at a container level, configure multiline via annotations, and Collectbeat ensures that the required Filebeat prospectors are spun up to stitch stack traces. A long standing problem that we have seen in our Kubernetes clusters is that, since we heavily rely on docker’s JSON log driver, performance is always a concern. Letting Filebeat decode each log line as a JSON payload is quite expensive. Also, there are a lot of use cases where a container may expose one of its many log files via stdout, but all others are written in specific file in the container. One such example is Apache Tomcat, where catalina.out’s logs are written into stdout, whereas access logs are not. We wanted to solve both these problems with an unconventional solution. Collectbeat was rewritten to accept log paths in the Pod’s annotations, and based on what is the underlying Docker file system, Collectbeat would spin up prospectors by appending the container’s filesystem path to the file path. This would let us tail log files present inside of the container, and helps us to not rely on JSON log file processing. We can also collect log files from different files written by a container. Collectbeat has become our defacto agent that sits on every node through DaemonSets in our Kubernetes clusters to collect logs and metrics. Collectbeat runs in both Filebeat mode and Metricbeat mode to be able to tail log files and collect metrics respectively. This is what our Node looks like: What are the features that Collectbeat has today? We are able to: Collect metrics from any Pod that exposes metrics that abide to all supported Metricbeat modules Collect logs written to stdout or files inside the Docker container Append Pod metadata on every log and metric collected Allow Pods to push metrics through Graphite protocol and parse them uniquely Stitch stack traces for application logs Today we run Collectbeat on over 1000 nodes shipping more than 3TB of logs and several billion data points per day. Our end goal is to put Collectbeat on every host in eBay and be able to collect logs and metrics from any application that is being deployed. Are we there yet? No, but we are slowly, but surely, getting there. There are still several more features that we have yet to crack, like being able to give QoS for all Pods so that all Pods are treated equally when shipping logs and metrics. We also want to be able to provide quotas and throttle workloads when applicable. We have greatly benefited from Collectbeat, and with great excitement we are happy to announce the open sourcing of Collectbeat . Putting our code out in the open will help us get feedback from the community and improve our implementation at the same time help others who are trying to solve the same problem as we are. So, go get github.com/ebay/collectbeat and let us know your feedback. A big shout out to all the folks in eBay who made this a reality: Vijay Samuel Ruchir Shah Ashwin Raveendran Amber Vaidya Saurabh Mehta Andy Santosa Rami El-Charif Suneet Nandwani Also, a big shout out to the Elastic folks from the Beats community who have helped us along the way: Monica Sarbu Tudor Golubenco Nicolas Ruflin Steffen Siering Carlos Pérez-Aradros Andrew Kroh Brandon Mensing Mike Hayes", "date": "2017-10-04"},
{"website": "Ebay-Engineering", "title": "Search Faster with eBay’s New Grouped Listings View", "author": ["Jon Glick"], "link": "https://tech.ebayinc.com/product/search-faster-with-ebays-new-grouped-listings-view/", "abstract": "Our new Grouped Listings feature makes it easier to find the product you want with just the click of a button. eBay has an unrivaled selection of more than 1.1 billion listings, but sometimes all that selection can be a lot of work to sort through. When you are looking for microwave ovens on eBay, there are almost 10,000 different listings. And as you scroll through the results, you’ll see listings for the same product again and again, just from different sellers. But wouldn’t it be nice to see the product you’re looking for just once in the search results, grouping together all seller listings so you could easily find the best deal? Plus, if that particular microwave wasn’t what you wanted, you wouldn’t want to see it repeatedly in the results. That is where our new Grouped Listings view comes in. For many searches, you’ll now see a [Group Similar Listings] button above the search results that enables you to condense like offerings in your search results so it’s easier to find what you really want. When you group the listings for that microwave oven search, those 10,000 listings condense over 10X into the 953 unique products they represent. We’ve also made the functionality “smart”-- the button will not appear in cases where most of the listings you’d see are unique like antique teapots . We’re using our vast repository of structured data across a wide range of products to group these listings, taking into account everything from unique identifiers like UPC codes and model numbers to product features obtained through artificial intelligence that understands natural language. Our search relevancy algorithms then leverage machine learning to show you the best offer for that product directly in the search results. Our new Grouped Listings feature is just another step toward evolving our search experience from focusing on individual listings to being structured around products. In the coming months, you’ll continue to see us enhancing the experience and expanding our investments in this area.  Grouped listings is currently live on desktop in the U.S., U.K., Germany and Australia, and will be available on mobile soon. Jon Glick is Vice President of Search Product at eBay, where his focus is on streamlining the search experience so it’s easier for people to find what they are looking for and get the best deals.", "date": "2017-10-09"},
{"website": "Ebay-Engineering", "title": "Is It the Right Product for You? eBay’s Newest Feature Lets You Ask", "author": ["Brian Livingston"], "link": "https://tech.ebayinc.com/product/is-it-the-right-product-for-you-ebays-newest-feature-lets-you-ask/", "abstract": "Learn how the new Questions & Answers feature lets you leverage the power of the eBay community to buy what you really want. We’ve all been there—shopping for that certain something online, reading product descriptions, reviews, clicking through images of the item—and we still can’t tell if this is the right product for us. Maybe it’s running shoes and you want to know how many uses you can get out of them. Maybe it’s a vacuum and you want to know how quiet is quiet? Or maybe you want to know if that toy is appropriate for your 6-year-old. These are the details we all search for but you can’t really find in product descriptions and images. Out of the 171 million buyers in the eBay community, someone knows the answer to your question—all you have to do is ask. Starting today, we added Questions & Answers to complement Reviews and help you feel good about your purchase because sometimes all you need is a little more information to know that the product you are looking at is the right one for you. You’ve always been able to ask the seller questions, but now you can get even more details from someone that has actually purchased the item within the eBay Community. This new Q&A section, found on the product pages, lets you ask your questions and even find a question that someone else has already asked so you can feel even more confident in what you are buying. It’s a way to pay it forward and help out a fellow buyer. While any eBay user can answer the question and help, we are also using machine learning to find experts in our community who have had relevant experiences with the product you have questions about. The eBay community has contributed millions of reviews and guides and they are already beginning to help answer questions from other members. As we receive more questions, artificial intelligence will power this feature and get smarter, all while our data scientists continue to figure out not only who might be best to answer certain questions based on their expertise on given products but also to understand what questions might be the most helpful for future buyers. Q&As are currently live on mobile web and desktop in the U.S. and will be coming to our app soon. So, next time you have a question about the product you’re looking at, ask away!  eBay’s active community of buyers and sellers are ready to help. BIO Brian Livingston is a Senior Product Manager at eBay where he focuses on increasing buyer confidence in purchases by leveraging the power of content from the eBay community.", "date": "2017-08-29"},
{"website": "Ebay-Engineering", "title": "Tiered Test Automation", "author": ["Ann Del Rio"], "link": "https://tech.ebayinc.com/engineering/tiered-test-automation/", "abstract": "As application code has evolved from monolithic to client-server, and to now micro-services, test automation has to evolve as well. Test automation relies very heavily on user interface and web services endpoint layers. That makes the test pass rates low. These layers are flaky due to factors such as data dependencies and inconsistencies, environmental stability, slow execution, and expensive maintenance of the test code. In a Tiered-Level Test Automation approach, the test code is written to follow a test pyramid popularized by Martin Fowler, where there is a minimal amount of focus given to the user interface and user actions tests. As the test code is written upstream, more scenarios and test permutations are written on the other tier levels of the automation. Black box testing tests the application in the eyes of the users. For manual testing, the test interacts with the page and verifies that the functionality is working as expected by visually checking the UI components and performing actions on them. Customer-Facing Public Website White box testing looks at the application code that is subject to test. Unit testing looks at the smallest testable parts of an application. Test automation should be heavy on unit tests. We implement unit testing by creating tests for individual methods. The technology stack used in our project is comprised of Java , JUnit , Mockito , JSON , and ObjectMapper . Our unit tests mock the dependencies, and pass them to the application method in question to run them through the business logic. Then they compare and assert whether the actual response is the same as the expected response that is stored in a JSON file. The following sample code is a unit test. The strategy and rationale for white box testing is to find out more of the issues and problems upstream. Integration tests can be written to verify the contracts of web services independently from the whole system. For example, if a configuration system is down or data is wiped out, does that mean the application code cannot be signed-off because the tests are failing? One test implementation is mocking the values from the configuration system, and then using them as the dependencies to run through the integrated business logic and assert whether the actual response is the same as the expected. The technologies used in the mock tests include JUnit Parameterized Tests , MockIto , PowerMockIto , JSON , and GSon . How are the unit tests different from the integration tests? The unit tests run through the individual methods, while the integration tests can call those methods all together. The services layer tests are run against the RESTful endpoints with only a few of the positive scenarios and invalid requests. Then the responses are validated with the expected data values, error codes, and messages. The technologies used are Java , Gson , and Jersey API. Use JSON to store the request and response, and parse and pass them as a data provider into the test methods. User Interface test automation is minimal and focuses on actions on the page, for example, editing and saving the page after making the customization changes. The technologies used are Java, Selenium WebDriver, HTTPClient, JSON, and Gson. The UI tests iterate through a collected list of web elements and actions, rather than individually taking each locator for each row and column. The save flow tests also integrate to the service response to verify against the source of truth. We are validating the data that is passed through the UI against the service response and comparing them to determine whether they both are equal. This strategy helps to validate more data on the fly rather than hard-coding the expected output. The functional flow is also verified through the integration with services. For example, the edit and save flows are tested by getting the service response, using it as the source of truth to validate the data that is saved, and verifying whether they are equal. Once again, this approach helps to validate more data on the fly rather than hard-coding the expected output in the test class or in some properties file. A snippet of the rows and columns of the Internal Tool’s UI that is the subject of the test Discovering issues upstream is more efficient and less expensive than finding them when the product is already developed and in production. The Tiered-Level Test Automation approach encourages developers to think and sets an example for where and when it is best to test the product. Implementing the tiered test automation was indeed a collaborative effort. Thanks to my colleagues, Kalyana Gundamaraju , Srilatha Pedarla , Krishna Abothu, and Manoj Chandramohan for their contributions to this test automation design. — Ann Del Rio", "date": "2017-10-10"},
{"website": "Ebay-Engineering", "title": "eBay Authenticate Makes Buying and Selling Luxury Handbags Easy", "author": ["Vipul Bahety"], "link": "https://tech.ebayinc.com/product/ebay-authenticate-makes-buying-and-selling-luxury-handbags-easy/", "abstract": "With our new program, sellers can authenticate their luxury bags, allowing shoppers to buy with confidence. So you want to sell that designer handbag that you aren’t using anymore? With our new authentication program , selling your luxury handbag is simple. Just start listing your eligible luxury handbag, and select “Sell with eBay Authenticate.” You can then ship the item for free to a team of industry experts. Now, sit back and let that team do the rest for you. Behind the scenes, experts authenticate your item through a multi-step process. They take professional photographs, determine the price of the item and then list that designer bag. When a buyer comes across that authenticated handbag, they can shop with confidence. We indicate on the listing page that the item’s authenticity is verified by industry experts. Plus, we’re guaranteeing that any bag you buy through the eBay Authenticate program is the real deal, so much so that if it isn’t, we’ll give you double your money back. And when the buyer receives the bag, it’s packaged in an elegant black box with a card verifying authenticity and item details. As a seller, when you enter the title of the handbag to list it, we leverage machine learning to determine initial eligibility. If the bag is eligible for Authenticate, you will see an option to create an order and send the handbag for authentication. Then, industry experts do the rest. eBay sellers can now use the service to list luxury handbags and wallets valued at $500 or more, and receive 80 percent of the final sales price when selling luxury handbags from 12 high-end brands, including Balenciaga, Burberry, Céline, Chanel, Christian Dior, Fendi, Goyard, Gucci, Hermès, Louis Vuitton, Prada, and Valentino. Currently, eBay Authenticate is live in the U.S. for consumer sellers listing luxury handbags within the twelve eligible brands. We are looking to expand to other items and our business sellers soon. BIO Vipul Bahety is a Senior Product Manager at eBay, where he focuses on enhancing consumer selling services to create a better experience for our customers. Neither eBay nor the industry experts are affiliated with, or endorsed by, any of the eligible brands. Services are provided independent of the eligible brands.", "date": "2017-10-24"},
{"website": "Ebay-Engineering", "title": "Significance Testing for Ratio Metrics in Experiments", "author": ["Liyun Chen"], "link": "https://tech.ebayinc.com/engineering/significance-testing-for-ratio-metrics-in-experiments/", "abstract": "Written by the Experimentation Analytics Team (with Experimentation Platform Product Team) We recently improved the ASP (Average Selling Price) metric calculation on our experimentation platform. As of Oct 31, 2016, we are reporting the ASP shift between test and control for all experiments. However, one question may come to your mind — how do we report it? It’s actually a question on how to report any ratio metrics. In this article I will explain the story and then describe how we disentangled the problem. ASP stands for average selling price, which seems very straightforward — the average of an item’s selling price on the eBay website. It’s not the listing price that you can browse directly on the eBay website. ASP only reflects the item price trend from completed transactions. When it comes to calculation, we simply compute ASP as the ratio of two other important metrics: Gross Merchandise Volume Bought (GMB) and Bought Item (BI) (or GMV and Sold Item, from the seller side). In any experiment, when we want to measure ASP’s lift between test and control groups, it’s simply a comparison of the test group’s ASP and the control group’s ASP. It’s exactly the same as all other metrics we report. However, when we calculate the ratio metric’s p-value , many questions come up. Non-buyers: How do we define ASP for GUIDs or users without any purchase? They have 0 GMB and 0 Bought Item count. Hence, what’s 0/0? It’s undefined! Selection bias: If we discard non-buyers and only compute ASP among buyers, then are we incorporating a selection bias in our measurement? We all know that a treatment may already drive a lift in number of buyers. Aggregation: At which level should we aggregate data to derive the standard deviation? For GMB, we usually aggregate to the user level and then calculate standard deviation. However, how can we aggregate the user-level ASP? Outliers: How do we do outlier capping? Can we just apply the convenient 99.9% capping that what we do for GMB? In fact, all four of these questions are common challenges for any ratio metrics. For instance, they apply as well to all conversion rates, exit rates, and defect rates. Therefore, we need to solve these four questions to develop a generic method to conduct significance tests for any ratio metrics. The answer to the first question is closely tied to the denominator of any ratio metrics. In the ASP case, ASP = GMB/BI, so ASP exists conditional on BI. Clearly, 0/0 does not make any mathematical sense. Therefore, we can only report ASP conditional on transactions. However, if we condition on transactions, we encounter possible selection bias between test and control transactions. Here, although we are not utilizing the advantage of randomization directly (that is, not a reduced-form estimate as in econometrics), we can still safely do so if we impose one assumption: we assume for a specific treatment, namely, BI and ASP lifts are independent. Therefore, we can decompose ASP’s lift from BI’s lift, and as long as we report BI’s lift; and when BI’s lift is small, ASP’s lift can be approximated by the difference between GMB lift and BI lift. (A more precise calculation requires a structural model and instrumental variables; we are not doing that for now). In conclusion, our decision is to report ASP conditional on transactions. For other ratio metrics, if the denominator is GUID or user count, then it’s just a natural unconditional ratio metric, and there will be no selection bias anyway. The reason for data aggregation is that we think a given user’s past behavior will be correlated with their future behavior. For example, we make recommendations on eBay website based on user’s past purchase patterns. Thus, there is a time-dependency (or auto-correlation), and we aggregate transaction-level data to the user level to get rid of such a correlation. So the answer to question #3 is still user-level aggregation. At the user level, we aggregate both GMB and BI. For the calculation of standard deviation, we apply the delta method so that ASP’s standard deviation will be a function of user-level GMB and BI. Fortunately, we report GMB and BI by default, so we have collected the raw materials already. For other ratio metrics, we need to aggregate both denominator and nominator to the user level. We always want to control any outlier’s impact to reduce standard deviation. Capping always depends on the metric and parameter we want to estimate. Do we want to control for users with extreme purchases, luxurious items, or bunches of cheap item purchases? Different concerns will lead to different capping choices, and we can test them all with the data. Alternatively, we can estimate a different parameter that is less impacted by outliers, or we can use statistical tests that rely less on the mean (say, quantile tests or rank-based methods). We would like to offer more test results in the future to help people understand how ASP’s distribution is affected in each experiment. Here are a few options. P0: Everything uncapped. P1: GMB and BI capped at GUID level. P2: Item price capped at item level, keep quantity uncapped. P3: Cap item price at item level, then cap quantity at item-GUID level. P4: Treat ASP as a weighted average, cap item price at item level, then cap BI at GUID level. Rank-based test: Wilcoxon rank-sum test to test difference in distribution. It’s a metric-specific choice, and we hope our options can inspire people. In summary, we are calculating ASP in this way: Define ASP conditional on transactions. Aggregate to the user level, and use delta method to calculate its standard deviation. Use user-level capping, the same as we do for GMB. It’s not perfect, but it requires less development time. We will keep monitoring the difference and make future enhancements if necessary. Typically, a ratio metric brings more challenges to significance testing. Here we illustrate ASP as an example to highlight major concerns and propose some solutions. We will keep monitoring ASP’s performance on the Experimentation Platform and make improvements over time.", "date": "2016-11-18"},
{"website": "Ebay-Engineering", "title": "Five Tools to Build Your Basic Machine Translation Toolkit", "author": ["Juan Rowda"], "link": "https://tech.ebayinc.com/engineering/five-tools-to-build-your-basic-machine-translation-toolkit/", "abstract": "If you are a linguist working with Machine Translation (MT), your job will be a lot easier if you have the right tools at hand. Having a strong toolkit, and knowing how to use it, will save you loads of time and headaches. It will help you work in an efficient manner, as well. As a Machine Translation Language Specialist at eBay, I use these tools on a regular basis at work, and that is why I feel comfortable recommending them. At eBay, we use MT to translate search queries and listing titles and descriptions into several languages. If you want to learn more, I encourage you to read this article . The text editor that comes with your laptop won’t cut it, trust me. You need an advanced text editor that can provide at least these capabilities: Deal with different encodings ( UTF , ANSI, etc.) Open big files, sometimes with unusual formats or extensions Do global search and replace operations with regular-expression support Highlight syntax (display different programming, scripting, or markup languages — such as XML and HTML — with color codes) Have multiple files open at the same time (that is, support tabs) This is a list of my favorite text editors, but there are a lot of good ones out there. Notepad++ is my editor of choice. You can open virtually any file with it, it’s really fast, and it will keep your files in the editor even if you quit it. You can easily search and replace in a file or in all open files, using regular expressions or just extended characters (control characters like \\n or \\t ). It’s really easy to convert to and from different encodings and to save all opened files at once. You can also download different plug-ins, like spellcheckers, comparators, etc. It’s free, and you can download it from the Notepad++ web site . Sublime Text is another amazing editor, and it’s a developers’ favorite. Personally, I find it great to write scripts. You can do many cool things with it, like using multiple selections to change several instances of a word at once, split a selection of words into different lines, etc. It supports regular expressions and tabs as well. It has a distraction-free mode if you really need to focus. It comes with a free trial period, and you can get it at the Sublime Text web site . Syntax highlighting, document comparison, regular expressions, handling of huge files, encoding conversion: Emeditor is complete. My favorite feature, however, is the scriptable macros. This means that you can create, record, and run macros within EmEditor — you can use these macros to automate repetitive tasks, like making changes in several files or saving them with different extensions. You can download it from the EmEditor web site . Quality Assurance tools assist you in automatically finding different types of errors in translated content. They all basically work in a similar way: You load files with your translated content (source + target). You optionally load reference content, like glossaries, translation memories, previously translated files, or blacklists. The tool checks your content and provides a report listing potential errors. You can find lots of errors using a QA tool: Terminology: where term A in the source is not translated as B in the target Blacklisted terms: terms you don’t want to see in the target Inconsistencies: same source segment with different translations Differences in numbers: when the source and target numbers don’t match Capitalization Punctuation: missing or extra periods, duplicate commas, and so on Patterns: certain user-defined patterns of words, numbers and signs (which may contain regular expressions to make them more flexible) expected to occur in a file. Grammar and spelling errors Duplicate words, tripled letters, and more Some QA tools you should try are Xbench and Checkmate . Xbench allows you to run the following QA Checks: Untranslated segments Segments with the same source text and different target text Segments with the same target text and different source text Find segments whose target text matches the source text (potentially untranslated text) Tag mismatches Number mismatches Double blanks Repeated words Terminology mismatches against a list of key terms Spell-check translations. Some linguists like to add all their reference materials into Xbench, like translation memories, glossaries, termbases, and other reference files, as the tool allows you to find a term while working on any other running application with just a shortcut. Xbench also has an Internet Search tab to run searches on Google. The list is pretty limited, but there are ways to expand it. Checkmate is the QA Tool part of the Okapi Framework, which is an open-source suite of applications to support the localization process. That means that the framework includes some other tools, but Checkmate is the one you want to perform quality checks on your files. It supports many bilingual file formats, like XLIFF, TTX, and TMX. Some of the checks you can run are repeated words, corrupted characters, patterns, inline code differences, significant differences in length between source and target, missing translations, spaces, etc. The patterns section is especially interesting; I will come back to it in the future. Checkmate also produces comprehensive error reports in different formats. It can also be integrated with LanguageTool , an open-source spelling and grammar checker. Why do you need a comparison tool? Comparing files is a very practical way to see in detail what changes were introduced, for example, which words were replaced, which segments contain changes, or whether there is any content added or missing. Comparing different versions of a file (for example, before and after post-editing) is essential for processes that involve multiple people or steps. Beyond Compare is, by far, the best and most complete comparison tool, in my opinion. With Beyond Compare, you can compare entire folders, too. If you work with many files, comparing two folders is an effective way to determine if you are missing any files or if a file does not belong in a folder. You can also see if the contents of the files are different or not. As defined by its website, AntConc is a “freeware corpus analysis toolkit for concordancing and text analysis.” This is, in my opinion, one of the most helpful tools you can find out there when you want to analyze your content, regardless the language. AntConc will let you easily find n‑grams and sort them by the number of occurrences. This is extremely important when you want to find patterns in your content. Remember: with MT, you want to fix patterns, not specific occurrences of errors. It may sound obvious, but finding and fixing patterns is a more efficient way to get rid of an issue than trying to fix each particular instance of an error. AntConc will also create a list of each word in your content, preceded by the number of hits. This is extremely helpful for your terminology work. There are so many things you can use this tool for, that it deserves its own article. In most cases, using a translation memory (TM) is a good idea. If you are not familiar with CAT tools (computer-assisted translation), they provide a translation environment that combines an editor, a translation memory, and a terminology database. The key part here is the TM, which is essentially a database that stores translations in the form of translation units (that is, a source segment plus a target segment), and if you come across the same or a similar source segment, the TM will “remember” the translation you previously stored there. CAT Tools make a great post-editing environment. Most modern tools can be connected to different machine translation systems, so you get suggestions both from a TM and from an MT system. And you can use the TM to save your post-edited segments and reuse them in the future. If you have to use glossaries or term bases, CAT tools are ideal, as they can also display terminology suggestions. When post-editing with a CAT tool, there are usually two approaches: you can get MT matches from a TM or a connected MT system (assuming, of course, that the matches are added to it previously), or you can work on bilingual, pre-translated files and store only post-edited segments in your TM. If you have never tried it, I totally recommend Matecat . It’s a free, open-source, web-based CAT tool, with a nice and simple editor that is easy to use. You don’t have to install a single file. They claim you will always get up to 20% more matches than with any other CAT tool. Considering that some tools out there cost around 800 dollars, what Matecat has to offer for free can’t be ignored. It can process 50+ file types; you can get statistics on your files (like word counts or even how much time you spent on each segment), split them, save them on the cloud, and download your work. Even if you never used a CAT tool before, you will feel comfortable post-editing in Matecat in just a few minutes. Another interesting free, open-source option is OmegaT . It’s not as user-friendly as Matecat, so you will need some time to get used to it, even if you are an experienced TM user. It has pretty much all the same main features commercial CAT tools have, like fuzzy matching, propagation, and support for around 40 different file formats, and it boasts an interface with Google Translate. If you never used it, you should give it a try. If you are looking into investing some money and getting a commercial tool, my personal favorite is Memoq . It has tons of cool features and overall is a solid translation environment. It probably deserves a more detailed review, but that is outside of the scope of this post. If you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2016-12-08"},
{"website": "Ebay-Engineering", "title": "Ready-to-use Virtual-machine Pool Store via warm-cache", "author": ["Sudeep Kumar"], "link": "https://tech.ebayinc.com/engineering/ready-to-use-virtual-machine-pool-store-via-warm-cache/", "abstract": "Conventional on-demand Virtual Machine (VM) provisioning methods on a cloud platform can be time-consuming and error-prone, especially when we need to provision VMs in large numbers quickly. The following list captures different issues that we often encounter while trying to provision a new VM instance on the fly: Insufficient availability of compute resources due to capacity constraints Desire to place VMs on different fault domains to avoid concentration of VM instances on the same rack Transient failures or delays in the service provider platform result in failure or an increase in time to provision a VM instance. Elasticsearch-as-a-service, or Pronto, is a cloud-based platform that provides distributed, easy to scale, and fully managed Elasticsearch clusters. This platform uses the OpenStack-based Nova module to get different compute resources (VMs). Nova is designed to power massively scalable, on-demand, self-service access to compute resources. The Pronto platform is available across multiple data centers with a large number of managed VMs. Typically, the time taken for provisioning a complete Elasticsearch cluster via Nova APIs is directly proportional to the largest time taken by the member node to be in a “ready to use” state (active state). Typically, provisioning a single node could take up to three minutes (95th Percentile) but can be up to 15 minutes in some cases. Therefore, in a fairly large size cluster, our platform would take a long time for complete provisioning. This greatly impacts our turnaround time to remediate production issues. In addition to provisioning time, it is time-consuming to validate new created VMs. There are many critical applications that leverage our platform for their search use cases. Therefore, as a platform provider, we need high availability to ensure that in a case of catastrophic cluster event (such as a node or an infrastructure failure), We can quickly flex up our clusters in seconds. Node failures are also quite common in a cloud-centric world, and applications need to ensure that there is sufficient resiliency built in. To avoid over-provisioning nodes, remediation actions such as flex-up (adding a new node) should ideally be done in seconds for high availability. New hardware capacity is acquired as racks from external vendors. Each rack typically has two independent fault domains with minimal resource overlap (For example, different networks), and sometimes they don’t share a common power source. Each fault domain hosts many hypervisors, which are virtual machine managers. Standalone VMs are provisioned on such hypervisors. VMs can be of different sizes (tiny, medium, large, and so on). VMs on the same hypervisor can compete for disk and network I/O resources, and therefore can lead to noisy neighbor issues. Nova provides ways to be fault domain- and hypervisor- aware. However, it is still difficult to successfully achieve guaranteed rack isolation during run-time provisioning of VM instances. For example, once we start provisioning VMs, there is no guarantee that we will successfully create VM instances on different racks. This depends entirely on the underlying available hardware at that point in time. Rack isolation is important to ensure high availability of Elasticsearch master nodes (cluster brain). Every master node in an Elasticsearch cluster must reside on a different rack for fault tolerance. (If a rack fails, at least some other master node in an another rack can take up active master role). Additionally, all data nodes of a given cluster must reside on different hypervisors for logical isolation. Our APIs must fail immediately when we cannot get VMs on different racks or hypervisors. A subsequent retry will not necessarily solve this problem. Solution The warm-cache module intends to solve these issues by creating a cache pool of VM instances well ahead of actual provisioning needs. Many pre-baked VMs are created and loaded in a cache pool. These ready-to-use VMs cater to the cluster-provisioning needs of the Pronto platform. The cache is continuously built, and it can be continuously monitored via alerts and user-interface (UI) dashboards. Nodes are periodically polled for health status, and unhealthy nodes are auto-purged from the active cache. At any point, interfaces on warm-cache can help tune or influence future VM instance preparation. The warm-cache module leverages open source technologies like Consul, Elasticsearch, Kibana, Nova, and MongoDB for realizing its functionality. Consul is an open-source distributed service discovery tool and key value store. Consul is completely distributed, highly available, and scalable to thousands of nodes and services across multiple data centers. Consul also provides distributed locking mechanisms with support for TTL (Time-to-live). We use Consul as key-value (KV) store for these functions: Configuring VM build rules Storing VM flavor configuration metadata Leader election (via distributed locks) Persisting VM-provisioned information The following snapshot shows a representative warm-cache KV store in Consul. The following screenshot shows a sample Consul’s web UI. Elasticsearch “is a highly scalable open-source full-text search and analytics engine. It allows you to store, search, and analyze big volumes of data quickly and in near real time. It is generally used as the underlying engine/technology that powers applications that have complex search features and requirements.” Apart from provisioning and managing Elasticsearch clusters for our customers, we ourselves use Elasticsearch clusters for our platform monitoring needs. This is a good way to validate our own platform offering. Elasticsearch backend is used for warm-cache module monitoring. Kibana is “built on the power of Elasticsearch analytics capabilities to analyze your data intelligently, perform mathematical transformations, and slice and dice your data as you see fit.” We use Kibana to depict the entire warm-cache build history stored in Elasticsearch. This build history is rendered on Kibana dashboard with various views. The build history contains information such as how many instances were created and when were they created, how many errors had occurred, how much time was taken for provisioning, how many different Racks are available, and VM instance density on racks/hypervisors. warm-cache module can additionally send email notifications whenever the cache is built, updated, or affected by an error. We use the Kibana dashboard to check active and ready-to-use VM instances of different flavors in a particular datacenter, as shown in the following figure. MongoDB “is an open-source, document database designed for ease of development and scaling.” warm-cache uses this technology to store information about flavor details. Flavor corresponds to the actual VM-underlying hardware used. (They can be tiny, large, xlarge, etc.). Flavor details consist of sensitive information, such as image-id , flavour-id , which are required for actual Nova compute calls. warm-cache uses a Mongo service abstraction layer (MongoSvc) to interact with the backend MongoDB in a secure and protected manner. The exposed APIs on MongoSvc are authenticated and authorized via Keystone integration. CMS (Configuration Management System) is a high-performance, metadata-driven persistence and query service for configuration data with support for RESTful API and client libraries (Java and Python). This system is internal to eBay, and it is used by warm-cache to get hardware information of various compute nodes (including rack and hypervisor info). System Design The warm-cache module is built as a pluggable library that can be integrated or bundled into any long running service or daemon process. On successful library initialization, a warm-cache instance handle is created. Optionally, a warm-cache instance can enroll for leader election participation. Leader instances are responsible for preparation of VM cache pools for different flavors. warm-cache will consist of all VM pools for every flavor across the different available data centers. The following figure shows the system dependencies of warm-cache. The warm-cache module is expected to bring down VM instance preparation time to few seconds. It should also remedy a lot of exceptions and errors that occur while VM instances get ready to a usable state, because these errors are handled well in advance of actual provisioning needs. Typical errors that are encountered today are nodes not available in Foreman due to sync issues and waiting for VM instances to get to the active state. The figure below depicts the internal state diagram of the warm-cache service. This state flow is triggered on every warm-cache service deployed. Leader election is triggered at every 15-minute boundary interval (which is configurable). This election is done via Consul locks with an associated TTL (Time-to-live). After a leader instance is elected, that particular instance holds the leader lock and reads metadata from Consul for each Availability Zone (AZ, equivalent to a data center). These details include information such as how many minimum instances of each flavor are to be maintained by warm-cache. Leader instance spawns parallel tasks for each AZ and starts preparing the warm cache based on predefined rules. Preparation of a VM instance is marked as complete when the VM instance moves to an active state (for example, as directed by an open-stack Nova API response). All successfully created VM instances are persisted on an updated warm-cache list maintained on Consul. The leader instance releases the leader lock on the complete execution of its VM’s build rules and waits for next leader election cycle. The configuration of each specific flavor (for example, g2-highmem-16-slc07 ) is persisted in Consul as build rules for that particular flavor. The following figure shows an example. In above sample rule, the max_instance_per_cycle attribute indicates how many instances are to be created for this flavor in one leadership cycle. min_fault_domain is used for the Nova API to ensure that at least two nodes in a leader cycle go to different fault domains. reserve_cap specifies the number of instances that will be blocked and unavailable via warm-cache. user_data is the base64-encoded Bash script that a VM instance executes on first start-up. total_instances keeps track on total number of instances that need to be created for a particular flavor. An optional group_hint can be provided that ensures that no two instances with the same group-id are configured on the same hypervisor. For every VM instance added to warm-cache, following information will be metadata is persisted on Consul: Instance Name Hypervisor ID Rack ID Server ID Group name (OS scheduler hint used) Created time Since there are multiple instances of the warm-cache service deployed, only of them is elected leader to prepare the warm-cache during a time interval. This is necessary to avoid any conflicts among multiple warm-cache instances. Consul is again used for leader election. Each warm-cache service instance registers itself as a warm-cache service on Consul. This information is used to track available warm cache instances. The registration has a TTL (Time-To-Live) value (one hour) associated with it. Any deployed warm cache service is expected to re-register itself with the warm-cache service within the configured TTL value (one hour). Each of the registered warm-cache services on Consul to elect itself as a leader by making an attempt to acquire the leader lock on Consul. Once a warm-cache service acquires a lock, it acts as a leader for VM cache pool preparation. All other warm-cache service instances move to a stand-by mode during this time. There is a TTL associated with each leader lock to handle leader failures and to enable leader reelection. In the following figure, leader is a Consul key that is managed by a distributed lock for the leadership role. The last leader node name and leader start timestamp are captured on this key. When a warm-cache service completes it functions in the leader role, this key is released for other prospective warm-cache service instances to become the new leader. The leadership time-series graph depicts which node assumed the leadership role. The number 1 in the graph below indicates a leadership cycle. When a leader has to provision a VM instance for a particular flavor, it first looks up for meta information for the flavor on MongoDB (via MongoSvc). This lookup provides details such as image-Id and flavor-Id . This information is used when creating the actual VM instance via NOVA APIs. Once a VM is created, its rack-id information is available via CMS. This information is stored in Consul associated with a Consul key $AZ/$INSTANCE , where $AZ is the Availability Zone and $INSTANCE is the actual instance name. This information is also then persisted on Elasticsearch for monitoring purpose. The following figure shows a high-level system sequence diagram (SSD) of a leader role instance: A Kibana dashboard can be used to check how VM instances in the cache pool are distributed across available racks. The following figure shows how many VM instances are provisioned on each rack. Using this information, Dev-ops can change the warm-cache build attributes to influence how the cache should be built in future. The following options are available for acquiring VM instances from the warm-cache pool: The Rack-aware mode option ensures that all nodes provided by warm-cache reside on different racks The Hypervisor-aware mode option returns nodes that reside on different hypervisors with no two nodes sharing a common hypervisor The Best-effort mode option tries to get nodes from mutually-exclusive hypervisors but does not guarantee it. The following figure illustrates the process for acquiring a VM. The following screen-shot includes a table from Kibana showing the time when an instance was removed from warm-cache, the instance’s flavor, data center information, and instance count. The corresponding metadata information on Consul for acquired VM instances is updated and removed from the active warm-cache list. Apart from our ability to quickly flex up, another huge advantage of the warm-cache technique compared to conventional run-time VM creation methods is that before an Elasticsearch cluster is provisioned, we know exactly if we have all the required non-error-prone VM nodes to satisfy to our capacity needs. There are many generic applications hosted on a cloud environment that require the ability to quickly flex up or to guarantee non-error-prone capacity for their application deployment needs. They can take a cue from the warm-cache approach for solving similar problems.", "date": "2016-11-14"},
{"website": "Ebay-Engineering", "title": "Transforming the Advertising Landscape on eBay", "author": ["Amir Green"], "link": "https://tech.ebayinc.com/product/transforming-the-advertising-landscape-on-ebay/", "abstract": "Get a behind the scenes look at the tech that’s revolutionizing advertising on eBay. When you’re shopping for an iPhone on eBay, you might see an ad for a mobile service deal to complement your purchase. Or maybe you’re browsing through eBay Motors section and you come across an ad for car insurance. This is because when you shop on eBay, there’s a lot more going on behind the scenes, with advertisers competing in real-time for a chance to get your attention and show you something else that you might be interested in. Let’s take that iPhone example: As soon as you hit “Search,” two distinct events are set off. eBay’s search engine looks for the best match to your search out of the 1.1 billion listings in our marketplace. eBay’s advertising exchange offers millions of advertisers the opportunity to show you an ad -- in real time! So, when you are deciding between the rose gold iPhone or the limited edition red one, various advertisers are competing for a chance to get your attention and show you their ad. This advertising process is called programmatic Real-Time-Bidding. It is an on-the-spot auction, and operates similar to a stock exchange, with publishers and advertisers communicating through an exchange powered by machines. On nearly every page load, eBay runs these types of real-time auctions for ad impressions. But, the standard way of programmatic advertising can slow down webpages and distract buyers from their shopping journey. We want you to get the best search results for that iPhone -- or whatever you are looking for -- as fast as possible, without letting ads slow down the process or disturb your shopping journey. Using programmatic science technology, we built an innovative, unified auction exchange called “parallel bidding.” By building our own advertising technology that leverages server-to-server programmatic pipes, we can improve advertising yield, reduce advertising latency, and enable data-driven control of advertising costs. In a process that only takes a few hundred milliseconds, parallel bidding sends out bid requests to multiple advertising bidders in the exchange in a parallel, simultaneous fashion and replaces the older and slower method of programmatic advertising that sends requests to bidders one at a time. This allows for a truly fair and unified auction, where all advertising bidders have an equal shot at winning the impression and drives up yield. This bidding process also collects bids for ad impressions behind the scenes on the server-side, speeding up the time it takes to load an eBay page and ads on your browser, and improving ad latency by up to 60%. And parallel bidding is in fact the auction house for our programmatic advertising, running and clearing an auction for each ad impression on eBay. The data it collects to make these auctions possible, coupled with eBay’s internal user behavior data, allows for intelligent pricing and cost control. Our goal is to create a valuable experience for both buyers and sellers, while also driving performance for advertisers. We strive to bring together two marketplaces: the shopping marketplace, connecting buyers and sellers; and the advertising marketplace, connecting advertisers and customers. We are continuing to work on parallel bidding with the goal of growing it so that it becomes the main way of showing ads on eBay, benefiting our entire marketplace. BIO Amir Green is a Senior Product Manager at eBay in New York City. He is product owner for the programmatic science team at eBay Advertising, focusing on unified auction mechanics, and building innovative technological solutions that bridge e-commerce and advertising technology.", "date": "2017-10-12"},
{"website": "Ebay-Engineering", "title": "Machine Translation Corpus Analysis", "author": ["Juan Rowda"], "link": "https://tech.ebayinc.com/engineering/machine-translation-corpus-analysis/", "abstract": "Statistical Machine Translation (SMT) needs considerably large amounts of text data to produce good translations. We are talking about millions of words. But it’s not simply any text data — it’s good data that will produce good translations. The challenge is how to make sense of all of these millions of words. What to do to find out whether the quality of a corpus is good enough to be used in your MT system? How do you know what to improve if you realize a corpus is not good? How to know what your corpus is about? Reading every single word or line is completely out of the question. Corpus analysis can help you find answers to these questions. It can also help you understand how your MT system is performing and why. It can even help you understand how your post-editors are performing. I will cover some analysis techniques that I believe are effective to understand your corpus better. To keep things simple, I will use the word “corpus” to refer to any text sample, either one used to produce translations or one being the result of a translation-related process. I’m going to cover two tools: AntConc and Python. The first one is a corpus analysis tool exclusively. The latter is a programming language (linguists, please, don’t panic!), but I’m going to show you how you can use a natural language processing module (NLTK) to dig into your corpora and also provide bits of code for you to try. AntConc and Python can be used in Windows, Mac, and Linux. As defined by its web site , AntConc is a “freeware corpus analysis toolkit for concordancing and text analysis.” It’s really simple to use. It contains seven main tools for analysis and has several interesting features. We will take a closer look at the details and how the tool can be used with the following examples. A great way to know more about your corpus is getting a list of all the words that appear in it. AntConc can easily create a list with all the words that appear in your corpus and show important additional information about them, like how many tokens are there and the frequency of each. Knowing which words appear in your corpus can help you identify what it is about; the frequency can help you determine which are the most important words. You can also see how many tokens (individual words) and word types (unique words) are there in a corpus. This is important to determine how varied (how many different words) your text is. To create a word list, after loading your corpus files, click the Word List tab and click Start . You’ll see a list of words sorted by frequency by default. You can change the sorting order in the Sort by drop-down. Besides frequency, you can sort alphabetically and by word ending. Frequency is often a good indicator of important words — it makes sense to assume that tokens that appear many times have a more relevant role in the text. But what about prepositions or determiners and other words that don’t really add any meaning to the analysis? You can define a word list range, that is, you can add stop words (words you want to exclude from your analysis) individually or in entire lists. Word lists are also very good resources to create glossaries. You can either use the frequency to identify key words or just go through the list to identify words that may be difficult to translate. This feature allows you to compare a reference corpus and a target corpus and then calculate words that are unusually frequent or infrequent. What’s the use for this? Well, this can help you get a better insight on post-editing changes, for example, and try to identify words and phrases that were consistently changed by post-editors. It’s safe to assume that the MT system is not producing a correct translation for such words and phrases. You can add these to any blacklists, QA checks, or automated post-editing rules you may be using. A typical scenario would be this: you use your MT output as the target corpus, and a post-edited/human translation (for the same source text, of course) as the source corpus; the comparison will tell you which words are frequent in the MT output that are not so frequent in the PE/HT content. “Vintage” here is at the top of the list. In my file with MT output segments, it occurs 705 times. If I do the same with the post-edited content, there are 0 occurrences. This means post-editors have consistently changed “vintage” to something else. It’s safe to add this word to my blacklist then, as I’m sure I don’t want to see it in my translated content. If I know how it should be translated, it could be part of an automated post-processing rule. Of course, if you retrain your engine with the post-edited content, “vintage” should become less common in the output. To add a reference corpus, in the Tool Preferences menu, select Add Directory or Add Files to choose your corpus file(s). Click the Load button after adding your files. Collocates are simply words that occur together. This feature allows you to search for a word in a corpus and get a list of results that show other words that appear next to the search term. You can see how frequent a collocate is and also choose if your results should include collocates appearing to the right of the term, to the left, or both. What’s really interesting about this is that it can help you find occurrences of words that occur near your search term and not necessarily next to it. For example, in eBay’s listing titles, the word “clutch” can be sometimes mistranslated. It’s a polysemous word , and it can be either a small purse or an auto part. I can do some analysis on the collocate results for clutch (auto parts) and see if terms like bag, leather, purse, etc., occur near it. You can also select how frequent a collocate needs to be in order to be included in the results. This is very useful to spot unusual combinations of words as well. It obviously depends on the language, but a clear example could be a preposition followed by another preposition. To use this feature, load your corpus files and click the Collocates tab. Select the From and To ranges — values here contain a number and a letter: L(eft)/R(ight). The number indicates how many words away from the search terms should be included in the results, and L/R indicates the direction in which collocates must appear. You can also select a frequency value. Enter a search term and click Start . All the results obtained with any of the tools that AntConc provides can be exported into several formats. This allows you to take your data and process it in any other tool. This is perhaps one of the most useful features in AntConc. Why? Because it allows you to find patterns . Remember that, when working with MT output, most of the time it’s not realistic to try to find or fix every single issue. There may be tons of errors with varying levels of severity in the MT output (especially considering the volumes of content processed by MT), so it does make sense to focus first on those that occur more frequently or that have a higher severity. Here’s a simple example: let’s assume that by looking at your MT output you realize that your MT system is translating the word “inches” into “centimeters” without making any changes to the numbers that usually precede that word, that is, “10 inches” is being consistently translated as “10 centimeters”. You could try to find and fix “1 centimeter”, “2 centimeters”, “3 centimeters”, etc. Instead, a much better choice would be to identify a pattern: “any number” followed by the word “centimeter” should be instead “any number” “inches”. This is an oversimplification, but the point is that identifying an error pattern is a much better approach than fixing individual errors. Once you have identified a pattern, the next step is to figure out how you can create some sort of rule to find/fix such pattern. Simple patterns made of word or phrases are pretty straightforward — find all instances of “red dress” and replace with “blue dress”, for example. Now, you can take this to the next level by using regular expressions. Going back to the inches example, you could easily find all instances of “any number” followed by centimeters with a simple regex like \\d+ centimeters , where \\d stands for any digit and the + sign stands for one or more (digits). Using the Clusters/N-Grams tool helps you find strings of text based on their length (number of tokens or words), frequency, and even the occurrence of any specific word. Once you open your corpus, AntConc can find a word or a pattern in it and cluster the results in a list. If you search for a word in your corpus, you can opt to see words that precede or follow the word you searched for. Results can be sorted by several criteria: By frequency (ideal to find recurring patterns — the more frequent a pattern is, the more relevant it might be) By word (ideal to see how your MT system is dealing with the translation of a particular term) By word end (sorted alphabetically based off the last word in the string) By range (if your corpus is composed of more than one file, in how many of those files the search term appears) By transitional probability (how likely it is that word2 will occur after word1; e.g., the probability of “Am” occurring after “I” is much higher than “dishwasher” occurring after “I”.) Let’s see how the Clusters tool can be used. I’ve loaded my corpus in AntConc. and I want to see how my system is dealing with the word case. Under the Cluster/N-grams tab, let’s select the Word check box, as I want to enter a specific search term. I want to see clusters that are three to four words long. And very important here, the Search Term Position option: if you select Left , your search term will be the first word in the cluster; if you select Right , it’ll be the last one instead. The following screenshots show how the Left/Right option selection affects the results. On left On right We can also use regular expressions here for cases in which we need more powerful searches. Remember the example about numbers and inches above? Well, numbers, words, spaces, letters, punctuation — all these can be covered with regular expressions. Let’s take a look at a few examples: Here I want to see all two-word clusters that start with the word “original”, so I’m going to use a boundary ( \\b ) before “original”. I don’t know the second word, it’s actually what I want to find out, so I’m going to use \\w , which stands for “any word”. All my results will then have the following form: original+word. Now I want to see all clusters, regardless of their frequency, that contain the words “price” OR “quality”. So, in addition to adding the boundaries, I’m going to separate these words with | (vertical bar) that simply stands for “or”. This is really useful when you want to check how the system is dealing with certain words — there’s no need to run separate searches since you can combine any number of words with | between them. Check the Global Settings menu for reference. For seasoned regex users, note that regex capabilities in AntConc are pretty modest and that some operators are not standard. If you are not familiar with this term, in a nutshell, an n-gram is any word or sequence of words of any size; a 1-gram is composed of one element, a 2-gram is composed of 2 elements, etc. It’s a term that defines the length of a string rather than its content. What’s great about this feature is that you can find recurring phrases without specifying any search terms. That is, you can easily obtain a list of, for example, all the 6-grams to 3-grams that occur more than 10 times in your corpus. Remember that clusters work in the opposite way — you find words that surround a specific search term. The n-gram search is definitely an advantage when you don’t know your corpus very well and you still don’t know what kind of issues to expect. It’s usually a good choice if it’s the first time you are analyzing a corpus — it finds patterns for you: common expressions, repeated phrases, etc. When working with n-grams, it’s really important to consider frequency. You want to focus your analysis on n-grams that occur frequently first, so you can cover a higher number of issues. What can you do with your findings, besides the obvious fact of knowing your corpus better? You can find recurring issues and create automated post-editing rules. Automated post-editing is a technique that consists in applying search and replace operations on the MT output. For instance, going back to our initial “inches” vs. “centimeters” example, you could create a rule that replaces all instances of number+centimeters with number+inches. Using regular expressions, you can create very powerful, flexible rules. Even though this technique was particularly effective when working with RBMT , it’s still pretty useful for SMT between training cycles (the process in which you feed new data to your system so it learns to produce better translations). You can also create blacklists with issues found in your MT output. A blacklist is simply a list of terms that you don’t want to see in your target, so for example, if your system is consistently mistranslating the word “case” as a legal case instead of a protective case, you can add the incorrect terms to the blacklists and easily detect when they occur in your output. In the same way, you can create QA checks to run in tools like Checkmate or Xbench . (Please note that this article is intended for a general audience and, if you are a Python expert, you may find some of the ideas below too basic. I’m not a Python expert myself, so I apologize in advance if the terminology I use here is not what experts use.) For those of you not familiar with Python , it’s a programming language that has been gaining more and more popularity for several reasons: it’s easy to learn and easy to read, it can be run in different environments and operating systems, and there’s a significant number of modules that can be imported and used. Modules are files that contain classes, functions, and other data. Without getting too technical, a module is code already written that you can reuse, without having to write it yourself from scratch. Quick example: if you want to write a program that will use regular expressions, you can simply import the re module, and Python will learn how to deal with them thanks to the data in the module. Modules are the perfect segue to introduce the Natural Language Processing Toolkit (NLTK). Let me just steal the definition from their site, www.nltk.org : “NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries…” Using Python and NLTK, there are quite a few interesting things you can do to learn more about your corpora. I have to assume you are somewhat familiar with Python (not an expert!), as a full tutorial would simply exceed the purpose of this post. If you want to learn more about it, there are really good courses on Coursera, Udemy, and YouTube, for example. I personally like Codeacademy’s hands-on approach. To follow these examples, you’ll need the following installed: Python 3.5 (version 2.7 works too, but some of these examples may need tweaking) NLTK Numpy (optional) To get corpora, you have two options: you can choose to use corpora provided by NLTK (ideal if you just want to try these examples, see how Python works, etc.) or you can use your own files. Let me walk you through both cases. If you want to use corpora from NLTK, open your Python’s IDLE, import the nltk module (you’ll do this every time you want to use nltk ), and then download the corpora: A new window will open, and you’ll be able to download one or more corpora, as well as other packages. You can find the entire list on the NLTK Corpora page. When working in Python, you can import (a) all available corpora at the same time or (b) a single corpus. Notice that (a) will import books (like Moby Dick and The Book of Genesis .) If you want to use your own files, you’ll have to tell Python where they are so it can read them. Follow these steps if you want to work with one file (but remember to import nltk first): Basically, I’m telling Python to open my file called reviews.txt saved in C . The “ r ” in front of the path is required for Python to read it correctly. I’m also telling Python that I want to read, not write on, this file. Then, I’m telling Python to read the contents of my file and store them in a variable called raw , to tokenize the content (“identify” the words in it), and to store those tokens in a variable named text . Don’t get scared by the technical lingo at this point: a variable is just a name that we assign to a bucket where we store information, so we can later make reference to it. What if you have more than one file? You can use the Plaintext Corpus Reader to deal with several plaintext documents. Note that if you follow the example below, you’ll need to replace sections with the relevant information, such as your path and your file extension. Here, I’m asking Python to import PlaintextCorpusReader , that my files have the txt extension, where the files are stored, and to store the data from my files into a variable called corpus . You can test if your data was correctly read just by typing the name of the variable containing it: corpus and text are the variables I used to store data in the examples above. Now that we are all set up and have our corpora imported, let’s see some of the things we can do to analyze it. We can get a word count using the len function. It is important to know the size of our corpus, basically to understand what we are dealing with. What we’ll obtain is a count of all words and symbols, repeated words included. If we wanted to count unique tokens, excluding repeated elements, we can follow this example. With the set function, we can get a list of all the words used in our corpus, that is, a vocabulary. A list of words is definitely useful, but it’s usually better to have them alphabetically sorted. We can also do that easily. Note that Python will put capitalized words at the beginning of your list. We can check how many times a word is used on average, what we call “lexical richness.” From a corpus analysis perspective, it’s good that a corpus is lexically rich, as theoretically the MT system will “learn” how to deal with a broader range of words. This indicator can be obtained by dividing the total number of words by the number of unique words . If you need to find out how many times a word occurs in your corpus, you can try the following. (Notice that this is case-sensitive.) One key piece of information you probably want to get is the number of occurrences of each token or vocabulary item. As we mentioned previously, frequent words say a lot about your corpus. They can be used to create glossaries, for example. One way to do this is using frequency distributions . You can also use this method to find how many times a certain word occurs. A similar way to do this is using the vocab function: Conversely, if you want to see the words that only appear one time, use the hapaxes function: If you only want to see, for example, the ten most common tokens from your corpus, there’s a function for that: We can have the frequency distributions results presented in many ways: One column >>> for sample in fdistcorpus:\n\tprint(sample)\n\n\t\nknowledge\nopening\nmystical\nreturn\nbound\nmoving\nBottom\nslam\nLord\nMs Here, I’m using a for loop . Loops are typically used when you want to repeat or iterate and action. In this case, I’m asking Python, for each token or sample in my corpus, to print said sample. The loop will perform the same action for all the tokens, one at the time, and stop when it has covered every single one of them. Here, I’m using a for loop . Loops are typically used when you want to repeat or iterate and action. In this case, I’m asking Python, for each token or sample in my corpus, to print said sample. The loop will perform the same action for all the tokens, one at the time, and stop when it has covered every single one of them. Tab-separated >>> fdistcorpus.tabulate()\n               ,              the              and                .                a               of               to                -               in               is             with              for               as                '                s              The               by               or               --               be             them               on              has Chart >>> fdistcorpus.plot()", "date": "2016-11-29"},
{"website": "Ebay-Engineering", "title": "NoSQL — Filling in the Gaps", "author": ["Rupesh Kartha", "Michael Chen"], "link": "https://tech.ebayinc.com/engineering/nosql-filling-in-the-gaps/", "abstract": "When you look for a NoSQL data store for your service, there are a number of choices. However, if your selection criteria are not simple, you find very few options, and you may even need to develop features to cover the gaps in the database. For a collection of services that we were developing, we needed a key-value store and a document store with high availability, high read performance, and high write performance. Some of the use cases needed this data store to be the system of record, needing point-in-time restore capability. We also needed “read-your-own-write” consistency for some of the operations. This blog is about how we came to use Couchbase 3.1 and how we addressed the gaps that were found. In our setup, there are three Couchbase clusters, one each in a data center, forming a cluster set. Each cluster has the full data set and has bidirectional Cross Datacenter Replication (XDCR) set up with each of the other two clusters in the cluster set. Each cluster has a replication factor of 2, that is, there are six copies of a document in the cluster set. The services read from and write to the primary node local to it. If the call to primary fails, the application uses its secondary node. This setup gives a highly available and eventually consistent distributed data store. Though the above setup gives high availability, one issue with the above setup is that when an entire cluster is down or a node is slow performing, there is no way for the application to detect and switch to another cluster or a better performing node. If the application is aware of all nodes in the cluster set and can choose the best performing node, availability improves and the service-level agreement (SLA) is more easily achieved. For this, a performance score, which is a weighted average of the query response time for that node, is assigned to the node and stored in a map. We assign more weight (say 90) to the existing score and less (say 10) to the new scores. With this strategy, scores change slowly, small spikes do not affect cluster selection, while large or sustained spikes cause a different cluster to be selected. The application chooses the best performing cluster set and optimal node (primary vs replica) at any given time. This allows for the application to automatically switch when there are external factors, such as prolonged network issues or Couchbase version upgrades. The chart below shows the performance during a manually simulated outage scenario. Each window represents a Couchbase cluster that is in a geographically different location. The application that is driving traffic is in the same geographic location as the upper left window. As the Couchbase cluster with the traffic is taken out with a vicious shutdown -r now across all the nodes, you can see the traffic in the upper left window take a dive, while it picks up a different data center shown in the upper right window. As the application continues to request data from Couchbase, it occasionally shifts traffic to the third data center, shown as spikes in the lower left window. This is because the application is constantly evaluating the “best” performing cluster and shunting traffic to it. Once the nodes in the “down” cluster finish rebooting and reform the cluster, the application can detect the cluster with trial queries and cause the traffic to shift back to it. A single Couchbase cluster is highly consistent. Given a single key, reads and writes are always to the same primary node and are immediately consistent. The trade-off that Couchbase offers is write availability. In a multiple-cluster setup, Couchbase sacrifices its strong consistency but gains much better availability. In the event that one node goes down, data for that node can’t be written in the affected cluster; however, you can choose to write to other clusters and rely on the XDCR setup to bring all your clusters back into consistency. Even though the multiple-cluster Couchbase was meant to be eventually consistent, some of our operations needed high (“read your own write”) consistency. There are many ways to impose a higher consistency than what the underlying storage technology provides. In the data write-read cycle, there are three options on which phase can be modified to increase consistency. Additional process can be imposed on the read phase, the write phase, or both phases. With no guarantees during the write phase, we can increase the consistency in the read phase by simply always reading data from all the clusters and either picking the latest or picking the value that has a quorum. Unfortunately, this does not guarantee “read your own write” consistency. When a value is updated, until the updated value is propagated to a quorum of nodes, the value returned is always the old value, unless “pick latest” is used. If the node that has the latest data is down, “pick latest” won’t work. We can enforce strong consistency in the write phase by making sure that the data is consistent before we return success on write. There are many ways to do this. Multi-write. The application can write to all the clusters all the time and wait until the writes all return successfully before returning a success to the caller. This does make the reads very fast and efficient. The complexity of the solution greatly increases when a node is down or the network is partitioned or multiple threads are attempting to update the same key. Read-back. The application can take advantage of XDCR by writing to the local cluster and performing reads to all clusters until all the reads fetch the value written. Once XDCR completes the transfer of documents and the correct values are fetched, the write operation can finally succeed. This solution also suffers from the same complexity increases for down nodes, partitioned network, or race conditions. Both of techniques focusing on this phase improve consistency most of the time and allow us to perform high-performance local reads most of the time, but it is too complex if we have to guarantee “read your own write” capability during node failures or when the network is partitioned. A popular approach in both-phase consistency requires quorums during both phases. Obtain a majority quorum upon write, and during the read phase obtain another quorum. We can mathematically choose the consensus threshold in each the read and write quorums to guarantee “read your own write” consistency. This communication diagram shows how expensive get and set calls are when we want to guarantee “read your own write” consistency. Every call requires network communication with every cluster that has the information. And as the number of clusters grows, the overhead with this approach can grow. It is best to provide this only to the use cases that absolutely need this. Discovering the data lag due to XDCR can give us the ability to make informed suggestions to our clients on when to actually use the very expensive “read your own write” consistency capability. In order for us to accurately measure how long it takes between writing the data and when data is available in the other clusters to be read, we simply generate heartbeats. With every heartbeat, we write the current timestamp to the local cluster and then immediately read the same document out of all the other clusters in parallel. When we take the difference between what timestamp was written, and what timestamp was received, we measure the data latency very accurately. The delta of the timestamp from the data read and the timestamp of when the data arrived can give the absolute staleness of the data from each cluster. From our measurements we found that the data lag is very close to the network latencies between the various clusters. Some of the use cases needed point-in-time backup and restore capability. Couchbase 3.X backup and restore tools were not yet mature, but it provided a Kafka connector to stream changes to Kafka based on DCP. This was used to implement point-in-time backup and restore. We set up a Kafka cluster to which the changes from one of the Couchbase clusters were written to. From Kafka, the readers can read the changes and write them to an Oracle database table. This table contains the changes to the Couchbase document along with the timestamp of the change. We developed a restore tool that gives the users the ability to select the data and the point in time from which restoration needs to be made. On execution, the tool extracts rows from the table, saves them to a file, and updates one of the Couchbase clusters, which then gets replicated. In addition to the standard Couchbase keys, application-level keys are stored. This allows us to perform selective restores. If some specific values are corrupted, the corrupted data alone can be restored. We avoid race conditions where data being restored might be actively updated from the service by leveraging the following approaches: Highly selective restore. Rarely would we want to restore everything back to a specific point in time. There is always a time window when the corruption occurs for specific attributes or users. This reduces the data volume and the chance of collision. Smaller window of collision. Because the data volume to be restored is small, the duration of the restore process is kept small. The goal is to have the restore complete in minutes rather than hours. In the worst case, where a collision does happen, it is straightforward to identify the documents that collided and run another restore with just the collided documents, so as to roll back the changes performed during the restore that had collisions as “corrupted” data. Over time, a lot of data can be collected. To maintain space, we used partition-dropping logic. The data is partitioned in Oracle using the LAST_MODIFIED_TS column modulo 24 months. Every month, the 24th oldest partition is dropped. NoSQL is great, but it may not be possible to find a product that meets all your needs. Spend the necessary time to evaluate your options. Prioritize your core needs and select the solution that best meets your priorities. Evaluate the gaps, and see if the gaps can be filled in by your application. A well-chosen NoSQL offering along with the proper application design and implementation can produce a highly available, low-latency, and scalable service to serve your needs.", "date": "2016-12-19"},
{"website": "Ebay-Engineering", "title": "Portico", "author": ["Leon Stein", "Wei Siong Lee"], "link": "https://tech.ebayinc.com/engineering/portico/", "abstract": "About three years ago, the Decide.com team joined eBay , and we started working on eBay Insights with a mission to bring powerful analytics to eBay sellers. Later, seller analytics became a key component of Seller Hub . Seller Hub is eBay’s one-stop portal, providing a unified platform for millions of eBay professional sellers. It is the hub where sellers get access to all the summaries, activities, tools, and insights about their businesses on eBay. Seller analytics in Seller Hub is powered by Portico — a data store that we built in house to support interactive, sub-second, complex aggregations on top of hundreds of billions of listing, purchase, behavioral activity tracking, and other kinds of records, with dozens of trillions of data elements, covering several years of listing history. Portico is currently handling millions of queries daily providing market GMV, traffic report, competitor analysis, and recommendations for unlikely to sell items through Performance and Growth tabs in Seller Hub. Implementing such a data store from scratch is a major undertaking, but after evaluating a dozen open-source and commercial solutions, we decided to go for it. As we describe Portico’s architecture below, we’ll point out some of the key considerations that led us to building this system ourselves. Generally, there are two approaches to big data analytics: OLAP -type systems, where aggregates are precomputed into “cubes” of various granularities (for example, Apache Kylin ) Brute-force scanning over data organized in columnar format (for example, Apache Parquet and Druid ) Our problem clearly called for the second, more flexible (albeit more complex) approach. Some important use cases, for example, comparing historical performance of a seller’s listing to its competitors, go well beyond manipulations with precomputed aggregates. Also, we needed the ability to drill down into the original un-aggregated records. Going into the columnar world, we had to discard job-oriented solutions targeted at “internal” analytics (most are Hadoop/HDFS-based), as we needed a highly available system that scales to millions of users with hundreds of concurrent queries and also provides consistent sub-second performance. We also wanted a solution that keeps most of the data in memory, as reading 100s of MB of data from disk on each query won’t scale with a large number of queries. So we were after a highly available distributed (many terabytes) in-memory columnar store. This has been done before. One inspiration came from the excellent Google PowerDrill paper, and another was Druid , mentioned above. Two main optimization ideas are behind columnar organization: Columnar data allows much better compression than row data (since column values tend to be similar, and many columns in analytics datasets tend to be sparse). Analytics queries usually touch a small subset of columns from a dataset, so the scanned data can be limited just to the columns that are needed. The next critical optimization question is how to organize related records in the columnar dataset to minimize the amount of data scanned by a typical query? The efficiency of brute-force scanning can be expressed as: Where is the number of records that went into computing the result, and is the total number of records that were evaluated. So, if the whole dataset is broken into many fragments (partitions), we want to minimize the number of scanned fragments by putting records that are likely to be used together into the same fragment and by providing a way to eliminate (prune) unneeded fragments prior to scanning. For eBay seller analytics use cases, there are three such groupings. Examples of listing categories are Wireless Routers or Women’s Jeans . Notice that items 2 and 3 are somewhat at odds: if data is broken into partitions by seller (some hash of seller ID, to have a reasonable number of partitions), each partition would contain listings belonging to many different categories, and if data is broken by leaf category, each partition would contain listings from many different sellers. An important observation is that sellers naturally focus on a few categories, so even very large sellers with millions of listings tend to be present in a relatively small number of categories. In fact, over 98% of eBay sellers are present in fewer than 100 leaf categories. Excluding 85% of eBay sellers who listed under 20 leaf categories from the sample set, over 86% of the remaining eBay sellers are present in fewer than 100 leaf categories. This observation led us to the following organization of our dataset: All records belonging to the same listing are kept together. Fragments are partitioned by leaf category. Large leaf categories are subpartitioned by seller ID hash, so the number of listings in each subpartition is kept at around 1 million. (One million is a very rough target, as there are sellers having millions of listings in the same category, but it still works pretty well for the majority of sellers/categories). Within each subpartition, data is broken into bite-size “pages” with a fixed number of listings (currently 50,000). Records within each subpartition are sorted by listing ID (across all pages). For category pruning, we keep a mapping from seller ID to the list of all categories where the seller ever listed. The above data organization efficiently handles many kinds of queries/aggregations: Looking up all listing data for a given listing ID (with known seller and category) requires a single page scan: since data within each subpartition is sorted by listing ID, we can locate the page containing the listing by looking at min/max listing ID values stored in page metadata. Aggregating listings in a given category (or a small set of categories) requires scanning just the pages belonging to these categories. Further page pruning can be achieved by evaluating page metadata. Aggregating listings for a given seller is similar to the above: we first look up all categories where the seller is present and prune subpartitions using the seller ID. A small percentage of listings change categories over the lifetime. We keep records for these listings separately in each category and combine them at query time. But this versatility comes at a cost: as new data is added daily to our dataset, it sprinkles all over the place, as new records are added to existing listings, and new listings are inserted at random positions within pages (eBay listing IDs aren’t strictly incremental), so no part of our dataset can be considered immutable. Effectively, the whole dataset has to be rebuilt daily from scratch. This differs radically from Druid’s approach, where data is organized as a time series, and historical fragments are immutable. Still, optimization for runtime performance and scalability outweigh the fixed (although high) cost of rebuilding the whole dataset for us. As an additional benefit, we’ve got daily snapshots of our dataset, so we can safely roll back and re-ingest new data to recover from data loss or corruption caused by bugs or data quality issues. Finally, rebuilding the whole dataset makes “schema migration” (introduction of new columns and new kinds of listing entities) almost a routine operation. As mentioned above, listing data consists of many different kinds of records (entities), and resembles a hierarchical structure: With data organized in columns, a mechanism is needed to align columns to the same logical record during scanning and also to recursively navigate columns in child records. To align columns at the same hierarchy level, we keep track of the currently scanned row position, and separately for the last accessed positions for all columns ever referenced by the query (query may access different columns on each row iteration). On each column access, we compare column position to the current row position, and skip over a range of column values (if necessary) to align that column to the current row. This alignment logic adds a few CPU cycles on each column access, and it results into a material overhead when scanning billions of values, but it simplifies the query API greatly. To align child entities, we introduce a special hidden “cardinality” column for each child containing the number of child records corresponding to the parent record. Incrementing the parent row position by 1 results in incrementing the child row position by the corresponding value contained in the cardinality column: With the above table, we present a columnar dataset in the query API as an iterable collection of rows that is conceptually familiar to every developer (in pseudo-code): The root (listing) iterator goes over all root records, and child iterators automatically terminate at the last child record for the currently iterated parent. Analytics datasets contain mostly numeric data, or data that can be mapped to numeric types. In fact, most of the common types can be mapped to 32-bit integers: 64-bit integers (longs) map to two 32-bit integers for low and high bytes Booleans (flags) map to 0 or 1 (compression described below takes care of unused bits) Floating-point numbers can be represented as one or two 32-bit integers depending on the required precision Dates map to the number of days from an arbitrary “base” date (we’ve picked 1/1/2000). Time of day (with seconds precision) maps to the number of seconds from midnight Currency amounts map to integer amounts in the lowest denomination (for example, US dollar amounts are expressed in cents) In cases when more than one 32-bit integer is needed for type mapping, we use multiple underlying integer columns to represent a single column in the dataset. Type conversions are expensive when scanning billions of values, so it’s important that basic comparison operations can be performed on unconverted values (that is, the mappings are order-preserving). We also use dictionaries for columns containing a relatively small number of distinct values, with each value converted to its index in the dictionary. Dictionaries work for any data type (for example, strings), including integers, in cases when a significant portion of distinct values are large by absolute value. Smaller numbers, like dictionary indexes, take less space when encoded/compressed. Dictionary indexes can be selected to preserve ordering, for example, lexicographic ordering for strings. Another benefit of dictionaries is that the number of occurrences of each distinct value can be stored along with the value in page metadata. This helps with page pruning (for example, skipping pages not containing listings of a specific type) and allows the collection of statistics about encoded data without actually scanning it (for example, what’s the percentage of listings of a given type in a category). We also support optional Bloom filters in metadata on columns that may be used for page pruning. For example, a Bloom filter on seller ID allows skipping pages that do not contain listings for specified sellers. For integer compression, we’ve developed a generic algorithm ( IntRunLengthCodec ) combining bit packing with Run-Length Encoding (RLE) and optional dictionary encoding. It detects whether RLE or bit packing is preferable for a block of numbers, and uses a heuristic on the percentage of distinct large values triggering use of dictionary. Apache Parquet uses a similar approach combining bit backing and RLE. During decoding, IntRunLengthCodec supports skipping over ranges of values that is significantly faster than iteration, improving performance of column alignment. For strings that do not fit into a dictionary, we use LZ4 compression in conjunction with IntRunLengthCodec to store string lengths. (We’ve started with Snappy , but LZ4 turned out a bit faster with similar compression ratios.) Both Snappy and LZ4 operate on blocks, allowing skipping over ranges of bytes for column alignment. Thus string columns are represented by two underlying columns, one with LZ4 compressed strings written back to back and another with the corresponding string lengths. Compression ratios are highly dependent on the data being compressed. On average we achieve compression ratio of ~30 for the whole dataset, bringing the compressed size down to several terabytes. We keep encoded column data separately from metadata that contains encoded column lengths and codec configurations. Metadata also contains dictionaries, serialized Bloom filters, and column statistics (min/max values, number of empty values, uncompressed data size, etc.) On disk, column data for a page is written to a single binary file with columns written sequentially back to back. Metadata is written as a (compressed) JSON file. At run time, column data is memory mapped from the binary file, and metadata is loaded into an on-heap structure. Pages belonging to the same subpartition are kept together as a “page set”. Page sets are dealt with atomically to ensure data consistency at subpartition level. When a newer version of subpartition page set is available, it’s loaded in memory, and the page set reference is swapped before unloading the older version. At the time of page set loading, we also precompute frequently used aggregates (for example, daily GMV ) and compact cross-references (for example, “relisting chains” where the same item was listed multiple times by a seller) across all pages. This data is stored on the heap together with the page set and can be used by queries. We also provide an LRU cache at page set level for query results, so frequently executed queries can bypass page scanning on repeated executions with same parameters. Portico is implemented in Scala, and we leverage Scala DSL capabilities to provide friendly schema and query APIs. The schema for a dataset is defined by extending the Schema class. (Code snippets are simplified, and class and method names are shortened for clarity): The methods starting with '$' define columns along with configuration. (The '$' prefix was chosen to avoid name collisions in subclasses). In the above code, key = true designates a column as a part of the schema key. Key columns are used when merging incremental data into a page (see “ Data ingestion ” below). Specifying filtered = true enables Bloom filter creation. Column and child names are captured from Schema class field names via reflection. Column specifications are encapsulated in the ColumnSpecSet class and can be obtained from a schema instance. At runtime, an instance of a ColumnSet representing a page (with references to memory-mapped column data and metadata) can be bound to an instance of Schema and scanned as follows: Care is taken to ensure that column-access methods compile into Java primitives, so there is no primitive boxing and unboxing during scanning. Column values can be converted into the target type during scanning. (Type annotation is added for clarity.) Column metadata is available directly from fields after binding, for example: To support schema migration, binding resolves conflicts between columns and children in the schema and in the bound page: Columns are matched by name. Columns found in the page but missing in the schema are dropped. Columns found in the schema but missing in the page are substituted with columns containing type-specific empty values (0 for integers). If column type in the schema differs from column type in the page, automatic type conversion is applied (at scan time). Similarly, children are matched by name with non-matching children dropped or defaulted to all empty columns. The above behavior can be overridden with an optional SchemaAdaptor passed into the bind() method. SchemaAdaptor allows custom type conversions, renaming columns and children, and also defaulting missing columns via a computation against other columns. Schema iteration behavior over hierarchical records is defined in the RecordSet trait that Schema implements. RecordSet along with ColumnSpecSet (obtained from schema or created separately) serve as the input into ColumnSetBuilder , constructing a sequence of ColumnSets (pages) with fixed number of root records: There are two other important implementations of RecordSet : StichedRecordSet takes a sequence of RecordSets and presents them as a single RecordSet . MergedRecordSet takes a RecordSet and a hierarchical collection of iterators over sorted incoming records (for example, parsed from CSV), and presents them as a single merged RecordSet . MergedRecordSet uses key fields to decide whether a record needs to be inserted or overwritten and to preserve the ordering of records throughout the hierarchy. Thus, to add new data to a subpartition comprised of a sequence of pages ( ColumnSets ), we go through the following steps: Bind existing pages to Schema (with optional SchemaAdaptor for schema migrations) to produce a sequence of RecordSets . Construct a single StichedRecordSet from the above sequence of RecordSets . Construct MergedRecordSet from StichedRecordSet with the incoming data. Pass MergedRecordSet to ColumnSetBuilder to produce the resulting sequence of pages. The above process relies on the fact that all records in the original ColumnSets and in the incoming data are sorted, and does not materialize intermediate RecordSets in memory (records are streamed), so it can be performed on inputs of arbitrary size. A Portico environment consists of one or more clusters. Each cluster contains all subpartitions comprising a dataset distributed across many nodes. Increasing the number of clusters increases redundancy and capacity (query throughput). The number of nodes within each cluster is determined by total heap and mapped memory needed to hold the dataset. Since memory-mapped files are paged from disk by the operating system, memory oversubscription is possible, however we try to keep oversubscription low to avoid excessive paging. All nodes in an environment know about all other nodes in all clusters, so a single query can be distributed across the whole environment. We use Zookeeper for cluster membership, and consistent hashing (based on the Ketama algorithm ) to minimize subpartition moves during rebalancing when nodes join or leave clusters. During rebalancing, nodes keep previously assigned subpartitions, so routing of queries to clusters that are being rebalanced is supported, improving environment availability and capacity. Partitions that are frequently accessed due to seasonal or geographical access patterns can be dynamically configured with additional replicas to improve performance and availability. With that, we achieve high availability and consistency of Portico environments, i.e. we guarantee that a successful query execution is performed exactly once against each requested partition/subpartition. Separation (by seller) and atomicity of subpartitions ensures that each listing is also evaluated exactly once. In addition to cluster nodes, the Portico environment contains “reference” nodes that serve partition mappings used to limit the number of partitions where a query is dispatched (pruning); in our case, mapping from seller ID to all leaf categories where seller is present, as discussed in “ Data organization .” Portico environments run in eBay’s internal OpenStack cloud (Connected Commerce Cloud or C3), and datasets are persisted in OpenStack cloud storage ( Swift ). We leverage the map/reduce paradigm for query execution. Native Portico queries are written by extending the Query trait parameterized by query result type R : The init() method binds context and execution parameters to the query. Context provides access to the execution framework, and can be used to lookup locally provided services or to execute subqueries. The execute() method scans a page and optionally produces a result. Query needs to define how results are combined in the merge() method. The execution framework invokes execute() method on each page for each partition or subpartition requested by the query. The merge() method is called recursively on the produced results, first with all results in a subpartition, then with all subpartition results in a partition, and finally with all partition results. The on...Result() callbacks can be optionally implemented by a query to apply special processing at these milestones during query execution. We provide two specializations of the Query trait: SchemaQuery , where scanning occurs over a pre-bound schema SqlQuery , supporting ad-hoc SQL execution (using Apache Calcite ) During query execution, the framework collects statistics on how many values were scanned and skipped for each column in each subpartition. These statistics are sent back to the client along with the query result, and can be used for query tuning. Clients submit queries to random nodes in a Portico environment over HTTP (all nodes are behind an HTTP load balancer). A named SchemaQuery can be submitted either as a precompiled JAR archive or as a Scala source text, in which case, Scala source is compiled into a JAR by the receiving node. JAR archives are distributed and stored on disk across clusters during execution, so subsequent query executions (with different parameters) don’t require JAR/source transfers. To isolate Portico clients, nodes use separate class loaders for each client, and same named query can be resubmitted/recompiled multiple times. The receiving node computes distribution of subpartitions requested by the query across nodes in the environment and drops subpartitions (pruning by seller ID hash) that are not accepted by the query. If multiple nodes (in different clusters or replicas within cluster) contain the same requested subpartition, a heuristic is applied to balance the expected number of scanned records on each node and the distribution of requests across replicas. Query is then dispatched in parallel to selected nodes via HTTP API described above. Node-local query execution is also performed in parallel with multiple pages scanned concurrently, using all available CPUs. After all local results are merged into a single result, it’s serialized back to the calling node, and then, after merging all node results, back to the client. We merge new data into the Portico dataset daily. The new data is coming from Data Warehouse and data science feeds as separate flat CSV files for each entity comprising listing. There is a single “ingest” server responsible for feed preparation. Each feed is broken into fragments by leaf category ID and sorted by the corresponding key fields. Incoming feeds are tracked in a MySQL database, and fragments are stored in Swift. After all fragments are ready, the ingest server triggers the start of merge process in a Portico environment via HTTP API. Nodes download incoming feed fragments from Swift for subpartitions they are responsible for and merge them into the corresponding page data, as described in “ Adding data .” With multiple clusters, the node responsible for merging each subpartition is selected deterministically using hashes of cluster states and subpartition key. Pages for merged subpartitions are uploaded to Swift, and updated across clusters in the environment via a node-to-node HTTP API with fallback to Swift. The ingest server monitors the overall progress of merging, and once merging is completed, it writes a build manifest to Swift, indicating a complete build. The ingest server also runs a report query to collect statistics (data sizes, record counts, etc.) for each partition in the new build, and persists this information in MySQL. This data is used to analyze sizing trends. Portico environment can process queries during data ingestion, however the capacity during ingestion is degraded due to CPU time being spent on merging. We perform ingestion in a pre-production Portico environment and distribute new dataset builds into production environments in different availability zones (data centers) via the HTTP API, with fallback to Swift. With several years of historical data, Portico supports a variety of analytics use cases, however, the historical data in Portico is at least one day old, as new data is merged daily. This gap is covered by near-real-time (NRT) data that is ingested from Kafka streams containing new and updated listings, purchases, small-window behavioral aggregates, etc. NRT data also temporarily covers historical data in case of delays in data warehouse extracts or historical data ingestion. We chose to colocate NRT data for subpartitions with historical data on the same nodes. This helps in combining NRT and historical data for subpartitions in one pass in queries, reducing network trips and result sizes. Like historical data, NRT data is broken into sequences of pages (page sets), however page size is determined by time window (for example, one hour) instead of the number of records. NRT page sets are grouped by day and dropped as new daily historical data arrives. Unlike with historical data, we keep different entities separately with flat schemas, so scanning NRT data requires separate passes for each entity. Still, we guarantee that each record key occurs at most once within a page, and all records in a page are ordered. The deduplication and ordering happens in the “current” page that is kept on the heap and where the new data is being appended. With Seller Hub expanding to the Big 4 markets, by November 2016, daily web traffic exceeded 4 million requests with an average of 400,000 unique visits. 99.47% of these requests achieve sub-second latency and all were ran on-the-fly on a dataset size of over 100 TB, compressed to a size of 3.6 TB and memory-mapped. With over three years of behavioral impression data and over four years of listing data, eBay’s sellers for the first time are able to perform real time analytics on their listing impressions, sales conversion rate, price trends, and those of their competitors. To achieve this performance, Seller Hub’s queries are optimized to take advantage of several features that were designed for best performance and optimum capacity utilization. As described in “ Data organization ,” most of our sellers tend to focus their businesses on a small subset of trending categories such as Fashion , Electronics , and DVDs , and trending categories tend to change with seasonality. These large leaf categories are sub-partitioned by seller ID hash and have most of their traffic balanced across the clusters. Also, from analyzing the access pattern of Seller Hub’s sellers and their geographic time zones, Portico is able to dynamically configure additional replica sets for these trending categories to further increase query performance. For seller-centric queries, queries make extensive use of Bloom filters and column statistics described in “ Page Structure ” to prune pages that do not contain listings in the specified date ranges for that seller. For category-level aggregation queries, such as finding last 30- and 365-days markets’ GMV , these results are pre-computed once during the loading of sub-partitions and cached for the day until the next daily build, as described in “ Data ingestion .” Lastly, over 80% of the queries are sent using default 30-day time window, caching the query result of this frequently accessed date range in the query cache, once computed, greatly improved the average performance. The Portico team is still working on exposing NRT data in Seller Hub. We would like to improve Calcite integration, primarily around projection and filter optimizations. We are following the Apache Arrow project and considering it as a potential foundation for native replacement of codecs and schema API. Powered by QuickLaTeX .", "date": "2017-01-12"},
{"website": "Ebay-Engineering", "title": "How eBay's Shopping Cart used compression techniques to solve network I/O bottlenecks", "author": ["Venkatesh Ramaswamy"], "link": "https://tech.ebayinc.com/engineering/how-ebays-shopping-cart-used-compression-techniques-to-solve-network-io-bottlenecks/", "abstract": "eBay’s data storage related to Shopping Cart information relies on two different data stores. There’s a MongoDB store that stores the entire cart contents for a user’s cart, and there’s an Oracle store that stores only the cart summary and basic cart details but has enough hints to recompute the full cart if needed. The Oracle store is used to overcome the persistence and consistency challenges (if any) associated with storing data in MongoDB. It’s easier to think of the MongoDB layer as a “cache” and the Oracle store as the persistent copy. If there’s a cache miss (that is, missing data in MongoDB), the services fall back to recover the data from Oracle and make further downstream calls to recompute the cart. All MongoDB carts are in JSON format, while the Oracle carts are stored in JSON format in a BLOB column. (These Oracle carts are user-sharded and are used only for OLTP reasons. All other filtering and querying to peek into cart data happens via a data warehouse and denormalized access patterns.) Figure 1. Storage architecture While this is not a discussion on the choice of technology (Oracle vs. MongoDB vs. any other database), we are hoping that our experience gives folks an insight into what it takes to identify, debate, solve, and (most importantly) roll out fixes rapidly to a site that handles hundreds of millions of calls across tens of millions of users each day. Late in 2016, the Shopping Cart service started experiencing a high number of misses at its caching layer, and at around the same time, we started getting alerts from eBay’s Ops team about our MongoDB replicas failing to catch up in a decent amount of time. (MongoDB runs in master-slave configuration and this description is worth a read if you are unfamiliar with the setup.) The setup, which was seemingly working “fine” for the past few years, had suddenly started to hit bottlenecks that were unrelated to any infrastructure or code change. Upon further investigation, this was narrowed down to MongoDB’s oplog hitting network I/O limits. You can think of the oplog as Oracle’s redo logs, except that this has a very critical purpose in making sure that the MongoDB read replicas are refreshed in as real time as possible. Due to this, the read replicas for a GET Cart operation, for example, were returning stale carts and were being rejected as a “cache” miss because of the incorrect (stale) version that was stamped on the cart. This meant that we were doing a lot more recomputes of users’ carts by falling back on the Oracle cart and using that to call downstream systems to formulate the full cart from the basic summary. Obviously, this was a bad thing since we were making our users wait more than normal. (Cache misses are bad! Recomputes are bad!) Before we go into specific solutions, we want to call out the numerous other conversations and options that were worth a mention but not selected into the final bucket. For example, once the replicas were not catching up, we ran trials on “hiding” some of the replicas to try and allow them to catch up faster without taking user traffic. We also played around with timeouts and periodic replica reboots, but none of them seemed to cause the issues to die down. The first obvious option was to split our JSON data into shardable sections. (Note that the entire cart was stored as a single JSON object.) This allows specific sections (only) to be written to disk and into a separate MongoDB cluster, which in turn reduces the number of writes and I/Os sent across the wire into the single master instance. The challenge with this approach was the re-write of the existing business logic to now understand a seemingly new schema. This option would use MongoDB’s set command to update only the specific values changing on each update. While this works in theory, the kind of updates happening on the cart JSON object would involve domino-effect updates, which would pretty much trigger a change across numerous sections of the JSON. Keeping this aside, there was no assurance from the MongoDB folks that this would reduce the amount of oplogs being written. Their concern was that given the large number of selecting updates within a document, it might trigger the entire document update — thus not helping. Given the desire to get our situation fixed quickly, without rewriting the business logic, compression seemed like another logical option to consider. Reducing the amount of data coming into the master should correspondingly reduce the amount of data flowing into the oplog. However, this would convert the data into a binary format and would need clients to understand compressed reads and writes. Considering various options and the timings for them, we chose Option 3 as the fastest, cleanest, and the most likely to succeed at first try. The biggest assumption was that the CPU hit taken by the clients on compression and decompression would be well worth the savings in terms of the latency on the MongoDB side. One advantage we had is that we had already embraced SOA, and our service encapsulated all the calls in and out of the MongoDB cluster, so it was only our service (“client” to MongoDB) that needed to change. Before we got started to do something as fundamental as this, we had to get our goals right first. Here were our simple premises. Allow carts to be compressed and persisted into MongoDB (no change of the data store at this time). Allow a choice of compression codecs and seamless shifting between reading with one codec and persisting/writing with another, in case there’s ever a need to change because of performance reasons. Allow reading of old, new, and intermediate carts, that is, forward and backward compatibility. Old carts can be read by new clients and should work. New carts can be read by old clients and should work. Old carts can be read and persisted as new carts. New carts can be read and persisted as old carts. Old carts can be read by new clients and should work. New carts can be read by old clients and should work. Old carts can be read and persisted as new carts. New carts can be read and persisted as old carts. Implementation should allow for uncompressed and compressed carts to be stored at the same time (not necessarily for the same exact cart). Ultimately, getting rid of raw uncompressed storage in favor of compressed carts. Make sure that there are no use cases needing real-time query of JSON data in the MongoDB store. (The new data will be compressed binary and won’t be query-able). After a quick round of deliberation, we moved to the next phase of evaluation. For our choice of codecs, we had four well-known Java implementations we could choose from: SNAPPY : Google-contributed compressor/de-compressor and heavily used in many Google products, externally and internally. The source code for the SNAPPY codec is available on GitHub . LZ4_HIGH : Open-source software, BSD 2-clause license, running in high-compression mode and requiring the storage capacity for decompression to work. The source code for the LZ4 codec is available on GitHub . LZ_FAST : Same as above, just running in fast compression mode. GZIP : the Java-provided implementation We wrote a benchmark with sample cart data (all sizes in bytes) from production across these libraries to choose what our initial choice of algorithm would be. To make sure that they are not impacted by CPU or I/O spikes on the local machine, these tests were run multiple times to make sure that the results (sample shown here below) are consistent. Here are some observations: SNAPPY seems to suffer from a slow start. The time taken for the first run is always notoriously high. It also has not-so-great compression ratios. We were surprised that SNAPPY performed so badly given how much talk it gets. LZ4_FAST decompression times are almost constant, independent of size. It also shows not-so-great compression ratios though. LZ4_HIGH provides great times for both compression and decompression and gets a bit slower at high data sizes. GZIP seems to get worse at high data sizes. Given these results, LZ4_HIGH seems to be the most optimal code for the following reasons. No slow start issues or performance quirks observed Linear time growth with data sizes Excellent overall performance for small to medium cart sizes (well into the 99th percentile cart size) and reasonable performance at very large cart sizes However, there’s one caveat. Decompression for LZ4_HIGH expects the output buffer size to be specified in the API call. The memory pre-allocation is likely what enables the faster decompression. It’s a price to be paid for the benefits but something we felt was useful enough to account for it in the final design. So while the decision was clear, the implementation was designed to have all four codecs available as choices to be possible to shift seamlessly between one codec or another (one of our goals as mentioned previously) depending on a future need. A sample payload of today’s shopping cart looks like this. The cart variable is the only interesting piece in the payload. Our new payload with support for compressed data looks like this. The cart variable remains unchanged, but the new elements, initially derived from the cart variable, provide support for the Compression feature. Each of the fields in the compressionMetadata field is described below: \"compressedData\" — The container that stores the compressed cart and metadata about the cart itself that will be used for compression/decompression. \"compressedCart\" : \"...Compressed cart object...\" — The compressed data for the cart field in the upper-level container. \"compressionMetadata\" — The subcontainer that holds the metadata required for decompression and hints for certain codecs (for example, LZ4_HIGH that needs the destination size ( uncompressedSize ) to work. \"codec\" : \"LZ4_HIGH\" — Stored when compression runs and used when decompression runs. \"compressedSize\" : 3095 — Stored when compression runs and used when decompression runs (used by LZ4_HIGH only, but we do not differentiate) \"uncompressedSize\" : 6485 — Stored when compression runs and used when compression runs (used by LZ4_HIGH only, but we do not differentiate) Note that the sizes of the compressed and decompressed data are also stored every time although they are only really used when the codec is LZ4_HIGH . While all of this helped with the need to seamlessly switch between codecs, it also acted as a great source of statistical information for compression metrics for us. Once the code rolled out to production, we had two approaches towards experimentation to make sure our implementation works seamlessly. The first was to go with an A/B test, the typical experimentation paradigm, and verify performance via standard reporting that’s already in place. The other option was via back-end config-driven testing. We chose the latter since we were confident that we could figure out issues with pure back-end metrics and have enough logging in place to identify issues. For example, we could make sure that the compressed and decompressed data matched the size stored in the metadata (else log a critical error). We also had alerting built into place that would give us an immediate read if any codec was mis-configured or failed to compress or decompress at run time. To add to it all, eBay’s services follow the 12-factor approach for the most part. The one factor we focus on here is the config-driven approach that enables us to play around with different codec settings. In summary, the whole rollout and validation process looked like this: Configure builds with compression feature = OFF as default. Roll out to all boxes, with a relaxed rollout template, making sure that there are no inadvertent impacts caused by the feature even when it is turned off. Bake for a few hours. Pick one random box and turn on compression config for that box ONLY in backwards-compatible mode. This mostly verifies compression, and very seldom is decompression expected to be triggered unless the same single box reads the previously compressed cart. Turn on the compression config for a few more boxes until all boxes compress but no one reads the compressed data yet. This causes even more increased traffic and worsen the current situation. Necessary evil. Turn on reading decompression config for one box. Make sure that this box is able to read the compressed field and use ONLY the compressed field. Like before, turn on reading decompression config for multiple boxes and then for all boxes. Verify across all boxes. Finally, here are the different configs we used: compressionEnabled = true / false — The master switch that controls whether this feature is enabled or not. compressionCodec = { SNAPPY , LZ4_HIGH , LZ4_FAST , GZIP } — One of the four choices that will be enabled, with LZ4_HIGH being the default choice. WRITE_MODE = One of three modes. NO_COMPRESS — Writes will not write the compressed data fields. DUAL — Writes will write the compressed data fields and the uncompressed/regular cart. COMPRESS_ONLY — Writes will write only the compressed data fields and null out the uncompressed/regular cart field. NO_COMPRESS — Writes will not write the compressed data fields. DUAL — Writes will write the compressed data fields and the uncompressed/regular cart. COMPRESS_ONLY — Writes will write only the compressed data fields and null out the uncompressed/regular cart field. READ_MODE = one of three modes NO_COMPRESS — Reads will read from the regular cart fields. VERIFY_COMPRESS — Reads will read from both the fields, use the regular cart field, but verify that the compression data is being decompressed correctly. Think of this as “audit” mode. COMPRESS_ONLY — Reads will directly read the cart from the compressed data fields, decompress it, and use it. NO_COMPRESS — Reads will read from the regular cart fields. VERIFY_COMPRESS — Reads will read from both the fields, use the regular cart field, but verify that the compression data is being decompressed correctly. Think of this as “audit” mode. COMPRESS_ONLY — Reads will directly read the cart from the compressed data fields, decompress it, and use it. Note that not all mode combinations are valid. For example, WRITE_MODE = COMPRESS_ONLY and READ_MODE = NO_COMPRESS is invalid, and care should be taken to avoid that combination. It might look like overkill, but the key goal to keep the site up and running at all costs with zero impact was playing on our minds all the time. We felt that the critical factor was to have all the different controls at your disposal for any eventuality. We can’t emphasize this enough. For any site that cares about its Users, this is a critical factor to be factored into any new feature that is rolled out. Our goal was no different, and the diagram in figure 2 captures some of the complexity we faced. A few things to keep in mind: Do not delete fields that are being deprecated. We added the compressedData field as a new one to our existing JSON structure. Make sure to separate out the code paths in a way and as close to the source of changing data. This practice almost always allows a better code-deprecation option in the future. Backward compatibility is critical even when new code is in the process of rollouts. Do not assume a 30 minute “incompatibility window” and everything will be fine after that. For example, you never know when things simply stall and you will be stuck in limbo for much longer. For example, Figure 2 shows how our code paths looked while we were in the midst of rollout and both the old and new services were taking traffic. By separating out the logic right at the DAO/DO layer, the rest of the business logic continued as if nothing changed. Figure 2. Code paths After a successful rollout, we saw some magical numbers. Our oplog write rates, which had been almost 150GB/hour, were down to about 11GB/hour, a 1300% drop! The average object size of the documents, which had been hovering around 32KB, was down to 5KB, a 600% drop! In fact, we even saw some improvements in our eventual service response times as well. So overall we achieved what we wanted with ZERO production issues, and all our bets and analysis paid fantastic dividends at the end, benefiting our customers. Figure 3. Before-and-after average object sizes. Figure 3 shows a screenshot from MongoDB’s Ops Manager UI tool that highlights the slight increase (when we were doing dual writes of compressed and uncompressed data) and the dramatic drop in the average object size from 32KB down to 5KB after we turned the feature on fully. Figure 4. Before-and-after oplog throughput Figure 4 shows another screenshot from MongoDB’s Ops Manager UI tool showing the slight increase and the dramatic drop in the oplog throughput after the feature was turned on fully. Post-release, we also went ahead and removed the “dead code,” which was basically the old logic that worked only on compressed data. Over a period of time, as older carts keep getting refreshed, there will never even be a hint that we even had uncompressed data at some point in time! So how did the machines running the (de-)compression do? Not too bad at all. There was hardly any noticeable change comparing CPU and JVM heap usages before and after the code rollout. Finally, here are some other scatter graphs for a random hour’s worth of data showing how the compression did in the Production environment. These graphs provide many other interesting observations, but those are left as an exercise for the reader. Figure 5. A scatter plot showing compression achieved for the uncompressed data plotted against the original (uncompressed) data Figure 6. A scatter plot showing the behavior of read times (that is, decompression) versus the size of the compressed data Figure 7. A scatter plot showing the behavior of write times (that is, compression) versus the size of uncompressed data", "date": "2017-01-31"},
{"website": "Ebay-Engineering", "title": "Effective Front-End Code Review", "author": ["Michael Woo"], "link": "https://tech.ebayinc.com/engineering/effective-front-end-code-review/", "abstract": "Whether it is open-source or in-house code, front-end or back-end code, the pull request (PR) model is widely used in the industry. Before a PR is merged, it is reviewed by peers, and the code is modified by the developer as needed, in an iterative and interactive process. Ideally, we want to spend quality time reviewing each modified line (and any unmodified but associated code that may be impacted) so that we don’t defer and build tech debt when code is added or modified. However, with pressing timelines, sometimes we simply don’t have enough time for code reviews. And when we do have time, we may not be doing a good job because we don’t have a clear set of team coding guidelines or we may not know what we should look for. Below are some suggestions about how code review can be done, with specific front-end things to focus on. Thorough and meaningful code reviews take time. We need to scope for code review just like any other task. For that to happen, everyone must believe in its purpose and importance — that means management and product managers must actively encourage code reviews and give developers the necessary time. The team has agreed on naming convention, styling, and coding standards, whether it is HTML, CSS, or JS. Styling standards give the code a consistent and professional look. A naming convention makes it easier to understand, search, build, and maintain. Coding standards make the code cleaner and easier to read, build, maintain, and debug. There are many popular standards out there. It is less important which one you choose, but it is crucial that the entire team follows one standard. If a rule is controversial within the team, discuss and decide, then change the standard and apply the new rule across the entire code base. Consistency is key. The team has agreed on what to review — from spaces and camelCase, to where business logic should be, to whether the new code is violating established architecture. Does the code follow agreed-upon standards and conventions? Does the markup use the same element ID name more than once? If so, change it to a class because each ID should be unique. If a reusable module contains an element ID, the ID is no longer unique once the module is reused in the same page. Remember that getting an element by ID returns only the first element that matches, which may cause issues when you have multiple elements with the same ID name. Are there unnecessary wrapper elements? If a <div> does not have a class, or the <div> has a class but the class has no associated styles, chances are that it is not needed. Remove it and check the layout. If the layout does not change, you do not need that wrapper. If the app uses a template language that allows complicated logic, are you over-using it, thus making the markup hard to read? Consider applying the logic to the view model BEFORE the view model is passed to the template. Does the markup contain the proper ARIA attributes for accessibility? Does the code follow agreed-upon standards and conventions? If you are using a CSS precompiler, such as LESS or SASS , are you nesting classes (or even worse, nesting IDs) because it looks more organized? Unnecessary nesting makes overriding CSS difficult; it also hurts performance and maintainability. Is CSS in modules namespaced to avoid name clashing? Consider a top-level wrapper class with the same name as the module. If you don’t want class nesting, consider using BEM convention . Are some class names so generic that name clashes are likely? This can happen with either other modules in the same app or with other libraries that you may use. Are regularly used CSS rules (or blocks of rules) repeated so often that they should become mixins? If the app uses a CSS library, such as Bootstrap or Skin , are you maximizing its use? Does your app redefine mixins available in the library, or worse, does your app have blocks of CSS where using a single class from the library would do? Are padding and margin used correctly? Is padding used when margin should have been, or vice versa? Does using percentage instead of pixel make sense? How about rem vs. px ? Are there duplicate rules for the same selector? Remove the duplicates. Are the same rules found for different selectors in the same module? Merge them. Are the same rules found across different modules? Put them in a shared CSS file. Are there too many nested classes? Are they necessary for override? Try removing some nesting. If the layout doesn’t change, they are not needed. Are IDs used for CSS? IDs are HIGHLY specific and need other IDs to override them. And if you must insist on using IDs for CSS, DO NOT nest IDs. First, nested IDs require the same number of nested IDs or more to override, which makes your CSS longer and more complicated. Second, and this relates to fundamental CSS principle — IDs are by definition unique. If an ID needs to be nested to work, it’s not unique. It should not be an ID. Make it a class instead. If the code uses media queries, see the “Media queries” section in this post . Does the code follow agreed-upon standards and conventions? Is there JSDoc or similar documentation, especially for exposed methods? If something is to be fixed/enhanced later, is there a TODO with a developer’s name next to it? Is a solution either too clever or too complicated? Is there a slightly longer but simpler and more maintainable solution? If you have to read the code (or its comments) several times to understand what it is trying to do, it is probably too complicated and should be simplified. Does each method do one specific task only? If not, break it up into smaller, focused tasks. It will help in reusability and unit testing. Is a method particular to one use case, with hard-coded values and hard-coded CSS selectors, and so on? Or is it generic and reusable, and takes parameters? Generic methods are scalable and easier to maintain, AND they are much easier to unit test. Does a block of code look familiar from elsewhere? If an existing util method does the same, use it. If there is a util method that is related but slightly different, update it. If the util does not exist, create it. Then, use that util and remove identical and similar blocks globally. If an app already uses a utility library such as Underscore or Lodash , is the PR introducing another similar library? Discuss with your team which library best suits your app. Is the app loading a heavy library when you use only a few methods in it? Is there a lighter alternative? Can you pull out just what you need? Does a function take too many parameters? If there are more than 4 (or some number set by your team), use an object to avoid the hassle of parameter ordering and null values. Are values and data cached? Any string, object, etc., should be cached if it is used more than once. Are variable and function names so short that they are not meaningful at a glance? Longer names (be reasonable, of course) are more likely to be meaningful and self-documenting. Furthermore, searching globally for a longer name will return fewer results, which makes code-digging easier. Is there code that looks like the following? Use Lodash’s get (or similar method from another library) to get an object property down a path. You can avoid long chains (it’s easy to miss a check) and undefined exceptions when the response is not what you expect. if (a && a.b && a.b.c && a.b.c[0] && a.b.c[0].d ......) {\n    ...\n} If using third-party libraries, is the app using deprecated methods? Use the latest APIs as recommended by the libraries. Are all console logs and debuggers removed? Are listeners removed before being re-added? Does the handler run multiple times on the same event on the same element? One way to check is by printing logs in the handler. Pay attention to elements (usually inputs) that are listening to multiple events, such as keydown , keypress , and keyup . When one user action causes all events to fire, such that the handler is run once for each fired event, is the app still behaving as expected? Even if it is behaving correctly, does it make the app less responsive? If the app architecture is based on the modular pattern, is one module referencing or modifying DOM elements of an unrelated module, thus breaking the pattern? Are all jQuery object variables prefixed with $ for quick identification? Are jQuery objects cached, if used more than once? Are jQuery calls chained? Are selectors cached in a string variable if used more than once? When no element is found for a selector and your code operates on this “missing” selector, jQuery fails silently, which is both GOOD and BAD at the same time. It is good because no error is thrown at run time; it is bad because the app may fail in other ways. A common reason is that the selector was changed and it was not updated globally. Cache the selector to minimize that risk. Is the code using jQuery event delegation to minimize number of listeners? How are you listening to events on elements that can be re-rendered? Via jQuery event delegation? That way, you don’t have to set up listeners again every time the element is re-rendered. It is easy to miss the re-add, especially when there are multiple render paths. Are you using deprecated methods? For example, use .on() instead of .bind() / .live() . One difficulty in front-end development is getting a web app to work well across devices, operating systems, and browsers. With the large number of device, operating system, and browser combinations in the wild, including the old and the native, working well everywhere is an impossible task for developers. The reasonable approach is to get your app’s traffic breakdown and test the most popular combinations during development. To review layout and browser quirks, an in-person code review is the most effective as the reviewer can play with the feature/fix while reviewing the code. (A very simple exercise, which is also very effective in finding layout bugs, is resizing the browser window.) If an in-person review is not feasible, attach screenshots of the app to the PR, showing the layouts in different forms: Desktop browser Tablet browser (both portrait and landscape mode) Phone browser (both portrait and landscape mode) Code review is for more than the pull request at hand. Use it as a chance to refine the team’s standards and best practices and to discuss implementation strategies. While code review is an essential part of any development, it is especially important for front end as the front-end world changes very quickly. Team discussion via review comments is an effective way for all team members to get on the same page, learn from each other, and improve as a team. How much time do you spend on code reviews? Do you scope for it in your sprint planning? Do you already look at some of the things mentioned above? What other specific things do you focus on?", "date": "2017-03-02"},
{"website": "Ebay-Engineering", "title": "An Approach to Achieve Scalability and Availability of Data Stores", "author": ["Mansi Narula"], "link": "https://tech.ebayinc.com/engineering/an-approach-to-achieve-scalability-and-availability-of-data-stores/", "abstract": "Today there has been an explosion of the web, specifically in social networks and users of ecommerce applications, that corresponds to an explosion in the sheer volume of data we must deal with. The web has become so ubiquitous that it is used by everyone, from the scientists in 1990s, who used it for exchanging scientific documents, to five-year-olds today exchanging emoticons about kittens. There comes the need of scalability, which is the potential of a system, network, or process to be enlarged in order to accommodate that data growth. The web has virtually brought the world closer, which means there is no such thing as “down time” anymore. Business hours are 24/7, with buyers shopping in disparate time zones. Thereby, a necessity for high availability of the data stores arises. This blog post provides a course of action required to achieve scalability and availability for data stores. This article covers the following methods to provide a scalable and highly available data stores for applications. Scalability: a distributed system with self-service scaling capability Data capacity analysis Review of data access patterns Different techniques for sharding Self-service scaling capability Data capacity analysis Review of data access patterns Different techniques for sharding Self-service scaling capability Availability: physical deployment, rigorous operational procedures, and application resiliency Multiple data center deployment Self-healing tools Well-defined DR tiering , RTO , RPO , and SOPs Application resiliency for data stores Multiple data center deployment Self-healing tools Well-defined DR tiering , RTO , RPO , and SOPs Application resiliency for data stores With the advent of the web, especially Web 2.0 sites where millions of users may both read and write data, scalability of simple database operations has become more important. There are two ways to scale a system: vertically and horizontally. This talk focuses on horizontal scalability, where both the data and the load of simple operations is distributed/sharded over many servers, where the servers do not share RAM, CPU, or disk. Although in some implementations disk and storage can be shared, auto scaling can become a challenge for such cases. The following measures should be considered as mandatory methods in building a scalable data store. Data capacity analysis: It is a very important task to understand the extreme requirements of the application in terms of peak and average transactions per second, peak number of queries, payload size, expected throughput, and backup requirements. This enables the data store scalability design in terms of how many physical servers are needed and hardware configuration of the data store with respect to memory footprint, disk size, CPU Cores, I/O throughput, and other resources. Review data access patterns: The simplest course to scale an application is to start by looking for access patterns. Given the nature of distributed systems, all queries to the data store must have the access key in all real-time queries to avoid scatter and gather problem across different servers. Data must be aligned by the access key in each of the shards of the distributed data store. In many applications, there can be more than one access key. For example, in an ecommerce application, data retrieval can be by Product ID or by User ID. In such cases, the options are to either store the data redundantly aligned by both keys or store the data with a reference key, depending upon the application’s requirements. Different techniques for sharding: There are different ways to shard the data in a distributed data store. Two of the common mechanisms are function-based sharding and lookup-based sharding.Function-based sharding refers to the sharding scheme where a deterministic function is applied on the key to get the value of shard. In this case, the shard key should exist in each entity stored in the distributed data store, for efficient retrieval. In addition, if the shard key is not random, it can cause hot spots in the system.Lookup-based sharding refers to a lookup table used to store the start range and end range of the key. Clients can cache the lookup table to avoid single point of failure.Many NoSQL databases implement one of these techniques for achieving scalability. Self-service scaling capability: Self-service scaling, or auto-scaling, can work as a jewel in the scalable system crown. Data stores are designed and architected to provide enough capacity to scale up front, but rapid elasticity and cloud services can enable vertical and horizontal scaling in the true sense. Self-service vertical scaling enables the addition of resources to an existing node to increase its capacity, while self-service horizontal scaling enables the addition or removal of nodes in the distributed data store via “scale-up” or “scale-down” functionality. Data stores need to be highly available for read and write operations. Availability refers to a system or component that is continuously operational for a desirably long length of time. Below are some of the methods to ensure that the right architectural patterns, physical deployment, and rigorous operational procedures are in place for a highly available data store. Multiple data center deployment: Distributed data stores must be deployed in different data centers with redundant replicas for disaster recovery. Geographical location of data centers should be chosen cautiously to avoid network latency across the nodes. The ideal way is to deploy primary nodes equally amongst the data centers along with local and remote replicas in each data center. Distributed Data stores inherently reduces the downtime footprint by the sharding factor. In addition, equal distribution of nodes across data centers causes only 1/nth of the data to be unavailable in case of a complete data center shutdown. Self-healing tools: Efficient monitoring and self-healing tools must be in place to monitor the heartbeat of the nodes in the distributed data store. In case of failures, these tools should not only monitor but also provide a way to bring the failed component alive or should provide a mechanism to bring its most recent replica up as the next primary. This self-healing mechanism should be cautiously used per the application’s requirements. Some high-write-intensive applications cannot afford inconsistent data, which can change the role of self-healing tools to monitor and alert the application for on-demand healing, instead. Well-defined DR tiering , RTO , RPO , and SOPs : Rigorous operational procedures can bring the availability numbers (ratio of the expected value of the uptime of a system to the aggregate of the expected values of up and down time) to a higher value. Disaster recovery tiers must be well defined for any large-scale enterprise, with an associated expected downtime for the corresponding tiers. The Recovery Time Objective (RTO) and Recovery Point Objective (RPO) should be well tested in a simulated production environment to provide a predicted loss in availability, if any. Well-written SOPs are proven saviors in a crisis, especially in a large enterprise, where Operations can implement SOPs to recover the system as early as possible. Application resiliency for data stores: Hardware fails, but systems must not die. Application resiliency is the ability of an application to react to problems in one of its components and still provide the best possible service. There are multiple ways that an application can use to achieve high availability for read and write database operations. Application resiliency for reads enables the application to read from a replica in the case of primary failure. Resiliency can also be part of a distributed data store feature, as in many of the NoSQL databases. When there is no data affinity of the newly inserted data with the existing data, a round-robin insertion approach can be taken, where new inserts can write to a node other than the primary when the primary is unavailable. On the contrary, when there is data affinity of the newly inserted data with the existing data, the approach is primarily driven by consistency requirements of the application. The key takeaway is that in order to build a scalable and highly available data store, one must take a systematic approach to implement the methods described in this paper. This list of methods is a mandatory, comprehensive list, but not exhaustive, and it can have more methods added to it as needed. Plan to grow BIG and aim to be 24/7 UP, and with the proper scalability and availability measures in place, the sky is the limit. Scalable SQL and NoSQL Data Stores Rapid Elasticity and Cloud Availability Patterns and Guidance Image by freeimageslive.co.uk – freebie.photography", "date": "2017-03-23"},
{"website": "Ebay-Engineering", "title": "Email Tech Is Now Ad Tech", "author": ["Alex Weinstein"], "link": "https://tech.ebayinc.com/engineering/email-tech-is-now-ad-tech/", "abstract": "eBay has come a long way in our CRM and email marketing in the past two years. Personalization is a relatively easy task when you’re dealing with just one region and one vertical and a hundred thousand customers. With 167M active buyers across the globe, eBay’s journey to help each of our buyers find their version of perfect was quite complex. Like many in our industry, we’ve had to deal with legacy systems, scalability, and engineering resource constraints. And yet, we’ve made email marketing a point of pride — instead of the “check mark” that we started from. Here’s our story. Our starting point was a batch-and-blast approach. Our outbound communications very much reflected our organizational structure: as a customer, I’d get a fashion email on Monday, a tech email on Tuesday, and a motors email on Wednesday. This of course wasn’t the kind of an experience we wanted to create. Additionally, for each of our marketing campaigns, we hand-authored targeting criteria — just as many of our industry colleagues do today in tools like Marketo and ExactTarget . This approach worked OK, but the resulting segment size was too large — in hundreds of thousands. This meant that we were missing out on the opportunity to treat customers individually. It also didn’t scale well — as our business grew internationally, we needed to add more and more business analysts; and the complexity of our contact strategy was becoming unmanageable. We wanted to create a structurally better experience for our customers — and our big bet was to go after 1:1 personalization using real-time data . We wanted to use machine learning to do the targeting, with real-time feedback loops powering our models. Since email is such a powerful driver for eCommerce, we committed to a differentiated experience in this channel. After evaluating multiple off-the-shelf solutions, we settled on building an in-house targeting and personalization system — as the size of the eBay marketplace is astounding, and many opportunities and issues are quite unique. We set a high bar: every time we show an offer to a customer, it has to be driven by our up-to-the minute understanding of what the customer did and how other customers are responding to this offer. Here are some examples of the scenarios we targeted: eBay has many amazing deals, and our community is very active. Deals quickly run out of inventory. We can’t send an offer to a customer and direct them to an expired deal. Thus, our approach involved open-time rendering of offers in email. Some of our retail events turn out to be much more popular than we anticipate. We want to respond to this real-time engagement feedback by adjusting our recommendations quickly. We thus built a feedback loop that shows an offer to a subset of customers; then, if an event is getting a much higher click-through rate than we expected, we show it to more customers. If it for some reason isn’t doing well — for example, if the creative is underperforming — the real-time “ bandit ” approach reduces its visibility. Both of these scenarios required us to have real-time CRM and engagement streams . That is, we needed to know when a customer opens an email or clicks on it, and based on this knowledge, instantaneously adjust our recommendations for other customers. This of course is miles away from a typical multi-step batch system based on ETL pipelines that most retailers have today. We were no different — we had to reengineer our delivery and data collection pipes to be real-time. The payoff, however, is quite powerful — this real-time capability is foundational to our ability to personalize better: both today and in the years to come. The resulting solution transformed email marketing at eBay: instead of hundreds of small, uncoordinated campaigns each month, we now have a small set of “flagship” campaigns, each of which is comprised of one or more offers. Each of the offers is selected at the open time of the email, and the selection is made based upon machine-learned model which uses real-time data. As a result, we saw significant growth in both engagement and sales driven by our emails. You’ll notice that this component-level personalization approach is all about treating email content as an ad canvas. The problem is fundamentally similar: once you’ve captured the customers attention — be it via a winning bid on an ad auction, or by having that customer open your email — you need to find the most relevant offer to show. Each email slot can be thought of as a first-party ad slot. This realization allowed us to unify our approaches between display advertising and email: the same stack now powers both. We extended this approach to scenarios like paid social campaigns, where Facebook would want to retrieve the offer from us a priori to manage their customer experience. We built a real-time push framework, where, whenever we find a deal that is better than what we previously apprised Facebook of, we immediately push that offer to Facebook. This creates a powerful cross-channel multiplier: if we happen to see the customer on the display channel, the same ad-serving pipeline is engaged — and our flagship deal-finding campaign can be served to that customer, too. This means that evolving our flagship campaigns — adding more sophisticated machine learning, improving our creatives — contributes to all channels that are powered by this pipeline, not just email. Orchestration across channels too becomes possible: we can choose to send an email to a customer with a relevant offer; if they don’t open it, we can then target them with a display ad, and an onsite banner; then, after showing the offer a set number of times across all channels, we can choose to stop the ad — implementing an effective cross-channel impression cap. And each condition for state transitions in this flow can itself be powered by a machine-learning model. eBay’s scale creates an admirable engineering challenge for a true CRM. By putting our customers, and their behavioral signals, at the top of our priority list, we were able to create an asset in our CRM platform that positions us well towards In this journey towards 1:1 personalization. A single real-time, event-driven pipeline we’ve built allows for coordinated, up-to-the-minute offers to be served — wherever we happen to see the customer. Alex Weinstein ( @alexweinstein ) is the Director of Marketing Technologies and CRM at eBay and the author of the Technology + Entrepreneurship blog , where he explores data-driven decision making in the face of uncertainty. Prior to eBay, Alex was the head of product development at Wetpaint, a personalization tech startup. Graphic: Rahul Rodriguez", "date": "2017-03-28"},
{"website": "Ebay-Engineering", "title": "Coding Kata Month", "author": ["Ben Kelly"], "link": "https://tech.ebayinc.com/engineering/coding-kata-month/", "abstract": "I’m very lucky to be working at eBay with some of the most talented people I know. More fortunate still perhaps that they indulge me in my regular experiments in making our department a better place to work. I’ve been thinking about what I perceive as deficiencies in coding kata and talking to one of my colleagues at EPD (European Product Development) about this since maybe midyear 2016. As we talked about the similarities and differences in martial arts and coding kata, we began to explore what we might do in order to shift the needle on current coding kata practice. To that end, we kicked off ‘Kata Month’ in December. It was very much an exploratory exercise to see what would happen if we solved the same kata every day for a month. Rather than do a kata until it was ‘solved’, what if we practiced it daily and with a view to deliberately practicing elements of coding? Truth be told, it very nearly did not happen, and I owe thanks to my manager Paul Hammond, who pushed me to kick off the exercise despite not being completely prepared. My tendency is to over-engineer and given the various pressures of our day to day I’d likely have delayed until January or February to try and have everything as I wanted it. As it turned out, we had enough in place and so with pretty much zero notice, I sent out the following email in December week 1: Hi all, For the next four weeks in the London office, we’ll be holding Coding Kata Month. Each day between 11 – 12, you’ll have one hour in which to participate. (Instructions below for week 1) In martial arts, constant, deliberate practice of fundamentals is key to attaining mastery. In Kendo, there are 10 kata (interestingly, they are done in pairs) — effectively 20 movements to learn. When I first started kendo, the kata were the ‘boring’ bits that I had to do in order to do the fun stuff (beating someone with a stick). The more I did them though, the more I realised there was a richness in them that I hadn’t seen (or had wilfully ignored). Yes, the movements are choreographed, but an understanding of the fundamentals ingrained in them is crucial. There is correctness of physical form, but also distance, timing, and things that are more difficult to perceive without practice — reading your opponent, their movement, their breathing, gauging their readiness. Deliberate practice to improve these fundamentals is key. The same is true for any skill, be it a musical instrument, carpentry, ballet and also programming. For the next month, we’re going to delve into deliberate practice for programming through kata. Monday to Thursday are kata day (implementation). Friday will be for code review/debrief — an opportunity for people to talk about what they learned. Instructions: Each day between 11:00 – 12:00 sharp Complete the Harry Potter coding kata within the constraints set for that day/week. Each time you begin, start from scratch. Go to our GitHub kata repository. Create a new repo named day1-<my initials>[-<my pair's initials>] . Clone your new repo. Open your IDE of choice and create new project in your new repo. Code… Go to our GitHub kata repository. Create a new repo named day1-<my initials>[-<my pair's initials>] . Clone your new repo. Open your IDE of choice and create new project in your new repo. Code… Commit after each Red/Green/Refactor cycle. At the conclusion of the kata: Include a text file listing the participants. Record any thoughts you think are relevant: learnings, assumptions, gripes, ideas, notes for next time, etc. Commit the above notes along with your code. Include a text file listing the participants. Record any thoughts you think are relevant: learnings, assumptions, gripes, ideas, notes for next time, etc. Commit the above notes along with your code. Week 1 – Individual Practice Mon –> Thursday — Code solutions Choose your language — you will be sticking with this language for a while, so choose carefully! Repeat the kata each day. Use the same language, same IDE. Friday –> Code review (group) On Friday, we’ll get together as a group and talk about what we learned and look at some different examples of your solutions. Weeks 2–4 will change things up a little. Here’s a taste of what is to come: Week 2 — Pairing Week 3 — Design variation and mobbing Week 4 — Open Honestly, I was a little taken aback at how enthusiastically the initiative was picked up by the teams. I figured they might get a kick out of it, but they grabbed the idea and ran with it. They talked about it over lunch, they talked about it across teams. After a long and challenging year, it was great to see the crew jumping in with so much energy. I dived in with equal enthusiasm. Honestly, I’d not coded in anger in well over a year, and I was painfully rusty. On day one, I realised how much I’d forgotten about TDD and got an embarrassingly small amount of code written. On day 2, I sort of hit my groove and worked out where I wanted to go with a solution. On day 3, I’d nailed a working solution to the problem, and by day 4, I knocked it out in about 20 minutes and started looking at how to evolve the data structures I’d chosen to make my solution extensible. I was feeling pretty good about myself. I sat down to pair with one of our programmers in week 2. At the end of the first session I had the humbling experience of seeing just how much I had to learn about TDD (not to mention intentional programming and various design patterns). The other thing it did was make me realise just how rich this area of kata could be. Having an interesting problem to solve was one thing, but putting together a repeatable solution that incorporates a contextually appropriate use of both fundamental and advanced programming skills has so much potential. I won’t give you a detailed rundown of the entire month; suffice it to say there were some interesting things to come out of it. Some of them code-related, some not. For example, we stipulated one hour for kata between 11 and 12 (just before most people go to lunch). The consensus was after a couple of weeks that this was quite disruptive to the day overall. The teams had standup in the morning, then a small amount of time to work before kata started, then lunch and then the afternoon. Productivity-wise, there was the general feeling that half the day was gone before any project work got done. For future iterations of kata month, we’ll kick off the day with kata. If nothing else, at least that way folks are starting the day writing code — something that you don’t always get to do despite best intentions. Another interesting thing that came out of our Friday review sessions was that some people were bored after ‘solving’ the kata. This was what I really wanted to address — that kata are not a thing to be ‘solved’, but a way to practice fundamentals. To some extent this was helped by the variety from week to week (individual, pairing, mobbing, etc.), but we also discussed using the time to work on weak points or selecting a different approach to solving the problem or even making more effective use of the IDE to do some of the heavy lifting. In hindsight, this might have been different if I’d spent more time setting the scene at the beginning, explaining how kata work in martial arts and what I was expecting. It also helped reinforce to me the importance of having a repeatable solution in place. Having a repeatable solution takes the ‘solving’ part out of the equation and lets you focus on practice of implementing a solution (more on that in a future post). At the end of the month, I ran a retro and put out a survey to the participants. I’d like to share some of the responses. It was interesting to see the various viewpoints of the people that participated, what their preconceptions and assumptions were, and how they changed over time. As far as our Friday sessions went, they were quite unstructured and in hindsight we could have made a lot more of them. We looked through some code, but with the exception of week 3 where we did an impromptu mobbing session, we didn’t really demo any writing of code. Given my views on kata as a visual teaching and learning aid, that feels like an opportunity missed. Setting expectations early on was also a recurring theme. I think there is a place for some amount of ritual to designate a mental shift required for working on kata. It need not be elaborate, but something that puts the practitioner in the mindset of deliberate practice. In that way, the goal and the aim is clear — execute the kata in order to practice your fundamentals. We talked also about the fact that this was a ‘greenfields’ kata and that it might be useful to try to do a kata along similar lines that was refactoring existing code that had issues of varying kinds. There are refactoring kata out there, but I quite like the idea of having kata that exist in pairs to exercise similar principles in both greenfields and brownfields situations, possibly even having kata whose solution works for one situation but needs refactoring to suit another. There are subtly different skills involved in selecting a particular design pattern to implement a solution versus recognising when existing code should be refactored to use that pattern. Since kata month finished, I’ve put together a small working group of interested folks with the aim of putting together some kata of our own. We’re working to that end now, to come up with a problem and a solution that is representative of the skills required by an EPD programmer. My intention, once we have something that works for us, is to then share those with the wider world. In the meantime, there is no shortage of kata ‘problems’ out there, but very few of them are accompanied with a solution. About the only one that springs to mind is Bob Martin’s Bowling Kata . I think there is certainly scope for other existing kata to similarly have repeatable solutions designed for them — not simply ‘solved’, but achieving a repeatable solution deliberately designed to exercise fundamentals and good design principles in context.", "date": "2017-03-06"},
{"website": "Ebay-Engineering", "title": "Healthy Team Backlogs", "author": ["Chris Gagné"], "link": "https://tech.ebayinc.com/engineering/healthy-team-backlogs/", "abstract": "Agile product owners use a backlog to organize and communicate the requirements for a team’s work. Product backlogs are deceptively simple, which can sometimes make them challenging to adopt for product owners who may be used to working with lengthy PRDs (“project requirement documents” or similar). Scrum most commonly uses the term product backlog . However, many product owners who are new to Scrum are confused by this term. Reasonable questions arise: Does this suggest that a team working on multiple products would have multiple backlogs? If so, how do we prioritize between them? Where do bugs get recorded? What happens if work needs to be done, but it isn’t associated with a product; do we create a placeholder? Therefore, we prefer the term team backlog . Our working definition of team backlog is “the maintained, ordered list of work that the team plans to do now or in the future.” This is a dense description, so let’s unpack it a little. We say the and team because each team needs a single source of truth to track their current and future work. If a team is working on multiple projects or products, all of the work for those stories should appear on a single, unified, team backlog. Teams do not generally share backlogs. Work includes almost everything that the development team needs to do. Features, bugs, technical debt, research, improvements, and even user experience work all appear on the same backlog. Generally speaking, recurring team meetings and similar events do not appear on the backlog. We say maintained because the backlog is a “living” artifact. The product owner and team must continually update and refine their backlog. Otherwise, the team will waste time doing useless work and chasing requirements. This requires several hours per week for the product owner and 1–2 hours per week for the team. It involves adding, ordering, discussing, describing, justifying, deleting, and splitting work. We say ordered list rather than prioritized list because the backlog is ordered, not just prioritized. If the backlog is only prioritized, there can be multiple items that are all “very high priority.” If the backlog is ordered, we communicate exactly in what order those “very high priority” tasks should be worked on. We say plans to do because we regularly delete everything from the backlog that we no longer plan to work on. Deleting unnecessary work is essential. Unnecessary work clutters up our backlog and distracts from the actual work. Now that we know what a backlog is, what makes a backlog healthy or not? While what makes for a good backlog is somewhat subjective — in the same way that what makes a good PRD could be subjective — there are 10 characteristics that we’ve found to be particularly important. Would you like to know if your backlog is healthy? Download this handy PDF checklist , print it out, then open up your backlog and follow along. For each criterion, take note of whether your backlog currently does, doesn’t, or only somewhat meets the criterion. In exchange for less than half an hour of your time, you’ll have good sense as to the health of your backlog and a few ideas for improvement. Focused, ordered by priority, and the team follows the order diligently At all times, anyone can look at the backlog and know what needs to be worked on next without ambiguity. Even if you have several “P1” issues, the team needs to know which P1 issue needs to be addressed next. Simply saying “they’re all important” will paralyze the team. Although the PO is responsible for the product backlog order and makes the final call, the PO should be willing to negotiate the order with their team. The team often has good insights that can mitigate dependencies or help the PO deliver more value. Stay focused on one thing at a time when possible to deliver value earlier and reduce context switching waste. At all times, anyone can look at the backlog and know what needs to be worked on next without ambiguity. Even if you have several “P1” issues, the team needs to know which P1 issue needs to be addressed next. Simply saying “they’re all important” will paralyze the team. Although the PO is responsible for the product backlog order and makes the final call, the PO should be willing to negotiate the order with their team. The team often has good insights that can mitigate dependencies or help the PO deliver more value. Stay focused on one thing at a time when possible to deliver value earlier and reduce context switching waste. Higher-value items towards the top, lower-value items towards the bottom In general, do high-value, low-cost work first (“lowest hanging fruit”). Next, do high-value, high-cost work because it is usually more strategic. Then, do low-value, low-cost work. Finally, eliminate low-value, high-cost work. You will almost always find something better to do with your time and resources, so don’t waste your time tracking it. It will be obvious if and when that work becomes valuable. Hint: You can use Weighted Shortest Job First or a similar technique if you’re having difficulty prioritizing. In general, do high-value, low-cost work first (“lowest hanging fruit”). Next, do high-value, high-cost work because it is usually more strategic. Then, do low-value, low-cost work. Finally, eliminate low-value, high-cost work. You will almost always find something better to do with your time and resources, so don’t waste your time tracking it. It will be obvious if and when that work becomes valuable. Hint: You can use Weighted Shortest Job First or a similar technique if you’re having difficulty prioritizing. Granular, ready-to-work items towards the top, loosely-defined epics towards the bottom Items that are at the top of the backlog will be worked on next, so we want to ensure that they are the right size to work on. The typical team’s Definition of Ready recommends that items take ≤ ½ of a sprint to complete. Delay decision-making and commitments — manifested as small, detailed, team-ready items — until the last responsible moment. There is little value specifying work in detail if you will not work on it soon. Due to learning and changing customer/company/competitive conditions, your requirements may change or you may cancel the work altogether. What is an Epic? An “epic” is simply a user story that is too large to complete in one sprint. It gets prioritized in the backlog like every other item. JIRA Tip: “Epics” in JIRA do not appear in the backlog for Scrum boards. As a result, they behave more like organizing themes than epics. Therefore, we suggest using JIRA’s epic functionality to indicate themes and user stories with the prefix “Epic: ”  to indicate actual epics. Items that are at the top of the backlog will be worked on next, so we want to ensure that they are the right size to work on. The typical team’s Definition of Ready recommends that items take ≤ ½ of a sprint to complete. Delay decision-making and commitments — manifested as small, detailed, team-ready items — until the last responsible moment. There is little value specifying work in detail if you will not work on it soon. Due to learning and changing customer/company/competitive conditions, your requirements may change or you may cancel the work altogether. What is an Epic? An “epic” is simply a user story that is too large to complete in one sprint. It gets prioritized in the backlog like every other item. JIRA Tip: “Epics” in JIRA do not appear in the backlog for Scrum boards. As a result, they behave more like organizing themes than epics. Therefore, we suggest using JIRA’s epic functionality to indicate themes and user stories with the prefix “Epic: ”  to indicate actual epics. Solutions towards the top, statements of need towards the bottom Teams can decide to start working on an item as soon as they know what customer needs they hope to solve. However, collaborating between product, design, development, and stakeholders to translate customer needs into solutions takes time. As with other commitments, defer solutioning decisions until the last responsible moment: Your ideal solution may change through learning or changing conditions such as customer, competitors, company, or even technology options. You may decide not to work on the problem after all. Teams can decide to start working on an item as soon as they know what customer needs they hope to solve. However, collaborating between product, design, development, and stakeholders to translate customer needs into solutions takes time. As with other commitments, defer solutioning decisions until the last responsible moment: Your ideal solution may change through learning or changing conditions such as customer, competitors, company, or even technology options. You may decide not to work on the problem after all. Your ideal solution may change through learning or changing conditions such as customer, competitors, company, or even technology options. You may decide not to work on the problem after all. 1½ to 2 sprints worth of work that’s obviously ready to work on at the top Teams sometimes surprise the product owner by having more capacity by expected. Having enough ready stories ensures that the team is: Unlikely to run out of work to pull into their sprint backlog during sprint planning. Able to pull in additional work during the sprint if they complete the rest of the work on their sprint backlog. It should be obvious what work is and isn’t ready to work on so that the team doesn’t have to waste time figuring it out each time they look at the backlog. Some teams prefix a story title with a “* ” to indicate a ready story (or a story that isn’t ready). Teams sometimes surprise the product owner by having more capacity by expected. Having enough ready stories ensures that the team is: Unlikely to run out of work to pull into their sprint backlog during sprint planning. Able to pull in additional work during the sprint if they complete the rest of the work on their sprint backlog. Unlikely to run out of work to pull into their sprint backlog during sprint planning. Able to pull in additional work during the sprint if they complete the rest of the work on their sprint backlog. It should be obvious what work is and isn’t ready to work on so that the team doesn’t have to waste time figuring it out each time they look at the backlog. Some teams prefix a story title with a “* ” to indicate a ready story (or a story that isn’t ready). Some teams prefix a story title with a “* ” to indicate a ready story (or a story that isn’t ready). The value of each piece of work is clearly articulated Your team should be able to understand why the work is important to work on. There are three primary sources of value (and you can define your own): User/Business Value : Increase revenue, reduce costs, make users happy Time Criticality : Must it happen soon due to competition, risk, etc.? Opportunity Enablement/Risk Reduction/Learning : Is it strategic? Is it necessary to enable another valuable item (for example, a dependency)? You won’t usually need a complex financial projection, just a reasonable justification as to why the item should be worked on next relative to all other known possibilities. Time previously spent with complex projections can instead be used to talk to customers and identify other opportunities. Your team should be able to understand why the work is important to work on. There are three primary sources of value (and you can define your own): User/Business Value : Increase revenue, reduce costs, make users happy Time Criticality : Must it happen soon due to competition, risk, etc.? Opportunity Enablement/Risk Reduction/Learning : Is it strategic? Is it necessary to enable another valuable item (for example, a dependency)? User/Business Value : Increase revenue, reduce costs, make users happy Time Criticality : Must it happen soon due to competition, risk, etc.? Opportunity Enablement/Risk Reduction/Learning : Is it strategic? Is it necessary to enable another valuable item (for example, a dependency)? You won’t usually need a complex financial projection, just a reasonable justification as to why the item should be worked on next relative to all other known possibilities. Time previously spent with complex projections can instead be used to talk to customers and identify other opportunities. The customer persona for the work is clearly articulated The “As a” part of the “As a ____, I can ___, so that ____” user story isn’t a mere formality; it’s an essential part of user-centered product development. Who is the customer? Who are you completing this work for? Even if you’re on a “back-end” team, keep the end-user in mind. Partner with your designer to identify your personas and reference them whenever possible. Is this feature for “Serious Seller Sally?” Can you imagine her personality and needs just as well as any of your friends? Example: “As Serious Seller Sally, I can list items using a ‘advanced’ flow so that I can get the options I need without the guidance for casual-sellers that only slows me down.” Tool Tip: Most teams and POs find it best to put just the “I can” part the user story (for example, “List items using a ‘advanced’ flow”) in the planning tool’s title field. Otherwise it can be harder to read the backlog. Put the entire user story at the top of your tool’s description field. The “As a” part of the “As a ____, I can ___, so that ____” user story isn’t a mere formality; it’s an essential part of user-centered product development. Who is the customer? Who are you completing this work for? Even if you’re on a “back-end” team, keep the end-user in mind. Partner with your designer to identify your personas and reference them whenever possible. Is this feature for “Serious Seller Sally?” Can you imagine her personality and needs just as well as any of your friends? Example: “As Serious Seller Sally, I can list items using a ‘advanced’ flow so that I can get the options I need without the guidance for casual-sellers that only slows me down.” Example: “As Serious Seller Sally, I can list items using a ‘advanced’ flow so that I can get the options I need without the guidance for casual-sellers that only slows me down.” Tool Tip: Most teams and POs find it best to put just the “I can” part the user story (for example, “List items using a ‘advanced’ flow”) in the planning tool’s title field. Otherwise it can be harder to read the backlog. Put the entire user story at the top of your tool’s description field. ≤ 100 items (a rule of thumb), and contains no work that — realistically — will never be done This is a general rule. If your team works on many very small items or has considerable work that you must track, your backlog could be longer. Assuming that each backlog item takes a minute to read and understand, 100 items alone would take over an hour and a half to process. Keeping our backlog limited like this makes it easier and faster to fully understand. A longer backlog is more likely to contain features that will never be built or bugs that will never be fixed. Keeping a short backlog helps us ensure that we triage effectively and delete items that we are unlikely to work on. This is a general rule. If your team works on many very small items or has considerable work that you must track, your backlog could be longer. Assuming that each backlog item takes a minute to read and understand, 100 items alone would take over an hour and a half to process. Keeping our backlog limited like this makes it easier and faster to fully understand. A longer backlog is more likely to contain features that will never be built or bugs that will never be fixed. Keeping a short backlog helps us ensure that we triage effectively and delete items that we are unlikely to work on. The team backlog is not a commitment A Scrum team cannot make a realistic, firm commitment on an entire team backlog because: It has not been through high-level design (for example, tasking at end of Sprint planning). The risk of missed dependencies and unexpected requests/impediments is too great. “Locking in” a plan that far into the future considerably restricts flexibility A Scrum team can make a valid commitment on a sprint backlog if there are no mid-sprint scope changes and few unexpected requests and impediments. A Scrum team cannot make a realistic, firm commitment on an entire team backlog because: It has not been through high-level design (for example, tasking at end of Sprint planning). The risk of missed dependencies and unexpected requests/impediments is too great. “Locking in” a plan that far into the future considerably restricts flexibility It has not been through high-level design (for example, tasking at end of Sprint planning). The risk of missed dependencies and unexpected requests/impediments is too great. “Locking in” a plan that far into the future considerably restricts flexibility A Scrum team can make a valid commitment on a sprint backlog if there are no mid-sprint scope changes and few unexpected requests and impediments. Backlog reflects the release plan if available If the team has conducted release planning, create pro forma sprints with items in your planning tool to reflect the release plan. If there are production release, moratorium, or similar dates, communicate those too. Update the release plan at end of each sprint as you learn. If the team has conducted release planning, create pro forma sprints with items in your planning tool to reflect the release plan. If there are production release, moratorium, or similar dates, communicate those too. Update the release plan at end of each sprint as you learn. Glad you asked. Here are four sample “sprints” that take good advantage of JIRA’s built-in functionality. Sprint 1 (active sprint) Sprint 2 (next sprint) Sprint 3 (future sprint) Sprint 4 (future sprint) Now you know what a healthy team backlog looks like. If you’ve filled out our printable checklist, mark off up to three items that you’ll work to improve over the next week or two with your teams. We hope this is of use to you!", "date": "2017-03-30"},
{"website": "Ebay-Engineering", "title": "Rheos", "author": ["Connie Yang"], "link": "https://tech.ebayinc.com/engineering/rheos/", "abstract": "Data IS the next currency.  The increased demand for real-time data across almost every business and technology platform has changed the world we live in. It is no different at eBay. Data IS the next currency.  The increased demand for real-time data across almost every business and technology platform has changed the world we live in.  It is no different at eBay. About two years ago, I was thrilled when I was asked to lead a development team to build a real-time data platform at eBay using Kafka . Initially, it was just for our Oracle change stream. In late 2015, we decided to expand it to a fully managed, secure, and easy-to-use real-time data platform, known as Rheos. The goal of Rheos is to provide a near real-time buyer experience, seller insights, and a data-driven commerce business at eBay. While Kafka has given us core capabilities in stream processing, managing a large, distributed, highly available, real-time data pipelines running on the cloud across security zones and data centers is hard without automation and core services. Hence, Rheos was built to provide the necessary life-cycle management, monitoring, and well-architected standards and ecosystem for the real-time streaming data pipelines. Currently, the pipelines consist of Kafka, Storm and stream processing applications. Shared and non-shared data streams can be running on these pipelines. By the end of 2016, nearly 100 billion messages flowed through the pipelines in Rheos daily. In 2017, Rheos is expected to handle 15 times the current traffic. So, how did we get there? Concepts At a very high level, Rheos has these concepts: Data taxonomy is a well-defined convention that classifies and catalogs events into proper namespaces for organizational, ease of discovery, and management purposes. Category is a top-level component in a namespace for a given stream type, for example, monitoring events, click stream events, business events, and so on. Stream captures the logical data flow that leads to a consumable data point in Kafka. The data flow may cut across one or more data points and stream processing units. Domain represents a shard or a group of related topics for a given stream type. Topics in the group are subject to a set of control parameters such as max partitions, max replica, max data retention period, max topic count, and service level agreement, just as examples. Namespace is used to classify the different data streams in Rheos. A namespace is composed of category, stream, and domain Lifecycle Management Service Lifecycle Management Service is a cloud service that provisions and provides full lifecycle management (LCM) for Zookeeper , Kafka, Storm, and MirrorMaker clusters. It is built on a modular architecture with a pluggable extension and frameworks. This combination allows it to create and perform LCM on a stream pipeline running on any cloud platforms (such as OpenStack , AWS , Google Cloud ). The Lifecycle Management Service allows you to provision, flex up/down a cluster, or replace a bad node in a cluster. In addition to its CLI API, it is equipped with a RESTful API that allows Rheos Management Service (see the Core Service below) to perform simple operation on a guest instance. For example, the management service can do a rolling start on a troubled Kafka cluster via the Lifecycle Manager API. Lifecycle Management Service architectural building blocks consist of these components API Server (REST and CLI) — a thin layer that parses, validates, and forwards requests to Task Manager Task Manager (RPC) — a stateful service that creates and executes orchestration workflows on a cluster of nodes Conductor — a component that is responsible for receiving heartbeat information from the guest instances Guest Agent — A lightweight agent that runs on the guest instance; responsible for executing a command from the Task Manager on the instance as well as sending heartbeat metrics to the Conductor Message Queue — a scoped, controlled, and secured way for the communication between the API Server, Task Manager, Conductor and the Guest Agent The pluggable extension includes these functions: Workflow Monitoring and metrics emitter and aggregator Authentication and authorization Configuration management IaaS (the underlying compute, storage, resource management, etc.) Core Service Rheos core service consists of the following components: Kafka Proxy Server, Schema Registry Service, Stream Metadata Service, and Management Service. The following picture captures how these components interact with each other. Rheos Kafka Proxy Server One of Rheos’ key objectives is to provide a single point of access to the data streams for the producers and consumers without hard-coding the actual broker names. This allows any open-source Kafka connectors, framework, and Kafka clients written in any programming language to seamlessly produce or consume in Rheos. To do this, we created a Rheos Kafka Proxy Server that handles Kafka TCP Protocol so that the Proxy Server can intercept any initial connection requests from the clients. Upon receiving the initial connection requests, the Proxy Server identifies which Kafka cluster the topic resides on via the Rheos Metadata Service (described below). Then, the actual broker cnames will be returned to the clients so that the clients can complete the final connection handshake with the brokers. In addition, Rheos Kafka Proxy Server also allows operations to easily replace a bad node or move a topic from one Kafka cluster to another with very little to no impact to the clients. Schema Registry Service To promote data hygiene in Rheos and ease of use for both stream producer and consumer, each event in Rheos must be identifiable with an Avro schema. Rheos has built a Schema Registry Service based on confluent.io Schema Registry . This service hosts data format definition, provides schema versioning and serialization information for each event type. In addition, Rheos users can view, insert, and update the schemas in the registry. Rheos Metadata Service Stream Metadata Service provides a system of record for each stream and the associated producer and consumer(s) that are known to the system. Prior to producing to or consuming from a stream, one must “register” the Kafka topic along with the associated schema, stream producer, and consumer with the Metadata Service. With this, Kafka topics, broker list along with the associated schemas can easily be discovered or browsed via Rheos REST API or Portal. More importantly, no hard coding of broker names in the client code ! In addition, the Metadata Service also makes it possible for our Management Service and Health Check System to seamlessly monitor, alert, and perform life cycle management operations on streams and the infrastructure that the streams run on. The recorded information includes the following items: The physical (cluster) location of a topic or a stream processing job/topology Data durability, retention policy, partition, producer, and consumer information Source and target data mirroring information Default configuration for Zookeeper, Kafka, and Storm Topic schema information And more Management Service Rheos performs stream, producer, and consumer life cycle management operations with a set of predefined Standard Operating Procedure (SOP) in the Management Service. Each SOP has a series of steps that can be performed on a guest instance via the Lifecycle Management Service. For example, Operations can initiate a rolling restart of a Kafka cluster using one of the SOPs. Health Check System This service monitors the health of each asset (for example, a Kafka, Zookeeper, or MirrorMaker node) that is provisioned through the Lifecycle Management Service in these aspects: Node state (up or down) Cluster health Producer traffic, consumer lags, or data loss It periodically samples data from Kafka topics, performs consumer lag checks, and end-to-end latency checks via Management Service. Upon anomaly or error detection, the service generates an alert via email and/or to eBay Operations. In addition, the Health Check Service records a consumer’s current offset with a timestamp in the primary and the secondary Kafka clusters. Producer traffic Producer traffic is closely monitored and can be viewed on the Rheos Portal. To provide a quick visual for a producer’s traffic trending or pattern, the current traffic volume of a stream domain (aka topic group with selected or all partitions) is overlaid on top of its yesterday’s traffic pattern. This way, one can quickly detect if there’s an anomaly with the current traffic. End-to-end latency A popular question everyone wants to ask is the data end-to-end latency or consumer lags in a stream pipeline. Rheos Health Check System provides a stream domain’s end-to-end latency by measuring two periods of time: From when an event is published to Kafka to the time when the event is consumed by a consumer From when an event is published to Kafka to the time when the broker writes to disk Stream consistency check To quickly remediate a problem in a stream, the Health Check System proactively monitors a set of in-sync replicas (ISR) for a given topic in a stream. In addition, it also ensures that the stream that the topic goes through is consistent spanning across one or more Kafka clusters. Node status Last but not the least, our Health Check System also monitors the state of each node in Rheos. At a high level, it provides a quick overview of the cluster health by checking these conditions: Whether a node is reachable or not Whether the primary workload (broker, Zookeeper, etc.) is running or not on a reachable node or not Whether a randomly selected node in a cluster can properly fulfil a request or not Rheos Mirroring Service In addition to Kafka’s cluster replication, Rheos Mirroring Service provides high data availability and integrity by mirroring data from source cluster to one or more target clusters. Built around Kafka’s MirrorMaker, the service is used to set up MirrorMaker instances and mirror a group of topics from one cluster to another via a REST API. Through the API, one can start and stop the mirroring of a topic group. Rheos Mirroring Service consists of these key components: Asset Agent is co-located on a mirroring compute node and responsible for reporting heartbeat metrics to a State Store. Mirror Manager is a REST service that starts and stops the mirroring of a topic group. It is equipped with the intelligence to properly distribute the MirrorMaker instances across the cluster based on a distribution strategy. Configurator is an Ansible playbook that resides on each MirrorMaker node. It is responsible for these functions: Creating the required Kafka producer/consumer properties for a topic group Creating the required directory structure for the instance along with the supervisor configuration Starting or stopping the MirrorMaker instance based on the given source to target mirroring configuration Creating the required Kafka producer/consumer properties for a topic group Creating the required directory structure for the instance along with the supervisor configuration Starting or stopping the MirrorMaker instance based on the given source to target mirroring configuration Mirror Bootstrap is a thin Java wrapper that registers and deregisters the MirrorMaker instance in the State Store prior to interacting with the underlying Mirror Maker instance. This allows us to capture the physical and the logical data mirroring activities. Using the Mirroring Service to achieve high availability As shown below, data can be mirrored from one region or availability zone to one or more regions or availability zones for highly availablity reasons. To do that, MirrorMaker instances are set up in the target locations to consume data from a source cluster and subsequently publish to target clusters. Using the Mirroring Service to move data across security zones In addition, Data Mirroring is used to provide data movement from one security zone to another. As shown below, MirrorMaker instances are set up in the target security zone to consume data from the source security zone over a TLS connection and subsequently publish the received data to the target clusters. How to access Kafka securely? To acquire a broker connection, a Rheos client must be authenticated by the eBay Identity Service via the Kafka SASL mechanism. Upon authentication, the client is then further authorized through Kafka’s default pluggable Authorizer via Zookeeper. In some cases, such as moving data across security zones, TLS is also enabled at the connection level. Conclusion Rheos has opened a new chapter in many aspects at eBay.  With Rheos, eBay data can now be securely extracted and moved from a data store, application, or other source to one or more locations in a real-time manner.  Stream processing has opened up new possibilities for eBay businesses, fraud detection, monitoring, analytics, and more at eBay.", "date": "2017-03-14"},
{"website": "Ebay-Engineering", "title": "Correcting Severe Errors Made by Machine Translation", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/engineering/correcting-severe-errors-made-by-machine-translation/", "abstract": "This article intends to show a few examples of severe errors made by machine translation engines that most of us want to prevent or correct. First, I will try to categorize what I would consider as a severe error created by MT: Errors with economic consequences for the company Errors due to offensive words Errors with legal or safety consequences Economic consequences come from errors that prevent a customer from doing business with the company. For eBay, these would be mostly issues that prevent buyers from buying. Customers of eBay start buying by entering a query to search for the items that they want to buy. That query entered in their native language is translated into English, for example, and then the English query goes on to find items. So it is critical that the translation of the query is appropriate to find the best results possible. When a query is translated in a way that does not bring results, this becomes a severe error because the customer is not buying from that translation. Our example comes from Brazilian Portuguese: when searching for an iPhone case, Brazilians will enter the term “ capinha ” for “case”, which is a diminutive form of the word. Most of the corpora used to train most MT engines may come from formally written sources, and these sources do not use diminutives very often. As a result, the translation of “ capinha ” from Portuguese into English may not translate into ”case”; actually, it may not translate it at all. The query for that search produces no results and this becomes an important error. This is something that we fixed and made our Brazilian customers happy. Another type of error that could have economic consequences would be the translation of “free shipping” as “paid shipping”, or the translation of “seller pays for shipping” as “buyer pays for shipping”. This could result in less buying. However, we haven’t seen this happen. Words could be offensive or inappropriate for being explicit language or have sexual connotations. We have these examples: Consider the word “female”. In many languages, the word female is translated differently if you are referring to a person or to an animal (or a mechanical part). If you are referring to a person, the translation for “female jacket” should sound like “feminine jacket” or “jacket for women”. If you are referring to an animal, the word is more on the anatomic side, expressing the idea of something being physically female. Preferably, the MT engine should not translate a female jacket as “anatomically” female and should translate with the meaning of a style. This is an issue that we found across several languages. To illustrate this, here is what happens in two languages. The other example is language-specific. There is a doll called Adora doll, where Adora is a brand. It turns out that adora is a word in Portuguese that means that you “adore”, that you “love” someone. The translation for “ boneca Adora ” coming from Portuguese turned into “love doll” in English, and the results of a search for “love doll” may not be the most suitable if someone is looking for a doll for children. Errors of legal or safety nature could come from converting units of measurement from one system (English units) to another (metric). This kind of issue is critical in medical translations, where the dosage of a medication in the English system turned into a metric unit, keeping the same number, could risk a person’s life! This is less likely to be an issue on an e-commerce environment. Also, our engine is trained to keep the same unit system, so we have not seen issues of that nature. If you know other examples of severe errors made by machine translation, we would love to hear from you and improve our system. And if you enjoyed this article, please check other posts from the eBay MT Language Specialists series.", "date": "2016-06-01"},
{"website": "Ebay-Engineering", "title": "Practical NoSQL resilience design pattern for the enterprise", "author": ["Donovan Hsieh", "Feng Qu"], "link": "https://tech.ebayinc.com/engineering/practical-nosql-resilience-design-pattern-for-the-enterprise/", "abstract": "Building applications resilient to infrastructure failure is essential for systems that run in a distributed environment, such as NoSQL databases. For example, failure can come from compute resources such as nodes, local/remote clusters, network switches, or the entire data center. On occasion, NoSQL nodes or clusters may be marked down by Operations to perform administrative tasks, such as a software upgrade and adding extra capacity. In this blog, you can find information on how to build resilient NoSQL applications using appropriate design patterns that are suitable to your needs. Note that the resilience design pattern described in this blog is complementary to other patterns, such as the Hystrix library . The difference is that it focuses on the following abstractions: NoSQL database architecture and capability abstraction, such as replication strategy, durability, and high availability Infrastructure and environment abstraction, such as standard minimal deployment, disaster recovery, and continuous operation Application abstraction, such as workload, performance, and access patterns Enabled by more efficient distributed consensus protocols, such as Paxos and Raft, and new thinking, such as flexible database schema, NoSQL databases have grown out of early adoption and are gaining popularity among mainstream companies. Although many NoSQL vendors tout their products as having unparalleled web-level scalability and capabilities such as auto-failover, not all are created equal in terms of availability, consistency, durability and recoverability. This is especially true if you are running mission-critical applications using heterogeneous NoSQL systems that include key-value (KV) pairs, column families (CF), and document and graph databases across a geographically dispersed environment. It becomes imperative that a well-run enterprise optimizes its management to achieve the highest operational efficiency and availability. To accomplish this, an enterprise should not only empower application developers with well-defined NoSQL database resilience design patterns so that they can be relieved from preventable infrastructure failure, but it should also provide a guaranteed-availability Service Level Agreement (SLA) on these resilience design patterns. Given the enormous complexity associated with integrating an enterprise-class infrastructure, such as eBay, developing meaningful and usable NoSQL resilience pattern is more than just one dimension — it comprises a multitude of coherent and intricately interconnected cogs such as these: Use case qualification Application persistence error handling Technology stack and engineering framework Data center infrastructure (for example, public/private/hybrid cloud) configuration and setup NoSQL database-agnostic and -specific resilience abstraction Operation and management best practice and standard operation procedure (SOP) Since NoSQL databases are not panaceas, ill-suited applications can disrupt operations and cause friction between stakeholders within the organization. The first step toward defining a meaningful NoSQL database resilience pattern is to ensure that only qualified use cases will use it. With that, we recommend the following guidelines when qualifying new NoSQL use cases. One main objective of a NoSQL resilience design pattern is that it should support a wide range of use cases across different NoSQL databases. To achieve this objective, we devised the following steps in our approach: Identify a meaningful NoSQL database architectural abstraction based on the CAP theorem , ACID/BASE properties , and performance characteristics. Categorize and define types of different resilience patterns based on workload, performance, and consistency properties that are meaningful to applications. Define a standardized minimal NoSQL deployment pattern for common small-to-medium, non-mission critical use cases. Define an enhanced design pattern to support mission-critical use cases that require high availability, consistency, durability, scalability, and performance. Define other design patterns to support non-conforming use cases, for example, standalone without disaster recovery (DR), and application sharding. Given that JSON has become the de facto standard data format for web-centric e-commerce businesses like eBay, this blog will use two of the top document-centric NoSQL databases (MongoDB and Couchbase) to illustrate our proposed resilience design pattern. Although a tutorial on MongoDB and Couchbase is beyond the scope of this blog, the high-level comparison between them in Table 1 helps illustrate their differences. It also helps explain how these differences influence respective resilience design patterns for each product. From the comparison above, we observe the following characteristics when designing a resilience pattern design: NoSQL databases tend to simplify their resilience pattern if they are based on peer-to-peer architecture. NoSQL databases tend to complicate their resilience pattern if they lack quorum read/write, an important capability to support global read and write consistency. Lastly, we found it helpful to organize resilience design patterns into different categories, as shown in the Table 2. For brevity, we will focus the examples on the following three categories: workload, durability, and application sharding. As mentioned earlier, one key objective of NoSQL resilience design patterns is to relieve applications and Operation of preventable infrastructure failure. It is our opinion that, disregarding the size or importance of applications, enterprise-class NoSQL clusters should be able to handle node, cluster, site or network partition failure gracefully with built-in preventive measures. Also, they should be able to perform disaster recovery (DR) within reasonable bounds. This assumption warrants the need of the “standard minimal deployments” described below. According to our experience managing some of the largest 24×7 heterogeneous NoSQL clusters in the world, we recommend the following “standard minimal deployment” best practice for enterprise-class MongoDB deployments: Whenever possible, nodes in the same MongoDB replica set will be provisioned in different fault domain to minimize any availability risk associated with node and network failure. Standard MongoDB deployment (for example, type 1 in Table 3) always includes third data center with two or more secondary nodes. For applications that do not have traffic in a third data center, light-weight arbiter nodes should be used for cross-data center quorum voting during site or total data center failure. With few exceptions (for example, type 2 below), a MongoDB replica set can be deployed in only one data center if applications do not require remote DR capability. The following diagram illustrates the MongoDB “standard minimal deployment” pattern. During primary node failover, one of the available secondary nodes in either data center will be elected as the new primary node, as shown in the following diagram. During site or data center failure, one of the available secondary nodes in other data centers will be elected as the new primary node, as shown in the following diagram. With peer-to-peer architecture, Couchbase’s “standard minimal deployment” does not require a third data center since it does not involve quorum voting when failing over to a new primary node. Arguably, increasing minimal nodes from four to five or six or more per data center can help strengthen operational viability should two or more nodes in the same data center fail at the same time (itself an unlikely scenario if they were provisioned in different fault domains in the first place). With that, we feel that a minimal four nodes per data center are sufficient for most Couchbase use cases to start with. The following diagram illustrates the Couchbase “standard minimal deployment” pattern where each data center/cluster has two copies of the same document (for example, P1 being the primary copy and R1 the eventually consistent replica copy). During automatic node failover, the Couchbase client SDK in applications will detect node failure and receive an updated failover cluster map with a topology containing the new location of replica documents that have been promoted to primary. This is shown in following diagram. Although Couchbase supports bidirectional Cross Data Center Replication (XDCR) between geographically dispersed clusters, its current implementation does not offer automatic cluster failover. To address this limitation, Couchbase will support a new feature called Multi-Cluster Awareness (MCA) in a future release (tentatively v4.x) to provide this capability, as shown in following diagram. Before we talk about the details of an individual resilience design pattern for each product, it helps to understand the capability of MongoDB and Couchbase in terms of high availability, consistency, durability, and DR across local and multiple data centers. The following table highlights their differences. From the above comparison, we observe one similarity between MongoDB and Couchbase in their support for high-availability writes or the lack of. This is because both products limit their writes to a single primary node only. Nonetheless, Couchbase does plan to support high-availability write for multiple data centers through its future Multi-Cluster Awareness feature, which will help alleviate this limitation. This is the reason why Couchbase’s standard minimal deployment pattern requires at minimum two data centers/clusters. Before we show MongoDB’s resilience design pattern examples, it is important to understand how data loss can happen in MongoDB if applications do not use a proper resilience design pattern. Let’s assume that you have a MongoDB replica set comprised of three nodes. The following operations illustrate how data loss can happen: First, the application writes five documents and receives write confirmation from the primary node only. The first three documents are replicated successfully at the second secondary node, and two documents are replicated at the third secondary node. The primary node fails before all five documents reach both of the two secondary nodes Quorum voting re-elects the second secondary node as the new primary node because it receives the third document, that is, more recently than the third secondary node The original primary node steps down to be a secondary and rolls back its fourth and fifth documents, since they didn’t reach other two secondary nodes In effect, the application loses the fourth and fifth documents even though it receives confirmation from the original primary node. The issue associated with this type of data loss (non-synchronous write) occurs because that application didn’t apply the “quorum write” option to majority nodes in the same replica set. In addition to “quorum write”, one should also enable MongoDB’s write-ahead logging journal file to further increase primary node write durability. Since non-synchronous write is just one type of issue that can happen to applications, it is not sufficient to develop a pattern just to solve it. Having various add-on patterns, like synchronous write and others, on top of the MongoDB standard minimal deployment patterns helps enrich the overall NoSQL database resilience capabilities. To achieve MongoDB replica-set-wide write durability, an application should use MongoDB’s WriteConcern = Majority option, where a write waits for confirmation from majority nodes across multiple data centers. During primary node failover, one of the majority secondary nodes having the latest committed write will be elected as the new primary node. One caveat of this pattern is that one should weigh the pros and cons associated with waiting for cross-data center majority nodes confirmation, since it may not suitable for applications that require short latency. This pattern is for applications that require high availability reads. With MongoDB’s master-slave replication architecture and ability to scale up to 49 secondary nodes across multiple data centers, it inherently supports highly available reads, provided that the number of healthy secondary nodes in any data center are capable of handling read traffic if one or more fail. Note that even though the current MongoDB replica set can scale up to 50 nodes, only 7 nodes can vote during primary node failover. The following diagram illustrates this capability. Use cases requiring extreme high writes can use MongoDB optional sharding since it allows horizontal write scaling across multiple replica sets (shards). Each shard (or replica set) stores part of the entire dataset as defined by a predefined shard key of the collection. With a well-designed shard key, MongoDB automatically distributes and balances read/write queries to designated shards. Although MongoDB sharding provides horizontal scale-out write capability, this pattern incurs consequent complexity and overhead and should be used with caution: An application should start with sharding using an appropriate shard key from the start instead of migrating from a single replica set as afterthought. This is because migrating a single replica set to a sharded cluster is a major undertaking from both development and operations perspectives. We recommend starting with a predefined number of shards capable of handling capacity and traffic in the foreseeable future. This helps eliminate the overhead associated with rebalancing when adding new shards. Note that MongoDB automatic shard balancing may introduce spikes during chunk migration and can potentially impact performance. Developers needs to understand behavior and limitation on how mongos , a software router process, on queries that do not include shard key, such as scatter gather operations . Developers should weigh the pros and cons of the overhead associated with running mongos as part of application servers vs. in separate nodes. The following diagram illustrates this capability. Although Couchbase does not support write-ahead logging or quorum write, it achieves high durability through following mechanisms: Local cluster durability — ReplicateTo and PersistTo functions Multi-cluster durability (to be released in a future v4.x release): Multi-Cluster Awareness (MCA) — Without application logic, this feature allows application developers and DBA to define rules on how applications should behave in the event of complete data center/cluster failover. Timestamp-based Conflict Resolution (TbCR) — Based on a server-synced clock, this feature provides Last-Write-Win (LWW) capability on bidirectional replication update conflict resolution to ensure correct cross-data center/cluster write durability. Multi-Cluster Awareness (MCA) — Without application logic, this feature allows application developers and DBA to define rules on how applications should behave in the event of complete data center/cluster failover. Timestamp-based Conflict Resolution (TbCR) — Based on a server-synced clock, this feature provides Last-Write-Win (LWW) capability on bidirectional replication update conflict resolution to ensure correct cross-data center/cluster write durability. For local cluster durability, Couchbase provides the following two functions through its client SDK: ReplicateTo — This function allows writes on the same document to be successfully replicated in memory for all copies in the local cluster. However, it does not guarantee writes to be persisted on disk, which may result in data loss if both primary and replica nodes fail before that happens. PersistTo — For increased durability, applications can use this function so that writes will not only be successfully replicated in memory but also persisted on disk in the local cluster. Note that even with the second PersistTo function, the current Couchbase version still does not guarantee writes to be successfully replicated to remote data centers/clusters. (This capability is explained in the next section, “ Couchbase Multi-cluster write durability pattern .”) The following operations illustrate how both functions work. Assume that the Couchbase topology contains four nodes per data center/cluster with each storing two copies of the same document replicated through XDCR between clusters. The application in Data Center 1 writes documentP1 to nodeN1. BeforeP1 is replicated to the replica node in the local cluster or to the remote data center/cluster, nodeN1 fails, and as a result the application suffers data loss. BeforeP1 reaches the remote data center/cluster, even thoughP1 has been replicated successfully in memory to the local cluster replica nodeN4, if bothN1 andN4 nodes fail, the application still suffers data loss. Using the ReplicateTo function can circumvent the failure described in step 3 , and using the PersistTo function can circumvent the failure described in step 4 , as shown in the following figure. Lastly, for multi-data center/cluster durability, use the design pattern described in “ Couchbase multi-cluster write durability pattern . With its Multi-Cluster Awareness and Timestamp-based Conflict Resolution features, Couchbase supports multi-cluster durability as shown below. In the absence of write-ahead logging or quorum write, and even though Couchbase provides sufficient support for local and multi-cluster durability, one should still ask this question: what is the likelihood that all primary and replica nodes fail in multiple data centers or even worse that all data centers fail completely at the same time? These two unlikely failure scenarios are shown in the following two diagrams. We feel that the odds are next to zero if one follows this proposed Couchbase durability design pattern. With its peer-to-peer architecture and XDCR’s bidirectional multi-cluster/data center replication capability, Couchbase affords users the flexibility to provision clusters with different sizes and shapes tailored for specific traffic/capacity and usage patterns. This flexibility is shown in the following diagram. On the other hand, the cluster-sizing calculation exercise can become complicated. This is especially true if it involves Multi-Dimensional Scaling . The motivation behind this design pattern is that almost all large-scale mission-critical use cases require high availability. According to our experience, it doesn’t matter which NoSQL database you use or how comprehensive the best practice and SOP you follow, sometimes simple mundane maintenance tasks can jeopardize the overall database availability. The larger the size of database, the more severe the damage it may suffer. This design pattern offers an alternative solution using a divide-and-conquer approach, that is, by reducing the NoSQL database cluster to a small and manageable size through the following mechanisms: Applications shard their data using modular, hash, round robin, or any other suitable algorithm. The DBA and Operations provide a standard-size NoSQL cluster to host each application-level shard. If needed, each application-level shard can further use built-in NoSQL vendor product sharding if it is available. Using MongoDB as an example, the following diagram illustrates this design pattern. One caveat associated with this pattern is that it requires a middle-tier data access layer to help direct traffic, an effort that must not be underestimated. In this blog, we described the motivation, consideration, and approach behind our proposed NoSQL resilience design pattern. We overviewed key differences between MongoDB and Couchbase in the context of resilience patterns. We also walked through three NoSQL resilience design pattern examples and a DB-agnostic application sharding pattern. In conclusion, we would like to suggest the following future work and direction on this topic: Provide end-to-end integration of proven NoSQL design patterns with application frameworks and also cloud provisioning and management infrastructure. Formalize the above NoSQL design patterns as officially supported products rather than just engineering patterns. Add other types of resilience patterns, such as high consistency. Add support for other NoSQL databases, for example, Cassandra. Collaborate with NoSQL vendors and develop new resilience patterns for new features and capabilities. The graphics in Non-synchronous writes are reproduced with the kind permission of Todd Dampier from his presentation “ Rock-Solid Mongo Ops “.", "date": "2017-02-14"},
{"website": "Ebay-Engineering", "title": "Browse eBay with Style and Speed", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/browse-ebay-with-style-and-speed/", "abstract": "One of the top initiatives for eBay this year is to provide a compelling browse experience to our users. In a recent interview , Devin Wenig has given a good overview of why this matters to eBay. The idea is to leverage structured data and machine learning to allow users to shop across a whole spectrum of value, where some users might desire great savings, while others may want to focus on, say, best selling products. When we started to design the experience, our first area of focus was mobile web. Similar to many other organizations, mobile web has been our highest growing sector. We wanted to launch the new browse experience on mobile web first, followed by desktop and native. The core design principles of the new mobile web browse experience were to keep it simple, accessible, and fast, really fast. On the front-end side of things, we made a couple of choices to achieve this. Lean and accessible — From the beginning we wanted the page to be as lean as possible. This meant keeping the HTML, CSS, and JS to a minimum. To achieve this goal, we followed a modular architecture and started building atomic components. Basically a page is a bunch of modules, and a module is built from other sub-modules, and so on. This practice enabled maximum code reuse, which in turn reduced the size of resources (CSS and JS) drastically. In addition, our style library enforced accessibility through CSS — by using ARIA attributes to define styles rather than just class names. This forces developers to write a11y -friendly markup from the beginning, instead of it being an afterthought. You can read more about it here . Code with the platform — The web platform has evolved into a more developer friendly stack, and we wanted to leverage this aspect — code with the platform vs. coding against it. What this meant was that we could reduce the dependency on big libraries and frameworks and start using the native APIs to achieve the same. For instance, we tried to avoid jQuery for DOM manipulations and instead use the native DOM APIs. Similarly, we could use the fetch polyfill instead of $.ajax etc. The end result was a faster loading page that was also very responsive to user interactions. BTW, jQuery is still loaded in the page, because some of eBay platform specific code is dependent on it, and we are working towards removing the dependency altogether. But our efforts did not stop there. The speed aspect was very critical for us, and we wanted to do more for speed. That is when we ran into AMP. The AMP project was announced around the same time we started the initial brainstorming for browse. It seemed to resonate a lot with our own thinking on how we wanted to render the new experience. Although AMP was more tuned towards publisher-based content, it was still an open source project built using the open web. Also, a portion of the traffic to the new browse experience is going to be from search engines, which made it more promising to look into AMP. So we quickly pinged the AMP folks at Google and discussed the idea of building an AMP version for the browse experience, in addition to the normal mobile web pages. They were very supportive of it. This positive reaction encouraged us to start looking into AMP technology for the eCommerce world and in parallel develop an AMP version of browse. Today we are proud to announce that the AMP version of the new browse experience is live, and about 8 million AMP-based browse nodes are available in production. Check out some of the popular queries in a mobile browser — Camera Drones and Sony PlayStation , for example. Basically adding amp/ to the path of any browse URL will render an AMP version (for example, non-AMP , AMP ). We have not linked all of them from our regular (non-AMP) pages yet. This step is waiting on few pending tasks to be completed. For now, we have enabled this new browse experience only in mobile web. In the next couple of weeks, the desktop web experience will also be launched. So how was the experience in implementing AMP for the eCommerce world? We have highlighted some of our learnings below. Best practices — One of the good things about AMP is that at the end of the day it is a bunch of best practices for building mobile web pages. We were already following some of them, but adoption was scattered across various teams, each having its own preference. This initiative helped us consolidate the list and incorporate these best practices as a part of our regular development life cycle itself. This made our approach towards AMP more organic, rather than a forced function. The other good side effect of this is even our non-AMP pages become faster. Less forking in code — This follows the previous point. Since we started following some of the AMP best practices for building regular pages, we were able to reuse most of the UI components between our non-AMP and AMP browse page. This resulted in less forking in code, which otherwise would have become a maintenance nightmare. Having said that, there is still some forking when it comes to JavaScript-based components, and we are still figuring out the best solution. AMP Component list — Although the AMP project’s initial focus was more towards publisher-based content and news feeds, the AMP component list was still sufficient to build a basic product for viewing eCommerce pages. Users will not be able to do actions on items (such as “Add To Cart”), but they still get a solid browsing experience. The good news is that the list is getting better and growing day by day. Components like sidebar , carousel , and lightbox are critical in providing a compelling eCommerce experience. Internal AMP platform — We have been thinking about leveraging the AMP ecosystem for our own search, similar to how Google handles AMP results. This plan is in very early stages of discussion, but the possibility of our search using AMP technology is very interesting. Infrastructure components — To launch an eBay page to production, a lot of infrastructure components automatically come into play. These are things like Global header/footer, site speed beacon kit, experimentation library, and the analytics module. All of them have some amount of JavaScript, which immediately disqualifies them from being used in the AMP version. This adds complexity in development. We had to fork few infrastructure components to support the AMP guidelines. They had to go through a strict regression cycle before being published, which added delays. Also, our default front-end server pipeline had to be conditionally tweaked to exclude or swap certain modules. It was a good learning curve, and over time we have also replaced our early quick hacks with more robust and sustainable solutions. Tracking — AMP provides user activity tracking through its amp-analytics component. amp-analytics can be configured in various ways, but it still was not sufficient for the granular tracking needs that eBay has. We also do stuff like session stitching, which needs cookie access. Creating an amp-analytics configuration to suit our needs was slowly becoming unmanageable. We need some enhancements in the component, which we are hoping to develop and commit to the project soon. We are excited to partner with Google and everyone else participating on the AMP Project to close the gap in launching a full-fledged eCommerce experience in AMP. We have created a combined working group to tackle the gap, and we will be looking into these items and more. Smart buttons — These enable us to do actions like “Add To Cart” and “Buy It Now” with authentication support. Input elements — User interactive elements are critical to eCommerce experiences, be they simple search text boxes or checkboxes. Advanced tracking — As mentioned earlier, we need more granular tracking for eBay, and so we have to figure out a way to achieve it. A/B Testing — This will enable experimentation on AMP. With items like these in place, AMP for eCommerce will soon start surfacing. We will also be looking into creating a seamless transition from the AMP view to a regular page view, similar to what the Washington Post did using Service Workers . This will enable users to have a complete and delightful eBay experience without switching contexts. We are also asked the question of if there is more focus towards web over native. The answer is NO. At eBay, we strongly believe that web and native do not compete each other. They indeed complement each other, and the combined ecosystem works very well for us. We will soon be launching these browse experiences in our native platforms. We are on our path to making eBay the world’s first place to shop and this is a step towards it. Thanks to my colleague Suresh Ayyasamy , who partnered in implementing the AMP version of browse nodes and successfully rolling it to production. — Senthil", "date": "2016-06-30"},
{"website": "Ebay-Engineering", "title": "Mastering the Fire", "author": ["Dmytro Semenov"], "link": "https://tech.ebayinc.com/engineering/mastering-the-fire/", "abstract": "“If you play with fire, you’re gonna get burned.”  ~ Anonymous There were several reasons we built the NodeJS stack at eBay and now offer it as part of our polyglot initiative. These reasons include an active open source community, development productivity, reliability, scalability, and speed. Community support and productivity proved to be true from the start, but when it comes to reliability, scalability, and speed, they all depend on developer culture. We use static code analysis tools, code reviews, unit tests, and regression testing to make sure our modules and applications work according to the spec. One isolated module can perform perfectly fine in its own test environment and as part of application, but once all the modules are packaged together and ready to roll, the app may turn out to be much slower than one expected, for example, due to logging too much data. This can become a tough time for the application stack provider who does not have answers. Thankfully, flame graphs came on the scene. They were really promising, but their promise turned out to be far from the reality. The flame graphs turned out to be hot like real flames. We touched them a few times, got burned, and backed off. The first time we approached them, flame graphs were available only in SmartOS, and one had to follow specific steps to generate them, and that was the problem, especially when one runs applications on a completely different platform. Addicted to the simplicity of Node, which just works, we found this option was far from simple, and we put it in reserve for tough cases that we could not solve some other way. The second time that we approached flame graphs, they were already available on Linux or OSX, but creating them still required a special setup and too many steps (including merging symbols with profile results) to get SVG charts in OSX. “It’s a living thing, Brian. It breathes, it eats, and it hates. The only way to beat it is to think like it.”  ~ Robert De Niro (as Donald ‘Shadow’ Rimgale), Backdraft , 1991 Meanwhile, we were using v8-profiler to generate profile data that we would load into the Chrome Profile tool, and then we would analyze the aggregation tree for any performance hot spots. It is a laborious task when one has to look at all the call stacks of a big application, and it demanded a lot of focus. We could not offer this solution to our application developers, as it would take too much of their time to troubleshoot. It was going to become a task for a special profile expert who would do a lot of profiling, get a lot experience, and be able to spot things easily and know where to look. This was not scalable. As a big project started knocking at our door, we had to figure out a better way to profile so that the application developers could do the work by themselves. We got an idea that if Chrome shows profile results in aggregated format, then there should be a way to calculate the same results by ourselves and present them as flame graphs by using one of the tools available. And we found our calculator and a suitable tool that was built to use JSON as profile data. All we needed to do is to put it all together . “Playing with fire is bad for those who burn themselves. For the rest of us, it is a very great pleasure.”  ~ Jerry Smith, National Football League tight end, Washington Redskins ‘65-77 The result is pretty exciting. We are now able to turn on profiling in production any time without restarting the server and look right into the problem via flame graphs with one click of a button. The results show the JavaScript part of the profiling (no native code), which is what developers want most of the time anyway when it comes to performance issues in their applications. It also works anywhere that can run Node. For example, developers now can profile right on their Macs or Windows machines without any special effort on their part. We have already successfully used it to find and optimize performance in platform code as well as in many applications that are soon to be rolled to production. We were able to quickly identify performance problems in production for one critical application when, after a fresh deployment, it started using 80% of CPU instead of the expected 20–30%. Below you can see the problem, it was loading templates over and over again with every request. The fix was simply to cache the templates at the first load. This first flame graph shows the application’s behavior before the fix. Total time spent on requests was 3500 msec. This next illustration shows a close-up view of the same flame graph, highlighting the trouble spots. This next flame graph shows the optimization we got after applying the fix. As you can see the rendering part became much smaller. The total time spent on all requests became 1100 msec. Most of the problems we discovered were not as big as the one that Netflix uncovered with flame graphs, but fixing them helped us save a lot on CPU usage. “Don’t let your dreams go up in smoke — practice fire safety.”  ~ Unknown Author There is still work to do. We need to train developers to read flame graphs. Otherwise this valuable tool can draw an undeserved negative perception and disappear from the developers’ toolset. After profiling many applications, we have also found common problems that we can highlight by default, and we can implement new rules for static code analysis to identify these problems. We have found it useful to profile the following areas with flame graphs: Application profiling during development Unexpected activity detection during memory leak analysis Capacity estimation based on CPU usage Issue troubleshooting at runtime in production Proactive smoke testing with live traffic in a special environment using a traffic mirror (cloning read requests and directing them to the target test box) Sampling and storing for future investigation To summarize our experience with Node and profiling, I would say that the successful employment of any language, no matter how promising, depends on the way it is used, and performance tools like flame graphs play a major role in helping the developer to accomplish what was claimed at the start.", "date": "2016-07-14"},
{"website": "Ebay-Engineering", "title": "Igniting Node.js Flames", "author": ["Mahesh Dathrika"], "link": "https://tech.ebayinc.com/engineering/igniting-node-js-flames/", "abstract": "“Simple things bring infinite pleasure. Yet, it takes us a while to realize that. But once simple is in, complex it out – forever.” ― Joan F. Marques Now that I have your attention, let me clear up the word “flames.” The flames that I’m referring to have nothing to do with fire. All I am talking about is performance tools in Node.js. When it comes to performance, everyone thinks of fighting fires, as many think performance optimization is a nightmare. Most of us think only that some individuals are masters in profiling. Anyone can become master in profiling when given simple tools. At eBay, we strive to make things simple and easy for our developers to use. During the course of Node.js development and production issues, we soon realized that profiling in Node.js is not an easy thing to do. Before jumping to the CPU profiling tool that simplified our lives, let me walk you through our journey that ended up in seeing flame charts from a completely different angle. With Brendan Gregg’s flame graph generation, it was much easier to visualize CPU bottlenecks. However, we need to run a small number of tools and scripts to generate these graphs. Yunong Xiao has posted an excellent blog on how to generate flame graphs using the perf command based on Gregg’s tools . Kernel tools like DTrace (BSD and Solaris) and perf (Linux) are very useful in generating stack traces from the core level and transform the stack calls to flame graphs. This approach gives us complete picture from Node internals, from the V8 engine all the way to JS code. However, running tools like this need some good understanding on tool itself and sometimes you need different OS itself. In most cases your production box and profiling box setup differ completely. This way makes it hard to investigate the issue going in production as one has to attempt to reproduce this issue in completely different environment. After managing to run the tools, you will end up with flame charts like this. Image source from Yunong Xiao’s blog Here are some pros and cons for this approach. Pros: Easy to find CPU bottleneck Graphical view Complete profile graph for native and JS frames. Cons: Complexity in generating graphs. Limited DTrace support by different platforms, harder to profile in DEV boxes The Chrome browser is just amazing. It is famous not only for its speed but also for its V8 engine , which is core to Node.js. In addition to these features, one tool that web developers love about Chrome is Developer Tools. There is one tool inside Developer Tools that is used to profile browser-side JS. The v8-profiler enables us to use server-side profile data in the Chrome Profile tool. Let us see how we can use this for profiling our Node.js application. Before using Profiles in Chrome, we have to generate some profiling data from our running Node.js application. We will use v8-profiler for creating CPU profile data. In the following code, I have created a route /cpuprofile for generating CPU profile data for a given number of seconds and then streaming the dump to a browser to open in Chrome. This sample code creates a CPU dump using v8-profiler . To generate CPU profile data, use these steps: Start your app. node index.js It’s a good idea to run ab to put some load on the page. It’s a good idea to run ab to put some load on the page. Access the CPU profile dump using http://localhost:8080/cpuprofile?duration=2 . A cpu-profile.cpuprofile will be downloaded from the server. Load the downloaded file cpu-profile.cpuprofile in Chrome using Developer Tools > Profiles > Load . Upon loading, you should see in your Profiles tab something like the following. Now that you have opened profile data, you can drill down the tree and analyze which piece of code is taking more CPU time. With this tool, anyone can generate profile data anytime with just one click, but just imagine how hard it is to drill down with this tree structure when you have big application. In comparison to Flame Graphs using Kernel Tools , here are some pros and cons. Easy generation of a profile dump Platform independent Profiling available during live traffic Chrome provides a graphical view for profile data, but the data is not aggregated and navigation is limited. Now that we have seen two different approaches for generating CPU profile data, let us see how we can bring in a nice graphical view like flame graphs to V8-profiler data. At eBay, we have taken a different approach to make it very simple and easy to use tool for our Node.js developers. We used V8-profiler data, applied the aggregation algorithm, and rendered the data as flame charts using the d3-flame-graphs module. If you look at the .cpuprofile file closely (created above), it is basically a JSON file. We came across a generic d3-flame-graphs library that can draw flame graphs in a browser using input JSON data. Thanks to “cimi” for his d3-flame-graphs module. After we made some modifications to the chrome2calltree aggregation algorithm and aggregated profile data (removed core-level CPU profile data), we could convert .cpuprofile data file to JSON that can be read by d3-flame-graphs , and the final outcome is simply amazing. Generate .cpuprofile on demand using v8-profiler as shown in Chrome Profiling Tool . Convert .cpuprofile into aggregated JSON format (source code). Load the JSON using d3-flame-graphs to render the flame graph on browser. This time access CPU flame graph on browser using the same URL ( http://localhost:8080/cpuprofile?duration=2 ) from Chrome Profiling Tool . The above flame chart shows only JS frames, which is what most Node application developers are interested in. expressjs (npm) v8-profiler (npm) d3-flame-graph (bower) Aggregation algorithm (modified) Easy and simple to generate flame graphs Doesn’t need special setup Platform independent Early performance analysis during development Graphical view integrated into every application Imposes 10% overhead during profiling To summarize, we have seen three different ways of profiling CPU in Node.js, starting from using OS-level tools to rendering flame graphs on a browser using simple open source JS code. Simple and easy-to-use tools help anyone master profiling and performance tuning. At eBay, we always strive to make some difference in our developers’ lives.", "date": "2016-06-15"},
{"website": "Ebay-Engineering", "title": "Human Evaluation of Machine Translation", "author": ["Olga Pospelova", "Juan Rowda"], "link": "https://tech.ebayinc.com/engineering/human-evaluation-of-machine-translation/", "abstract": "Machine translation (MT) evaluation is essential in machine translation development. This is key to determining the effectiveness of the existing MT system, estimating the level of required post-editing, negotiating the price, and setting reasonable expectations. Machine translation output can be evaluated automatically, using methods like BLEU and NIST, or by human judges. The automatic metrics use one or more human reference translations, which are considered the gold standard of translation quality. The difficulty lies in the fact that there may be many alternative correct translations for a single source segment. Human evaluation, however, also has a number of disadvantages. Primarily, it is a costly and time-consuming process. Human judgment is also subjective in nature, so it is difficult to achieve a high level of intra-rater (consistency of the same human judge) and inter-rater (consistency across multiple judges) agreement. In addition, there are no standardized metrics and approaches to human evaluation. Let us explore the most commonly used types of human evaluation. Judges rate translations based on a predetermined scale. For example, a scale from 1 to 5 can be used, where 1 is the lowest and 5 is the highest score. One of the challenges of this approach is establishing a clear description of each value in the scale and the exact differences between the levels of quality. Even if human judges have explicit evaluation guidelines, they still find it difficult to assign numerical values to the quality of the translation (Koehn & Monz, 2006). The two main dimensions or metrics used in this type of evaluation are adequacy and fluency. Adequacy, according to the Linguistic Data Consortium, is defined as “how much of the meaning expressed in the gold-standard translation or source is also expressed in the target translation.” The annotators must be bilingual in both the source and target language in order to judge whether the information is preserved across translation. A typical scale used to measure adequacy is based on the question “How much meaning is preserved?” 5: all meaning 4: most meaning 3: some meaning 2: little meaning 1: none Fluency refers to the target only, without taking the source into account; criteria are grammar, spelling, choice of words, and style. A typical scale used to measure fluency is based on the question “Is the language in the output fluent?” 5: flawless 4: good 3: non-native 2: disfluent 1: incomprehensible Judges are presented with two or more translations (usually from different MT systems) and are required to choose the best option. This task can be confusing when the ranked segments are nearly identical or contain difficult-to-compare errors. The judges must decide which errors have greater impact on the quality of the translation (Denkowski & Lavie, 2010) . On the other hand, it is often easier for human judges to rank systems than to assign absolute scores (Vilar et al., 2007). This is because it is difficult to quantify the quality of the translation. Human judges identify and classify errors in MT output. Classification of errors might depend on the specific language and content type. Some examples of error classes are “missing words,” “incorrect word order,” “added words,” “wrong agreement,” “wrong part of speech,” and so on. It is useful to have reference translations in order to classify errors; however, as mentioned above, there may be several correct ways to translate the same source segment. Accordingly, reference translations should be used with care. When evaluating the quality of eBay MT systems, we use all the aforementioned methods. However, our metrics can vary in the provision of micro-level details about some areas specific to eBay content. As a result, one of the evaluation criteria is to identify whether brand names and product names (the main noun or noun phrase identifying an item) were translated correctly. This information can help in identifying the problem areas of MT and focusing on the enhancement of that particular area. Some types of human evaluation, such as error analysis, can only be conducted by professional linguists, while other types of judgment can be performed by annotators who are not linguistically trained. Is there a way to cut the cost of human evaluation? Yes, but unfortunately, low-budget crowdsourcing evaluations tend to produce unreliable results. How then can we save money without compromising the validity of our findings? Start with a pilot test — a process of trying out your evaluation on a small data set. This can reveal critical flaws in your metrics, such as ambiguous questions or instructions. Monitor response patterns to remove judges whose answers are outside the expected range. Use dynamic judgments — a feature that allows fewer judgments on the segments where annotators agree, and more judgments on segments with a high inter-rater disagreement. Use professional judgments that are randomly inserted throughout your evaluation job. Pre-labeled professional judgments will allow for the removal of judges with poor performance. Human evaluation of machine translation quality is still very important, even though there is no clear consensus on the best method. It is a key element in the development of machine translation systems, as automatic metrics are validated through correlation with human judgment. If you enjoyed this article, please check other posts from the eBay MT Language Specialists series.", "date": "2016-06-26"},
{"website": "Ebay-Engineering", "title": "Finite-State Machine for Single-Use Code Authentication", "author": ["Senthilkumar Gopal"], "link": "https://tech.ebayinc.com/engineering/finite-state-machine-for-single-use-code-authentication/", "abstract": "eBay strives to excel at security and to identify new and improved secure mechanisms to allow users to seamlessly access their account and in the meantime ensure that the fraudulent and malicious users are kept at bay. This is a balancing act that every internet platform player, major and minor, performs every day. Passwords are one such mechanism used to secure a user’s account but with not much success. They have always been a major pain point for users, as they need to be maintained with a unique combination of hard-to-guess and hard-to-remember numbers, alphabets and special characters. Ironically, this leads users to create passwords that are hard to remember but easy for hackers to brute force. With new websites and platforms cropping up each day, users are required to remember multiple passwords becoming a daily struggle. Passwords are one of the weakest links in our attempt to secure user accounts, because many users don’t use a strong, complex one or reuse the same password across multiple services, making themselves vulnerable to phishing and other types of attacks. To relieve users from ever needing to remember their password and to move towards the utopia of a “password-free” world, eBay has released the ability of using one-time code to log in, which can be delivered to the users’ phones, a personal and important device that they own. Along with the ease of getting disposable one-time codes, this also helps while traveling or when using a public computer or network, where users can log in with a one-time code rather than running the risk of having their regular passwords hijacked via a key logger, malware, or even a compromised network. More details about the feature available in this release statement Any user can now use the “Sign in with a single-use code” link on the log-in page to request that a one-time code be delivered to their registered phone number via text messaging. Then the user can type in the code from the text message into the input field, securely getting into the account without the hassle of remembering or exposing the original password. The one-time codes are short-lived and cannot be transferred between sessions, making them highly isolated and secure in comparison to regular passwords, which can be used across any number of devices. Given its criticality, the application architecture need to be robust and secure but also better manageable and configurable. The structure should provide better code readability, which in turn ensures high quality code in the production environment. One of the important characteristics of the application architecture is that the finite state machine used for generating and validating the one-time code must conform with all the rules that were set up such as expiry, retry attempts, etc. A finite-state machine (FSM) is a mathematical model of computation used to design both computer programs and sequential logic circuits. It is conceived as an abstract machine that can be in one of a finite number of states. The machine is in only one state at a time; the state it is in at any given time is called the current state. It can change from one state to another when initiated by a triggering event or condition; this is called a transition. A particular FSM is defined by a list of its states, and the triggering condition for each transition. Wikipedia contributors, “Finite-state machine,” Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Finite-state_machine&oldid=731346054 (accessed August 18, 2016). A simple representation of a Turnstile state machine is shown below. FSM provides many advantages over traditional rule-based systems. States, transitions, events, and their conditions maintained in a modular and declarative fashion Pluggable nature of transitions and conditions Abstraction of states, actions, and their roles Clarity of behavior during firing of events Most importantly, ease of ensuring secure state management To illustrate, imagine a user has requested a one-time code and received it on their phone. However, while entering the password, they mistyped it incorrectly for more times than the allowed number of attempts. In the case of a rule engine or a simple if/else sequence, there is a high probability of a bug being introduced, as there is no state maintenance — only rule/logic evaluations, which can allow the user to provide the correct code and log in successfully even after exceeding the allowed number of attempts. However, on a state machine, once the user attempts exceed the limits, the state machine transitions to an AttemptExceeded/Failed state, making it impossible for the user to attempt a code validation even with the correct code. This structure guarantees that the shortcomings of a sequence model execution are not possible in a state machine. Rather than writing a FSM from scratch, we evaluated two widely used frameworks based on maturity and support: Spring StateMachine and squirrel-foundation . Spring StateMachine seemed more suited to a daemon application, such as Zookeeper, and was not ready for a multi-threaded environment, such as a web application [ Issue#170 ]. Moreover, squirrel-foundation provided the ability to add custom event types and actions, which are explained in more detail below, helping make the decision to use the squirrel-foundation framework to model the finite state machine. A simple representation of the state machine used to generate and validate the one-time code for phone is provided below. As illustrated, the one-time code validation moves between different states of the machine based upon the Action of the user, the current state of the machine, and the conditions configured between them. For instance, when a user requests a code for the first time, if the Send failed for some reason, the state machine moves the transaction to a final FAILED state, rendering the transaction inert and effectively terminated. Elaborating on the use case discussed earlier, where the user exceeds the number of incorrect code attempts, if the user performs the action of Requests Code retry, the retry-limit-exceeded condition fires and the state machine moves the current state to FAILED, terminating the transaction and preventing the user from re-using the session or the code further. In order to explain the usage and set-up better, following are some of the important parts of the State machine modeling code snippets. These are not complete nor compilable as is, and they are truncated for brevity. The squirrel-foundation framework allows configuring the State Machine once and create new instances of the state machine for each thread without incurring the expensive creation time. The newSM() method is invoked with the beginning state of the current transaction to get the State Machine ready, and when the events are fired, the State Machine takes care of identifying the next state to transfer to. Each of the transitions from one state to another state is managed as an external transition and guarded by conditions. For the Phone StateMachine, the positive transitions are configured similar to below. As illustrated, the Condition<T> is an interface provided by the squirrel-foundation framework for configuring the conditions specific to a state-to-state transition. A successful Boolean response to the isSatisfied() method fires the transition that the condition satisfies. NOTE: If more than one Condition fires for a transition from one initial state to two different end states, an exception is thrown. It is imperative that each condition is mutually exclusive with other conditions for the same initial state. Even for error scenarios, such as expired or attempts exceeded, the state transition is simple to configure, maintain, and change, ensuring the isolation of the change and easy testability. For persistence and initialization, the State Machine is backed by a database. The database helps in reading the initial state, which helps in starting the State Machine and also in storing the resolved state after firing of the event in the State Machine. The framework provides appropriate hooks such as afterTransitionCompleted and afterTransitionDeclined for persisting the states. Squirrel-foundation also provides a mechanism to identify other unchecked exceptions using afterTransitionCausedException , which is useful for alerting and monitoring purposes. Even though the squirrel-foundation framework satisfied all the needs, there was no available structure to bind a pre-Action to an Event. For example, the code has to be sent to the user before the state machine is triggered for INITIAL state. Similarly, the code should be validated against the database and relevant context results should be properly populated before firing the state machine. This was achieved by creating a custom state machine and overriding the fire() method to perform an associated Action and then firing the StateMachine. Squirrel-foundation provides the freedom of defining custom types for all necessary parameters such as Events, Actions, Context etc., which makes it possible for each PhoneEvent to be configured with an associated Action. The state machine is a well-known structure for managing processing, and this is just one example of how a complex logical structure can be represented in a simple but effective and maintainable manner. This structure allows developers to manage changes and configure values effectively and almost bug-free. Currently, the above State machine is being configured with better listeners for effective logging, self-healing mechanisms in case of failures, changeover to use RxJava for state persistence, and logging as future enhancements.", "date": "2016-08-30"},
{"website": "Ebay-Engineering", "title": "Scalable and Nimble Continuous Integration for Hadoop Projects", "author": ["Deepak Vasthimal"], "link": "https://tech.ebayinc.com/engineering/scalable-and-nimble-continuous-integration-for-hadoop-projects/", "abstract": "The Experimentation Platform at eBay runs around 1500 experiments that are responsible for processing over hundreds of terabytes of reporting data contained in millions of files using a 2500+ node Hadoop infrastructure and consuming thousands of computing resources. The entire report generation process contains well over 200 metrics. It enables millions of customers to experience small and large innovations that enable them to buy and sell products in various countries in diverse currencies and using diverse payment mechanisms in a better way everyday. The Experimentation Reporting Platform at eBay is developed using Scala, Scoobi, Apache Hive, Teradata, MicroStrategy, InfluxDB, and Grafana, submitting hundreds of Map/Reduce (M/R) jobs to a Hadoop infrastructure. The platform contains well over 35,000 statements and over 25,000 lines of code in around 300 classes. We use Jenkins to set up continuous integration (CI), and one of the challenges for humongous projects involving Hadoop technologies is slow-running unit and integration tests. These test cases run one or several M/R jobs in a local JVM, and that effort involves a considerable amount of set-up and destruction time. As additional automated test cases are added to increase code coverage, they have adverse impacts on overall build completion time. One solution that can improve CI run time is to run automated test cases in a distributed and concurrent manner. This technique helped improve CI running time of the Experimentation Reporting Platform at eBay from 90 minutes to ~10 minutes, thus paving the way for a truly scalable CI solution. This CI involves more than 1,800 unit test cases written in Scala using ScalaTest and Mockito. Jenkins provides support for multi-configuration build jobs. A multi-configuration build job can be thought of as a parameterized build job that can be automatically run on multiple Jenkins nodes with all the possible permutations of parameters that it can accept. They are particularly useful for tests where you can test your application using a single build job but under a wide variety of conditions (different browsers, databases, and so forth). A multi-configuration job allows you to configure a standard Jenkins job and specify a set of slave servers for this job to be executed on. Jenkins is capable of running an instance of the job on each of the specified slaves in parallel, passing each slave ID as a build parameter and aggregating JUnit test results into a single report. The problem boils down to this. There are a number of Jenkins slave nodes, and we have to split all JUnit tests into batches, run all batches in parallel using the available slaves, and aggregate all the test results into a single report. The last two tasks (parallel execution and aggregation) can be solved using built-in Jenkins functionality, namely, multi-configuration jobs (also known as matrix builds). There are a number of different ways that you can set up a distributed build farm using Jenkins, depending on your operating systems and network architecture. In all cases, the fact that a build job is being run on a slave (and how that slave is managed) is transparent for the end-user: the build results and artifacts will always end up on the master server. It is assumed that the Jenkins master server has multiple slave nodes configured and ready for use. A new multi-configuration build is created as shown below. This set-up results in the creation of a multi-configuration project on Jenkins that requires additional configuration before it can be functional. Assuming the project is set up on Git, you can provide the Git SSH URL and build trigger settings as shown here. Now comes the important part that allows you to choose the list of slave machines on which an individual batch of test cases can be executed. In this example, five machines are selected (slave4 is not visible) on which the build will be triggered. In this set-up, a master machine (the part of the distributed CI job that runs on the Master node) dictates the entire run. A multi-configuration build runs as-is on every slave (including the master) machine. Every build receives a $slaveId as a build parameter that allows the script to be written appropriately. The build configuration part of CI involves invocation of a shell script. This shell script performs the following activities. Determine a list of test cases classes that need to be executed. Once the list is obtained, it is shuffled. This process occurs only on the master. Send the complete list to all the slaves. Split the complete list of test cases into batches equal to number of slave machines. Execute each batch on a node (slaves or master) The master node waits for each slave node to complete execution. Each part of distributed CI job runs on the slaves and the master, but the console log of each is available only on the master. As a result, the computation of the number of total number of tests occurs on the master. The following shell script performs the above listed tasks. Once the multi-configuration project is created, it can be run as follows. Each configuration runs a subset of automated test cases on a separate CI node (machine). This allows the entire CI job to complete execution in a distributed manner. This solution is scalable, and as additional automated test cases are added, the execution speed can be maintained by simply adding additional CI slave nodes. Currently, the batch of unit test case classes are distributed randomly across CI machines. There might be a batch that has slower running test cases, thereby slowing down the execution time of the CI build. Better task allocation times can be achieved by recording test class execution times in any relational database so as to analyze them and construct robust batches of unit test classes with uniform execution times. Distributed and concurrent execution of unit test cases has allowed the Experimentation Reporting Platform at eBay to build a CI solution with 1,800+ unit test cases with more than 70% statement and branch coverage. The test cases are a mix of Hadoop tests and non-Hadoop unit tests. Each commit triggers a distributed build that finishes in a timely manner (approximately 10 minutes), allowing the committer to quickly verify each commit. “Speeding Up Hadoop Builds Using Distributed Unit Tests,” by Ilya Katsov Jenkins: The Definitive Guide Book by John Ferguson Smart", "date": "2016-09-12"},
{"website": "Ebay-Engineering", "title": "The Sprinting Pachyderm: Improving Runtime Performance of Your Big Data Application", "author": ["Subrahmanya Harve"], "link": "https://tech.ebayinc.com/engineering/the-sprinting-pachyderm-improving-runtime-performance-of-your-big-data-application/", "abstract": "Disclaimer: No elephants were harmed while writing this blog post! Big Data applications have become ubiquitous in software development. With treasure troves of data being collected by companies, there is always a need to derive business sense from the data quickly or risk losing their temporal context. Companies typically enable the quick pace by decoupling the tasks of setting up and maintaining infrastructure and platform administration from application software development, allowing application developers to focus purely on solving business problems. At the Experimentation team in eBay, we deal with a massive scale of computing numerous statistical measures for over 100 metrics across tens of dimensions. To get an idea of the scale, try sampling (pun intended!) these approximate numbers — We support more than 250 experiments simultaneously, generate about 30 TB per month, and process around 20 TB every day, running more than 300 Map/Reduce (MR) jobs every day. While application developers tend to focus their attention by adding functionality, poor run-time performance of jobs sometimes comes as a rude surprise. In this blog, I present some of the levers that a developer can employ to mitigate performance issues of Big Data applications. Here I will assume that there exists a data processing and storage platform, such as a Hadoop cluster, that MR applications are being developed using some high-level library such as Scoobi (a Scala library), and that jobs are scheduled using an enterprise scheduler such as Automic (UC4), though some of these techniques are applicable across variants of the architecture. When you design MR applications, some key elements need to be factored into the design phase. Here are some of my guiding principles for designing MR applications. Store intermediate and final data with the express purpose of assisting with convenient processing in subsequent stages. It might be obvious to store intermediate data so that it is convenient for the MR job to process it, but the same advantage may be applicable if the final output data is also stored conveniently, because yet another stage may be added as an enhancement in the future. Create self-sufficient logical units (folders) of storage. Here self-sufficiency simply means denormalization. Self-sufficiency allows for a better possibility of parallelism, and better parallelism translates to better amortization of processing costs for a suite of business use cases (or an application as a whole). The trade-off is data duplication but with cheap storage costs. Higher storage requirements should be a good alternative to absorbing higher processing cost, which translates to poor application performance and therefore affects business. To mitigate the higher storage requirements, have a robust purging and archival policy. Design should be flexible enough to accommodate iterative changes later, applied as layers. That is, design should be such that a smaller layer of design change can be easily added on top of the existing design. For example, we could later add a design layer to group the logical units referred to earlier and apply processing on the groups. Start by setting a self-constrained SLA for each MR application. Continuously try to improve processing performance to be less than the SLA. Over time, the SLA will settle on a realistic time. Allow a breach of the SLA only if there is a corresponding increase in input. Use the right tool for the right job. Sometimes it may be necessary to use a library or a tool such as Hive, Spark, or Tez that is not standard for the application being developed because of inherent advantages of the tool, such as joins or in-memory performance. Using the right tool helps prevent re-inventing the wheel if there is a tool already available to use but not yet part of the development environment. A seasoned MR application developer knows the advantages of using Combiners. Among other advantages, they help reduce shuffle time, reduce network clogging, and prevent OOM errors on the reducer side. But what if it’s not easy to write Combine operations for records? In real world Big Data applications, records have complex structures with attributes of Arrays, Maps, etc. Combined records form new records that need to combined again, and care should be taken that the end result of combining n records is the same for multiple executions of the job with Combine operations occurring in any order. That is, Combine operations should not violate associate properties. Here is a great methodology to employ in such cases. Perform Combine operations to generate only partial aggregations and form the map output, and then have the final reduce phase perform the originally intended aggregation for a given reduce key. For example, suppose there is a map of attributes in every record with string keys and double values. Our original intended aggregation is to generate the variance of those double values. In this case, the general method is to use Combiners to generate map-local variance values and then use a Reducer to generate variance across all the map-local variances for a given key. Comparing that with the variances generated without using Combiners for the same key, we find that the values are not the same. The following example illustrates the concept. Formula for Variance = ∑((X i – X M ) 2 )/n Where: X i is a value in the series for i in the range between 1 to n X M is the mean of all the values in the series n is the total number of values in the series We can rewrite the formula as = (X 1 -X M ) 2 + (X 2 -X M ) 2 + (X 3 -X M ) 2 …/ n We can further reduce this to = ((X 1 ) 2 + (X 2 ) 2 + (X 3 ) 2 + … + (nX M ) 2 -2X M (X 1 +X 2 +X 3 …)) /n There are now three parts of the above formula that can independently become candidates for Combine functions. X 1 + X 2 + X 3 …  (sum of values) 1 + 1 + 1 + … n  (Number of Values) (X 1 ) 2 + (X 2 ) 2 + (X 3 ) 2 …  (sum of squares of values) The above three parts form the components necessary to perform final variance calculations in the final Reduce phase. These three parts (or partial aggregations) are computed in the Combiners and written out as part of the map output. Note that the X M can also be computed by just computing (1)÷(2) and then using this value in the formula for Variance. Hence this methodology can be applied to several other such complex computations as long as the computations can be broken down into partial components/aggregations. I refer to this as “staggered computations” in MR jobs and have found them to be highly performant. MR applications can be very simple in nature, such as adding a static text to every row in the input, or they can be very complex with a lot of control flows, filters, etc. Sometimes, a lot of objects can end up being created just for the life of processing the record, discarded, and then recreated during the processing of another record. Such applications provide a lot of opportunity to cache objects. Caching objects, as we all know, prevents most memory issues from persisting and relieves the application from memory pressure. Hierarchical queues allow for the cluster to be composed of a hierarchy of queues, where each child queue is dedicated to a tenant. If hierarchical queues are enabled on the cluster, they can help alleviate the issue of resource hogging by some tenants on the cluster and allow for more available clusters per tenant. This in turn allows for better predictability of your job’s run times and also for better capacity management of the queue resources. Hadoop clusters these days are multi-tenant, so job performance depends heavily on the prevailing workload. Analyze the load patterns on the queue/cluster and look for spiky usage patterns. Ideally, we want to remove spikes from the usage patterns and smooth the curve of usage. This is not an easy task since spikes show up at different times of the day, based on the number of tenants and the number of jobs per tenant. However, once quiet periods of load patterns are identified, the Scheduler that submits the jobs can be tuned to leverage the resources available during the quiet period. This is called job throttling or dynamic scheduling, where jobs are submitted not all at once, but a few at a time. Slowly increase the threshold after observing consistent results. This solution will work only after consistent load patterns are observed. If the load pattern cannot be nailed down to its generic form, then this solution will have no effect on job performance. After the job is submitted, there is no way to control the rate at which resources (mappers/reducers) are allocated to the job, which means throttling can be done only at the time of job submission. In enterprise schedulers such as UC4 by Automic, jobs are queued to be submitted dynamically by writing simple UC4 scripts and by using a nifty parameter called maxParallel that decides the upper limit of jobs to be run in parallel. Hadoop supports various file formats such as Text, CSV, Sequence File, though anyone can implement a new file format support by implementing the corresponding FileFormat interfaces and SerDes. While choosing a file format for Hadoop applications, the key factors to consider are performance, compression, and schema evolution. The aforementioned formats all have performance, compression, and schema disadvantages for large datasets. Schema evolution In practice, Big Data applications evolve over time and so does the data structure/schema. Data being output by one application may be getting consumed by many other applications or end users. The data files in the above formats themselves do not say anything about the schema and have to be parsed every time a file is read to understand the underlying schema which impacts the overall performance of consuming such data. Fields get added, removed, renamed frequently during the evolution of the application. While compression data can prove to be advantageous for storage and transit of data, these advantages must be evaluated against the increase in processing cost during decompression. That is there is a trade off between I/O-bound or CPU-bound compression codecs. There are various codecs, such as gzip, bzip2, LZO, and Snappy, that may be used in conjunction with various output file formats. Choose a compression type that allows for splittable input or partial processing of data. Test out the various combinations of codecs with the output file formats to find out which combination works best for storage, transit, and processing of data for your MR application. Enable compression for both intermediate output and final output to get a sense of the runtime performance advantage. File formats have an impact on the read/write performance of data which is also important for overall application performance but is not directly related to processing performance. If you have more than a few Hive applications or a lot of users querying Hive tables on processed data, then there are file formats that optimize such use cases. More recent file formats, such as RC (Row Columnar), ORC (Optimized Row Columnar), and Parquet, are moving in this direction. All things considered, I recommend using the Avro format , which offers great advantages in schema evolution and compression support (blocks), allows for input splitting, and hence is better for overall performance. Active monitoring of running jobs is essential to understand their performance characteristics. If there is no monitoring in place, then it takes an inordinate amount of time just to understand why a particular job is failing post facto. Needless to say, profiling an MR application is critical to understand its behavior. Counters are a convenient way to monitor the MR application. They can be used to detect skews and anomalies in use cases and failures, and they even create a measure of certain business events. However, counting adds some overhead to the processing and should be used carefully. Counters are updated not only at the task level but at the overall application level, too. Hadoop itself restricts the number of user-defined counters to about 120. Having useful counters helps developers understand the profile of the application in terms of memory and CPU. In Scoobi, a simple construct allows us to look up and increment a counter representing business events. MR jobs can fail due to a variety of reasons, but these failures can be classified into two categories: failures due to the platform or Hadoop cluster and failures due to data states. If the failures are due to the Hadoop cluster, then the chances are that they were due to some transient condition on the cluster such as a data node containing a replica not being reachable or a network glitch making name node unavailable briefly. It does not make sense to manually restart every time such a failure occurs. Auto-remediation is a key part of making the schedule of jobs tolerant to the failures of the cluster. On a scheduler such as UC4, it is quite easy to detect a failure condition and just auto-restart and resubmit the failed job. We would, however, need to cap the number of restarts to a reasonable threshold and send out alerts if remediation was impossible. Applications are typically just a series of computations. By wrapping such computations with timing functions, it is possible to estimate the breakdown of processing costs for all those computations. Further, the processing costs can be saved in user-defined counters to be able to look at the numbers as aggregates. This allows us to optimize specific functions for better performance. Below are two timing functions that capture the processing costs of other computations represented as functions. It does not take long after beginning to write MR applications for you to hit the last-reducer problem. Essentially, this means there is one or a few reducers stuck doing a lot of processing. More often than not, this is due to skewness in your data. This is a hard problem to solve, especially since skewness is observed on the Reducer side. Skew can occur either in the number of Reducer groups or in the data distribution per Reducer group. Here the Reducer group is nothing but a fragment of the data characterized by a single Reducer key. It is relatively easier to solve data skew in the number of Reducer groups by simply having the code output a different key on Mappers breaching a threshold of the number of keys, so that a Reducer with fewer Reducer groups observed may get more of other reducer groups to process. This requires a little bit of processing cost to be absorbed at the Mapper side, but it’s worth trying if the observed runtime performances are significantly better. A simple hash partitioner can be used whenever there is a breach of this threshold number. The skew due to data distribution across reducer groups is a much tougher problem to solve. Here data skew is observed when one or a few Reducer groups have a large number of records to process compared to most of the Reducer groups. This type of skew is harder to solve because the framework is agnostic to the logic of operation being performed in the Reducer. One of the solutions to handle this type of skew is to break up the Reducer workload into two or more stages, in such a way as to mitigate the issue. This approach, however, might need the developer to rework the logic of the Reducer, especially the non-final stages. In other situations, if you are doing a join of two datasets where one of the datasets is heavily skewed while the other is not, you might benefit from using Block Joins. As the name suggests, a block join splits up the skewed data into blocks and replicates the non-skewed dataset into every block. The overall processing cost is higher but is spread more evenly across compute nodes, and using Block Joins also prevents the OOM from occurring on hot compute nodes. There is a definite advantage in trying to tune the job parameters to get better performance of the jobs. However, job parameters are very subjective to the job being tuned. Hence, the parameter values that are good for one job may have an adverse effect on another job. Let’s look at some of the parameters to tune that may easily result in faster runtimes. mapreduce.input.fileinputformat.split.minsize and mapreduce.input.fileinputformat.split.maxsize The above two parameters can be tuned to decide the workload of a mapper within a range. Tuning this range results in a corresponding change in the number of mappers being allocated for the job. For large applications, it is good to cap the mapped.max.split.size at 128 MB. More mappers mean better parallelism and likely faster run time. However, this should be balanced with the prevailing load on the cluster. If the number of maps configured is not available in one pass, runtime performance of the job will increase, thereby annulling the effect of smaller runtimes of individual maps. scoobi.mapreduce.reducers.min and scoobi.mapreduce.reducers.max Sometimes, due to skewness in data, more reduce groups might be getting processed in the same reducer. In such cases, it may be beneficial to increase the number of available Reducers so that the reduce groups are better distributed. Scoobi allows this by tuning the parameters scoobi.mapreduce.reducers.min and scoobi.mapreduce.reducers.max . mapreduce.task.io.sort.factor , mapreduce.task.io.sort.mb and mapreduce.map.sort.spill.percent Before partitions are written into files as map output, the map outputs are sorted in memory. The above three parameters influence this sorting behavior. The sorting itself occurs by default using the QuickSort algorithm. mapreduce.task.io.sort.factor is a number that represents the number of simultaneous streams allowed to be merged at a time. mapreduce.task.io.sort.mb is the size of the buffer memory to use for sorting the map outputs. In practice, we have seen good performance when this setting is 1/4 the total memory setting for the map task. mapreduce.map.sort.spill.percent is a value between 0 and 1 that represents a percent threshold at which the spill of the buffer will start. By default, this setting is at 0.8 (but it depends on individual cluster deployments). mapreduce.map.memory.mb , mapreduce.reduce.memory.mb , mapreduce.map.java.opts , mapreduce.reduce.java.opts mapreduce.map.memory.mb is the amount of memory allocated to each map task, and it should be tuned in the same way as in regular Java applications. Similarly, there exists a parameter for the Reducer task. mapreduce.map.java.opts and mapreduce.reduce.java.opts allow other parameters to be included, such as those related to GC. dfs.block.size Represents the block size in MB that output files should be broken into. Each block will reside in one data node. It is recommended to try to run mappers in map-local state, which means that the entire data needed for a mapper is present on one single node. For this reason, it is better to keep dfs.block.size >= mapred.min.split.size and dfs.block.size <= mapred.max.split.size . If min.split.size is significantly larger than dfs.block.size , mappers would have to read data from other blocks over network, which adds to the run time of the job. mapreduce.reduce.shuffle.parallelcopies This is the number of parallel copies that are allowed to be requested by a reducer. This number represents the number of mappers that can simultaneously serve map output data to the reducers. Tune this to see which setting helps reduce the shuffle time for the job. dfs.replication , dfs.client.block.write.locateFollowingBlock.retries , dfs.client.block.write.retries dfs.replication indicates the number of replicas to be stored on the cluster. If it makes sense, reduce the number of replicas so that the overall storage footprint goes down. The trade-off is that jobs may fail if data nodes with these fewer replicas are simultaneously unavailable. dfs.client.block.write.locateFollowingBlock.retries indicates the number of retries that a job client must make to locate a suitable end-point data node before deciding to fail the job during writes. dfs.client.block.write.retries indicates the number of retries that a job must make while writing the output of the job. dfs.replication indicates the number of replicas to be stored on the cluster. If it makes sense, reduce the number of replicas so that the overall storage footprint goes down. The trade-off is that jobs may fail if data nodes with these fewer replicas are simultaneously unavailable. dfs.client.block.write.locateFollowingBlock.retries indicates the number of retries that a job client must make to locate a suitable end-point data node before deciding to fail the job during writes. dfs.client.block.write.retries indicates the number of retries that a job must make while writing the output of the job. mapred.compress.map.output and mapred.map.output.compression.codec It can be beneficial to compress the map output so that it occupies a smaller storage footprint, This setting also provides the advantage of faster shuffle time over the network. mapred.compress.map.output represents a boolean value to toggle between allowing compression or not for the map output. The LZO codec offers reasonable performance, even though compression is not great compared to others such as Snappy. However, YMMV. Performance engineering of an MR application is a long, drawn-out affair. It is never a finished task, since application profile, input data, size, and other characteristics change over time. The purpose of this post is to provide a direction to design and develop MR applications specifically with performance in focus. There are plenty of other ideas, parameters, and tricks for many programming languages out there to learn, implement, and take lessons from practical observations. Go ahead and create your own Sprinting Pachyderm!", "date": "2016-08-12"},
{"website": "Ebay-Engineering", "title": "eBay Releases Dynamic Application Security Testing Proxy as Open Source", "author": ["Srinivasa Rao Chirathanagandla"], "link": "https://tech.ebayinc.com/engineering/ebay-releases-dynamic-application-security-testing-proxy-as-open-source/", "abstract": "In an effort to contribute to the open-source community for security, Global Information Security (GIS) at eBay released its DAST Proxy as open-source software. DAST Proxy is a life-cycle management tool for dynamic application security scans that has a unique feature set. It is available for download and contribution under the MIT License at https://github.com/eBay/DASTProxy . DAST Proxy has work flows that help users record browser actions and submit them to a backend scan engine, such as AppScan. It updates the user with the scan status and publishes the scan results. It supports automation integration and has a set of RESTful web services that can be seamlessly integrated into any existing Selenium (or any other automation framework) functional test cases for security testing. DAST Proxy also works with all the browser-based test cases for both web and mobile applications. This section explains how to conduct a manual dynamic security scan using DAST Proxy. To start, the user is required to have two browsers installed. On Browser 1, the user obtains a proxy host and port generated by the DAST server on DAST Proxy’s home page. The user then inserts this host and port into Browser 2’s proxy settings. Once the proxy is set up, DAST Proxy records all the web traffic between Browser 2 and the QA server and stores it in a HAR file. The same file is then submitted to the back-end scan engine for thorough dynamic security testing. DAST Proxy polls the back-end engine for the status and resultant vulnerabilities and stores them in the database, which is accessible to the user via the DAST Proxy dashboard. Recording the scan and submitting it to a back-end scan engine, such as AppScan Dashboard with list of scans, vulnerabilities, and payloads Integration with JIRA system Ability to rerun the scans from the dashboard Support for manual API-end point testing with browser plug-ins, such as Postman ZAP (OWASP Zed Attack Proxy project) integration Selenium integration NT OBJECTives integration", "date": "2016-09-19"},
{"website": "Ebay-Engineering", "title": "A Glimpse into Experimentation Reporting at eBay", "author": ["Arun Akkinapalli"], "link": "https://tech.ebayinc.com/engineering/a-glimpse-into-experimentation-reporting-at-ebay/", "abstract": "Around 1500 A/B tests are performed on eBay across different sites and devices on a yearly basis. Experimentation forms a key business process at eBay and plays an important role in the continual improvement of business performance through the optimization of the user experience. Different data insights from these tests enable users to answer important questions such as “How will this new product feature benefit eBay?” or “Does this new page layout improve user engagement and increase GMB?” Testing allows business units to conceptually explore new ideas with respect to page content and style, search algorithms, product features, etc., which can vary from subtle to radical variations. Such test variations can be easily targeted to segments of the total customer population based on the desired percentage/ramp up and contextual criteria (geographic, system or app-specific), providing a level of assurance before launching to a broader audience. Lifecycle of an experiment at eBay All the experiments begin with an idea. The first step is to prepare a test proposal document, which has the summary of what is being tested, why it’s being tested, the amount of traffic assigned, and what action is going to be taken once the results are published. This document will be reviewed and approved in TPS council meetings every week. The next step is for the test operations team to interact with the product development team to find the right slot for the test schedule, understand the impact of interaction with other tests, and then set up the experiment, assigning necessary traffic to treatment and control, and launch it after smoke testing (a minimal amount of traffic is assigned to make sure everything is working as expected) is successful and necessary validation steps are completed. The next step is the launch of the experiment. Tracking the experiment will immediately begin, and data is collected. Different reports providing necessary insights will be generated on a daily basis and for the cumulative period through the data collection period. The final results will be published to a wider audience after the experiment is complete. This completes the life cycle of an experiment. This post will provide a quick overview of the reporting process. Before going further, let’s define some basic terms related to experimentation. Guid: A visitor is uniquely identified with a GUID (Global Unique ID). This is a fundamental unit of our traffic that represents the browser on a machine (PC or handheld) visiting the site. It is identified from the cookies that a particular eBay site drops on the user’s browser cache. UserId: A unique ID assigned to each registered user on the site. Event : Every activity of the user captured on the site. Session : All the activity of the user until 30 minutes of inactivity elapses within a day. The aggregate of many events constitute a session. GUID MOD: 100% of the eBay population is divided into 100 different buckets. A Java hash will convert the GUID into a 10-digit hash, and the modulo of the GUID is extracted from this hash, which represents the bucket that the GUID is assigned. A specific GUID will never fall into two different GUID MODs. Treatment and control: The feature to be tested is referred as “treatment,” and “control” is the default behavior. Versions: Any change in the experiment during the active state will create a new version of the experiment. Major and Minor versions are created based on the change’s impact on the experiment. Classifier: A classifier is one of the primary dimensions on which we slice the data and report for different dimensions and metrics under it: Total GUID Inclusive (TGI) — All the GUIDS that are qualified for a particular treatment or control Treated — All the GUIDS that have seen the experience of the treatment Untreated — All the GUIDS that are qualified but have not seen the experience of the treatment Total GUID Inclusive (TGI) — All the GUIDS that are qualified for a particular treatment or control Treated — All the GUIDS that have seen the experience of the treatment Untreated — All the GUIDS that are qualified but have not seen the experience of the treatment The following figure shows a simplified view of the reporting process. Let us outline the upstream data sets that the process depends on. The data is stored in Hadoop and Teradata systems. User data: Event-level raw data of the user activity on the site, updated every hour. Transaction and activity data: Data sets that capture the metric-level activity of the user such as bid, offer, watch, and many more. Experiment metadata: Metadata tables that provide information about the experiments, treatments, GUID MOD, and various other parameters. Every day the process first checks for the upstream data sets to be loaded, and stage 1 will be triggered after all the data sets are available. In this stage, detail data sets at the GUID and Session levels are generated from the event level data. Treatment session: This is one of the primary data set which has GUID and Session-level data at the treatment and version levels. There are various indicators of different dimensions that we will not cover in this post. Transaction detail data set: All the activity of GUID and Sessions related to transaction metrics such as revenue are captured here. This data set will not have any treatment-level data. Activity Detail data set: Same as the transaction detail data set but for activity-level metrics such as bid, offer, bin, watch and so on, which are captured here. There are around six more data sets we generate on a daily basis. We will not go in details about them in this post. All the processing happens on Hadoop, and the data will be copied to Teradata for analysts to access them. The data sets generated in stage 1 act as upstream data sets for stage 2. In this stage, lots of data transformations and manipulations happen. Data is stored at the GUID, treatment, and dimension levels and stored in Hadoop. This data will not be moved to Teradata, because this stage acts like an intermediate step for our process. Outlier capping is applied to the metrics from the data sets populated from stage 2 to handle extreme values. The output from the stage 2 is fed into the stage 3, which is the summary process. The data will be aggregated at the treatment, version, and dimension levels, and all the summary statistics are calculated in this step. The data is stored in Hadoop and copied over to Teradata for MicroStrategy to access this information and publish different reports. Post-stratification is an adjustment method in data analysis. It is used to reduce the variance of estimations. In stratification, subjects are randomized to treatment and control at the beginning of the experiment. After data collection, they are stratified according to pre-experiment features, so that subjects are more similar within a stratum than outside that stratum. The overall treatment effect is then estimated by the weighted average of treatment effects within individual strata. Because the variance of the overall treatment effect estimation consists of variance due to noise and variance due to differences across strata, stratifying experiment subjects removes variance due to strata difference, and thus variance of the estimated overall treatment effect is reduced. This process runs in parallel and generates stratified transactional metrics. The processing happens in Hadoop, and data is copied over to Teradata for access to different reports. Scala, Hive, SQL, SAS, R, and MicroStrategy are some of the technologies and statistical packages we use throughout the process. Most of the processing happens in Hadoop, and minor manipulations occur in Teradata. This concludes the main topic of this post. One of the critical aspects during this process is data quality, as inaccurate results can have impact on the decisions to be made. In the next post, we will talk about different data quality initiatives and how we are tackling them. Happy Testing!", "date": "2016-09-14"},
{"website": "Ebay-Engineering", "title": "Experience the Lightning Bolt", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/experience-the-lightning-bolt/", "abstract": "Three months back we announced how we are transforming the shopping experience at eBay, enabling our users to browse with style and speed. Our goal was to provide an engaging experience not only to users who are within the eBay site, but also to mobile users accessing eBay from external platforms like Google and Twitter. This is where AMP technology comes into play. We implemented an AMP version for our new product browse experience, along with the regular mobile web pages, and launched them in June. At that time we did not make our AMP content discoverable to Google, as we had a few pending tasks to be completed. Also, AMP links surfaced in Google search results only for publisher-based content and not for eCommerce. Things have changed now. Google announced that they are opening AMP beyond the news industry to include eCommerce, travel and so on. From our end, we wrapped up the pending items and linked the AMP pages from non-AMP pages to make them discoverable. Today we are happy to announce that users around the globe will start seeing eBay AMP links in Google search results and experience the lightning bolt — instant loading. We have close to 15 million AMP-based product browse pages, but not all will appear as AMP right away. This feature is being ramped up and will eventually surface. Check out some of the popular queries in a mobile browser — “ iPhone 6 no contract ” and “ canon digital cameras ,” for example. The AMP lightning bolt appears next to links as an indication. AMP for eCommerce is now a reality. eBay AMP link in Google search results (left); eBay AMP product browse page (right) Following our initial launch in June, we did a couple of things to make AMP ready for prime time. We outline a few these efforts here. Understanding how users interact with our pages is critical for us to provide the most optimized experience. The back-end system that powers the new product browse experience is designed in such a way that it constantly collects users’ screen activity, learns from it, and optimizes the experience for subsequent visits. For example, if users interact more often with a module that appears below the fold in the screen, then in future visits to the same browse page, that module will start appearing above the fold . Our non-AMP page has a custom analytics library that does the reporting to the back end. AMP has a component ( amp-analytics ) for doing this. In our initial AMP launch, we used this component just to track page impressions. It provides a fairly exhaustive tracking mechanism. But what we wanted was more granular control at an element level, where each element dictates what it wants to track. We started working with the AMP team on this and came up with a spec . We went ahead implemented the spec and contributed it back to the open-source project. With the implementation in place, we were able to achieve a robust and advanced analytic system that reports user interactions like click, scroll, and visibility to our backend, which in turn optimizes the subsequent visits. We mentioned in our previous blog that most of the code is shared between the AMP and non-AMP pages. Even with this code sharing, there were still small feature inconsistencies between the two versions. We closed these gaps, fixed the inconsistencies, and put a process in place to make sure they do not creep in. Having said that, there were certain UI components and behaviors that we were not able to achieve in the AMP version due to restrictions. Some of these components are eCommerce-specific. We are working with the AMP team to add them to the component list so everyone can benefit. A good example would be tabbed UI component, and there is already a feature request to get this implemented. During the initial launch, we put manual effort into managing assets (CSS and JavaScript) between the AMP and non-AMP versions. In the AMP version, there should be no JavaScript, and all CSS should be inline, whereas in the non-AMP version, both CSS and JavaScript should be bundled and externalized. Doing this manually was not ideal. Our asset pipeline tool, Lasso , had a solution for this — conditional dependencies . We created an AMP flag that gets initialized to true if the request is AMP and then set as a Lasso flag . The pipeline gets access to it and automatically bundles, externalizes, or inlines resources based on the conditions. This was a big time saver and ended up being very efficient. We are not done yet; in fact, we are just getting started. We have a bunch of tasks lined up. Beyond AMP — We know AMP pages are fast. But what about the subsequent pages the user visits? Currently when users click on a link in the AMP page, a new tab opens, and the destination page is loaded there. In our case, the mobile web version of the destination page is loaded. We want that experience also to be as fast and consistent as the AMP experience. There is an AMP component ( amp-install-serviceworker ) to achieve this goal, and our top priority is to leverage this utility and create a seamless transition from the AMP to the target page. We are also discussing with the Google team about how to avoid the new tab and continue the experience in the same window. Cache freshness — AMP content is served from Google AMP cache, and the cache update policy can be found here . What this means to eBay is, for popular product queries, users always see fresh content. But for certain extremely rare queries, a few users may end up seeing stale content. While this is not a common scenario, there is an AMP component ( amp-fresh ) in the works to fix this. We will be integrating this component as soon as it is ready. In the meanwhile, we have a script that we manually run for a few products to update the AMP content in cache. Unified version — Currently we have two versions of the new browse pages — AMP and non-AMP. The AMP version shows up to users searching in Google, and the non-AMP version shows up to users searching within eBay. Although both of them are highly optimized, look the same, and share most of the code, updating both versions is still a maintenance overhead. In addition, we always need to watch out for feature parity. In the future, based on how AMP pages are performing, we may choose to have one mobile version (AMP) and serve it to all platforms. We are very excited to provide the AMP experience to our mobile users coming from Google. We have been playing with it for a while, and it is indeed lightning fast. Mobile browsing can be slow and sometimes frustrating, and this is where AMP comes in and guarantees a consistent and fast experience. We hope our users benefit from this new technology. Senthil Padmanabhan | Principal Engineer at eBay", "date": "2016-09-21"},
{"website": "Ebay-Engineering", "title": "Visualizing Machine Translation Quality Data — Part I", "author": ["Juan Rowda"], "link": "https://tech.ebayinc.com/engineering/visualizing-machine-translation-quality-data-part-i/", "abstract": "We are, no doubt, living some of the most exciting days of the Information Age. Computers keep getting faster, smartphones are ubiquitous. Huge amounts of data are created daily by amazingly diverse sources. It is definitely easier than ever to gather data for language services buyers and providers, but it looks like the localization industry is really not doing a lot to harness all this information. Overall, and with a few exceptions, of course, the industry seems to be missing out on this and is not fully leveraging, or at least trying to understand, all these wonderful bits and pieces of information that are generated every day. Perhaps the issue is that too much information can be hard to make sense out of and may even feel overwhelming. That is, precisely, the advantage of data visualization. In this series of articles, I will cover three different tools you can use to visualize your translation quality data: Tableau, TAUS DQF, and Excel. This article is part 1 and will only focus on general information and Tableau. Perhaps the single most important point of data visualization is that it allows you to assimilate information in a very natural way . An enormous amount of information that is difficult to take in when in a table suddenly makes sense when presented and summarized in a nice chart. Patterns and trends may become easier to spot and, sometimes, even obvious. Correlations may pop up and give you much-needed business or strategic advantages, allowing you to effectively act on your information. How does this apply to translation and localization practices? Well, there simply is a lot of information you can measure and analyze, for example: Productivity Vendor performance MT system performance Tool performance Financials Process efficiency, etc. At eBay, we use data visualization to track our vendors’ performance, the quality of our MT output for different language combinations, details on the types of issues found in our MT output, what types of issues we are finding in our vendors’ deliverables, and more. Let’s take a minute to examine what is necessary to make a visualization effective. I’m by no means an expert on this subject, you’ll notice, but based on my experience and research, these are the key points to consider: First of all, be clear . What are you trying to find out with a chart? What do you want to bring the attention to? What are you trying to say? Transmitting a clear message is a priority. Be selective : don’t cram columns and lines in a visualization just because. Carefully plan the data points you want to include, assessing if they contribute or not to the main purpose of your message. This can be difficult, especially if you have too much information – you may feel tempted to add information that might not add any value at all. Keep your audience in mind, and be relevant . Shape your message to answer the questions they may have. Discard any information they may find unnecessary. Project managers may be interested in financials and the percentage of on-time deliveries, and engineers on process efficiencies, while language managers may be focused on quality and language performance. Put some thinking on what’s the best way to represent the information and how you can make the most important information stand out. It’s usually a good idea to include trends, highlight patterns, and make meaningful correlations obvious. Tableau is perhaps one of the most popular visualization programs available. The concept is simple: Tableau can read your data, from a simple Excel file or a database (among several other options), parse it, and turn the information into dimensions and measures. And here’s the best part: you can simply drag and drop those dimensions and measures onto columns and rows, and Tableau will generate charts (or views , as they like to call them) for you. Automatically. Effortlessly. And it comes with an amazing range of chart options and customization options that may seem overwhelming when you start using the software but, once you get the hang of it, make total sense. Let’s look at some examples: This chart shows in a very simple way how vendors are performing for each of the two content types we are working with at the moment, that is, titles and descriptions. It becomes evident that Vendor 2 may be the best option for descriptions while Vendor 5 is underperforming when it comes to titles. Now, let’s imagine we want to analyze how post-editors for the different languages are doing, again based on the content type. We can take a look at how many errors reviewers found for each of them. Here it becomes evident that German post-editors are doing great with descriptions, but they are struggling with titles, as there’s a big difference in the position of the blue columns. We can also see that Spanish and French seem to be above the error average. Italian, Portuguese and Russian don’t show major changes from one content type to the other. Now we want to dig deeper into the errors our reviewers are finding, and for that, we are going to look at the different types of errors by language. Looking at this chart, it seems like the biggest problem are mistranslations. This is a good hint to try to find out why is this happening: Is the source too complex? Are post-editors not doing enough research? Are we providing the right reference material? On the other hand, data seems to indicate that terminology is not really a big problem. We could infer that our glossaries are probably good, our tool is showing the right glossary matches, and our translators are subject matter experts. We can also see that French has many more issues than Italian, for example. Tableau will easily let you swap your columns and rows to change the way the data is presented. In the example below, the focus is now on error categories and not on the number of errors found. However, what I don’t like in this view is that the names of the error categories are vertical and are hard to read — it is possible to rotate them, but that will make the chart wider. There are plenty of options you can try to create a view that shows exactly what you want, in the best possible way. We can also see that French has many more issues than Italian, for example. Tableau will easily let you swap your columns and rows to change the way the data is presented. In the example below, the focus is now on error categories and not on the number of errors found. However, what I don’t like in this view is that the names of the error categories are vertical and are hard to read — it is possible to rotate them, but that will make the chart wider. There are plenty of options you can try to create a view that shows exactly what you want, in the best possible way. Here’s a very simple one to quickly see what are the busiest months based on the number of words processed. Now we want to look at edit distance — analyzing this information can help us figure out, for example, MT performance by language, considering that a low edit distance indicates less post-editing effort. I’m going to include the wordcount, as well, to see the edit distance in context. I can ask Tableau to display the average edit distance for all languages by placing a line in the chart. The blue dots indicate that German is the language with the lowest edit distance, with an average of 21.15. This may be an indication that my DE output is good, or at least better than the rest of the languages. The red dots for Italian are all over the place, which may indicate that the quality of my IT MT output is inconsistent — just the opposite of Portuguese, with most purple dots concentrated in the center of the chart. I can ask Tableau to display the average edit distance for all languages by placing a line in the chart. The blue dots indicate that German is the language with the lowest edit distance, with an average of 21.15. This may be an indication that my DE output is good, or at least better than the rest of the languages. The red dots for Italian are all over the place, which may indicate that the quality of my IT MT output is inconsistent — just the opposite of Portuguese, with most purple dots concentrated in the center of the chart. In this final example, let’s assume we want to see how much content our reviewers are covering; ideally, they should be reviewing 50% of the total wordcount. Here we can see, by language, how many words we’ve processed and how many were reviewed. You can quickly see that the wordcount for French doubles the Russian wordcount. You can also easily notice that the FR reviewer is not covering as much as the rest. This may indicate that you need another reviewer or that the current reviewer is underperforming. Compare this to Portuguese, where the difference between total words and reviewed words is minimal. If we only need to review 50% of the content, PT reviewer is covering too much.", "date": "2016-10-03"},
{"website": "Ebay-Engineering", "title": "Griffin — Model-driven Data Quality Service on the Cloud for Both Real-time and Batch Data", "author": ["Alex Lv"], "link": "https://tech.ebayinc.com/engineering/griffin-model-driven-data-quality-service-on-cloud-for-both-real-time-and-batch-data/", "abstract": "At eBay, when people use big data (Hadoop or other streaming systems), measurement of data quality is a significant challenge. Different teams have built customized tools to detect and analyze data quality issues within their own domains. As a platform organization, we think of taking a platform approach to commonly occurring patterns. As such, we are building a platform to provide shared infrastructure and generic features to solve common data quality pain points. This will enable us to build trusted data assets. Currently it is very difficult and costly to validate data quality when we have large volumes of related data flowing across multiple platforms (streaming and batch). Take eBay’s Bullseye Personalization Platform as an example: every day we have to validate the data quality for ~600M records. Data quality often becomes an enormous challenge in this complex environment and at this massive scale. Our investigation found the following gaps at eBay: No end-to-end unified view of data quality from multiple data sources to target applications that takes account of data lineage. This results in a long delay in identifying and fixing data quality issues. No system to measure data quality in streaming mode through self-service. The need is for a system with a simple tool for registering data assets, defining data quality models, visualizing and monitoring data quality, and alerting teams when an issue is detected. No shared platform and API service. Each team should not have to apply and manage its own hardware and software infrastructure to solve this common problem. With these needs in mind, we decided to build Griffin, a data quality service that aims to solve these shortcomings. Griffin is an open-source solution for validating the quality of data in an environment with distributed data systems, such as Hadoop, Spark, and Storm. It creates a unified process to define, measure, and report quality for the data assets in these systems. You can see Griffin’s source code at its home page on GitHub. Accuracy measurement: Assessment of the accuracy of a data asset compared to a verifiable source Data profiling: Statistical analysis and assessment of data values within a data asset for consistency, uniqueness, and logic Anomaly detection: Pre-built algorithmic functions for the identification of events that do not conform to an expected pattern in a data asset Visualization: Dashboards that can report the state of data quality Real-time: The data quality checks can be executed in real time to detect issues faster. Extensible: The solution can work with multiple data systems. Scalable: The solution is designed to work on large volumes of data. It currently runs on ~1.2 PB of data. Self-serviceable: The solution provides a simple user interface to define new data assets and rules. It also allows users to visualize the data quality dashboards and personalize their view of the dashboards. Griffin has been deployed at eBay and is serving major data systems. It takes a platform approach to providing generic features to solve common data quality validation pain points. To detect data quality issues, the key process is as follows. The user registers the data asset. The Model Engine creates a data quality model for the data asset. The Model Engine calculates metrics. Any data quality issue is reported through email or the web portal. The following BPMN (Business Process Model and Notation) diagram illustrates the system process. The following sections describe each step in detail. The user can register the data set to be used for a data quality check. The data set can be batch data in an RDBMS (for example, Teradata), a Hadoop system, or near real-time streaming data from Kafka, Storm, and other real-time data platforms. Normally, some basic information should be provided for the data asset, including name, type, schema definition, owner, and other items. After the data asset is ready, the user can create a data quality model to define the data quality rules and metadata. We can define models for different data quality dimensions, such as accuracy, data profiling, anomaly detection, validity, timeliness, and so on. The model or rule is executed automatically (by the Model Engine) to get the sample data quality validation results in a few seconds for streaming data. “ Data quality model design ” introduces the details of how the Model Engine is designed and executed. The models are running on Spark. They can calculate data quality values for both real-time and batch data. Large-scale data can be handled in a timely fashion. After the data quality values are calculated, the metrics value is generated based on the calculation results and persisted in the MongoDB database. If any metrics value is below its threshold, an email notification is triggered and the end user is notified as soon as any data quality issue occurs. Finally, all metrics values are displayed in the web portal, so that the user can analyze the data quality results through Griffin’s built-in visualization tool and then take action. To accomplish this process, we designed three layers for the entire system, as shown in the following architecture design diagram: Data collection and processing layer Back-end service layer User interface The key component of this layer is our Model Engine . Griffin is a model-driven solution, and the user can choose various data quality dimensions to execute data quality validation based on a selected target data set or source data set (as the golden reference data). It has a corresponding library supporting it in the back end for measurements. We support two kinds of data sources: batch data and real-time data. For batch mode, we can collect the data source from our Hadoop platform by various data connectors. For real-time mode, we can connect with messaging systems like Kafka to achieve near real-time analysis. After retrieving the data, the Model Engine computes data quality metrics in our Spark cluster. On the back-end service layer, we have three key components. The Core Service is responsible for metadata management, such as model definition, subscription management, user customization, and so on. The Job Scheduler is responsible for scheduling the jobs, interacting with Model Engine, saving metrics values, sending email notifications, etc. RESTful web services accomplish all the functions of Griffin, such as registering data sets, creating data quality models, publishing metrics, retrieving metrics, and adding subscriptions. Developers can develop their own user interfaces using these web services. We have a built-in visualization tool for Griffin. It’s a web front-end application that leverages AngularJS and eCharts to give you an effective tool for performing data quality activities. Here are some screenshots. Besides the built-in UI, developers can easily develop other kinds of user interfaces by calling the RESTful services provided by Griffin. Currently, Griffin support three types of models: Accuracy Data profiling Anomaly detection Accuracy provides the measurement of the accuracy rate for a data asset. Data profiling provides a way to perform data assessment by investigating the characteristics of subject data sets. Anomaly detection provides the ability to predict data issues by applying some mathematical algorithms. Given a data set, does this target data set accurately represent the “real-world” values that they are expected to represent? We can define “real-world” values as a source of truth or golden reference data set, which could come from upstream after some data processing logic or from the user’s requirement directly or from a third-party’s certified data. Now we know how to get a golden data set and target data set, and furthermore, if we know how to compare the target data set against the golden data set by defining some mapping rules, we can measure the accuracy of the target data set. For example, if a source file has 100 records, but in the target file only 95 records exactly match with records in the source file, then the accuracy rate is 95/100 * 100% = 95.00%. Creating an accuracy model takes three steps: The user defines the golden data set (as the source of truth). In our solution, the user can register the golden data set first or just select an existing one in the next step. The user defines mapping rules between the target data set and the golden data set. In our solution, the user can define mapping rules by selecting corresponding columns (fields) in the UI page. The user submits the job, and back-end jobs calculate the accuracy model. This section describes how the back end measures the accuracy dimension of a target data set T, given the source of truth as golden data set S. To measure the accuracy quality of target dataset T, the basic approach is to calculate the discrepancy between the target and source data sets by going through their contents and examining whether all fields are exactly matched as below, Our two data sets are too big to fit in one box, so our approach is to leverage the MapReduce programming model by distributed computing. The real challenge is how to make this comparing algorithm generic enough to relieve data analysts and data scientists from coding burdens and at the same time keep it flexible enough to cover most accuracy requirements. The conventional way is to use SQL joins to calculate this, like scripts in Hive, but this SQL-based solution can be improved since it has not considered the unique natures of the source data set and target data set in this context. Our approach is to provide a generic accuracy model, after taking into consideration the special natures of the source data set and target data set. Our implementation is in Scala, leveraging Scala’s declarative capability to accommodate various requirements and running in a Spark cluster. Data quality issues can be identified via different data profiling types. Profiling results can be compared with documented expectations, and an alert report is triggered if the result doesn’t meet the expectations. There are three types of profiling provided in our framework: Simple statistics generates null, unique, and duplicate count profiles. For example, the null count profile reports the count of null values in the selected column. It helps the customer to identify problems in the data, such as an unexpectedly high ratio of null values in a column. An example is to profile an Email Address column and discover an unacceptably high volume of missing email addresses. Summary statistics generate max, min, mean, and median number profiles. For example, for Age, the value usually should be less than 150 and greater than 0. The user can do range checking with the max/min profile on the Age column. Advanced statistics generates the frequency of pattern profiles, expressed with regular expressions. For example, a pattern profile of a United States Zip Code column might produce the regular expressions \\d{5}-\\d{4} , \\d{5} , and \\d{9} . If you see other formats, your data likely contains values that are not valid or in an incorrect format. Our data profiling mechanism is based on the column summary statistics functions provided in MLib of Spark, which enables us to calculate only once for all basic statistics on Number data type columns. Fast profiling of big data, since our framework is based on Spark Auto-scheduling for data profiling after model creation Visualization including a history trend The goal of anomaly detection is to identify cases that are unusual within data that is seemingly homogeneous. Anomaly detection is an important tool for detecting data quality issues. For now, we have implemented some statistical detection functions by using the Bollinger Band and MAD (Mean Absolute Deviation) algorithms to find those data sets whose total count falls out of expected region. The expected region is calculated based on the history trend of each day’s total count. Our anomaly detection also allows users to adjust parameters in the algorithm as needed and dynamically show the results after changing the parameters, so that anomaly detection is customized for the specific user. Let’s take MAD as an example, the MAD of a data set is the average distance between each data value and the mean. These steps calculate the MAD: Find the mean (average). Find the difference between each data value and the mean. Take the absolute value of each difference. Find the mean (average) of these differences. The following diagram shows the formula of MAD: The calculation of Bollinger Bands is similar to that of MAD. For more information, refer to Wikipedia’s article about Bollinger Bands . Griffin is deployed in production at eBay and provides centralized data quality service for several eBay systems (for example, the Bullseye Personalization Platform, Hadoop data sets, and site-speed data). Griffin validates more than 800M records daily. We will introduce Griffin to more eBay systems, making it the unified data quality platform within eBay. We will support more data quality dimensions, such as validity, completeness, uniqueness, timeliness, and consistency. We will develop more machine-learning algorithms to detect even deeper relationships within data content and find data quality issues.", "date": "2016-10-12"},
{"website": "Ebay-Engineering", "title": "Customizing Spring Security with Multiple Authentications", "author": ["Benoy Antony", "Karthik Chandrasekar"], "link": "https://tech.ebayinc.com/engineering/customizing-spring-security-with-multiple-authentications/", "abstract": "Spring Boot offers an easier way to create new web applications or web services. The Security module in the Spring framework enables us to plug in different authentication mechanisms. In some cases, we needed to provide multiple authentication mechanisms for our web service. These authentication mechanisms can be standard or custom. We had a similar requirement while working on an in-house project to develop a web service for distributing and renewing Kerberos key tabs. The project is named Kite , and it is a web service built on Spring Boot. We initially added SPNEGO to authenticate users of our Kite service. To enable security and add SPNEGO, we needed to make changes to our pom.xml . The relevant POM changes are shown here: We also needed to add Java code to configure SPENGO authentication. The relevant parts of the Java code related to hooking up SPENGO authentication are shown below: These POM and source code changes enable users to authenticate via Kerberos. Some of our users needed an alternate form of authentication, where the users present a one-time-use token. To hook up our custom token authentication, we took the following steps: Implement token authentication logic as TokenAuthenticationFilter by extending AbstractAuthenticationProcessingFilter . Plug in TokenAuthenticationFilter via FilterRegistrationBean . The custom token-based authentication filter extends AbstractAuthenticationProcessingFilter . Here we override the doFilter to implement our custom authentication logic. Note that in authUserByToken , we created a SecurityUser object, and this needs to be of the format user@REALM . To plug in the new authentication mechanism, we can use the FilterRegistrationBean . The code snippet below is added to the SecurityConfig above: With these code changes, we can add our custom authentication logic in Spring in addition to the existing authentication mechanisms.", "date": "2016-09-26"},
{"website": "Ebay-Engineering", "title": "Monitoring Anomalies in the Experimentation Platform", "author": ["Deepak Vasthimal", "Roopa Penmetsa"], "link": "https://tech.ebayinc.com/engineering/monitoring-anomalies-in-the-experimentation-platform/", "abstract": "The Experimentation platform at eBay runs around 1500 experiments that are responsible for processing over hundreds of TBs of reporting data contained in millions of files using Hadoop infrastructure and consuming thousands of computing resources. The entire report generation process contains well over 200 metrics, and it enables millions of customers to experience small and large innovations that enable them to buy and sell products in various countries in diverse currencies and using diverse payment mechanisms in a better way everyday. The Experimentation reporting platform at eBay is developed using Scala, Scoobi, Apache Hive, Teradata, MicroStrategy, InfluxDB, and Grafana. Our user-behavior tracking platform enables us to gain insights into how customers behave and how products are used and unlock the information needed to build the right strategies for improving conversion, deepening engagement, and maximizing retention. The eBay platform contains hundreds of applications that enable users to search for products, view specific product, and engage in commerce. These applications are running on numerous servers in data centers across the world, and they log details of every event that occurs between a user and eBay (in a specific application), such as activity (view product, perform search, add to cart, and ask questions, to name a few) and transaction (BID, BIN, and Buyer Offer, for example), including the list of experiments that a user is qualified for and has experienced during that event. Tracking data is moved from application servers to distributed systems like Hadoop and Teradata for post-processing, analytics, and archival. Any experiment that runs on the Experimentation platform can experience anomalies that need to be identified, monitored, and rectified in order to achieve the goal of that experiment. Traffic corruption. An experiment is set up to ensure that it receives an approximately equal share of unique visitors, identified by GUID (global unique identifier) or UID (signed-in), throughout its life cycle. At times, this traffic share is significantly skewed between experiment (the new experience) and control (the default experience), potentially resulting in incorrect computation of bankable and non-bankable metrics. This is one of the critical anomalies and is carefully monitored. Tag corruption. The vast amounts of user activity collected by eBay application servers include information (tags) about the related list of experiments that a user is qualified for. Any kind of corruption or data loss can significantly hamper metrics computed for any experiment. Here are some typical reasons for these anomalies: GUID reset: GUIDs are stored on browser cookies. Any kind of application error or mishandling of browser upgrades can cause GUID resets against either the experiment or the control, resulting in traffic corruption. Cache refresh: eBay application servers maintain caches of experiment configurations. A software or hardware glitch can cause the caches on these servers to go out of sync. This problem can lead to both traffic and tag corruption. Application anomalies: Web pages are served by application servers. These application servers invoke several experimentation services to determine the list of experiments that a user is qualified for, based on several factors. Application servers can incorrectly log this information, thereby corrupting essential tags because of incorrect encoding, truncation, and application errors. This problem results in both traffic and tag corruption. Anomalies in experiments are detected daily and ingested into InfluxDB, an open-source time-series database, visualized with Grafana. InfluxDB is an open-source database, specifically designed to handle time-series data with high availability and high performance requirements. InfluxDB installs in minutes without external dependencies, yet is flexible and scalable enough for complex deployments. InfluxDB offers these features, among many others. InfluxDB possesses on-the-fly computational capabilities that allow data to become available within milliseconds of its capture. InfluxDB can store billions of data points for historical analysis. InfluxDB aggregates and precomputes time-series data before it is written to disk. Grafana provides a powerful and elegant way to create, explore, and share dashboards and data with your team. Grafana includes these features among many others: Fast and flexible client-side graphs with a multitude of options Drag-and-drop panels, where you can change row and panel widths easily Support for several back-end time series databases, like InfluxDB, Prometheus, Graphite, and Elastic Search, with the capability to plug in custom databases Shareable links to dashboards or full-screen panels The Experimentation reporting platform leverages both InfluxDB and Grafana to monitor anomalies in experiments. It supports the following features. The home page consists of a bird’s-eye view of all anomalies, broken at various levels like channel, business (application), and country. Every anomaly has certain threshold beyond which it needs to be further analyzed. The Gauge panel in Grafana enables us to do just that. Any anomaly can be further analyzed in a drill-down view that shows details of that anomaly, which is again broken down at various levels. Grafana allows quick duplication of each panel with a view that can be be easily modified. The user can select either an SQL or a drop-down interface to edit queries. There are several occasions during the triaging process when we need to quickly check if a given experiment or channel or country is experiencing any anomalies. The search feature provided by Grafana (through templating) allows us to do just that. The user can type or select from a drop-down to view details of all anomalies for a specific combination of filters. Every dashboard can be customized and shared across the organization. InfluxDB (v 0.11-1) is installed on a single node, and so is Grafana (v 3.0.2). Each of these are hosted on the eBay cloud with 45 GB of memory, 60GB of disk space, and Ubuntu 14.04. Each day, around 2000 points are ingested into InfluxDB using a Scala client with ingestion time of around few seconds. Currently, the system contains seven months of historical anomaly data, taking around 1.5 GB disk space in InfluxDB and consuming approximately 19 GB of RAM. Anomaly data is archived on HDFS for recovery in case of system failure. This dataset is minuscule compared to vast amounts of data that can be handled by InfluxDB , especially when assisted by its capability to be set up as a cluster for fault tolerance, which unfortunately is not supported beyond v 0.11-1 . The anomaly monitoring platform is the cornerstone for monitoring anomalies in experiments at eBay. It is becoming a single point for monitoring, sharing, and searching for anomalies in experiments for anyone in the company who runs experiments on the Experimentation platform. Its ability to be self-service (thanks to Grafana) in terms of creating new dashboards for new datasets is what makes it stand out. There are several measures and metrics that determine if a experiment is experiencing an anomaly. If the thresholds are breached, the experiment is flagged and a consolidated email notification is sent out. It’s always been discussed in Grafana circles as to when alerting is coming (Winter has come, so will alerting), and it seems that alerting is actually coming to Grafana, enabling users to set alert thresholds for every metric that is being monitored, right from the dashboard . Grafana and Grafana Live Demo InfluxDB and InfluxDB Benchmark InfluxDB Scala Client , by paulgoldbaum GIFs were created using GIPHY Capture . The block diagram was created using Gliffy .", "date": "2016-10-06"},
{"website": "Ebay-Engineering", "title": "Apache Eagle: Secure Hadoop in Real Time", "author": ["Arun Manoharan"], "link": "https://tech.ebayinc.com/engineering/eagle-is-your-hadoop-data-secured/", "abstract": "Co-Authors: Chaitali Gupta and Edward Zhang Update:  Eagle was accepted as an Apache Incubator project on October 26, 2015. Today’s successful organizations are data driven. At eBay we have thousands of engineers, analysts, and data scientists who crunch petabytes of data everyday to provide a great experience for our users.  We execute at massive scale using data to connect our millions of users in global commerce. In recent years, Hadoop has become one of the most popular choices for Big Data analytics. eBay uses Hadoop to generate value from data for improving search experience, identifying and optimizing relevant advertisements, enriching our product catalogs, and performing click stream analysis to understand how eBay customers leverage our marketplace. Accordingly, we have a wide variety of workloads that include Hive, MapReduce, Spark, HBase, and hundreds of petabytes of data. In this era of Big Data, security is ever more critical. At eBay, Hadoop data is secured. Our security approach follows these four pillars: access control, perimeter security, data classification, and data activity monitoring. Early in our journey, we recognized there was no product or solution that adequately supported our data activity monitoring needs given the scale and variety of use cases at eBay. To address this gap, eBay built Eagle. Eagle is an open-source Data Activity Monitoring solution for Hadoop to instantly detect access to sensitive data or malicious activities, and to take appropriate actions. We believe Eagle is a core component of Hadoop data security, and we want to share this capability with the open-source community. We will be open sourcing Eagle through the Apache Software Foundation. We are looking forward to working with the open-source development community. Here are some of the Eagle data activity monitoring use cases: Anomalous data access detection based on user behavior Discovery of intrusions and security breaches Discovery and prevention of sensitive data loss Policy-based detection and alerting Key Eagle qualities include the following: Real time: We understand the importance of timing and acting fast in case of a security breach. So we designed Eagle to make sure that the alerts are generated in a sub-second and that the anomalous activity is stopped if it’s a real threat. Scalability: At eBay, Eagle is deployed on multiple large Hadoop clusters with petabytes of data and 800 million access events every day. Ease of use: Usability is one of our core design principles. We have made it easy to get started . It takes only a few minutes to get up and running with Eagle sandbox, and examples and policies can be added with a few clicks. User profiles: Eagle provides capabilities to create user profiles based on user behavior in Hadoop. We have out-of-the box machine-learning algorithms that you can leverage to build models with different HDFS features and get alerted on anomalies. Open source: Eagle is built ground up using open-source standards and various products from the Big Data space. We decided to open-source Eagle to help the community, and we are looking forward to your feedback, collaboration, and support. Extensibility : Eagle is designed with extensibility in mind. You can easily integrate Eagle with existing data classification and monitoring tools. 1.a Eagle Architecture Eagle provides a programming API for extending Eagle to integrate any data source into the Eagle policy evaluation framework. For example, Eagle HDFS audit monitoring collects data from Kafka, which is populated from the NameNode log4j appender or from the logstash agent. Eagle Hive monitoring collects Hive query logs from running jobs through the YARN API, which is designed to be scalable and fault-tolerant. 2.1 Stream processing API: Eagle provides a stream processing API which is an abstraction on Apache Storm, but is also extensible to other streaming engines. This abstraction allows developers to easily assemble data transformation, filtering, external data join, etc. without being physically bound to a specific streaming platform. The Eagle streaming API also allows developers to easily integrate business logic with the Eagle policy engine. Internally, the Eagle framework compiles business logic execution DAG into program primitives of the underlying stream infrastructure—for example, Apache Storm. Here is an example of events and alerts processing in Eagle: 2.2 Alerting framework: The Eagle alerting framework includes a stream metadata API, a policy engine provider API for extensibility, and a policy partitioner interface for scalability. The stream metadata API allows developers to declare the event schema including what attributes constitute an event, what each attribute’s type is, and how to dynamically resolve attribute values at runtime when the user configures a policy. The policy engine provider API allows developers to plug in a new policy engine easily. The WSO2 Siddhi CEP engine is the policy engine that Eagle supports as a first-class citizen. A machine-learning algorithm is also wrapped into the framework as one type of policy engine. Eagle’s extensible interface allows you to plug in different policy engines: The policy partitioner interface allows policies to be executed on different physical nodes in parallel. It also allows you to define your own policy partitioner class. These capabilities enable policy and event evaluation in a fully distributed fashion. 1.b Scalable Eagle policy execution framework Scalability: Eagle supports policy partitioning to support a large number of policies. 2.3 Machine-learning module: Eagle provides capabilities to define user activity patterns or user profiles for Hadoop users based on the user behavior in the platform. The idea is to provide anomaly detection capability without setting hard thresholds in the system. The user profiles generated by our system are modeled using machine-learning algorithms and used for detection of anomalous user activities, where users’ activity pattern differs from their pattern history. Currently Eagle uses two algorithms for anomaly detection: Eigen-Value Decomposition and Density Estimation. The algorithms read data from HDFS audit logs, slice and dice data, and generate models for each user in the system. Once models are generated, Eagle uses the Storm framework for near-real-time anomaly detection to determine if current user activities are suspicious or not with respect to their model. The block diagram below shows the current pipeline for user profile training and online detection. 1.c User profile offline training & anomaly detection architecture Eagle online anomaly detection uses the Eagle policy framework, and the user profile is defined as one of the policies in the system. The user profile policy is evaluated by a machine-learning evaluator extended from the Eagle policy evaluator. Policy definition includes the features that are needed for anomaly detection (same as the ones used for training purposes). A scheduler runs a Spark-based offline training program (to generate user profiles or models) at a configurable time interval; currently, the training program generates new models once every month. The following are some details on the algorithms. 2.3.1 Density Estimation —In this algorithm, the idea is to evaluate, for each user, a probability density function from the observed training data sample. We mean-normalize a training dataset for each feature. Normalization allows datasets to be on the same scale. In our probability density estimation, we use a Gaussian distribution function as the method for computing probability density. Features are conditionally independent of one another; therefore, the final G aussian probability density can be computed by factorizing each feature’s probability density. During the online detection phase, we compute the probability of a user’s activity. If the probability of the user performing the activity is below threshold (determined from the training program, using a method called Mathews Correlation Coefficient), we signal anomaly alerts. 1.d Showing user behavior histogram on one dimension 2.3.2 Eigen-Value Decomposition —Our goal in user profile generation is to find interesting behavioral patterns for users. One way to achieve that goal is to consider a combination of features and see how each one influences the others. When the data volume is large, which is generally the case for us, abnormal patterns among features may go unnoticed due to the huge number of normal patterns. As normal behavioral patterns can lie within very low-dimensional subspace, we can potentially reduce the dimension of the dataset to better understand the user behavior pattern. This method also reduces noise, if any, in the training dataset. Based on the amount of variance of the data we maintain for a user, which is usually 95% for our case, we seek to find the number of principal components k that represents 95% variance. We consider first k principal components as normal subspace for the user. The remaining (n-k) principal components are considered as abnormal subspace. During online anomaly detection, if the user behavior lies near normal subspace, we consider the behavior to be normal. On the other hand, if the user behavior lies near the abnormal subspace, we raise an alarm as we believe usual user behavior should generally fall within normal subspace. We use the Euclidian distance method to compute whether a user’s current activity is near normal or abnormal subspace. 1.e Showing important user behavior components 3.1 Policy Manager: Eagle Policy Manager provides a UI and Restful API for users to define policies. The Eagle user interface makes it easy to manage policies with a few clicks, mark or import sensitivity metadata, perform HDFS or Hive resource browsing, access alert dashboards, etc. Here is a s ingle-event evaluation policy (one user accessing a sensitive column in Hive): Here is a window-based policy (one user accessing /tmp/private 5 times or more within 10 minutes): 3.2 Query Service: Eagle provides a SQL-like service API to support comprehensive computation for huge sets of data—comprehensive filtering, aggregation, histogram, sorting, top, arithmetical expression, pagination, etc. Although Eagle supports HBase for data storage as a first-class citizen, a relational database is supported as well. For HBase storage, the Eagle query framework compiles a user-provided SQL-like query into HBase native filter objects, and then executes it through the HBase coprocessor on the fly. Eagle data activity monitoring is currently being used for monitoring the data access activities in a 2500-node Hadoop cluster, with plans to extend it to other Hadoop clusters covering 10,000 nodes by the end of this year. We started with a basic set of policies on HDFS/Hive data and will be ramping up more policies by the end of this year. Our policies range from access patterns, commonly accessed data sets, predefined queries, Hive tables, columns, and HBase tables, through to policies based on user profiles generated by ML models. We have a wide range of policies to stop data loss, data copying to unsecured location, sensitive data access from unauthorized zones, etc. The flexibility of creating policies in Eagle allows us to expand further and add more complex policies. In addition to data activity monitoring, at eBay the Eagle framework is used extensively to monitor the health of nodes, Hadoop apps, core services, and the entire Hadoop cluster. We have also built in a lot of automation around remediation of nodes, which helped us reduce our manual workload to a large extent. Below are some of the features we are currently working on and will be releasing in the next version: Machine-learning models for Hive and HBase data access events Extensible API for integration with external tools for reporting and data classification New module for Hadoop cluster monitoring in the next version of Eagle HBase access monitoring Hadoop job performance monitoring Hadoop node monitoring HBase access monitoring Hadoop job performance monitoring Hadoop node monitoring Please visit https://github.com/eBay/Eagle for more information.", "date": "2015-10-23"},
{"website": "Ebay-Engineering", "title": "Congruent Numbers Part I", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/congruent-numbers-part-i/", "abstract": "Usually my posts have a connection to eBay, but this time I’m writing about a recreational math problem that caught my attention. One of the most famous facts of mathematics is that a triangle with sides of length 3, 4, and 5 is a right triangle, which means it can be drawn with a horizontal base and a vertical side, forming a right angle. Since the area of a triangle is one-half base times height, this triangle has area . Is there a right triangle with area 5? Of course there is, since a right triangle with a base of length 1 and height 10 has area 5. But the third side (the hypotenuse) has length , which is not an integer. If you require all three sides of a triangle to be integers, then there is no right triangle with area 5. A brute-force enumeration verifies this, and easily shows that 6 is the smallest, and the next few after 6 that are the area of a right triangle are 24, 30, 54, 60. So asking if there is a right triangle with integer area appears to be of little interest. But what if you allow rational sides? It’s possible for the height and width to be non-integer fractions, but have integer area. And in fact there is such a triangle with area . The equation shows that a triangle with sides , and is a right triangle. The area is . An integer that is the area of a right triangle having all sides of rational length is called congruent. What numbers are congruent? Detecting them has something in common with detecting primes. There is a fast test to see if a number is prime or composite, but it’s a lot harder to obtain direct evidence that a number is composite by producing its factors. Similarly, there is a fast test to see if a number is congruent, but it is hard to find the side lengths , , that demonstrate directly that is congruent. The reason it can be hard to find the sides is that they can be fractions with a lot of digits. For example, the sides of a right triangle with area are I found tables on several web sites containing the side lengths for all congruent numbers less than 1000, but I believe they are all copies of the same table, because they all have an identical glitch at which I will explain shortly. In this posting and the next I will explain my efforts to compute the sides of a triangle with area . I got interested in this problem because I wondered how it was done. I wanted to compute sides for . I wanted to check if the existing table can be improved. I found the existing table can indeed be improved, as I now explain. The rational sides demonstrating that is congruent are not unique. For example, 6 is the area of a triangle with sides (3,4,5) and also . I’ll always try to find the example with the smallest denominators. So I prefer (3,4,5) (denominator of 1) to (denominator of 70). After I wrote my own code to generate a table of side lengths, I discovered that the web table doesn’t always give sides with the smallest denominator. Two examples are below, where the triangle sides are , , . How big (how many digits) are the numerator and denominator of the sides? I used the web table to plot the number of digits needed for the denominator of the length of the hypotenuse against the area . The plot is below. The vertical axis is which is the number of base-10 digits in the denominator . Note that prime values of (blue) generally require more digits than composite numbers do. The straight line suggests that the number of digits needed for the denominator increases linearly with . In other words, the size of is exponential in . In this section I explain in detail a search method for finding the sides for the triangle of area that can easily find the fractions for , which are A brute-force exhaustive search for would find the fractions and by trying all with integers , having up to digits (and similarly for ). That is possibilities, where . This search would take a long time, . There are some simple tricks that make it much faster to find the fraction by brute force. Going back to sides for the congruent number , Rewrite the equation to have a common denominator. The numerators satisfy A triple of such integers is called a pythagorean triple. This link between congruent numbers and Pythagorean triples holds in general, as summarized below: So from rational numbers , , and you get integers and that are Pythagorean, meaning is a square. I’ll always assume that any factor common to all of , and has been removed, in other words that . Also note that if is congruent, then multiplying the sides of its triangle by the integer shows that is congruent. And conversely, if you have a triangle showing that is congruent, divide the sides by to get a triangle for . So I only need to consider square-free . Here are the first few square-free congruent numbers. Note that in each case the denominator of is the product of the denominators of and . A proof that this is always so is in the final post of this three-part series. Pythagorean triples can be written in the form (1) The proof of this can be found on the web and in most number theory textbooks under the name Euclid’s Formula, but I also give a proof in the final post of this series. The formulas can be rewritten as (2) So there is a 1-1 correspondence between , , and , . Specifically, starting with , , you rewrite with a common denominator to get , , and then use ( 2 ) to get , . Conversely, given and use ( 1 ) to compute , , , and then get using (3) Finally, reduce to get , similarly for . This means that a brute-force search can be done using only two integers and (with instead of four integers , , , and . What is their size? If etc. have digits, then etc. have digits and , have digits, so brute force is now . Huge but smaller than . Here is a table of the first few square-free congruent numbers represented as and : Note that and are always of opposite parity (that is, one is odd and the other even). As I’ll explain in the final post, once you remove any common factors from then and must be of opposite parity. This is the glitch in the web table I mentioned earlier. For , the web table gives , , which are both odd. If you use them to generate you will find they all have a common factor of 2. If you remove this factor and recompute , you get , with . A major improvement in exhaustive search can be made by noting the special form of and . You can see the pattern in the following table of and , where they are written in partially factored form. For entries in the table, is a square, or a square times a divisor of , and similarly for . As shows, both and can have divisors. Stated more formally, and where . Since this is true for all numbers in the table above, it’s plausible that it’s true in general, and I’ll show that in the final post. This property means you only need to search up to the square root of and . In other words, you only need to check values of and similarly for , so exhaustive search takes steps when . This  can be done in less than a minute, at least on my laptop. I won’t bother to give the code, because although it easily computes the sides for it can’t get the side lengths for more challenging areas such as . In the next post I’ll explain an improved algorithm using elliptic curves and give code for the SageMath system that can solve . Powered by QuickLaTeX", "date": "2016-01-28"},
{"website": "Ebay-Engineering", "title": "The API Journey: Or How We Built a New Family of Modern Services", "author": ["Tanya Vlahovic"], "link": "https://tech.ebayinc.com/engineering/the-api-journey-or-how-we-built-a-new-family-of-modern-services/", "abstract": "An API — or application programming interface — is an intermediary that enables applications to interact. It is a contract that specifies how applications talk to one another. Further, an API creates a separation between a service provider and its consumers. Essentially, it decouples their implementations. As long as the contract stays intact, API providers may continue changing their code and underlying dependencies without destabilizing clients. APIs are a big deal. After dealing with SOAP-based legacy APIs for years, eBay started a journey to deliver a new, modern family of APIs for sellers and buyers. Our principal goal was to design a set of interfaces that will meet business objectives, attract developers, replace our legacy APIs, and be long-lived. This is not an easy job. As I mentioned, APIs are a contract, and as such, they cannot be changed in ways that break existing integrations. APIs should evolve and grow with the business, so they must also be expandable and flexible. Now, that is hard. Our challenge was to create a vision for the API, plan ahead and design a stable contract that will last for years, even as we add business capabilities. Here is how we did it. “Ensuring and validating that assets and artifacts within the architecture are acting as expected and maintaining a certain level of quality.” — Gartner To achieve consistency across the APIs, we followed a governance process and compliance model. One of our most important goals was improving the quality of the APIs by defining and enforcing standards and policies. We established a governance process that was objective, transparent, manageable, and explicit. Our compliance model for web services is platform- and tenant-agnostic and fits well into eBay’s overall API maturity model. Levels of compliance are specified by a set of characteristics and capabilities that may be measured and assessed. This helps to identify and quantify a realistic target maturity for our APIs in a given period. (And, it is testable!) “Unfortunately, people are fairly good at short-term design, and usually awful at long-term design.” — Roy Fielding First and foremost, the API blueprint is the starting point. At eBay, a blueprint describes detailed API design enough to verify, implement, maintain, and extend capabilities in the future. Designing APIs has many analogies to building a house. Without a proper blueprint, pressure to deliver on time often leads to a poor design. To further frame an analogy to house construction, working without a blueprint causes shortcuts similar to building a bathroom in the kitchen since the plumbing work has already been done there. The challenge lies in finding a balance between our agile product development methodology and time needed to come up with a detailed design. Implementation becomes straightforward once there is a blueprint and clear understanding of what needs to be done. For our new family of APIs, we followed our interface design method (“IDM”). The IDM is the process of arriving at an underlying model and a stable contract. It starts with capturing use cases by specifying actors, concepts, and actions, and then deriving entity relationships. Further, nouns are identified from the entities and verbs from the actions. The final phase of the IDM process is determining resource representation and specifying authorization details. The currently supported funding model is the cost per sale (CPS). Inventory is partitioned based on following dimensions: eBay-defined item categories Seller-defined item categories Item price range Brand Fulfillment cost Item condition We followed pragmatic RESTful principles with standard implementations around cross-cutting concerns: error handling, security, behavioral tracking, and operational metrics. APIs represent the consumer’s view of the capabilities, and the URI names must be meaningful to developers. Our URI pattern takes a consumer-centric approach by providing a consistent, predictable, and understandable names across APIs. This pattern makes an API intuitive and easy to discover and consume. In most of the cases, the new APIs use JSON for resource representations. It is compact and easy to parse and translate. For certain use cases, supporting additional formats is straightforward, since our RESTful architecture style leaves room for such flexibility. So far, we have managed to stick to standard formats and media types. OAuth 2.0 protocol is leveraged to address security and data privacy concerns. Here, the challenge was to balance the need of fine-grained scopes that protect data and activities while managing the scope policies. APIs are more than pure design and implementation. They include documentation, technical support, terms of use, and various operational aspects. Bringing transparency to the process through frequent discussions between architects, product owners, and engineering teams was crucial. Getting feedback from technical writers helped to achieve vocabulary consistency across APIs. For sure, all of the teams were aligned on what the success is: to build APIs that developers will love and want to use. We delivered modern RESTful APIs that cover a subset of our overall marketplace capabilities and follow industry standards, well-established patterns, and best practices. Still, they are powered by a model that is flexible and extensible enough to capture new opportunities that might come in the future. Our journey is not yet complete. We are engaging customers, listening to feedback, and encouraging adoption of the new APIs, all to bring our new, long-term public API program to reality. Our goal is a large and powerful ecosystem of developer applications that add value and benefits to our buyers and sellers. Finally, we want to continue transforming our business by exposing valuable eBay solutions and capabilities to empower developers.", "date": "2016-10-19"},
{"website": "Ebay-Engineering", "title": "Peer Groups in Empirical Bayes", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/peer-groups-in-empirical-bayes/", "abstract": "In a post from February, I sang the praises of Empirical Bayes, and showed how eBay uses it to judge the popularity of an item. This post discusses an important practical issue in using Empirical Bayes which I call “Peer Groups”. (Update, August 29, 2017:  I just discovered that the book, Computer Age Statistical Inference by Efron & Hastie, also discusses peer groups in section 15.6, although they call it “relevance” rather than “peer groups.”) First, a quick summary of the February post . The popularity of an item with multiple copies available for sale can be measured by the number sold divided by the number of times the item has been viewed, sales/impressions for short. The problem was how to interpret the ratio sales/impressions when the number of impressions is small and there might not be any sales yet. The solution was to think of the ratio as a proxy for the probability of sale (call it ), and use Bayes theorem to estimate . Bayes theorem requires a prior probability, which I estimated using some of eBay’s voluminous sales data. This method is called Empirical Bayes because the prior is determined empirically, using the data itself. That brings me to peer groups. When computing the prior probability that an item gets a sale, I want to base it on sales/impressions data from similar items, which I call a peer group. For example if the item is a piece of jewelry, the peer group might be all items of jewelry listed on eBay in the past month. I can get more specific. If the item is new, then the peer group might be restricted to new items. If the list price is $138, the peer group might be further restricted to items whose price was between $130 and $140, and so on. Once you have identified a peer group and used it to estimate the prior probability, you use Bayes theorem to combine that with the observed count of sales and impressions to compute the probability of sale. This is the number you want—the probability that the next impression will result in a sale. It is called the posterior probability, to distinguish it from the prior probability. There’s a tension in selecting the peer group. You might think that a peer group more strongly constrained to be similar to the item under consideration will result in a better prior and therefore a better estimate of the probability of a sale. But as the peer group gets smaller and smaller, the estimate of the prior based on the group becomes noisier and less reliable. Which finally brings me to the subject of this post. In the case where the peer group is specified by a continuous variable like price, you can get the best of both worlds—a narrowly defined peer group and a lot of data (hence low noise) to estimate the prior parameters. The idea is modeling. If the prior depends on the price , and if there is a model for the dependence, the same data used to compute the prior can be used to find the model. Then an item of price is assigned the prior given by the model at , which is essentially the peer group of all items with exactly price . Since this prior is a prediction of the model, it indirectly uses all the data, since the model depends on the entire data set. What is needed to apply Bayes theorem is not a single probability , but rather a probability distribution on . I assume the distribution of is a Beta distribution , which has two parameters. Specifying the prior means providing values of and . So our idea is to see if there is a simple parametrized function that explains the dependence of and on the price . The beta distribution has a mean of . As a first step, I examine the dependence of (rather than and ) on price. The fit to the power law is very good. The values of and are noisier than . But I do know one thing: sales/impressions is small, so that is small, and therefore so . It follows that if and fit a power law, so would . Thus a power law for and is consistent with the plot above. Here are plots of and . Although somewhat noisy, their fits to power laws are reasonable. And the exponents add as expected: the exponent for is , for is 0.35, and for is . Once the form of the dependence of and on price is known, the Empirical Bayes computations proceed as usual. Instead of having to determine two constants and , I use Empirical Bayes to determine four constants , , , and , where The details are in the February posting, so I just summarize them here. The are computed using maximum likelihood as follows. The probability of seeing a sales/impressions ratio of is and max likelihood maximizes the product or equivalently the log Instead of maximizing a function of two variables , maximize Once you have computed , then an item with sales out of impressions at price has a posterior probability of . When people hear about the peer group problem with a beta distribution prior, they sometimes suggest using beta regression. This suggestion turns out not to be as promising as it first seems. In this section I will dig into beta regression, but it is somewhat of a detour so feel free to skip over it. When we first learn about linear regression, we think of points on the plane and drawing the line that best fits them. For example the -coordinate might be a person’s height, the coordinate is the person’s weight, and the line shows how (on the average) weight varies with height. A more sophisticated way to think about linear regression is that each point represents a random variable . In the example above, is a height, and represents the distribution of weights for people of height . The height of the line at represents the mean of . If the line is , then has a normal distribution with mean . Beta regression is a variation when has a beta distribution instead of a normal distribution. If the satisfy they are clearly not from a normal distribution, but might be from a beta distribution. In beta regression you assume that is distributed like where the mean is , or perhaps a function of . The theory of beta regression tells you how to take a set of and compute the coefficients and . But in our situation we are not given from a beta distribution. The beta distribution is the unknown (latent) prior distribution. So it’s not obvious how to apply beta regression to get and . Empirical Bayes is a technique that can work well for problems with large amounts of data. It uses a Bayesian approach that combines a prior and a particular item’s data to get a posterior probability for that item. For us the data is sales/impressions for the item, and the posterior is the probability that an impression for that item results in a sale. The prior is based on a set of parameters, which for a beta distribution is . The parameters are chosen to make the best possible fit of the posterior to a subset of the data. But what subset? There’s a tradeoff. If the subset is too large, it won’t be representative of the item under study. If it is too small, the estimates of the parameters will be very noisy. If the subset is parametrized by a continuous variable (price in our example), you don’t need to decide how to make the tradeoff. You use the entire data set to build a model of how the parameters vary with the variable. And then when computing the posterior of an item, you use the parameters given by the model. In our example, I use the data to compute constants . If the item has price , sales and impressions, then the parameters of the prior are and and the estimated probablity of a sale (the posterior) is . Powered by QuickLaTeX", "date": "2015-10-28"},
{"website": "Ebay-Engineering", "title": "Congruent Numbers Part II", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/congruent-numbers-part-ii-2/", "abstract": "The method of my first post is too slow to find the side lengths for a triangle of area , which has many digits: In the 1984 book by Koblitz, Introduction to Elliptic Curves and Modular Forms , computation of the side lengths for is credited to the mathematician Don Zagier. Now, however, it can be computed quickly by anyone with access to the SageMath system. The Python code given below computes the values for in less than one minute on my Mac laptop. This post has three parts: I derive equations that produce triangle side lengths for congruent numbers; I give Python code that implements the equations; and I present two tables of congruent numbers. The first table is for , and is a variant of the web table. The second is an incomplete table for constructed using the Python code. Exhaustive search is too slow to find the sides of the triangle with area ; finding them requires a new idea. That idea is to reduce the search to a problem in elliptic curves (the same idea used to prove Fermat’s Last Theorem!). An elliptic curve is the next level of complexity after a quadratic curve. A quadratic curve is ; an elliptic curve is . For the congruent number problem, Koblitz’s book Introduction to Elliptic Curves and Modular Forms shows the connection between a congruent and the elliptic curve . I summarize it here. The connection involves adding points. For a conic section like a circle each point can be represented by , and two points , with angles , can be added to get a third point with angle . In a formula with a similar formula for . So the formula for is a rational function of , where is our special addition, not vector addition. In the same way, two points on an elliptic curve can be added, and there is a formula for the sum. If and then the formula for (I don’t need ) is If is a point of this simplifies to In that equation , and are rational numbers. I need a formula in terms of their numerators and denominators. So write (1) Then Next, use Koblitz page 47, which says that if is the sum of a point on with itself, then you get rational sides with area for sides , with so Or since , This gives an algorithm that can find the sides’ lengths for area , and is fast enough to work for . Use SageMath to find a point on . Next use equation ( 1 ) to define , , , and the equation after that to get and . And use the equations directly above to get the side lengths and . Here is the code that implements the algorithm of the previous section. Here is a table with the side lengths of congruent numbers. Like the web tables, it gives and rather than the side lengths , , but I give and in factored form. In addition it differs from the web table at , , , and . The changes at and give fractions with a smaller denominator. The change at fixes the glitch wherein both and are both odd. The change at adds a second solution with the same denominator. I remarked before that there are multiple values of , (or equivalently , , ) for each , and that I would pick the one with the smallest . This doesn’t always select a unique triangle. For example has both and , both with . Other such examples are And now, here’s the table. Also available as a text file. One of the motivations for writing the code in this posting was to try computing the side lengths for . Of the 358 square-free congruent numbers between 1000 and 2000, the code was able to find sides for all but 50. I’d be thrilled to hear about more sophisticated algorithms that can compute these missing entries.  Here are the entries I was able to compute, also available as a separate file. Powered by QuickLaTeX", "date": "2016-02-10"},
{"website": "Ebay-Engineering", "title": "Packaging for Performance", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/packaging-for-performance/", "abstract": "An interesting topic of discussion in recent times has been around static resource (JS/CSS) packaging for web applications. Craig Silverstein’s and Rebecca Murphey’s write-ups on this topic provide great insights into the reality of packaging in today’s frontend engineering world. The main question that comes up is, should the strategy for JavaScript and CSS bundling (which is a current performance best practice) change when migrating to HTTP/2 ? Although in theory the benefit of bundling—which is to reduce the number of HTTP requests—becomes void with HTTP/2, the reality is that we are not there yet. The above articles are proof of that reality. At eBay we did similar research a few months back when prepping the site for HTTPS, and our findings were the same. In this post, I will outline the approach that we have taken towards packaging and the performance benefits of our approach. The majority of eBay pages followed a naïve pattern for bundling resources. All the CSS and JavaScript for a page were bundled into one resource each, with the CSS included in the head tag and JS at the bottom. Though it was good in terms of reducing the number of HTTP requests, there were still opportunities to improve — the main one being a better use of browser caching. As users navigate through eBay pages, every unvisited page needs to download the whole JS and CSS, which includes the same core libraries (jQuery etc.) used in previous pages. With our plan towards moving to HTTPS (and HTTP/2), we knew this coarse-grained packaging would not be effective. Our study, and others’ studies, also indicated that avoiding bundling altogether and loading resources individually would still not be effective in terms of performance. We needed a balance, and that’s when we came up with our own packaging solution. Our first step was to identify the core JS and CSS libraries used across all eBay pages and to aggregate them as one resource. To do this, we created an internal Node.js module called Inception . This module includes all the common JS and CSS modules and will be added as a dependent by each domain team (owners of the various eBay pages). The identified core JS libraries were jQuery, marko (templating engine), marko-widgets (UI component abstraction), and in-house analytics and tracking libraries. For CSS, we have our own library called Skin, from which we picked the core, button, icons, dialog, and form modules. The package bundler we use at eBay is Lasso . The Inception module, which plugs in along with Lasso, provides the following functionalities: Enforces all domains (buying, selling, browse, checkout, etc.) to follow the exact version of the core JS and CSS libraries. Non-compliance will result in build failures. Bundles the Inception resources as one URL with the same URL address across all domains. Examples are inception-hashcode.js and inception-hashcode.css . Enables domain teams to still include any of the Inception JS/CSS libraries as a part of their own module dependencies. The Lasso optimizer will de-dupe libraries and ensure only one copy is sent to the browser. This functionality is critical for two reasons . First, we want to promote module-level encapsulation, so that when domain teams are building modules, they are free to add a dependency on a core library (say skin-button ) without worrying about duplication. This also makes the module work standalone. Secondly, domain teams should not bear the overhead of tracking what is or isn’t present in Inception. They should be able to include any dependency they want, and the tooling can take care of the optimization. Now with Inception in place, we started seeing these benefits: Browser caching : One of the drawbacks mentioned earlier—bundling all resources as one URL—is poor leverage of browser caching. Inception fixes this drawback. Since the same URL is used across all domains for the core JS and CSS libraries (which BTW is the majority of the payload), browser caching is heavily utilized as users navigate through various eBay experiences. This caching is a massive improvement in terms of performance, especially for slow connections. In addition, with newer browser versions supporting code caching , we might also avoid the parse and compile times for the big Inception JS bundle. Library consistency : Another problem that we saw in our previous bundling system was lack of consistency in core library versions used across domains. Since domains were maintaining the core libraries, users navigating from one domain to another might, for instance, get different versions of jQuery or button styles. The result is not only UI inconsistency but also implementation inconsistency across domains. This issue is also fixed with Inception, as it is a central place to manage core libraries. Path to Progressive Web Apps : With all domain pages having the same core library dependencies, transition between them becomes easy, since only the application-specific JS and CSS has to be downloaded on each navigation. This approach will enable us to build our web apps using an Application Shell Architecture , thus paving the way to making eBay a Progressive Web App . We have already explored a similar route in the past (within a domain) using the Structured Page Fragments approach, and we have seen our perceived performance increase immensely. Easy upgrade path : Finally, Inception also enables us to upgrade to newer versions of core libraries from a central place. Inception itself follows semantic versioning , so all domain teams depending on Inception will get the updates uniformly in a semantic manner. Upgrades were problematic previously, as we had to chase individual teams to do the upgrades manually. Now with the core libraries being taken care of through Inception, what about the remaining resources in a page—i.e., application/domain-specific CSS and JS? For each domain we again came up with a packaging approach where we split the resources into two buckets: constants and variables. Constants : The CSS and JS resources that remain the same on every request are bucketed as constants. These mainly pertain to the key UI components in each domain, which remain unaltered to various request parameters. The constant modules are bundled as one resource, and they again benefit from browser caching. When a user revisits a page, this bundle usually hits the browser cache and gets a performance advantage. Variables : A small portion of the resources in each page vary based on request attributes. These variations might be due to experimentation, user sign-in status, business logic, etc. Such resources are bucketed as variables and have their own bundling, which happens at runtime. They will have the least cache-hit ratio and probably will need to be downloaded over the network for a new session. To summarize, every page will have six resource bundles (3 for CSS and 3 for JS), and each bundle will have its own purpose. All URLs are hashed based on content; thus cache busting is automatically taken care of. Inception — bundles the core CSS and JS libraries. Highest in payload. Constants — bundles the unchanging application CSS and JS. Middle-range in payload. Variables — bundles the varying CSS and JS in an application. Least in payload. In the current state, this packaging strategy seems to be the best fit for us in terms of performance. It has created the right balance between number of HTTP requests and browser caching. As we start migrating towards HTTP/2 next year, we will further evaluate this approach and try to come up with more fine-grained bundling solutions, and of course with performance being the key.", "date": "2016-01-04"},
{"website": "Ebay-Engineering", "title": "Announcing Pulsar Reporting: Near-Real-Time Metrics Reporting Framework", "author": ["Jason Liu"], "link": "https://tech.ebayinc.com/engineering/announcing-pulsar-reporting-near-real-time-metrics-reporting-framework/", "abstract": "We are excited to announce the first open-source release of Pulsar Reporting. Earlier this year, we announced http://gopulsar.io , an open-source project that included Pulsar Pipeline, a real-time analytics platform and stream processing framework. One of the frequently requested features for Pulsar has been integration with a metrics store for visualizing the near-real-time metrics. We’ve provided this feature with this release, which adds the Pulsar Reporting API and the Pulsar Reporting UI Framework under the same license terms. The public GitHub repository is https://github.com/pulsarIO . Pulsar Reporting is an extensible data visualization and reporting framework designed to provide real-time insights from Pulsar Pipeline. The framework includes a rich set of charting widgets and a visual reporting editor for users to easily create reports. It has a robust data query engine that can be extended to support many different types of data sources. With the Pulsar Reporting Framework, users can quickly create multi-dimensional and interactive reports that include drill-down and slice-and-dice capabilities. Near-real-time reports – Building reports based on near-real-time data that auto-refreshes at specified intervals Visual reporting editor – Generating reports without writing any code Rich charting widgets – Creating multiple chart types:   line, bar, histogram, pie, stack,  datatable, etc. Reporting API – Querying data with human-friendly SQL or program-friendly structured JSON Dynamic data source management – Adding or removing data sources with no down time Security and permissions – Managing authentication and access control Druid Kafka extension – Ingesting real-time data from Kafka into Druid AngularJS-based hierarchical UI framework – Easily adding and extending reports Bootstrap-based responsive design – Being able to use Pulsar Reporting on different sizes of screens The Pulsar Reporting Framework complements Pulsar, an open-source, real-time analytics platform and stream processing framework. Pulsar generates huge amounts of data, and visualization is the best way to provide intuitive and meaningful insights into that data. However, building dashboards and reports for big data from scratch is cumbersome and error-prone. The Pulsar Reporting Framework allows user to create reports easily and quickly without requiring complex data processing and UI logic. The raw events and session events from Pulsar Pipeline flow to Kafka using the Pulsar Kafka channel. The Druid cluster then ingests the raw events as well as the sessions from Kafka topics into two tables, one for sessions and one for events. Both tables are indexed in one-second granularity to enable real-time reporting. The Pulsar Reporting API provides an abstract layer to access the tables. The Reporting UI gets the data from the API to build different charts. Get session metrics using the SQL API： Endpoint: http://<API_Server>/prapi/v2/sql\nMethod: POST\nBody: {\"sql\" : \"SELECT (count(session) - sum(retvisitor)) * 1.0 / count(session) newSessionRate, sum(sessionDuration) * 1000 totalSessionDurations, count(session) sessions, sum(sessionDuration) totalSessions, sum(totalpagect) totalPages, country, trafficSource FROM pulsar_session WHERE site=0 and country='usa' GROUP BY country, trafficSource ORDER BY sum(totalpagect) ASC limit 20\",\n\"intervals\": \"2015-10-11 03:00:32/2015-10-18 01:00:32\",\n\"granularity\": \"day\"} Get page views by traffic source using the structured JSON API： Endpoint: http://<API_Server>/prapi/v2/realtime\nMethod: POST\nBody: {\"metrics\" : [ \"pageviews\" ], \"dimensions\" : [ \"trafficsource\" ], \"filter\" : \"site=0\" } We have open-sourced the Pulsar Reporting Framework, and we plan to continue developing the code in the open. We welcome your suggestions and contributions. Here are some of the features we are thinking about. Pathing and funnels Exporting reports Expanding support to additional data sources based on community interest Integrating with Pulsar.js, a client-side Javascript library to generate Pulsar events for the web Please visit http://gopulsar.io for source code, documentation, and more information.", "date": "2015-11-05"},
{"website": "Ebay-Engineering", "title": "Congruent Numbers Part III", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/congruent-numbers-part-iii/", "abstract": "Before demonstrating the claims made in Part I and Part II on this topic, let me mention two simple facts about integer solutions to . First, if some number divides both and , it also clearly divides . So and have a common factor if and only if , , and do. Or saying it another way, and are relatively prime if and only if , , and are. From now on I always assume that this is true for , , and . The second fact is that and have opposite parity, that is, one is even and the other odd. It’s clear that and can’t both be even, since then they would have a common factor of 2. To see that and can’t both be odd, note that the square of an odd number is so the square of an odd number is congruent to 1 modulo 4. This means that if and are both odd, the left side of is congruent to 2 modulo 4. But then the right-hand side is even, so is of the form and is congruent to 0 modulo 4, contradiction. With these two facts out of the way, let me show that a Pythagorean triple is always of the form (1) It is easy to check that if , and can be written this way in terms of and then . But where does the formula come from, and why are there always such and ? A Pythagorean triple can be thought of as a point on a unit circle: (2) Each point corresponds to a single point on the -axis, as in the left half of the figure below. The slanted line connects to a point on the -axis. The key fact is that each pair corresponds to a single number . And is rational if and only if is rational. This can be seen from explicit formulas linking and . To get those formulas consult the right half of the figure where the triangle with sides , is proportional to the triangle with sides . So The second equation shows that if and are rational then so is . And the last two equations show the converse. But they show more. Writing in reduced form (i.e. and relatively prime) and combining with ( 2 ) shows Equating numerators suggests that and . But there is a subtlety: this is guaranteed only if and are relatively prime. For example , but there are no and with and . The small print below gives two ways to turn the suggestion into a proof. To see that the relative primality of and implies the relative primality , suppose that a complex integer divides both of . If is a real integer, then it divides both and , contradicting their relative primality. Similarly if is purely imaginary. That means I can assume that . By conjugating, it follows that and so both and divide . Since these are distinct, the real integer divides which is again a contradiction. The final detail is that although , they could be associates, that is . But that would imply so which after taking norms becomes , and that contradicts the statement from the beginning of this post that and are of opposite parity. Formula ( 1 ) gives an easy way to show why there isn’t a triangle with integer sides and area 5. If then that formula shows that there is a pair , with and When this has no integer solution because three of the factors must be 1, the other 5. Since is the largest of the four factors, it would have to be 5 and , which is clearly impossible. Since the numbers , , , and are used to compute the sides , and , you can remove any common factors if necessary since that won’t change the side lengths , , . So I will always assume that , or using a common shorthand . Proposition: If represents a square-free congruent triangle and , then . Proof: By contradiction. Suppose . Then some prime divides . First suppose so but . Then , , and are integers so . It follows that one of or is even. This means that one of , is divisible by 4, the other by 2, so . Since It follows that , but and is square free. This is a contradiction, so . Similarly, if is an odd prime, and divides , , and but not , then , contradiction. So . I use this to give a shortcut in the exhaustive search over and . You only need to consider a subset. In the lemma below, the parity of is , i.e. odd or even. Lemma: For a square-free congruent triangle, if then and , are of opposite parity. And such pair of generates a triangle with . Proof: First assume . If and have the same parity, then from equation ( 1 ) all of , and are even, contradicting the proposition. If and have a common factor, so do , and , so that is not possible either. For the other direction, note that since and have different parities, then is odd and is even so . Thus you only need to consider an odd prime . If then so so and similarly . It follows that in exhaustive search, you only need to consider , that are relatively prime and of opposite parities. Note: this is the glitch in the web table I mentioned earlier. For , the web table gives , , which are both odd. If you use them to generate , remove the common factor of 2, and then recompute , you get , with . Next, I’ll verify the important speedup, which is that and are almost squares. Proposition: If , are relatively prime and generate a square-free congruent number, then and where (in particular, and ). Proof: Combine equation ( 1 ) and to get If a prime has then or . If the latter, but since , is impossible, and so is impossible so . Therefore every factor of is either a square or a factor of . It follows that where each factor of divides . If any factor of is a square then incorporate it into after which . Similarly . Since , so . We observed earlier that in our tables, the denominator of times the denominator of equals the denominator of . Here is the proof that it is true in general. Repeating the equations: Even though , and can have a common factor in which case and . Lemma: With the usual assumption that , then . Proof: First check that so that . If not, then there is a with and . From and (3) it follows that (or perhaps ). But and implies , contradiction. Next from ( 3 ) every prime factor of must divide either or . Start with and . Then consider the prime factors of one-by-one. For each factor , remove it from either the numerator and denominator of or from . After the removal process becomes and becomes and since and are completely reduced, , and so . Since each factor of was removed, so and thus . Finally, write The last equation shows that . If not, then it writes in a more reduced form, contradicting the first part of the proof. I previously showed so it follows that . And the first part of the proof showed that . Powered by QuickLaTeX", "date": "2016-03-03"},
{"website": "Ebay-Engineering", "title": "Performance Tuning on Quartz Scheduler", "author": ["Sheldon Shao"], "link": "https://tech.ebayinc.com/engineering/performance-tuning-on-quartz-scheduler/", "abstract": "Quartz is a popular open source job scheduling library for Java Application. eBay uses it in many projects to schedule jobs. It does very well under lower load. However, Quartz runs into trouble under a heavy load. A lot of misfired triggers occur, the executor threads can’t get tasks, and hundreds of jobs get stuck in the triggers table. So we have to do performance tuning on it. This article describes how we narrowed down the issue and optimized Quartz. Quartz jobs aren’t able to be scheduled and executed. A lot of jobs in the table simple_triggers wait for execution, but a few in fired_triggers . The simple triggers should have the REPEAT_INTERVAL set, which means they are repeat jobs. TIMES_TRIGGERED means how many times it has been triggered. Tons of “Handling the first 20 triggers that missed their scheduled fire-time …” logs in the log file. Database session increased, and many sessions are wait on “SELECT * FROM qrtz_LOCKS WHERE SCHED_NAME = ‘ {SCHED_NAME} ‘ AND LOCK_NAME = ‘TRIGGER_ACCESS’ FOR UPDATE”. Before we can understand why this happens, let’s learn about misfire. Here is the explanation from Quartz ‘s official website: A misfire occurs if a persistent trigger “misses” its firing time because of the scheduler being shutdown, or because there are no available threads in Quartz’s thread pool for executing the job. The different trigger types have different misfire instructions available to them. By default they use a ‘smart policy’ instruction – which has dynamic behavior based on trigger type and configuration. When the scheduler starts, it searches for any persistent triggers that have misfired, and it then updates each of them based on their individually configured misfire instructions. When you start using Quartz in your own projects, you should make yourself familiar with the misfire instructions that are defined on the given trigger types, and explained in their JavaDoc. More specific information about misfire instructions will be given within the tutorial lessons specific to each trigger type. For example, there is a job that should be triggered with 10-second interval. Let’s consider “0s” as a timestamp. At “0s”, it was acquired by QuartzSchedulerThread and passed to ExecuteThread to execute. The NEXT_FIRE_TIME was set to “10s.” Unfortunately, it took more than 60 seconds and didn’t finish within 10 seconds, so it missed the trigger of “10s” “20s” .. “60s.” After “70s,” the MisfireHandler finds it was misfired, so the NEXT_FIRE_TIME should be recovered to “80s.”  That’s the “smart policy” instruction for repeat simple triggers. Quartz supports clusters, so we are able to configure many instances within one cluster. It needs to use the database LOCK to coordinate the UPDATE on tables triggers and fire_triggers. Quartz uses standard row lock “SELECT * FROM … FOR UPDATE” for MySQL. A graph helps understand TRIGGER_ACCESSLOCK . If a new job stores something in the “triggers” table, it must obtain TRIGGER_ACCESS once the LockOnInsert is true(by DEFAULT). QuartzSchedulerThread also needs to obtain the LOCK after it acquires the trigger and to fire the trigger ( triggersFired ) MisfiredHandler obtains TRIGGER_ACCESS to recover the misfired triggers and updates the NEXT_FIRE_TIME for misfired triggers. We saw this problem on production many times. Here are details of this problem. One instance has only a few jobs executing. Once misfires start occurring, reducing the number of instances will help the system to recover. Based on the logs and database information, we tried to reproduce the problem locally using the following steps. We set up MySQL database locally. We copied the MisfireExample from Quartz’s existing examples. We changed the configuration to point Quartz to use the MySQL database. We modified the MisfireExample to support multiple instances, so that we could run multiple instances locally. We set the system to generate triggers to repeat 5 times with 3 seconds interval every 500ms. After these changes and running five MisfireExample instances, it’s easy to reproduce the problem. Here is what we can see with the behavior the same as that on production. A lot of triggers accumulated in the “simple_triggers” table. A few jobs were fired in “fired_triggers”. Lots of misfired information was printed in the console. Many MySQL sessions were waiting on “SELECT * FROM qrtz_LOCKS WHERE SCHED_NAME = ‘SCHED_NAME’ AND LOCK_NAME = ‘TRIGGER_ACCESS’ FOR UPDATE”. Stopping the storage of new triggers did not help to recover the triggers. Stopping 3 or 4 instances increased the fired triggers. The system will go back to normal once more jobs were executed. As #5, the job generator only generates 2 triggers every minute in one instance. Even when the generating frequency is very low, the system didn’t recover. So that means StoreJobAndTriggers isn’t a key role in this scenario. The problem is that the MisfireHandler and QuartzSchedulerThread compete for “TRIGGER_ACCESS” LOCK. Each instance has one MisfireHandler and one QuartzSchedulerThread. Also if you notice the misfired information printout, it happened around one second. That means it took around 1 second to update 20 rows each time. Another fact is that QuartzSchedulerThread acquires ONE trigger each time once it obtains the TRIGGER_ACCESS” LOCK. It is a high-speed operation compared with the slow speed of MisfireHandler. Here is a graph that indicates that why fewer instances were better than more instances when it was running into misfire problem. Fewer instances mean that QuartzSchedulerThread has more chances to obtain the LOCK. The above chart shows the test result of each optimization. We generated 500 enable/disable traffic jobs at once and started two instances of Quartz to process them. It took around 270 minutes to finish all the jobs when using the original code. But it took only 36 minutes with Quartz batch mode. Quartz supports a batch mode. With batch mode, the QuartzSchedulerThread is able to acquire jobs based on the active executor thread count. When we configure under this mode, the triggers can be executed faster, and the number of fired triggers is same as the total thread count of all instances. The following code is the method of creating the Quartz scheduler. We can set maxBatchSize and batchTimeWindow to leverage the batch mode. public void createScheduler(String schedulerName, String schedulerInstanceId, ThreadPool threadPool, ThreadExecutor threadExecutor, JobStore jobStore, Map<String, SchedulerPlugin> schedulerPluginMap, String rmiRegistryHost, int rmiRegistryPort, long idleWaitTime, long dbFailureRetryInterval, boolean jmxExport, String jmxObjectName, int maxBatchSize, long batchTimeWindow) throws SchedulerException We set maxBatchSize as same as the number of executor threads. The batchTimeWindow should be based on how many tasks triggered in a specific period.  We set it as 1 second in our code. Let the updating job data task execute before obtaining the lock. The Quartz executing thread needs to obtain the TRIGGER_ACCESS LOCK once a stage is completed. It updates Job Data and the state in the trigger table after it obtained the lock. Updating job data takes a lot time because the job data needs to be serialized and stored to the job detail table.  Usually there is only one executor thread updating one job’s data. So it isn’t necessary to do it within the LOCK. When we moved the “updating job data” step into our own code, it reduced the time on lock. After this change, it only took 27 minutes to finish all the 500 jobs. The following chart shows the change. Our job has multiple stages. One stage can be run in any instance independently. The job data should be stored into database permanently. Also it needs to update the trigger state after one stage completed. Executing all the stages in one executing thread and reducing the lock usage would be a good improvement. Quartz uses the database lock in a cluster environment. Jobs will stack with regular configuration when under heavy load. Using batch mode can improve performance quite a bit. And also trying to reduce the lock times would help.", "date": "2016-01-14"},
{"website": "Ebay-Engineering", "title": "GZinga: Seekable and Splittable Gzip", "author": ["Ruchir Shah", "Mahesh Somani"], "link": "https://tech.ebayinc.com/engineering/gzinga-seekable-and-splittable-gzip/", "abstract": "Generally, data compression techniques are used to conserve space and network bandwidth. Widely used compression techniques include Gzip, bzip2, lzop, and 7-Zip. According to performance benchmarks , lzop is one of the fastest compression algorithms, while bzip2 has a high compression ratio but is very slow. Gzip offers the lowest level of compression. Gzip is based on the DEFLATE algorithm, which is a combination of LZ77 and Huffman coding. The zlib software library writes compressed data with a Gzip wrapper. (Note that in this post we will use Gzip and zlib interchangeably.) With Gzip, compression is as fast as or faster than serial writes on disk, and the compression ratio is far better than with lzop. For decompression as well, Gzip performs better than other algorithms. These are the reasons why Gzip is one of the most popular and widely used data compression techniques in the industry. However, Gzip has limitations: you cannot randomly access a Gzip file, and you cannot split a Gzip file in the case of Hadoop map-reduce jobs. As a result, Gzip is slower in those scenarios, and it does not leverage the power of parallel processing. The eBay Cloud Platform team is happy to announce the open-source project GZinga , which aims to provide two additional capabilities for Gzip-compressed files: Seekable: Random search within Gzip files Splittable: Division of a Gzip file into multiple chunks It is common to collect logs from production applications and use them to debug and triage issues. Log files are generally rolled over periodically (hourly, daily) or based on size. To save disk space, log files are stored in a compressed format such as Gzip. In most outage scenarios, developers are interested in looking at logs for a particular activity or around a certain timestamp. Scanning an entire file to find the log for a particular time period is costly. The data for these logs can be stored on commodity storage like Hadoop. However, to take advantage of Hadoop’s capabilities, small chunks of large Gzip files need to be processed in parallel (for files beyond a few hundred MBs). The idea of GZinga originated as a means to provide optimal performance for reading/processing Gzip files. Though we have looked at options for log files, the technique we’ve developed can apply to other use cases—textual documents, web crawls, weather data, etc. In this section, we will discuss generating Gzip files with meta information that will be used to perform random access in those files. We use two techniques from the Gzip specification —multiple compressed blocks and the Gzip header flags format—to make files seekable. A Gzip file consists of a series of “members” (compressed data sets). The format of each member is specified as follows: As a Gzip file can have a series of such members, an actual Gzip file can look like this: Such multiple compressed blocks can be achieved using the Z_FULL_FLUSH option, according to the zlib manual . These blocks help us in reading a file from any arbitrary location after jumping to its inner header location. The main advantages of this feature are that it is not necessary to uncompress the entire file and it is possible to read from different locations. Only the requested compressed block will be uncompressed, which improves performance considerably. As the deflater gets reset for every block, compression efficiency (both size and processing) diminishes. However, we’ve observed that when data is compressed using zlib, then the impact on the amount of compression is minimal. The following table shows the difference in compression and timing for logs, with and without the multiple-blocks approach. In all of these scenarios, 60 blocks are written—1 per minute for an hourly log file. It is evident that when the size per block is greater than 1 MB, the processing and space overhead is minimal and can be ignored. (Note that a smaller block size may not be suitable to use, as it is reasonably fast to decompress such blocks). The header section has provision for extra comments, according to the Gzip specification. Each header has the following format : The 4th byte of the header represents the flag information: Bit 4 can store a zero-terminated file comment at the end of the block. We can use this flag to store an index for the file. The interface for providing the index is shown below in the GZipOutputStreamRandomAccess class : The comment section in the header field will contain information about the index collected up until that point. It should be noted that comment blocks are ignored by standard libraries that don’t look for comments or extra fields. The comments are added as a name / value pair as shown below: At the time of the file close operation, an extra block is added with a header that contains information about all indices, and a footer without any data. This extra block can be used in locating the header deterministically while reading, as described in the next section. The effective size of the file increases due to the comment block as described below (in a test with 60 blocks): To take advantage of an index written in the header, the file will first be open for random access and the last few chunks of data will be read. Then the header needs to be located and the index loaded into memory. Byte comparison (reading in reverse order) will be employed to locate the header, and then the index information will be extracted. Based on the requested index, the read marker will be jumped to the required location and the stream will be passed to Gzip for uncompression of the data. The interface for jumping to the desired location and reading the metadata information is shown in the below GZipInputStreamRandomAccess class: Reading the index and then jumping to the desired location improves performance dramatically. The following table provides the numbers for jumping to the last block of data and reading one record from there: We also observed that it takes the same amount of time irrespective of which index needs to be jumped to before reading one record. Overall, this approach provides significant improvement in performance, with the extent of improvement depending on file size (e.g., 10-50x gain). SeekableGZipDataInputStream is an extension of Hadoop’s FSDataInputStream class to enable random access for Gzip files that are stored inside Hadoop. Using this extended class, one can jump to any location in a Gzip file stored in Hadoop and read required data from there with much faster performance. Here is an example of using this class to perform random access: In many Hadoop production environments, you get Gzipped files as the raw input. When putting these Gzipped files into Hadoop, an MR job runs with only 1 map task, as a Gzip file is not splittable. This fact is the biggest disadvantage to Gzip, because it defeats the real power of Hadoop. But when we generate Gzip files with the above Seekable approach, then it is possible to split a Gzip file and feed it to an individual map task for processing. A Gzip file with multiple compressed blocks allows for splitting the file based on the start of a new header. In our approach, we implemented a SplittableCompressionCodec interface for the GzipCodec class. Using this interface, when a split is being invoked with the “start” and “end” position of the split provided, it locates the start of the header from the provided “start” (and “end”) position and sets the new “start” (and “end”) pointer once located. For example, given the following split as the original (or default): The new “start” and “end” location will be set as shown below: Compressed data before the “start” position will be processed by the previous split (both the “start” and “end” positions will point to the next header). In order to use this feature, one needs to set the io.compression.codec property in the configuration object before submitting the MR job. Instead of org.apache.hadoop.io.compress.GzipCodec for Gzip files, the value should be io.gzinga.hadoop.SplittableGzipCodec: This value can also be set in the mapred-site.xml file to take effect for all MR jobs. Another important property to set is for splitting the size. Property mapreduce.input.fileinputformat.split.maxsize indicates the maximum split size. The number of splits will depend upon the value of this property and the actual Gzip file size. The greater the number of splits, the faster the MR job. Below are performance numbers indicating improvement with respect to splits (where the input Gzip file size is 280MB and the MR job is to perform the wc command on the file): As these performance numbers show, the map task took 127 seconds when run with a single split, compared to 52 seconds when run with 3 map tasks. Also note that when the split size decreases (and the number of splits increases), the map task time decreases accordingly. Gzip provides one of the best compression algorithms and is widely used in the industry. But with its lack of support for random access search and file splitting, it is not able to leverage the power of parallel and distributed processing systems like Hadoop. With the introduction of the Seekable and Splittable features, Hadoop access can be achieved with high performance.", "date": "2015-10-09"},
{"website": "Ebay-Engineering", "title": "embedded-druid:  Leveraging Druid Capabilities in Stand-alone Applications", "author": ["Ruchir Shah"], "link": "https://tech.ebayinc.com/engineering/embedded-druid-leveraging-druid-capabilities-in-stand-alone-applications/", "abstract": "Co-authors:  Ramachandran Ramesh, Mahesh Somani, and Sankar Venkatraman The eBay Cloud Platform team is happy to announce the open-source project embedded-druid , which aims to provide Druid capability for a reasonably small amount of data without involving the complexity of multi-node setup. That is, embedded-druid is Druid but with a single JVM process. Druid is an open-source data store designed for real-time exploratory analytics on large data sets. Combining a column-oriented storage layout, a distributed, shared-nothing architecture, and an advanced indexing structure, Druid allows for the arbitrary exploration of billion-row tables with sub-second latencies. It also supports fast aggregations and OLAP queries. Druid is proven technology for executing OLAP kinds of queries involving Big Data, and for providing sub-second response times. Given its distributed, shared-nothing architecture for large amounts of data, Druid has multiple components: real-time node, historical node, broker node, co-ordinator node, deep storage, MySQL, ZooKeeper, etc. If the input data size is small (say, up to hundreds of millions of rows), then the overhead involved in deploying Druid might be excessive; one might prefer to use in-memory database systems like Apache Derby or PostgreSQL if the report requirement is very simple (such as grouping by some dimension or retrieving topN values). There are numerous use cases where the input is not “ Big Data ” but rather medium or small data requiring OLAP-like capabilities, such as grouping by multiple dimensions and handling percentile and other aggregation functions. For example, in eBay we generate operational metrics reports for applications that run on multiple machines across data centers. This report contains total request counts, average request durations, etc. across different dimensions, such as the type of request, data center, request status, dependency, etc. Each application owner might view different types of information from this report – top hosts with errors, slowest requests by request type / data center, or requests by error code, as just a few examples. Given the dynamic nature of such queries, if Druid capabilities can be leveraged without deployment complexity, then the lives of developers, debuggers, and analysts can be made much easier. Let us assume a simple database that represents the number of characters added for a particular page in Wikipedia. We have the following columns representing the dimensions for our data: Here is the metric we are interested in: The following table shows sample data for the above schema: We want to generate different reports, such as “How many edits were made on page AB grouped by gender?” or “What is the histogram for edits made on page JB in SF?” The following sections walk through how to generate such reports using embedded-druid. Currently, embedded-druid supports loading CSV files, for which the implementation class CSVLoader is available. One needs to first provide a list of all columns available in the CSV file (including metrics), a list of dimensions, and a column specifying the timestamp (if available). For example, for the Wikipedia schema mentioned above, the CSV file might have data in this format: The following code creates the Loader object required to load this data in memory: Druid generates segment and index files for the given input. Once the Loader object is created, one needs to specify segment and index files required for query purposes. This specification includes the available dimensions and the kind of aggregator function to be used for querying. For example, if one is interested in querying values like total count, max, min, total sum, and percentiles, then the following AggregatorFactory objects need to be created: To create segment and index files locally, one needs to create a QueryableIndex object as follows: By default, segment files are created at the location System.getProperty(\"druid.segment.dir\"). If this property is not set, then the files will be created at the temporary location System.getProperty(\"java.io.tmpdir\") + File.separator + \"druid-tmp-index-\". Therefore, if one wants to create segment files at a specified location, then  property “druid.segment.dir” needs to be set first. Once segment files are created, one can execute different kinds of queries using the QueryableIndex object. For example, if one wants to execute GroupByQuery for the above mentioned schema, then the code looks like this: Similarly, here is the code snippet for executing TopNQuery: We are planning to extend this work by providing (and/or integrating) REST APIs for ingestion and for querying Druid data. For visualization, we also plan to integrate with an easy-to-use UI like Grafana . These enhancements will help users analyze data quickly and surface meaningful information promptly.", "date": "2016-02-05"},
{"website": "Ebay-Engineering", "title": "Clojure for Fun and Profit", "author": ["David Vilar Alcala"], "link": "https://tech.ebayinc.com/engineering/clojure-for-fun-and-profit/", "abstract": "When we decided to build our Shutl iOS application we had some decisions to make. One of these decisions was the technology to use to build the back end. This back end is a pretty simple service. First, it provides an API that combines and transforms data retrieved from different services so the client can present it. Secondly, it listens to events and notifies the clients accordingly. Both of these use cases could benefit from or require asynchronous processing. At Shutl, Ruby is our default programming language, so usually it would be a very straightforward decision to choose Sinatra and Sidekiq. But the team is always willing to try new things, and Clojure has caught our attention for some time. Our back-end project sounded like a good opportunity to test out Clojure. It wasn’t a critical component, but it could potentially grow bigger. The logic would be pretty simple, mostly about transforming data, and our design goals — concurrency and parallelization — played to Clojure’s strengths. So we decided to give it a shot. As complete beginners, we took a look at the Clojure ecosystem. Although some web frameworks exist in Clojure, the general approach seems to be combining small libraries that do one thing well. After some research, this was our initial setup: Ring running Jetty as a web server and Compojure as the routing library Midje as the test framework Korma to access the database and Ragtime to run migrations core.async for the asynchronous processing java-apns , a Java client to use the Apple Push Notification service Logback , a successor of log4j Hystrix and Hystrix-clj for resilience and fault tolerance One big advantage of using Clojure is that it runs on the JVM and offers idiomatic forms to invoke Java code. Clojure therefore makes it very easy to use Java libraries and gives you access to the great ecosystem of Java libraries, like Logback. For some of them (I am looking at you, Hystrix), wrappers also allow a more idiomatic usage. Coming from a Ruby perspective into Clojure, we needed a different mindset but we were halfway there. We embraced immutability a long time ago, functions as first-class citizens are a familiar concept, and Ruby is also dynamically typed. There were only two things we had to get used to: zero objects and lots of parentheses. The first one was easier than expected; it is kind of easy to get used to the way you write code in Clojure. You write small, concise functions and then you combine them. So simple, so easy. But what about the syntax? I am not going to lie. It can be overwhelming at the beginning. Then, as you use it, you discover one of the things I love the most about Clojure: it is a very simple language. But simple doesn’t mean powerless, just the opposite. Being a dialect of Lisp, the core of the language is very small but the standard library is complete and well documented. Clojure allows you to focus on what really matters — what your code is supposed to do — rather than on learning how to do things. This simplicity, combined with the use of the REPL, allows you to easily test and increase your knowledge. And a tiny, little detail: it is fun. A few months later, the application went live and everybody was happy. It has been running in production since the fall of 2015, we haven’t had any issue, and it is performing really well. Obviously, we still have lots to learn. We didn’t use every single feature in the language (did somebody say writing macros?), but just because we didn’t need them. We are very proud of what we built. All in all the experience was very positive. It was so positive that recently we decided to use Clojure as the back-end language for a new product… stay tuned!", "date": "2016-02-12"},
{"website": "Ebay-Engineering", "title": "How Our CSS Framework Helps Enforce Accessibility", "author": ["Ian McBurnie"], "link": "https://tech.ebayinc.com/engineering/how-our-css-framework-helps-enforce-accessibility/", "abstract": "Spot the difference….You can’t! To a sighted user it appears we have two identical button elements. A user interface control not only needs to look like a certain control, it must be described as that control too. Take for example a button, one of the simplest of controls. There are many ways you can create something that looks like a button, but unless you use the actual button tag (or button role – more on roles later), it will not be described as a button. Why does it need to be described as a button? Users of AT (assistive technology), such as a screen reader, may not be able to see what the control looks like visually; therefore it is the job of the screen reader to describe it aurally. A screen reader, such as VoiceOver for Mac OSX and iOS, can do this job only if we, the developers, ensure the correct semantics are present in our HTML code. In the table below, compare and contrast the accessibility tree attributes for each element  (hint: click each image to view at full size). VoiceOver uses the accessibility tree to convey to the user what it knows about the web page. You will see that for the fake button, there is nothing in the tree to identify the span element as a button. Quite simply, VoiceOver does not know this element is intended to be a button. “To click this button press CTRL-OPTION-SPACE.” Accessibility tree screenshots taken from Mac OSX Accessibility Inspector What’s also interesting is that if you look at the ‘Actions’ section of the tree, the real button has an ‘accessibilityPerformPress’ action, while the fake button does not. Armed with this information, VoiceOver can also describe how to interact with the element (e.g., press CTRL-OPTION-SPACE ). No such information will be communicated for the fake button. We can safely say that this fake button is not accessible, because the AT doesn’t know what it is or how to interact with it. It appears our fake button is accessible only to people who can see the screen and use a mouse. Oh dear – this fake button has excluded a large number of our users from being able to buy items! You might be wondering, “Who on earth would use a span or div tag for a button?” You might now also be thinking, “What on earth does Swiss cheese have to do with any of this?” In the Swiss cheese model of accident causation, risk of a threat becoming a reality is mitigated by differing layers and types of defenses that are “layered” behind each other. For example, we might use code linting, code reviews, accessibility checkers, and manual testing to help ensure that this button is properly described. We liken these separate layers to multiple slices of Swiss cheese, stacked side by side – hence the name. Is there anything cheese can’t do? Although many layers of defense lie between hazards and accidents, there are flaws in each layer that, if aligned, can allow the accident to occur. What if we could also write our CSS framework in a way that acts as another layer in our line of defense? Read on to find out how! Continuing on from our previous ‘fake button’ example, let’s suppose the developer had created the following rules to make the span element appear visually like a button: The dreaded fake button (although you still can’t tell, just by looking at it) What we have here is the proverbial cart before the horse. The developer has styled the element before describing its purpose . One way in which we can create the necessary description (the horse) is to require a role attribute. We’ll go into more detail on the role attribute later, but here’s the interesting bit – we can leverage attribute selectors and re-write our CSS like so: Under the skin: our attribute selector has now exposed the fake button for the fraud that it is! Our selector now ensures that a button will visually appear like a button only if it has first been described as a button. You can almost think of this as TDD (test-driven development). If the HTML does not pass our ‘test’, the visual style will not be applied. It’s important to know that nearly all elements have a default implicit role, and these default roles do not need to be specified in the HTML – to do so would be redundant. No prizes for guessing what the default role of a button element is. Yes, it’s button ! You might think that it was easy enough for us to convert a span into an accessible button using the button role, but in actual fact our work would not be finished there. Adding a role does not add behavior. A fully accessible button must be keyboard focusable and it must be invokable with SPACE and ENTER keys too. A button element gives this behavior for free; a span element – even with a role of button – does not, and we must implement its behavior by hand. So please, and I really can’t emphasize this strongly enough, do everybody a favor and always use an actual button element for buttons. The only real reason you might have for using the button role is when progressively enhancing a link into a button using JavaScript; for example, to make the link open an overlay instead of a new page – which is exactly what we do on eBay. As with spans and divs, allowing anchor tags for buttons does re-open the door to misuse and abuse (think ‘faux’ buttons); and though it is possible to enforce the correct usage with clever use of attribute selectors, it’s a little more convoluted and therefore beyond the scope of this post. Again, we can enforce this markup requirement by rewriting our CSS selector like so: Horse, cart, & driver: the element now has the appearance, description, and interaction of a button Finally, no more span and div tags for buttons. Our CSS framework simply does not allow it. So far we’ve looked at a simple example of how CSS selectors can force developers to put the proper semantics in place – whether implicitly or explicitly. But what about state? If an element has state (a checked checkbox for example), it is not sufficient to describe only what the element is; we must also describe what state it is in. Developers often fall into exactly the same trap as before: they convey the state visually but not aurally. In the following code example, the developer has used a modifier class of btn--disabled in order to alter the opacity and background-color of the button: Our ‘ghosted out’ button appears visually disabled Modifier class is a BEM (Block, Element, Modifier) concept. Throughout this article we will be using a variation of BEM in order to structure and distinguish our class names. You might be thinking that this isn’t really disabled. If so, you are quite right. This button will not be described as disabled and it will not behave as disabled. Again, you might be thinking, “Who actually does this kind of stuff?”, but fear not, our CSS selectors can again protect us from this manner of profanity: As you can see, the previous modifier class will no longer cut the mustard. It is removed from the selector entirely and the HTML disabled property takes its place. Only when this property is applied in the markup will the button be well and truly disabled for all users. So far, none of this is particularly earth-shattering, I’m sure you agree, but it sets the stage nicely for moving onto more complex controls and widgets, where we must start delving deeper into the world of WAI-ARIA (commonly referred to as just ARIA for short). HTML gives us only a limited set of controls such as buttons, links, and the various form value inputs. What about menus, tabs, carousels, overlays, etc. – how do we describe those? Yes, you guessed it – ARIA comes to our rescue. ARIA gives us many more roles beyond a simple button, and these roles, in conjunction with a multitude of states and properties, open up a whole new set of desktop-like user interface controls and widgets for us to play with. Just make sure you read the instructions before diving in. You do read the instructions don’t you? Look out for more controls in HTML5, such as menu and dialog. In fact, you might be interested to know that both the menu and dialog tags started out life as ARIA roles before they were introduced as bona fide HTML elements. Don’t get too excited, though – neither have cross browser support at the time of this writing. In the next section we will look at an example of such a widget and demonstrate how we can use ARIA to influence the way we write CSS selectors in order to enforce accessible markup. A tabs widget allows the layered stacking of two or more content panels, whereby only one panel of content can be visible at any time. A list of clickable tabs allows the user to swap out the visible panel. This all happens on the client, without a full page reload (i.e., the client is stateful). By decluttering the user interface in this way we can say that a tabs widget follows the principle of progressive disclosure . Using tabs, the user can switch between “Sign In” or “Register” without a full page reload. It is critical that our interface is not only visually identifiable as a tabs control (I’ve seen designs that struggle even to meet this criterion!), but also aurally . Without any tab-related HTML tags, how do we achieve this? A seasoned developer might set out initially to create the tabs as a list of clickable page anchors for the tabs, with a group of divs acting as anchor targets for the tab panels: This is a perfectly reasonable approach to begin with. Page anchors are often well suited as the starting point for tabs, because in the case of JavaScript being unavailable they ensure at least some basic functionality when clicked (i.e., the browser will scroll to the content of the relevant panel). However, when JavaScript does become available, care must be taken to prevent the default link behavior so as to not interfere with tab semantics and behavior. Let me be very clear about this: links are not the same as tabs! This technique of making core content and functionality available pre-CSS and pre-JavaScript is called progressive enhancement . Progressive enhancement is the safest and surest way to guard against the unknown (e.g., script timeout, script failure, scripting disabled) and to ensure your core experience remains backwards and forwards compatible in all HTML-capable browsers. We will assume that all layout-related styling is in place for the links (i.e., they are neatly spaced out horizontally), and that by default the visible state of all panels is hidden, with only the ‘active’ panel displayed. Let’s then suppose our developer chooses to visually convey the selected ‘tab’ state using only an underline (a veritable tour de force of minimalism, I know): It would now take only a small amount of JavaScript for our developer to turn this into a “functioning” tabs widget by preventing the default link action (i.e., prevent it navigating to the URL fragment) and toggling the ‘selected’ and ‘active’ modifier classes accordingly; and indeed our developer might be tempted to stop there. But although this control looks like a tabs widget, it will currently be described only as a list of links (scroll down to see the accessibility tree). No clues are given as to the dynamic, stateful nature of the widget. Screen reader users attempting to follow one of these links are going to be surprised when nothing happens after invoking the link, and equally surprised when no navigation occurs. They are left guessing as to what type of control they might be interacting with. Not a good experience. Let’s fix it so that if developers try to use our amazingly awesome CSS to style their tabs like ours (go on, admit it, you want that underline too), the styles will appear only if they have the correct accessible markup in place. To achieve the correct markup for tabs, just as with our simple button example, we can replace class names with ARIA roles and states. Luckily, ARIA gives us a set of tab-related roles: tablist tab tabpanel We can also leverage the following global ARIA states: aria-selected aria-hidden aria-controls aria-labelledby Whilst it would be entirely possible to continue on with our demonstration of progressive enhancement by applying the above roles and states to override our previous link -based markup, it does add some additional complexities which might distract us from the primary topic at hand. So, rather than getting bogged down in those details, let’s drop the progressive enhancement for now and pretend we live in a magical world where JavaScript is always on, is always available, and always works. Actually, to be honest, it’s not just a JavaScript issue. Some people would argue that by using list-based markup, we also provide for a reasonable semantic fallback in the cases where the tab & tablist roles are not supported by the user’s browser & AT combo. It will make most sense if we show you the new HTML first this time, rather than the CSS, and hopefully, without the cognitive clutter of the list and link tags, our end goal is now a little clearer. You will quickly see that the core DOM structure remains almost identical: With these new ARIA roles in place, our tabs will now actually be described as tabs by assistive technology. Likewise, when our JavaScript toggles the ARIA selected state, this state will also be conveyed to our users. Note that AT actually requires two additional ARIA properties that are not present in our markup: aria-controls (on the tabs) and aria-labelledby (on the tabpanels). These ARIA properties are not typically used as styling hooks on tabs, so we will leave them out for the sake of code brevity; but be sure to include them when building your own tabs widget! Okay, so we are nearing the end now, but first we must finish up our CSS. Our selectors must become a contract for the accessible HTML above. Where before we had classes for BEM blocks and elements, now we have ARIA roles. Where before we had classes for BEM modifiers, now we have ARIA states: Personally, I’m a big fan of BEM, but it’s nice where possible like this to be able to replace it with something a little more real, if you know what I mean. One other rule we have enforced in our selectors is the tabindex attribute. Keyboard accessibility for tabs must be implemented in JavaScript using a roving tabindex technique; this is because the tabs in a tablist are selected using the arrow keys, not the tab key (the tab key is actually used to exit the list of tabs). While not strictly necessary to ensure the correct description is given, this selector helps ensure that the correct attribute values are in place for roving tabindex behavior. It’s up to you whether you want to go this far, into the realm of behavior-testing, in your own selectors. We must always remember that correctly describing a UI control is only part of making it accessible. The user expectation is that it behaves like that control too. Therefore we must also ensure that the correct accessible behavior is in place. For example, a button must always be ‘clickable’ with SPACE and ENTER keys. Sadly, this kind of behavior is often the first thing to go missing when developers try rolling their own buttons using span or div tags. More complex controls such as tabs, menus, or autocomplete will typically require a more significant amount of JavaScript in order to make sure the control fully behaves according to its description. We have seen that each layer of the web frontend has its own responsibilities in terms of creating accessible UI controls: HTML provides the aural description and some built-in behavior CSS provides the visual style and interaction clues JS provides any missing behavior not provided by ARIA or HTML HTML provides behavior, without the need for JavaScript, for built-in tags such as links, buttons, and form controls. For the purpose of this blog post, our focus has been primarily HTML and CSS. HTML is fundamental in laying solid foundations for accessible UI controls and widgets, and we have shown how those foundations can be enforced by use of CSS attribute selectors. So, the next time you find yourself creating a class name like ‘active’, ‘hidden’, ‘on’, or ‘off’ – stop and think instead how you might be able to leverage HTML properties or ARIA states in your selectors. Likewise, if you find yourself creating a class name like ‘btn’, ‘tab’, or ‘dialog’ – also stop and think about how you might be able to leverage an existing HTML tag or ARIA role. Thank you for reading. I hope you enjoyed it. If you are interested in more accessibility-related articles in future, be sure to leave a comment below! Finally, if you are interested in learning more about our CSS framework, watch this space for an upcoming announcement and further details. We are currently applying the finishing touches to the framework before releasing it as open source. The Swiss Cheese Model: https://en.wikipedia.org/wiki/Swiss_cheese_model https://en.wikipedia.org/wiki/Swiss_cheese_model Finding out the default, implicit role of any HTML element: http://www.w3.org/html/wg/drafts/html/master/semantics.html http://www.w3.org/html/wg/drafts/html/master/semantics.html Full list of ARIA widget roles: http://www.w3.org/TR/wai-aria/roles#widget_roles_header http://www.w3.org/TR/wai-aria/roles#widget_roles_header Full list of ARIA widget states & properties: http://www.w3.org/TR/wai-aria/states_and_properties#attrs_widgets_header http://www.w3.org/TR/wai-aria/states_and_properties#attrs_widgets_header Finding out how an ARIA widget is supposed to behave: http://www.w3.org/TR/wai-aria-practices/#aria_ex http://www.w3.org/TR/wai-aria-practices/#aria_ex Providing keyboard navigation for widgets (roving tabindexes and aria-activedescendant): http://www.w3.org/TR/2009/WD-wai-aria-practices-20090224/#kbd_generalnav http://www.w3.org/TR/2009/WD-wai-aria-practices-20090224/#kbd_generalnav Decluttering the user interface using progressive disclosure: https://en.wikipedia.org/wiki/Progressive_disclosure https://en.wikipedia.org/wiki/Progressive_disclosure Emphasizing the core content of your site using progressive enhancement: https://en.wikipedia.org/wiki/Progressive_enhancement https://en.wikipedia.org/wiki/Progressive_enhancement BEM methodology: https://en.bem.info/method/ https://en.bem.info/method/", "date": "2015-11-04"},
{"website": "Ebay-Engineering", "title": "Announcing Neutrino for Load Balancing and L7 Switching", "author": ["Blesson Paul"], "link": "https://tech.ebayinc.com/engineering/announcing-neutrino-for-load-balancing-and-l7-switching/", "abstract": "The eBay PaaS team is pleased to announce the open-sourcing of Neutrino , a software load balancer developed to do L7 switching and load balancing for eBay’s test infrastructure. Built on Scala and Netty, Neutrino uses the Java Virtual Machine as its run-time environment. It can run on any commodity hardware as long as JVM 7+ is available. Our team was looking for options to replace eBay’s hardware load balancers, which are expensive, inflexible, and unable to keep up with the demand, especially in our test environments. We considered two options: either use an open-source product like HAProxy, or else build an in-house solution. At a high level, the SLB would need to satisfy the following requirements: L7 switching using canonical names L7 switching using canonical names and URL context L7 switching based on rules – for example, routing traffic based on the HTTP header, authentication header value, etc. L4 switching Ability to send the traffic logs to API endpoints Automated cluster management using eBay PaaS; the SLB should be able to read the network topology (which is stored in a database and can be accessed through API) and reconfigure itself Support for the most common load-balancing algorithms, such as Least Connection and Round Robin. The framework should also be extensible to add more algorithms in the future. Ability to run on bare metal, a VM, or a container No loss of traffic during reconfiguration HAProxy is the most commonly used SLB across the industry. It is written in C and has a reputation for being fast and efficient (in terms of processor and memory usage). It can do L4 switching as well as L7 switching using canonical names and URL context. However, HAProxy does not satisfy the requirement for L7 switching based on rules, sending logs to API endpoints, or adding new load-balancing algorithms. Reading the configuration from a database or an API can be achieved through another application, but this solution is not optimal. We found that extending HAProxy to support these features would be tough, as would be adding more load-balancing algorithms. For these reasons, our team needed to think about developing an SLB in-house. Neutrino was built in Scala, using the Netty server, with the above requirements in mind. Neutrino can do L7 routing using canonical names, URL context, and rules. Its highly extensible pipeline architecture enables new modules to be hooked into the pipeline with minimal effort. Developers can add new switching rules and load-balancing options easily. New modules can be added to send logs to API endpoints or to load the configuration file from a database or API. Because Neutrino uses the JVM run-time environment, developers can use either Java or Scala to add modules. Modular and pluggable architecture Neutrino supports easy extensibility for new routing and resolving policies. A customizable pipeline enables adding new modules in the request and response channel. Neutrino supports easy extensibility for new routing and resolving policies. A customizable pipeline enables adding new modules in the request and response channel. Switching options L7 routing can be based on canonical names, URL context, or rules. L4 routing is achieved by multiple port numbers instead of floating IPs. L7 routing can be based on canonical names, URL context, or rules. L4 routing is achieved by multiple port numbers instead of floating IPs. Pluggable pipeline New components for the pipeline are easy to build. The run-time environment is JVM, and so these components can be built on any language that runs on the JVM. New components for the pipeline are easy to build. The run-time environment is JVM, and so these components can be built on any language that runs on the JVM. Horizontal scalability Any number of SLBs can run in parallel, in containers, in VMs, or on bare metal. Any number of SLBs can run in parallel, in containers, in VMs, or on bare metal. High availability and performance Due to its distributed architecture, Neutrino is highly available, and can support very high throughput levels. We have measured upwards of 300+ requests per second on a 2-core VM. Reconfiguration of the Neutrino cluster – including addition of machines, rules, etc. – causes no loss of traffic. Due to its distributed architecture, Neutrino is highly available, and can support very high throughput levels. We have measured upwards of 300+ requests per second on a 2-core VM. Reconfiguration of the Neutrino cluster – including addition of machines, rules, etc. – causes no loss of traffic. Traffic metrics and configuration APIs Traffic metrics and configuration are exposed as APIs . Metrics can be easily published to Graphite . Neutrino is also extensible to push metrics to other metrics systems. Traffic metrics and configuration are exposed as APIs . Metrics can be easily published to Graphite . Neutrino is also extensible to push metrics to other metrics systems. Neutrino’s SLB strength lies in its programmability and customizable nature. It is very easy to customize and adapt to an existing topology. The Neutrino documentation includes examples of customizations. Neutrino can run in a Docker container or a VM. Since Neutrino is built as a library, it can also be easily integrated and shipped along with other applications. Neutrino can be dynamically re-configured during run time without bringing down the connections. By default, Neutrino can read the configuration from a file. Configuration can be updated either within a given interval or on demand through an API. It is also very easy to move the configuration to a database or an API resource. By default, Neutrino supports two load-balancing options: Round Robin Least Connection Support for additional options is provided by adding a custom load balancer . Neutrino also supports three switching options by default: L7 switching using canonical names – With this option, every HTTP request will have a URL address in the HOST header. Neutrino can look at the header and route traffic to the appropriate pool. For example, two pools could have their own canonical names: cname1.com and cname2.com. L7 context-based switching – This option is useful if the same canonical name is used for two services, but with different URL suffixes. For example, cname1.com/first could point to one pool, and cname1.com/second could point to the other. L4 switching based on port numbers – In this case, every service will be identified using a port number, and traffic coming to each port number will be sent to the corresponding pool. For example, cname1.com:1185 could point to one pool, and cname1.com:1186 could point to another. If none of the above switching options satisfies the requirements, custom pool resolvers can be added. The following diagram illustrates the switching options. Git Hub repository: https://github.com/eBay/Neutrino Neutrino website: http://neutrinoslb.io/ Thanks to Chris Brawn and Srivathsan Canchi for their contributions to this project.", "date": "2016-02-17"},
{"website": "Ebay-Engineering", "title": "Machine Translation: Search Queries at eBay", "author": ["Tatyana Badeka"], "link": "https://tech.ebayinc.com/engineering/machine-translation-search-queries-at-ebay/", "abstract": "All eBay-generated content is currently translated by our talented localization team, whereas eBay’s user-generated (UG) content is handled by our Machine Translation (MT) engine. It is common knowledge that UG text can get pretty noisy due to typos, non-dictionary terms, etc. At eBay, however, MT deals with more than that. We work with multiple  types of UG content – search queries, item titles, and item descriptions – and each presents its own challenges. This post discusses one of those content types: search queries. Translating search queries (SQ) is a very important step in providing our customers with a localized shopping experience, because before even opening a listing, customers look through the search results to choose the listings of interest. What we offer our users is an opportunity to search for items in their native language by automatically translating their queries into the language of the market (English, German) and matching their queries against our inventory. Search on eBay is a complex process (think of polysemy , broad context, the variety of inventory, etc.); try adding a machine translation step to that! Here are some of the main challenges we face when translating SQ. Training set ( see SMT for more ). Queries are translated from the user’s language, which means there has to be a separate training corpus for this direction in every language. Being able to use actual post-edited queries for training is very helpful. Lack of context. As you can imagine, search queries are quite short; the average length is 1-3 words (iPhone; iPhone case; blue iPhone case). The training data therefore has very little context for the engine to learn from. Polysemy. Given that search queries provide zero category information, polysemous terms are number one candidates for an error. Categories are listed for search results , but we have no way to know which category a user had in mind when he/she typed “pipe” in the search field – was it Plumbing, Motors, or Collectibles? Which translation do we choose? The same issue applies to all languages. Domain limitations. This goes hand in hand with polysemy. Guessing the user intent and choosing the right category is not the only problem with polysemic terms; sometimes we also need to keep in mind legal aspects, domain specifics, and even shipping policies.  For instance, some possible meanings of a term might point to items that would be very expensive or illegal to ship, or that for other reasons are very unlikely to be sold on eBay. In this case, we might give preference to a less common meaning that is more likely to be sold and bought on eBay. Language trends. Sometimes the most obvious and common translation, the one that would be offered in a dictionary, is not suitable for our purposes. Dictionaries cannot keep up with the fast-changing language culture and do not always reflect the current trends. This situation is especially true for clothing and gadgets. Fashion is changing, technologies are developing, and new words are emerging – or new meanings are assigned to existing terms. A lot of English words are also getting adopted by other languages (or transliterated, in the case of Russian), often with a local “flavor.” We must keep up with these trends. Here’s a specific example: A simple word “шапка”, which means “hat” in Russian and would be translated as a hat in any other circumstances, should be translated as “beanie” on eBay, because when Russian users search for “шапка”, what they have in mind looks like a beanie; the search results for “hat” are much more diverse and less relevant. Translating search queries correctly means helping eBay users find what they want. Providing an accurate, grammatically correct translation of a query is never enough; what we always keep in mind is user intent and relevance of the results.", "date": "2016-03-07"},
{"website": "Ebay-Engineering", "title": "Diversity in Search", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/diversity-in-search/", "abstract": "This is a post about how to rank search results, specifically the list of items you get after doing a search on eBay. One simple approach to ranking is to give a score to each item. For concreteness, imagine the score to be an estimate of the probability that a user will buy the item. Each item is scored individually, and then they are presented in score order, with the item most likely to be purchased at the top. This simple approach makes a lot of sense. But there are some situations where it can work poorly. For example suppose that there are forty results, 20 are Sony and 20 are Panasonic. And suppose that all the Sony items have a score that is just a tiny bit better than any of the Panasonic items. Then the first 20 results will be Sony, giving the incorrect impression that we only have Sony. It would be better to show at least one Panasonic near the top, preferably high enough to be “above the fold”. In other words in this case I want to take diversity into account. An ultra-simple method to handle this case would be to add a small amount of random noise to each score. But I think you can see that this is somewhat of a kludge, and doesn’t address the problem of balancing score and diversity head-on. One of the first principled methods for diversity goes back fifteen years to the paper of Jaime Carbonell and Jade Goldstein “ The use of MMR, diversity-based reranking for reordering documents and producing summarization ” in Proceedings of SIGIR ’98. Their idea is a simple one. Find a way to measure the similarity of items, where presumably two Sony items would be more similar than a Sony item and a Panasonic one. Then trade off the score and the similarity. In more detail, they propose placing items on the results page one step at a time. The first step places the item with the highest score, and records its index in the set of placed items I . The second step examines every unplaced item and compares their scores with their similarity to the item i (where I ={ i }). If P ( j ) is the score of j (P for probability) then each item j is evaluated using what they call marginal relevance: λP ( j )−(1− λ ) S ( i , j ). The marginal relevance is higher when the score P is better, and it is higher when the similarity to i is less. The parameter λ determines the relative importance of these two numbers. The item chosen for the second slot is the one with maximum marginal relevance (MMR): The MMR item is placed on the results page, and its index is added to I . In general the algorithm computes and places the resulting item on the result page as well as adding its index in I . This seems an improvement over adding random noise, but is unsatisfying for several reasons. Once it places an item, it is reluctant to place a second similar item. This may be fine for web search where you only want to see one instance of each kind of page, but is not so good for e-commerce where you’d like to compare multiple items. Also, there are almost certainly several dimensions of similarity (brand, cost, new vs used, etc), and it’s not clear how to combine them all into a single similarity function. Rakesh Agrawal et. al. have a paper “ Diversifying search results ” from WSDM ’09 that doesn’t address any of those objections, but does address a different problem: how do you pick λ ? They propose an algorithm that doesn’t have an arbitrary tuning parameter. They suppose that each item is in a category and that the demand (for a fixed query) of each category is known. This maps well with eBay search, since we know the category demand for our top queries. How to use this info? They imagine that users who issue query Q are diverse—that some are looking for items in category C 1 while others want an item in a different category C 2 , and that this diversity is captured in the category demand table. The paper gives an algorithm that maximizes the chance that a user will find an item that in their desired category. Let’s call this the Agrawal algorithm. The Agrawal algorithm is a step above MMR in elegance because it doesn’t require fiddling with an arbitrary constant like . It works well for categories, but what about other forms of diversity like brand or item condition (new vs used). Just as we record category demand, we could record demand for brands, item condition, etc. But then we would need a way to soup-up the Agrawal algorithm to combine these different demands, and presumably would need to prioritize them. And like MMR, it heavily penalizes an item if a similar one already exists, which is not appropriate for e-commerce. A paper by Michael Welch et. al. “ Search result diversity for informational queries ”, WWW ’11 addresses one of the objections to Agrawal. They assume that users want to see more than one document per category. Specifically, they introduce a random variable J representing the number of items a users wants to see. You have to provide an estimate for the probability distribution of J (i.e. find numbers p j = Pr( J = j )) and then the algorithm uses that in its ranking. But it still doesn’t address our requirement to gracefully handle multiple dimensions of diversity. There’s another factor to take into account. With multiple dimensions, we are unlikely to have a good estimate of the demand. For example if the dimensions are category, brand and condition, we would ideally want the demands for each tuple: for example (category=TV, Brand=Sony, condition=Used). For all these reasons, we abandoned the train of thought Carbonell → Agrawal → Welch, and took a different approach. For each query we specify which dimensions are important, together constraints in the form of a max and/or min. For example for the query “flat screen TV”, we might want at least 10% new items, and at most 50% with brand Samsung. This fits in nicely with our search architecture, which lets us customize the search behavior for a specific query by putting info into DSBE, a database indexed by query. Also we expect that in the common case the min/max constraints aren’t violated, and so the exact value of the constraints aren’t critical. You can think of this as a very simple agent system. Each dimension has an agent with constraints (max/min). Each agent monitors the construction of the results page, and tries to keep their constraint satisfied. They won’t always succeed, so think of the requirements as being soft constraints. Whenever you have multiple constraints, your first question should be how to prioritize them. This falls out almost automatically from the agent algorithm, which I now describe. Each agent monitors the construction of the result set. At each stage it checks on its constraints. If any are violated, it scans through the unplaced items looking for an item that will get the result set closer to satisfying its constraints. This is the candidate . If the agent’s constraint is violated, it is unhappy and it quantifies its unhappiness by summing two terms. The first term measures how much the placed items deviate from the agent’s constraints. The more deviation, the more unhappiness. The second term involves the candidate. There will be a penalty for placing the candidate on the SRP instead of the default item that would otherwise be placed, because the candidate has a lower score ( P ) then the default. Summarizing the terminology: Candidate : An item proposed by the agent for the next spot on the results page. If no agent proposes a candidate, the default item is the unplaced item of highest score. Deviance : How far the placed items deviate from the agent’s constraints. Larger means more deviation. Penalty : The penalty (in reduced score) for placing the candidate instead of the default. Larger means more of a penalty, i.e. more gap between the scores. Unhappiness : The agent’s unhappiness. If unhappiness>0 then the agent’s candidate will be a contender for the next placed item. Now back to question of prioritizing. It falls out automatically. If multiple agents are unhappy, we simply pick the one with the largest unhappiness score, and use its candidate as the next item to be placed. This approach isn’t hassle-free. You need to pick constraints (max/min) for each query. And you need to quantify deviance and penalty. Or in other words, find how to weight them, which is reminiscent of the parameter λ in MMR. But we prefer this because it is efficient, handles multiple constraints, and is not too sensitive to the exact values of max and min. For most queries we expect the constraints to hold with the default ranking, and so placing an agent’s candidate is the exception rather than the rule. The description in the previous section was abstract in that it talked about deviance and penalty without offering a way to compute them. Here is one way. A typical constraint requires that the fraction items with property P be at least f (this is a min constraint). P might be that an item is used, or has brand Sony, or is newly listed. The straightforward way to compute deviation from a constraint is to look at the number of items with P placed so far and compare it to the number needed to meet the constraint. Suppose there are n items placed so far, and that k of them are in P .  Since the constraint is min = f we’d like at least n items. If k < nf the constraint isn’t met so set the deviance to nf − k . Or more precisely deviance is ( nf − k ) + where x + = x when x > 0 and 0 otherwise. A very simple way to compute the penalty is to compare the score of the candidate item I C with the item that would otherwise be placed, I default .  The penalty is S ( I C ) − S ( I default ). Since items are sorted in score order, the penalty is always nonnegative. Unhappiness is computed using the formula unhappiness = deviance − λ × penalty.  Imagine  deviance > 0 but the only way to fix it is by placing an item with a huge penalty. The large − term makes unhappiness negative, so the agent isn’t unhappy after all. In other words, agents are altruistic and don’t insist on their constraint if it results in placing a poor item. The parameter controls the tradeoff between score and diversity. As a preliminary formula, unhappiness is But there’s one issue remaining. Suppose the constraint asks for at least 10%, and suppose the first item placed does not have the desired property. Then n = 1, f = 0.1, and k = 0 so ( nf − k ) + =(0.1) + =0.1 and the constraint is already unhappy and will be in the running to have its proposed items placed next. These seems like jumping the gun, since the goal is 10% but only one items has been placed. There are at least two ways to account for this. The first would be rounding: replace ( nf − k ) + with round( nf − k ) + .   But I think of constraints as something only triggered in exceptional circumstances, and so prefer to have unhappiness only go negative at the last possible minute. In other words, if it is still possible to satisfy nf ≤ k on the next round, then don’t trigger the constraint. Writing this as an equation, suppose I pass on this constraint and the next item turns out to not have P . Then k stays the same and n becomes n + 1. But on the next round I pick an item with P . Then k becomes k + 1 and n becomes n + 2. So I can satisfy the constraint in the future if ( n + 2) f ≤ k + 1. So replace ( nf− k ) + with (( n +2) f − ( k +1)) + . The formula becomes There are two types of constraints based on a property P . The first is a min constraint, which asks that the fraction of items with P be greater than f min . The second is max constraint, asking for the fraction to be less than f max . Most properties are something very specific, like item=used. But there is also an any property. This would be used (for example) to keep any one seller from dominating the results. A max constraint with property seller, and using any asks that there be no seller id with more than of the results. The any property doesn’t make sense with the min constraint. The any property is also a nice way to do duplicate (or fuzzy) dup removal. We assign a hash to each item, so that duplicate (or similar) items have the same hash. Then using a max with f = 1/50 means (roughly) that each item will appear only once on each 50-item page. If λ = 0 the requirement is absolute (unless there are competing agents). But by setting λ to a small number, we can allow dups if the alternate is to show very low quality items. I close with a more formal description of the agent-based diversity algorithm. It can be implemented quite efficiently. In MMR (as well as Welch and Agrawal), at each step you must scan all unplaced items (think O ( n 2 )). In our algorithm, you scan each item once for the entire algorithm (although this is done separately for each constraint). Scanning is implemented by having a pointer per constraint. The pointer moves down the unplaced list as the algorithm proceeds. Powered by QuickLaTeX", "date": "2014-11-26"},
{"website": "Ebay-Engineering", "title": "NoSQL Data Modeling", "author": ["Donovan Hsieh"], "link": "https://tech.ebayinc.com/engineering/nosql-data-modeling/", "abstract": "Data modeling for RDBMS has been a well-defined discipline for many years. Techniques like logical to physical mapping and normalization / de-normalization have been widely practiced by professionals, including novice users. However, with the recent emergence of NoSQL databases, data modeling is facing new challenges to its relevance. Generally speaking, NoSQL practitioners focus on physical data model design rather than the traditional conceptual / logical data model process for the following reasons: Developer-centric mindset – With flexible schema (or schema-free) support in NoSQL databases, application developers typically assume data model design responsibility. They have been ingrained with the notion that the database schema is an integral part of application logic. High-performance queries running in massive scale-out distributed environments – Contrary to traditional, centralized scale-up systems (including the RDBMS tier), modern applications run in distributed, scale-out environments. To accomplish scale-out, application developers are driven to tackle scalability and performance first through focused physical data model design, thus abandoning the traditional conceptual, logical, and physical data model design process. Big and unstructured data – With its rigidly fixed schema and limited scale-out capability, the traditional RDBMS has long been criticized for its lack of support for big and unstructured data. By comparison, NoSQL databases were conceived from the beginning with the capability to store big and unstructured data using flexible schemas running in distributed scale-out environments. In this blog post, we explore other important mindset changes in NoSQL data modeling: development agility through flexible schemas vs. database manageability; the business and data model design process; the role of RDBMS in NoSQL data modeling; NoSQL variations that affect data modeling; and visualization approaches for NoSQL logical and physical data modeling. We end the post with a peak into the NoSQL data modeling future. One highly touted feature in today’s NoSQL is application development agility. Part of this agility is accomplished through flexible schemas, where developers have full control over how data is stored and organized in their NoSQL databases. Developers can create or modify database objects in application code on the fly without relying on DBA execution. The result is, indeed, increased application development and deployment agility. However, the flexible schema is not without its challenges. For example, dynamically created database objects can cause unforeseen database management issues due to the lack of DBA oversight. Furthermore, unsupervised schema changes increase DBA challenges in diagnosing associated issues. Frequently, such troubleshooting requires the DBA to review application code written in programming languages (e.g., Java) rather than in RDBMS DDL (Data Definition Language) – a skill that most DBAs do not possess. In old-school software engineering practice, sound business and (relational) data model designs are key to successful medium- to large-scale software projects. As NoSQL developers assume business / data model design ownership, another dilemma arises: data modeling tools. For example, traditional RDBMS logical and physical data models are governed and published by dedicated professionals using commercial tools, such as PowerDesigner or ER/Studio. Given the nascent state of NoSQL technology, there isn’t a professional-quality data modeling tool for such tasks. It is not uncommon for stakeholders to review application source code in order to uncover data model information. This is a tall order for non-technical users such as business owners or product managers. Other approaches, like sampling actual data from production databases, can be equally laborious and tedious. It is obvious that extensive investment in automation and tooling is required. To help alleviate this challenge, we recommend that NoSQL projects use the business and data model design process shown in the following diagram (illustrated with MongoDB’s document-centric model): Figure 1 Business Requirements & Domain Model: At the high level, one can continue using database-agnostic methodologies, such as domain-driven design , to capture and define business requirements Query Patterns & Application Object Model: After preliminary business requirements and the domain model are produced, one can work iteratively and in parallel to analyze top user access patterns and the application model, using UML class or object diagrams. With RDMS, applications can implement database access using either a declarative query (i.e., using a single SQL table join) or a navigational approach (i.e., walking individual tables embedded in application logic). The latter approach typically requires an object-relational mapping (ORM) layer to facilitate tedious plumbing work. By nature, almost all NoSQL databases belong to the latter category. MongoDB can support both approaches through the JSON Document model, SQL-subset query , and comprehensive secondary indexing capabilities. JSON Document Model & MongoDB Collection / Document: This part is where native physical data modeling takes place. One has to understand the specific NoSQL product’s strengths and weaknesses in order to produce efficient schema designs and serve effective, high-performance queries. For example, modeling social network entities like followed and followers is very different from modeling online blogging applications. As such, social networking applications are best implemented using Graph NoSQL databases like Neo4j, while online blogging applications can be implemented using other flavors of NoSQL like MongoDB. Interestingly enough, old-school RDBMS data modeling techniques still play a meaningful role for those who are new to NoSQL technology. Using document-centric MongoDB as an example, the following diagram illustrates how one can map a relational data model to a comparable MongoDB document-centric data model: Figure 2 In the relational world, logical data models are reasonably portable among different RDBMS products. In a physical data model, design specifications such as storage clauses or non-standard SQL extensions might vary from vendor to vendor. Various SQL standards, such as SQL-92 and the latest SQL:2008 as defined by industry bodies like ANSI/ISO, can help application portability across different database platforms. However, in the NoSQL world, physical data models vary dramatically among different NoSQL databases; there is no industry standard comparable to SQL-92 for RDBMS. Therefore, it helps to understand key differences in the various NoSQL database models: Key-value stores – Collections comprised of unique keys having 1- n valid values Column families – Distributed data stores in which a column consists of a unique key, values for the key, and a timestamp differentiating current from stale values Document databases – Systems that store and manage documents and their metadata (type, title, author, creation/modification/deletion date, etc.) Graph databases – Systems that use graph theory to represent and store data as nodes (people, business, accounts, or other entities), node properties, and edges (lines connecting nodes/properties to each other) The following diagram illustrates the comparison landscape based on model complexity and scalability: Figure 3 It is worth mentioning that for NoSQL data models, a natural evolutionary path exists from simple key-value stores to the highly complicated graph databases, as shown in the following diagram: Figure 4 For conceptual data models, diagramming techniques such as the Entity Relationship Diagram can continue to be used to model NoSQL applications. However, logical and physical NoSQL data modeling requires new thinking, due to each NoSQL product assuming a different native structure. One can intuitively use any of the following three visualization approaches, using a document-centric data model like MongoDB as an example: Native visual representation of MongoDB collections with support for nested sub-documents (see Figure 2 above) Pros – It naturally conveys a complex document model through an intuitive visual representation. Cons – Without specialized tools support, visualization results in ad-hoc drawing using non-uniform conventions or notations. Reverse engineering selected sample documents using JSON Designer (see Figure 5 below) Pros – It can easily reverse engineer a hierarchical model into a visual representation from existing JSON documents stored in NoSQL databases like MongoDB. Cons – As of this writing, JSON Designer is available only on iPhone / iPad. Furthermore, it does not include native DB objects, such as MongoDB indexes. Figure 5 Traditional RDBMS data modeling tools like PowerDesigner (see Figure 6 below) Pros – Commercial tools support is available. Cons – it requires tedious manual preparation and diagram arrangement to represent complex and deeply nested document structure. Figure 6 In a future post, we’ll cover specific data model visualization techniques for other NoSQL products such as Cassandra, which is based on the Column Family structure. Like any emerging technology, NoSQL will mature as it becomes mainstream. We envision the following new data modeling opportunities for NoSQL: Reusable data model design patterns (some product-specific and some agnostic) to help reduce application development effort and cost Unified NoSQL model repository to support different NoSQL products Bi-directional, round-trip engineering support for (data) model-driven design processes and tools Automated data model extraction from application source code Automated code-model-data consistency validation and consistency conformance metrics Strong control for application / data model change management, with proactive tracking and reconciliation between application code, embedded data models, and the actual data in the NoSQL databases", "date": "2014-10-10"},
{"website": "Ebay-Engineering", "title": "Announcing Kylin: Extreme OLAP Engine for Big Data", "author": ["Luke Han"], "link": "https://tech.ebayinc.com/engineering/announcing-kylin-extreme-olap-engine-for-big-data/", "abstract": "We are very excited to announce that eBay has released to the open-source community our distributed analytics engine: Kylin ( http://kylin.io ). Designed to accelerate analytics on Hadoop and allow the use of SQL-compatible tools, Kylin provides a SQL interface and multi-dimensional analysis (OLAP) on Hadoop to support extremely large datasets. Kylin is currently used in production by various business units at eBay. In addition to open-sourcing Kylin, we are proposing Kylin as an Apache Incubator project. The challenge faced at eBay is that our data volume has become bigger while our user base has become more diverse. Our users – for example, in analytics and business units – consistently ask for minimal latency but want to continue using their favorite tools, such as Tableau and Excel. So, we worked closely with our internal analytics community and outlined requirements for a successful product at eBay: Sub-second query latency on billions of rows ANSI-standard SQL availability for those using SQL-compatible tools Full OLAP capability to offer advanced functionality Support for high cardinality and very large dimensions High concurrency for thousands of users Distributed and scale-out architecture for analysis in the TB to PB size range We quickly realized nothing met our exact requirements externally – especially in the open-source Hadoop community. To meet our emergent business needs, we decided to build a platform from scratch. With an excellent team and several pilot customers, we have been able to bring the Kylin platform into production as well as open-source it. Kylin is a platform offering the following features for big data analytics: Extremely fast OLAP engine at scale : Kylin is designed to reduce query latency on Hadoop for 10+ billion rows of data. ANSI SQL on Hadoop: Kylin supports most ANSI SQL query functions in its ANSI SQL on Hadoop interface. Interactive query capability: Users can interact with Hadoop data via Kylin at sub-second latency – better than Hive queries for the same dataset. MOLAP cube query serving on billions of rows: Users can define a data model and pre-build in Kylin with more than 10+ billions of raw data records. Seamless integration with BI Tools: Kylin currently offers integration with business intelligence tools such as Tableau and third-party applications. Open-source ODBC driver: Kylin’s ODBC driver is built from scratch and works very well with Tableau. We have open-sourced the driver to the community as well. Other highlights: Job management and monitoring Compression and encoding to reduce storage Incremental refresh of cubes Leveraging of the HBase coprocessor for query latency Approximate query capability for distinct counts (HyperLogLog) Easy-to-use Web interface to manage, build, monitor, and query cubes Security capability to set ACL at the cube/project level Support for LDAP integration The idea of Kylin is not brand new. Many technologies over the past 30 years have used the same theory to accelerate analytics. These technologies include methods to store pre-calculated results to serve analysis queries, generate each level’s cuboids with all possible combinations of dimensions, and calculate all metrics at different levels. For reference, here is the cuboid topology: When data becomes bigger, the pre-calculation processing becomes impossible – even with powerful hardware. However, with the benefit of Hadoop’s distributed computing power, calculation jobs can leverage hundreds of thousands of nodes. This allows Kylin to perform these calculations in parallel and merge the final result, thereby significantly reducing the processing time. As an example, suppose there are several records stored in Hive tables that represent a relational structure. When the data volume grows very large – 10+ or even 100+ billions of rows – a question like “how many units were sold in the technology category in 2010 on the US site?” will produce a query with a large table scan and a long delay to get the answer. Since the values are fixed every time the query is run, it makes sense to calculate and store those values for further usage. This technique is called Relational to Key-Value (K-V) processing. The process will generate all of the dimension combinations and measured values shown in the example below, at the right side of the diagram. The middle columns of the diagram, from left to right, show how data is calculated by leveraging MapReduce for the large-volume data processing. Kylin is based on this theory and is leveraging the Hadoop ecosystem to do the job for huge volumes of data: Read data from Hive (which is stored on HDFS) Run MapReduce jobs to pre-calculate Store cube data in HBase Leverage Zookeeper for job coordination The following diagram shows the high-level architecture of Kylin. This diagram illustrates how relational data becomes key-value data through the Cube Build Engine offline process. The yellow lines also illustrate the online analysis data flow. The data requests can originate from SQL submitted using a SQL-based tool, or even using third-party applications via Kylin’s RESTful services. The RESTful services call the Query Engine, which determines if the target dataset exists. If so, the engine directly accesses the target data and returns the result with sub-second latency. Otherwise, the engine is designed to route non-matching dataset queries to SQL on Hadoop, enabled on a Hadoop cluster such as Hive. Following are descriptions of all of the components the Kylin platform includes. Metadata Manager: Kylin is a metadata-driven application. The Metadata Manager is the key component that manages all metadata stored in Kylin, including the most important cube metadata. All other components rely on the Metadata Manager. Job Engine: This engine is designed to handle all of the offline jobs including shell script, Java API, and MapReduce jobs. The Job Engine manages and coordinates all of the jobs in Kylin to make sure each job executes and handles failures. Storage Engine: This engine manages the underlying storage – specifically the cuboids, which are stored as key-value pairs. The Storage Engine uses HBase – the best solution from the Hadoop ecosystem for leveraging an existing K-V system. Kylin can also be extended to support other K-V systems, such as Redis . REST Server: The REST Server is an entry point for applications to develop against Kylin. Applications can submit queries, get results, trigger cube build jobs, get metadata, get user privileges, and so on. ODBC Driver: To support third-party tools and applications – such as Tableau – we have built and open-sourced an ODBC Driver. The goal is to make it easy for users to onboard. Query Engine: Once the cube is ready, the Query Engine can receive and parse user queries. It then interacts with other components to return the results to the user. In Kylin, we are leveraging an open-source dynamic data management framework called Apache Calcite to parse SQL and plug in our code. The Calcite architecture is illustrated below. (Calcite was previously called Optiq, which was written by Julian Hyde and is now an Apache Incubator project.) At the time of open-sourcing Kylin, we already had several eBay business units using it in production. Our largest use case is the analysis of 12+ billion source records generating 14+ TB cubes. Its 90% query latency is less than 5 seconds. Now, our use cases target analysts and business users, who can access analytics and get results through the Tableau dashboard very easily – no more Hive query, shell command, and so on. Support TopN on high-cardinality dimension: The current MOLAP technology is not perfect when it comes to querying on a high-cardinality dimension – such as TopN on millions of distinct values in one column. Similar to search engines (as many researchers have pointed out), the inverted index is the reasonable mechanism to use to pre-build such results. Support Hybrid OLAP (HOLAP): MOLAP is great to serve queries on historical data, but as more and more data needs to be processed in real time, there is a growing requirement to combine real-time/near-real-time and historical results for business decisions. Many in-memory technologies already work on Relational OLAP (ROLAP) to offer such capability. Kylin’s next generation will be a Hybrid OLAP (HOLAP) to combine MOLAP and ROLAP together and offer a single entry point for front-end queries. Kylin has already been open-sourced to the community. To develop and grow an even stronger ecosystem around Kylin, we are currently working on proposing Kylin as an Apache Incubator project. With distinguished sponsors from the Hadoop developer community supporting Kylin, such as Owen O’Malley (Hortonworks co-founder and Apache member) and Julian Hyde (original author of Apache Calcite, also with Hortonworks), we believe that the greater open-source community can take Kylin to the next level. We welcome everyone to contribute to Kylin. Please visit the Kylin web site for more details: http://kylin.io . To begin with, we are looking for open-source contributions not only in the core code base, but also in the following areas: Shell Client RPC Server Job Scheduler Tools For more details and to discuss these topics further, please follow us on twitter @KylinOLAP and join our Google group: https://groups.google.com/forum/#!forum/kylin-olap Kylin has been deployed in production at eBay and is processing extremely large datasets. The platform has demonstrated great performance benefits and has proved to be a better way for analysts to leverage data on Hadoop with a more convenient approach using their favorite tool. We are pleased to open-source Kylin. We welcome feedback and suggestions, and we look forward to the involvement of the open-source community. Editor's Note: The eBay Inc. Kylin Team contributed to this article.", "date": "2014-10-20"},
{"website": "Ebay-Engineering", "title": "Don't Build Pages, Build Modules", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/dont-build-pages-build-modules/", "abstract": "We are in an interesting phase of rethinking frontend engineering across eBay Marketplaces, and this blog summarizes where we are heading. Modular programming is a fundamental design technique that has been practiced since the dawn of software engineering. It is still the most recommended pattern for building maintainable software, and the Node.js community fully embraces this design philosophy. Most Node.js modules are created using smaller modules as building blocks to achieve the final goal. Now with Web Components gaining momentum, we decided to change our approach toward building frontend web applications. Modularity was already in our frontend codebase, but only in the scope of a particular language. Most of our shared JavaScript (overlays, tabs, carousel, etc.) and CSS (buttons, grid, forms, icons, etc.) were written in a modular fashion. This is great, but when it comes to a page or a view, the thinking was still about building pages, as opposed to building UI modules. With this mindset, we found that as the complexity of pages grew, it became exponentially more difficult to maintain them. What we wanted was a simple way to divide a page into small and manageable pieces, and to develop each piece independently. This is when we came up with the notion, “Don’t build pages, build modules.” In general, everyone understands and agrees with the concept of frontend modules. But to make the concept a reality, we needed to deviate from our current style of web development. Decomposition: First, we wanted to move away from the idea of directly building a page. Instead, when a requirement comes in the form of a page, we decompose it into logical UI modules. We do so recursively until a module becomes FIRST . This means a page is made up of a few top-level modules, which in turn are built from sub-modules, very similar to how JavaScript modules are built in the Node.js world. There are common styles and JavaScript (e.g., jQuery) that all modules depend on. These files together become a module of their own (e.g., a base module) and are added as dependencies of other modules. Engineers start working independently on these modules, and a page is nothing more than a grid that finally assembles them. DOM encapsulation: We wanted all of our frontend modules to be associated with a DOM node and for that node to be the module’s root element. So we place all client-side JavaScript behavior like event binding, querying the DOM, jQuery plugin triggers, etc. within the scope of the module’s root element. This gives perfect encapsulation to our modules by making them restrictive and truly independent. Obviously we needed some sort of JavaScript abstraction to achieve this encapsulation, and we decided to go with the lightweight widgets functionality (named Marko Widgets ) offered by RaptorJS . Marko widgets are a small module (~4 KB) providing a simple mechanism for instantiating widgets and binding them to DOM elements. Instantiated widgets are made into observables, and can also be destroyed when they need to be removed from the DOM. To bind a JavaScript module to a DOM node, we simply used the w-bind directive as shown below: When rendered, the Marko widgets module tracks which modules are associated with which DOM nodes, and automatically binds the behavior after adding the HTML to the DOM. Some of our modules, like our tracking module, had JavaScript functionality, but no DOM node association. In those scenarios, we use the <noscript> tag to achieve encapsulation. With respect to CSS, we name-space all class names within a module with the root element’s class name, separated with ‘-‘ such as gallery-title, gallery-thumbnail, etc. Packaging: The next big question was how do we package the modules? Frontend package management has always been challenging, and is a hotly debated topic. Packaging for the same asset type is quite straightforward. For instance, in JavaScript once we nail down a module pattern ( CommonJS , AMD , etc.), then packaging becomes easy with tools like browserify . The problem is when we need to bundle other asset types like CSS and markup templates. Here, our in-house Raptor Optimizer came to the rescue. The optimizer is a JavaScript module bundler very similar to browserify or webpack , but with a few differences that make it ideal for our module ecosystem. All it needs is an optimizer.json file in the module directory, to list out the CSS and markup template ( dust or marko ) dependencies. For JavaScript dependencies, the optimizer scans the source code in the current directory, and resolves them recursively. Finally, an ordered, de-duped bundle is inserted in the CSS and JavaScript slot of the page – for example: Note that the markup templates will be included only when rendering on the client side. Including them otherwise will unnecessarily increase JavaScript file size. Going modular also meant changing the way files were structured. Before applying modularity to the frontend codebase, teams would typically create separate top-level directories for JavaScript, CSS, images, fonts, etc. But with the new approach it made sense to group all files associated with a module under the same directory, and to use the module name as the directory name. This practice raised some concerns initially, mainly around violating a proven file structuring scheme, and around tooling changes related to bundling and CDN pushing. But engineers quickly came to an agreement, as the advantages clearly outweighed the disadvantages. The biggest benefit is that the new structure truly promoted module-level encapsulation:  all module-associated files live together and can be packaged.  In addition, any action on a module (deleting, renaming, refactoring, etc., which happen frequently in large codebases) becomes super easy. We wanted all of our modules to follow the Law of Demeter – meaning two independent modules cannot directly talk to each other. The obvious solution was to use an event bus for communication between client-side modules. We evaluated various eventing mechanisms, with the goal of having it centralized and also not introducing a large library dependency. Surprisingly, we settled on the eventing model that comes with jQuery itself. jQuery’s trigger , on , and off APIs do a fantastic job of abstracting out all eventing complexities, for both DOM and custom events. We wrote a small dispatcher wrapper, which handles interactions between modules by triggering and listening to events on the document element: Modules can now use the $.dispatcher to trigger and listen to custom events without having any knowledge about other modules. Another advantage of using the jQuery DOM-based eventing model is that we get all event dynamics (propagation and name-spacing) for free. Some teams prefer to create a centralized mediator module to handle the communication. We leave that to engineers’ discretion. One of the biggest advantages of frontend modules is they perfectly fit in the multiscreen world. Flows change based on device dimensions, and making a page work either responsively or adaptively on all devices is not practical. But with modules, things fall in place. When engineers finalize the modules in a view, they also evaluate how they look and behave across various screen sizes. Based on this evaluation, the module name and associated view model JSON schema are agreed upon. But the implementation of the module is based upon the device. For some modules, just a responsive implementation is sufficient to work across all screens. For others, the design and interactions (touch or no-touch) would be completely different, thus requiring different implementations. However different the implementations may be, the module name and the view model powering it would be the same. We indeed extended this concept to the native world, where iOS and Android apps also needed the same modules and view models. But the implementation is still native (Objective-C or Java) to the platform. All clients talk to the frontend servers, which are cognizant of the modules that a particular user agent needs and respond with the appropriate view model chunks. This approach gave us a perfect balance in terms of consistency and good user experience (by not compromising on the implementation). Companies like LinkedIn have already implemented a view-based JSON model that has proved successful. The granularity of the view model is decided by engineers and product managers together, depending on how much control they need over the module. The general guideline is to make the view model’s JSON as smart as possible and the modules dumb (or thin), thus providing a central place to control all clients. All of the other benefits of modular programming come for free: Developer productivity – engineers can work in parallel on small contained pieces, resulting in a faster pace. Unit testing – it has never been easier. Debugging – it’s easy to nail down the problem, and even if one module is faulty others are still intact. Finally, this whole approach takes us closer to the way the web is evolving. Our idea of bundling HTML, CSS, and JS to create an encapsulated UI module puts us on a fast track to adoption. We envision an ideal future where all of our views, across devices, are a bunch of web components. As mentioned earlier, we are indeed in the process of rethinking frontend engineering at eBay, and modularization is one of the first steps resulting from that rethinking. Thanks to my colleagues Mahdi Pedramrazi and Patrick Steele-Idem for teaming up and pioneering this effort across the organization. – Senthil Frontend Engineer", "date": "2014-10-02"},
{"website": "Ebay-Engineering", "title": "Async Fragments: Rediscovering Progressive HTML Rendering with Marko", "author": ["Patrick Steele-Idem"], "link": "https://tech.ebayinc.com/engineering/async-fragments-rediscovering-progressive-html-rendering-with-marko/", "abstract": "At eBay, we take site speed very seriously and are always looking for ways to allow developers to create faster-loading web apps. This involves fully understanding and controlling how web pages are delivered to web browsers. Progressive HTML rendering is a relatively old technique that can be used to improve the performance of websites, but it has been lost in a whole new class of web applications. The idea is simple: give the web browser a head start in downloading and rendering the page by flushing out early and multiple times. Browsers have always had the helpful feature of parsing and responding to the HTML as it is being streamed down from the server (even before the response is ended). This feature allows the HTML and external resources to be downloaded earlier, and for parts of the page to be rendered earlier. As a result, both the actual load time and the perceived load time improve. In this blog post, we will take an in-depth look at a technique we call “Async Fragments” that takes advantage of progressive HTML rendering to improve site speed in ways that do not drastically complicate how web applications are built. For concrete examples we will be using Node.js , Express.js and the Marko templating engine (a JavaScript templating engine that supports streaming, flushing, and asynchronous rendering). Even if you are not using these technologies, this post can give you insight into how your stack of choice could be further optimized. To see the techniques discussed in this post in action, please take a look at the accompanying sample application . Progressive HTML rendering is discussed in the post The Lost Art of Progressive HTML Rendering by Jeff Atwood, which was published back in 2005. In addition, the “Flush the Buffer Early” rule is described by the Yahoo! Performance team in their Best Practices for Speeding Up Your Web Site guide. Stoyan Stefanov provides an in-depth look at progressive HTML rendering in his Progressive rendering via multiple flushes post. Facebook discussed how they use a technique they call “BigPipe” to improve page load times and perceived performance by dividing up a page into “pagelets.” Those articles and techniques inspired many of the ideas discussed in this post. In the Node.js world, its most popular web framework, Express.js , unfortunately recommends a view rendering engine that does not allow streaming and thus prevents progressive HTML rendering. In a recent post, Bypassing Express View Rendering for Speed and Modularity , I described how streaming can be achieved with Express.js; this post is largely a follow-up to discuss how progressive HTML rendering can be achieved with Node.js (with or without Express.js). A page that does not utilize progressive HTML rendering will have a slower load time because the bytes will not be flushed out until the complete HTML response is built. In addition, after the client finally receives the complete HTML it will then see that it needs to download additional external resources (such as CSS, JavaScript, fonts, and images), and downloading these external resources will require additional round trips. In addition, pages that do not utilize progressive HTML rendering will also have a slower perceived load time, since the screen will not update until the complete HTML is downloaded and the CSS and fonts referenced in the <head> section are downloaded. Without progressive HTML rendering, a server/client waterfall chart might be similar to the following: The corresponding page controller might look something like this: As you can see in the above code, the page HTML is not rendered until all of the data is asynchronously loaded. Because the HTML is not flushed until all back-end services are completed, the user will be staring at a blank screen for a large portion of the time. This will result in a sub-par user experience (especially with a poor network connection or with slow back-end services). We can do much better if we flush part of the HTML earlier. A simple trick to improve the responsiveness of a website is to flush the head section immediately. The head section will typically include the links to the external CSS resources (i.e. the <link> tags), as well as the page header and navigation. With this approach the external CSS will be downloaded sooner and the initial page will be painted much sooner as shown in the following waterfall chart: As you can see in the chart above, flushing the head early reduces the time to render the initial page. This technique improves the responsiveness of the page, but it does not significantly reduce the total time it takes to make the page fully functional. With this approach, the server is still waiting for all back-end services to complete before flushing the final HTML. In addition, downloading of external JavaScript resources will be delayed since <script> tags are placed at the end of the page (assuming you are following best practices) and don’t get sent out until the second and final flush. Instead of flushing only the head early, it is often beneficial to flush multiple times before ending the response. Typically, a page can be divided into multiple fragments where some of the fragments may depend on data asynchronously loaded from various back-end services while others may not depend on any asynchronously loaded data. The fragments that depend on asynchronously loaded data should be rendered asynchronously and flushed as soon as possible. For now, we will assume that these fragments need to be flushed in the proper HTML order (versus the order that the data asynchronously loads), but we will also show how out-of-order flushing can be used to further improve both page load times and perceived performance. When using “in-order” flushing, fragments that complete out of order will need to be buffered until they are ready to be flushed in the proper order. As an example, let’s assume we have divided a complex page into the following fragments: Each fragment is assigned a number based on the order that it appears in the HTML document. In code, our output HTML for the page might look like the following: The Marko templating engine provides a way to declaratively bind template fragments to asynchronous data provider functions (or Promises ). An asynchronous fragment is rendered when the asynchronous data provider function invokes the provided callback with the data. If the asynchronous fragment is ready to be flushed, then it is immediately flushed to the output stream. Otherwise, if the asynchronous fragment completed out of order then the rendered HTML is buffered in memory until it is ready to be flushed. The Marko templating engine ensures that fragments are flushed in the proper order. Continuing with the previous example, our HTML page template with asynchronous fragments defined will be similar to the following: The data provider functions should be passed to the template as part of the view model as shown in the following code for a sample page controller: In this particular example, the “search results” async fragment appears first in the HTML template, and it happens to take the longest time to complete. As a result, all of the subsequent fragments will need to be buffered on the server. The resulting waterfall with in-order flushing of async fragments is shown below: While the performance of this approach might be fine, we can enable out-of-order flushing for further performance gains as described in the next section. Marko achieves out-of-order flushing of async fragments by doing the following: Instead of waiting for an async fragment to finish, a placeholder HTML element with an assigned id is written to the output stream. Out-of-order async fragments are rendered before the ending <body> tag in the order that they complete. Each out-of-order async fragment is rendered into a hidden <div> element. Immediately after the out-of-order fragment, a <script> block is rendered to replace the placeholder DOM node with the DOM nodes of the corresponding out-of-order fragment. When all of the out-of-order async fragments complete, the remaining HTML (e.g. </body></html> ) will be flushed and the response ended. To clarify, here is what the output HTML might look like for a page with out-of-order flushing enabled: One caveat with out-of-order flushing is that it requires JavaScript running on the client to move each out-of-order fragment into its proper place in the DOM. Thus, you would only want to enable out-of-order flushing if you know that the client’s web browser has JavaScript enabled. Also, moving DOM nodes may cause the page to be reflowed, which can be visually jarring to the user and result in more client-side CPU usage. If reflow is an issue then there are tricks that can be used to avoid a reflow (e.g., reserving space as part of the initial wireframe). Marko also allows alternative content to be shown while waiting for an out-of-order async fragment. To enable out-of-order flushing with Marko, the client-reorder=\"true\" attribute must be added to each <async-fragment> tag, and the <async-fragments> tag must be added to the end of the page to serve as the container for rendered out-of-order fragments. Here is the updated <async-fragment> tag for the search results fragment: The updated HTML page template with the new <async-fragments> tag is shown below: In combination with out-of-order flushing, it may be beneficial to move <script> tags that link to external resources to the end of the first chunk (before all of the out-of-order chunks). While the server is busy preparing the rest of the page, the client can start downloading the external JavaScript required to make the page functional. As a result, the user will be able to start interacting with the page sooner. Our final waterfall with out-of-order flushing will now be similar to the following: The final waterfall shows that the strategy of out-of-order flushing of asynchronous fragments can significantly improve the load time and perceived load time of a page. The user will be met with a progressive loading of a page that is ready to be interacted with sooner. To allow HTML to be served in parts, chunked transfer encoding should be used for the HTTP response. Chunked transfer encoding uses delimiters to break up the response, and each flush results in a new chunk. If gzip compression is enabled (and it should be) then flushing the pending data to the gzip stream will result in a gzip data frame being written to the response as part of each chunk. Flushing too often will negatively impact the effectiveness of the compression algorithm, but without flushing periodically then progressive HTML rendering will not be available. By default, Marko will flush at the beginning of an <async-fragment> block (in order to send everything that has already completed), as well as when an async fragment completes. This default strategy results in efficient progressive loading of an HTML page as long as there are not too many async fragments. For improved usability and responsiveness, there should not be a long delay between rendered HTML being displayed to the user in the web browser and behavior being attached to the associated DOM. At eBay, we use the marko-widgets module to bind behavior to DOM nodes. Marko Widgets supports binding behavior to rendered widgets immediately after each independent async fragment, as illustrated in the accompanying sample app . For immediate binding to work, the required JavaScript code must be included earlier in the page. For more details, please see the marko-widgets module documentation. It is important to note that as soon as a byte is flushed for the HTTP body, then the response is committed; no additional HTTP headers can be sent (e.g., no server-side redirects or cookie-setting), and the HTML that has been sent cannot be “unsent”. Therefore, if an asynchronous data provider errors or times out, then the app must be prepared to show alternative content for that particular async fragment. Please see the documentation for the marko-async module for additional information on how to show alternative content in case of an error. The Async Fragments technique allows web developers to maximize the benefits of progressive HTML rendering to produce web pages that have improved actual and perceived load times. Developers at eBay have found the concept of binding HTML template fragments to asynchronous data providers easy to grasp and utilize. In addition, the flexibility to support both in-order and out-of-order flushing of async fragments makes this technique applicable for all web browsers and user agents. The Marko templating engine is being used as part of eBay’s latest Node.js stack to improve performance while also simplifying how pages are constructed on both the server and the client. Marko is one of a few templating engines for Node.js and web browsers that support streaming, flushing, and asynchronous rendering. Marko has a simple HTML-based syntax, and the Marko compiler produces small and efficient JavaScript modules as output. We encourage you to try Marko online and in your next Node.js project. Because Marko is a key component of eBay’s internal Node.js stack, and given that it is heavily documented and tested, you can be confident that it will be well supported. Patrick Steele-Idem is a member of eBay’s platform team who enjoys writing open-source software and improving how web applications are built. He is the author of RaptorJS , a suite of open-source front-end power tools that are being used within and outside eBay. You can follow Patrick on Twitter at @psteeleidem .", "date": "2014-12-08"},
{"website": "Ebay-Engineering", "title": "The Power of Perceived Performance", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/the-power-of-perceived-performance/", "abstract": "Recent years have seen a huge influx of SPAs — Single Page Applications. Though they enhance user experience, implementing SPAs for large-scale web applications is indeed a complex task. At eBay, we faced a similar challenge when we wanted to migrate a couple of our key desktop flows (search and item pages) to an app-like experience, from the current state of full page refreshes. Some of the key challenges were Server and client sync: Solving this challenge is super critical for e-commerce applications. Both the browser and the server should maintain the state of the app. At any point in time the URL should be portable, meaning it should render the page markup the same way every time. From an e-commerce perspective, three main circumstances make this point critical:   SEO, browser refreshes (especially for items ending soon), and URL sharing. Code redundancy: This point follows from the previous one. In order for the app to be rendered on the server, all logic built in JavaScript for the browser should be replicated on the server. The result, however, is a maintenance nightmare, especially for large distributed teams. Although there are solutions like rendr and PhantomJS to simulate the browser environment on the server, they don’t work at scale. Performance penalty for the first hit: Most SPA frameworks out there require the initial rendering on page load to happen on the browser. This is an anti-performance pattern and has proven to be a bad way to go for many (see, for example, Twitter’s experience ). In addition, with initial rendering on the browser we lose the huge benefits of the preload scanners in modern browsers. This point, then, is a reiteration that server-side rendering is not just an add-on, but a must. Browser back/forward: This may come as a surprise to many, but from what we have observed, even the slightest deviation from the default back/forward actions of the browser has impacts on consumer behavior. Users are so accustomed to these actions (mainly in the desktop environment) that we need to make sure they work as expected. This is not much of an SPA challenge, but it is something to keep in mind. Considering the above facts and still wanting to build a seamless experience, we decided to go the PJAX route. PJAX (pushState + AJAX) is a technique that delivers a fast browsing experience, without the SPA overhead. It has worked well for biggies like Twitter and GitHub. When looking to implement a PJAX library, we learned about YouTube’s SPF — Structured Page Fragments— from the 2014 Velocity conference (yes, SPF not SPA; we know it’s confusing). A quick dive into SPF indicated it was pretty much what we wanted. Moreover, the contributors to SPF responded promptly to all our queries and enhancement requests, thus enabling us to get started quickly. So what does SPF offer? Application code remains intact : For SPF, we don’t have to change the way we build applications. Also, no specialized client treatment is required. The only change needed was to add a conditional hook (based on a particular request URL param) in our server response pipeline to respond with HTML in JSON format, instead of with standard HTML. This benefit is huge to us, as development teams can build and maintain applications while being agnostic about how client-side navigations might happen. Markup is on server : With SPF, markup is always rendered on the server. Along with the code maintenance benefit, this feature also removes the dependency on client hardware specifically for rendering. Although client machines are getting increasingly powerful, we still have a sizable set of our global users on the lower hardware spectrum whom we need to support. Our previous attempts at client-side rendering for these users were not fruitful. On a side note, we are very interested in trying out React for client-side rendering after initial load. Moving to SPF provided an opportunity for us to clean up JavaScript and CSS. We did two types of optimization. JavaScript events : Our pages did not have a standard way of handling events — some were handled at an individual element level and some used delegation. This situation was not ideal, as complex pages were sometimes sluggish because they had tons of events. Now with PJAX, we’ve brought in a standard:  to widgetize our UI modules and delegate events at the widget container level. This standard has made event handling more efficient. Furthermore, we needed to re-initialize only those widgets that were changed on page navigation. Resource bundling : Most pages were bundled in a way that there was only one JavaScript and one CSS URL per page. All library JS (jQuery, Raptor , tracking, utils, etc.) and CSS ( skin ) were combined with application JS and CSS, making them one URL each. While this was good for reducing HTTP requests, it also meant anti-caching. When a user navigates from one page to another, the entire CSS and JS have to be downloaded and executed; library files were the big chunk of this overhead, which was unnecessary. With SPF, this approach would fail right away, since it involves a single page context as well as executing the same library code (like jQuery), which would result in unintended behaviors. To fix this situation, we took on the task of creating two resource bundles for key pages — bundling all common JS and CSS shared across pages as one resource, and bundling the application JS and CSS per page as the second resource. This solution saves a lot of time in terms of resource parsing and execution, as only the small amount of application JS and CSS has to be processed on each navigation. Also, in SPF mode the resource processing happens only for the first navigation; for repeated views, the previously executed CSS and JS can be leveraged. Now back to why the title “The Power of Perceived Performance.” Moving to SPF measurably increased performance on each navigation. But we had a problem:  the performance gain was not visually perceptible. In an internal demo, the feedback was “yeah, it seems to be a little quicker, but nothing else is different.” We were scratching our heads about what was missing. Finally, it all came down to one thing — a progress indicator. Yes, we did not have progress indicators when users navigated pages in SPF mode. Transitions or progress indicators mask the slowness in applications. There has been much research around this phenomenon, and we actually experienced the humongous impact it has. Close observation of all major websites that use the PJAX technique reveals they use some sort of progress indicator. For instance, Twitter navigation uses a small throbber, replacing the bird icon in the static header. GitHub replaces the icon right next to a file or folder with a throbber. YouTube shows a red progress bar at the top to indicate the progress of a user’s action. When we were considering how to implement a transition for SPF-based navigation, a lot of fancy ideas came up. From internal testing, the feedback we received was clear :  more than the fancy stuff, customers just need an elegant transition. We ended up with a real-time progress bar similar to YouTube’s. With the progress indicator in place, we did another internal launch. This time, the feedback was unanimous: “Wow! The page looks fast.” It was surprising how a tiny progress indicator could change the perception of an entire application. The performance numbers with and without the progress indicators were the same. But just with that indicator, the application feels much faster. This is the real power of perceived performance. As a bonus, avoiding the re-parse and re-execution of large CSS and JavaScript on each navigation made our page interactions ready instantly. Currently the PJAX-based navigation is enabled within the item page and is in production A/B testing. Search is next, and soon other key flows will follow. The ultimate goal is ONE eBay desktop experience. Huge thanks to YouTube engineers Alex Nicksay , David Phillips, and Marcel Duran for their continuous support throughout the migration. Last but not least, thanks to my colleagues Karthik, Yaniv, Vidya, and Raghu for teaming up and making this is a reality.", "date": "2015-01-05"},
{"website": "Ebay-Engineering", "title": "A Case Study in Empirical Bayes", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/a-case-study-in-empirical-bayes/", "abstract": "Empirical Bayes is a statistical technique that is both powerful and easy to use.  My goal is to illustrate that statement via a case study using eBay data.  Quoting the famous statistician Brad Efron, Empirical Bayes seems like the wave of the future to me, but it seemed that way 25 years ago and the wave still hasn’t washed in, despite the fact that it is an area of enormous potential importance. Hopefully this post will be one small step in helping Empirical Bayes to wash in! The case study I’ll present comes from ranking the items that result from a search query. One feature that is useful for ranking items is their historical popularity. On eBay, some items are available in multiple quantities. For these, popularity can be measured by the number of times an item is sold divided by the number of times it is displayed, which I will call sales/impressions (S/I). By the way, everything I say applies to any ratio of counts, not just sales and impressions. The problem I want to discuss is what to do if the denominator is small. Suppose that items typically have 1 sale per 100 impressions. Now suppose that a particular item gets a sale just after being listed.  This is a typical item that has a long-term S/I of about 0.01, but by chance it got its sale early, say after the 3rd impression.  So S/I is 1/3, which is huge. It looks like an enormously popular item, until you realize that the denominator I is small: it has received only 3 impressions.  One solution is to pass the problem downstream, and give the ranker both S/I and I. Let the ranker figure out how much to discount S/I when I is small.  Passing the buck might make sense in some situations, but I will show that it’s not necessary, and that it’s possible to pass a meaningful value even when I is small. How to do that?  Informally, I want a default value of S/I, and I want to gradually move from that default to the actual S/I as I increases. Your first reaction is probably to do this by picking a number (say 100), and if I < 100 use the default, otherwise S/I. But once you start to wonder whether 100 is the right number, you might as well go all the way and do things in a principled way using probabilities. Jumping to the bottom line: the formula will be (S + α)/(I + γ). This clearly satisfies the desire to be near S/I when S and I are large. It also implies that the default value is α/γ, since that’s what you get when S=I=0.  In the rest of this post I will explain two things.  First, how to pick α and γ (there is a right way and a wrong way).  And second, where the shape of the formula (S + α)/(I +γ) comes from.  If you’re familiar with Laplace smoothing then you might think of using (S+1)/(I+1), and our formula is a generalization of that.  But it still begs the question — why a formula of this form, rather than, for example, a weighted sum . The formula (S + α)/(I +γ) comes by imagining that at each impression, there is a probability of an associated sale, and then returning the best estimate of that probability instead of returning S/I. I’ll start with the simplest way of implementing this idea (although it is too simple to work well). Suppose the probability of a sale has a fixed universal value , so that whenever a user is shown an item, there is a probability that the item is sold. This is a hypothetical model of how users behave, and it’s straightforward to test if it fits the data. Simply pick a set of items, each with an observed sale count and impression count. If the simple model is correct, then an item with impressions will receive sales according to the binomial formula: Here is the number of impressions and the number of sales. As mentioned earlier, this whole discussion also works for other meanings of and , such as is clicks and is impressions. To test the simple model, I can compare two sets of data. The first is the observed pairs . In other words, I retrieve historical info for each item, and record impressions and sales. I construct the second set by following the simple model: I take the actual number of impressions , and randomly generate the number of sales according to the formula above. Below is a histogram of the two data sets. Red is simulated (the model), and blue is actual. The match is terrible. Here is some more detail on the plot: Only items with a nonzero sale count are shown. In the simulation there are 21% items with S=0, but the actual data has 47%. So we need to go to a more sophisticated model. Instead of a fixed value of , imagine drawing from a probability distribution and plugging it into the inset equation, which is then used to get the random . As you can see in the plot below, the two histograms have a much more similar shape than the previous plot, and so this model does a better job of matching the actual data. Now it all boils down to finding the distribution for . Since , that means finding a probability distribution on the interval [0, 1]. The most common such distribution is the Beta distribution, which has two parameters, and . By assuming a Beta distribution, I reduce the problem to finding and (and yes, this α is the same one as in the formula (S + α)/(I +γ)). This I will do by finding the values of and that best explain the observed values of and . Being more precise, associated to each of historical items is a sale count and an impression count , with . I was perhaps a bit flip in suggesting the Beta distribution because it is commonly used. The real reason for selecting Beta is that it makes the computations presented in the Details section below much simpler. In the language of Bayesian statistics, the Beta distribution is conjugate to the binomial. At this point you can fall into a very tempting trap. Each is a number between 0 and 1, so all the values form a histogram on [0,1]. The possible values of follow the density function for the Beta distribution and so also form a histogram on [0,1]. Thus you might think you could simply pick the values of and that make the two histograms match as closely as possible. This is wrong, wrong, wrong. The values are from a discrete distribution and often take on the value 0. The values of come from a continuous distribution (Beta) and are never 0, or more precisely, the probability that is 0. The distributions of and of are incompatible. In my model, I’m given and I spit out by drawing from a Beta distribution. The Beta is invisible (latent) and indirectly defines the model. I’ll give a name to the output of the model: . Restating, fix an and make a random variable that produces value with the probability controlled indirectly by the Beta distribution. I need to match the observed (empirical) values of to X, not to Beta. This is the empirical Bayes part. I’ll give an algorithm that computes and later. But first let me close the loop, and explain how all this relates to (S + α)/(I + γ). Instead of reporting S/I, I will report the probability of a sale. Think of the probability as a random variable — call it . I will report the mean value of the random variable . How to compute that? I heard a story about a math department that was at the top of a tall building whose windows faced the concrete wall of an adjacent structure. Someone had spray-painted on the wall “don’t jump, integrate by parts.” If it had been a statistics department, it might have said “don’t jump, use Baye’s rule.” Baye’s rule implies a conditional probability. I want not the expected value of , but the expected value of conditional on impressions and sales. I can compute that from the conditional distribution . To compute this, flip the two sides of the | to get . This is , which is just the inset equation at the beginning of this post! Now you probably know that in Baye’s rule you can’t just flip the two sides, you also have to include the prior. The formula is really . And is what we decided to model using the Beta distribution with parameters and . These are all the ingredients for Empirical Bayes. I need , I evaluate it using Baye’s rule, the rule requires a prior, and I use empirical data to pick the prior. In empirical Bayes, I select the prior that best explains the empirical data. For us, the empirical data is the observed values of . When you do the calculations (below) using the Beta distribution as the prior, you get that the mean of P is (S + α)/(I + γ) where γ = α + β. How does this compare with the simplistic method of using S/I when I > δ, and η otherwise? The simplistic formula involves two constants δ and η just as the principled formula involves two constants α and γ. But the principled method comes with an algorithm for computing α and γ given below. The algorithm is a few lines of R code (using the optimx package). I’ll close by filling in the details. First I’ll explain how to compute and . I have empirical data on items. Associated with the -th item ( ) is a pair , where might be the number of sales and the number of impressions, but the same reasoning works for clicks instead of sales. A model for generating the is that for each impression there is a probability that the impression results in a sale. So given , the probability that is . Then I add in that the probability is itself random, drawn from a parametrized prior distribution with density function . I generate the in a series of independent steps. At step , I draw from , and then generate according to the binomial probability distribution on : Using this model, the probability of seeing given is computed by averaging over the different possible values of , giving I’d like to find the parameter that best explains the observed and I can do that by maximizing the probability of seeing all those . The probability seeing is , the probability of seeing the whole set is and the log of that probability is . This is a function of , and I want to find the value of that maximizes it. This log probability is conventionally called the log-likelihood . Since I’m assuming is a beta distribution, with , then becomes The calculation above uses the definition of the beta function and the formula for the beta integral If you don’t want to check my calculations, is just the beta-binomial distribution, and you can find its formula in many books and web pages. Restating, to find , is to maximize the log-likelihood , specifically And since the first term doesn’t involve or , you only need to maximize The method I used to maximize that expression was the optimx package in R. The final missing piece is why, when I replace S/I with the probability that an impression leads to a sale, the formula is . I have an item with an unknown probability of sale . All that I do know is that it got sales out of impressions. If is the random variable representing the sale probability of an item, and is a random variable representing the sale/impression of an item, I want , which I write as for short. Evaluate this using Baye’s rule, The term can be ignored. This is not deep, but can be confusing. In fact, any factor involving only and (like ) can be ignored. That’s because so if it follows that can be recovered from using In other words, I can simply ignore a and reconstruct it at the very end by making sure that . I know that For us, the prior is a beta distribution, . Some algebra then gives The symbol ignores constants involving only and . Since the rightmost term integrates to 1, the proportionality is an equality: For an item with I want to know the value of , but this formula gives the probability density for . To get a single value I take the mean, using the fact that the mean of is . So the estimate for is This is just (S + α)/(I + γ) with γ = α + β. There’s room for significant improvement. For each item on eBay, you have extra information like the price. The price has a big effect on S/I, and so you might account for that by dividing items into a small number of groups (perhaps low-price, medium-price and high-price), and computing , for each. There’s a better way, which I will discuss in a future post. Powered by QuickLaTeX", "date": "2015-02-06"},
{"website": "Ebay-Engineering", "title": "HDFS Storage Efficiency Using Tiered Storage", "author": ["Benoy Antony"], "link": "https://tech.ebayinc.com/engineering/hdfs-storage-efficiency-using-tiered-storage/", "abstract": "At eBay, we run Hadoop clusters comprised of thousands of nodes that are shared by thousands of users. We store hundreds of petabytes of data in our Hadoop clusters. In this post, we look at how to optimize big data storage based on frequency of data usage. This method helps reduce the cost in an effective manner. It is now common knowledge that commodity hardware can be grouped together to create a Hadoop cluster with big data storage and computing capability. Parts of the data are stored in each individual machine, and data processing logic is also run on the same machines. For example: A 1,000-node Hadoop cluster with storage capacity of 20 TB per node can store up to 20 petabytes (PB) of data. All these machines have sufficient computing power to fulfill Hadoop’s motto of “take compute to data.” Different types of datasets are usually stored in the clusters, which are shared by different teams running different types of workloads to crunch through the data. Each dataset is enhanced and enriched by daily and hourly feeds through the data pipelines. A common trait of datasets is heavy initial usage. During this period the datasets are considered HOT . Based on our analysis, we found there is a definite decline in usage with time, where the stored data is accessed a few times a week and ages to being WARM data. In the next 90 days, when data usage falls to a few times a month, it is defined as COLD data. So data can be considered HOT during its initial days, then it remains WARM in the first month. Jobs or applications use the data a few times during this period. The data’s usage goes down further; the data becomes COLD , and may be used only a handful of times in the next 90 days. Finally, when the data is very rarely used, at a frequency of once or twice per year, the “temperature” of the data is referred to as FROZEN . In general, a temperature can be associated with each dataset. In this case, temperature is inversely proportional to the age of the data. Other factors can affect the temperature of a particular dataset. You can also write algorithms to determine the temperature of datasets. HDFS supports tiered storage since Hadoop 2.3 . How does it work? Normally, a machine is added to the cluster, and local file system directories are specified to store the block replicas. The parameter used to specify the local storage directories is dfs.datanode.data.dir . Another tier, such as ARCHIVE, can be added using an enum called StorageType . To denote that a local directory belongs to the ARCHIVE tier, the directory is prefixed in the configuration with [ARCHIVE]. In theory, multiple tiers can exist, as defined by a Hadoop cluster administrator. For example: Let’s add 100 nodes that contain 200 TB of storage per node to an existing 1,000-node cluster having a total of 20 PB of storage. These new nodes have limited computing capability compared to the existing 1,000 nodes. Let’s prefix all the local data directories with ARCHIVE. These 100 nodes now form the ARCHIVE tier and can store 20 PB of data. The total capacity of the cluster is 40 PB, which is divided into two tiers – the DISK tier and the ARCHIVE tier. Each tier has 20 PB. For this example, we will store the heavily used HOT data in the DISK tier, which has nodes with better computing power. For WARM data, we will keep most of its replicas in the DISK tier. For data with a replication factor of 3, we will keep two replicas in the DISK tier and one replica in the ARCHIVE tier. If data is COLD, we will keep at least one replica of each block of the COLD data in the DISK tier. All the remaining replicas go to the ARCHIVE tier. When a dataset is deemed FROZEN, which means it is almost never used, it is not optimal to store it on a node that has lots of CPU power to run many tasks or containers. We will keep it on a node that has minimal computing power. Thus, all the replicas of all blocks of FROZEN data can move to the ARCHIVE tier. When data is first added to the cluster, it gets stored in the default tier, DISK. Based on the temperature of the data, one or more replicas are moved to the ARCHIVE tier. Mover is used for data movement from one storage tier to another tier. Mover works similarly to Balancer except that it moves block replicas across tiers. Mover accepts an HDFS path, a replica count, and destination tier information. Then it identifies the replicas to be moved based on the tier information, and schedules the moves between source and destination data nodes. Many improvements in Hadoop 2.6 further support tiered storage . You can attach a storage policy to a directory to denote it as HOT, WARM, COLD, or FROZEN. The storage policy defines the number of replicas to be located on each tier. It is possible to change the storage policy on a directory and then invoke Mover on that directory to make the policy effective. Based on the data temperature, some or all replicas of data could be on either tier. But the location is transparent to applications consuming the data via HDFS. Even though all the replicas of FROZEN data are on ARCHIVE storage, applications can still access it just like any HDFS data. Because no computing power is available on ARCHIVE nodes, mapped tasks running on DISK nodes will read the data from ARCHIVE nodes, which leads to increased network traffic for the applications. If this occurs too frequently, you can declare the data as WARM/COLD, and Mover can move one or more replicas back to DISK. The determination of data temperature and the designated replica movement to pre-defined tiered storage can be fully automated. Tiered storage is enabled in one of the very large clusters at eBay. The cluster had 40 PB of data. We added 10 PB of additional storage with limited computing power. Each new machine could store 220 TB. We marked the additional storage as ARCHIVE. We identified a few directories as WARM, COLD, or FROZEN. Based on their temperature, we moved all or a few replicas to the ARCHIVE storage. The price per GB of the ARCHIVE tier is four times less than the price per GB on the DISK tier. This difference is mainly because machines in the ARCHIVE tier have very limited computing power and hence lower costs. Storage without computing is cheaper than storage with computing. We can use the temperature of the data to make sure that storage with computing is wisely used. Because each block of data is replicated a few times (the default is three), some replicas can be moved to the low-cost storage based on the temperature of the data. HDFS supports tiered storage and provides the necessary tools to move data between tiers. Tiered storage is enabled on one of the very large clusters at eBay to archive data. Benoy Antony is an Apache Hadoop committer who focuses on HDFS and Hadoop security. Benoy works as a software engineer in the Global Data Infrastructure team at eBay.", "date": "2015-01-12"},
{"website": "Ebay-Engineering", "title": "Developer Anarchy Day", "author": ["Maria Turnau"], "link": "https://tech.ebayinc.com/engineering/developer-anarchy-day/", "abstract": "Have you ever imagined what would happen if you let software developers work on what they want? Well, we did it. For one day. And here are the results… “OK, listen: there is no backlog today.” When we first heard these words from Megan instead of the usual beginning of standup, we didn’t know what to expect. Further explanation wasn’t elaborate either. There was only one rule: you need to demonstrate something at the end of the day. We had different reactions. We were happy (“Great! A break from the day-to-day tasks!”), shocked (“What did they do with my safe and predictable to-do column! Help!”), and insecure (“Can I really finish something to show in just one day, with no planning, estimating, or design?”). So that was it. For one full (work) day, all developers in our team at Shutl (an eBay Inc. company) were supposed to forget about ongoing projects, deadlines, and unfinished tasks from the day before. We could work on whatever we wanted. We could pair or work individually. We could work on a DevOps task, on a working feature, or just on a prototype. We could develop a feature on an existing application or create a brand new project. The first thing we did was a small brainstorm where we described our ideas. It was not obligatory, but it helped in forming pairs and getting some encouragement for our little projects. Then we just started coding. Now, let me give some background behind this idea. You may have heard about “programmer anarchy” in context of development processes and company culture. In a few words: letting engineers make decisions on what and how they develop in order to meet critical requirements, and getting rid of “managers of programmers” from your development process. Fred George, the inventor of the idea , implemented it in a couple of companies. There was also a big buzz about how Github works with no managers (or rather with everyone being a manager ). These are great examples to read and think about. There are different opinions about this philosophy. Certainly, developing a culture and process that leaves all decisions to developers requires courage, time, money, and a certain kind of people in your team. You have to think very carefully before applying developer anarchy as a day-to-day rule. We asked ourselves if there was anything we could do without changing our processes and getting rid of our managers, but still gain inspiration from the concepts of developer anarchy? We reckoned we could, and Developer Anarchy Days were born! Introducing Developer Anarchy Days required very little preparation or changes in our organization. No planning or product management before it began was required. We did have some discussions prior to the event on whether it should be a spontaneously picked day or a planned and scheduled action. We decided for mix of both. Team members would get a ‘warning’ email some days in advance so that they could start thinking about it, but the actual day was a surprise. The concept is very lightweight and open to interpretation. The premise is simple. Give your developers a day without a backlog or predefined tasks and let their creativity take over. This method has benefits to whatever team composition you may have. Less experienced developers get a chance to expand their skills and their self-confidence as they gain experience in owning and delivering something in a short time frame. More experienced developers get a chance to try out some new technologies they’ve been itching to experiment with. Pairing is always an option (and encouraged), so that there is someone to help and learn from. What if the team is not an agile team at all? Well, that’s actually a great opportunity to taste a bit of agility. What can be more agile than delivering in just one day? It depends on how you define wasted time. If you see it as any time not spent directly on delivering pre-defined business requirements/stories, then yes, it is wasted time. You could say the same about avoiding technical debt, working on chores, organizing meetings, or playing ping-pong after lunch. As with any other culture-related thing, it is hard to say. You may waste one day on building software no one will ever look at again. On the other hand, you may learn something, make developers more motivated, invent internal tools that improve efficiency, and even develop some great new innovations to help achieve business goals. Yes and no. It’s probably not enough time to develop something production-ready, but that’s not the intention. It’s more about trying something new, developing a prototype, creating a small internal tool, or just presenting new ideas to the team. For that, we’ve found that one day is enough. You can make it longer and spend a couple of days on building small projects in small teams. This may be more effective in complex and usable projects, but also requires more preparation, such as some planning considering ongoing project roadmaps and probably announcing the event earlier so everyone can prepare potential ideas for the projects. Developer Anarchy Days have a lot in common with hackathons, hackfests, codefests, or hack-days. They’re all about intensively developing and presenting creative ideas. The main difference is that hackathons are usually bigger events in the form of competition, very often involving participants from outside of the company. They require proper event organization, including marketing, proper venue, food, and infrastructure. Usually, the promotional aspect of it is very important. You don’t need all this to organize a Developer Anarchy Day. Developers show that they are able to make decisions and explore creative ideas Engineers get a chance to come up with ideas from a technological perspective – something that businesses may sometimes miss Developers feel more motivated, because they are doing something of their own Developers experience how it is when they have to not only deliver something on time but also limit the project to something they can show and sell to others Developers can feel like a product manager and understand their job better The event breaks the routine of everyday (sometimes monotonous) deliveries The event gives everyone an opportunity to finally do stuff that we thought would be nice, but doesn’t bring any direct or indirect business value (e.g. internal tools) Finally, the event allows time to try some new technology or crazy idea! OK, let’s go back to Shutl and our very first Developer Anarchy Day. It was a busy day, but definitely a fun one. Everyone felt responsible for finishing what they began on time. After all, we all had to present something. Some of us were pairing; some decided to give it a go by themselves. Although we love pairing , it is good to get away from it from time to time. First thing the next morning, we presented our work. The variety and creativity of our little projects was beyond all expectations! Here are couple of examples. As Shutl has a service-oriented architecture, our everyday work (as everyone’s DevOps) involves logging into multiple boxes. One of our engineers spent Developer Anarchy Day building a super useful command line tool that automates the process of logging in to specific environments without having to ssh into multiple boxes and remember server names. We’ve used it every day since, making our lives easier. Every day we gather lots of feedback from our customers. The stars they give in their reviews though are a bit impersonal. You can learn much more by analyzing the language of the feedback comments. A pair of Shutl developers spent a day building a language sentiment analyzer that allowed us to get a sense of the general mood of our customers, based on the words they used. Another Shutl engineer decided to be more DevOps for that day. He experimented with some new tools and demonstrated immutable deployments with CloudFormation and Chef. Looking for common or possible use cases of our services, we realized that it would be really convenient to use Shutl to pick up and deliver items sent by private sellers on Gumtree or eBay . We have Shutl.it , which allows customers to deliver items from point A to B. The idea was to create a shareable link that pre-fills Shutl.it with pick-up information so any retailer or private seller can offer Shutl as an easy delivery option. We definitely had fun and learned something. Actually, we now use “Easy login” every day and “Predefined orders” inspired some things on our roadmap. No surprise here. It was genuinely positive. What can be better for us nine-to-five workers than a little bit of anarchy, especially when it lasts only one day, after which we quickly revert back to comfort and security of prioritized backlog and product management. We all agreed that we want to repeat anarchy on a regular basis. And we do. It has become an important part of our work culture.", "date": "2015-03-09"},
{"website": "Ebay-Engineering", "title": "Search Engine Journal: How eBay’s Search Technology Helps Users Find Your Listings", "author": ["Margaret Dornbusch"], "link": "https://tech.ebayinc.com/engineering/search-engine-journal-how-ebays-search-technology-helps-users-find-your-listings/", "abstract": "Dan Fain, Vice President of Search Technology at eBay Inc., joined Kelsey Jones of the Search Engine Journal for a Marketing Nerds podcast to discuss eBay’s proprietary search technology, which helps both sellers and users in search results. The podcast, How eBay’s Search Technology Helps Users Find Your Listings , touches on how eBay’s evolving machine learning search technology helps users find the listings they are looking for. Dan describes the parts of each listing that eBay’s algorithm searches to contextually find the best listings for users’ searches, as well as recommend additional listings users might be interested in. The discussion covers the following topics: Whether eBay search utilizes users’ prior search behavior to influence search results The top areas in a product listing that are crawled first Why search is so important to eBay What eBay is looking forward to in the future of our search technology How eBay is treating mobile search", "date": "2015-04-10"},
{"website": "Ebay-Engineering", "title": "The Zombie Apocalypse Retrospective!", "author": ["Akash Bhalla"], "link": "https://tech.ebayinc.com/engineering/the-zombie-apocalypse-retrospective/", "abstract": "I’ve written in the past that I believe that retrospectives should be a creative process, and I like to engage the brain using interesting visuals and ideas. I’ve attempted to employ this philosophy at Shutl (an eBay Inc. company) by trying to use a different theme for every retrospective I’ve run. (A recent example of a theme I found through funretrospectives.com is the catapult retro . ) Then a few weeks ago, I made a comment to one of our engineers, Volker , that you could pretty much take any situation you can think of and turn it into a retrospective idea; thus the challenge of a Zombie Apocalypse- themed retro was born! I was first introduced to retrospectives in 2007. Back then, a typical retro would follow the starfish format (or some variation). However, over the past few years I’ve started to see some limitations with such formats. In an attempt to address the more common anti-patterns , I’ve been moving towards a slightly adapted format. I now try to incorporate action items into the brainstorming section, both to streamline the time taken and to focus the group on constructive conversation. This format achieves a few things: Shortens the overall time taken by having the group identify not only what’s helping/hindering the team, but also what they can carry forward to improve their performance in the future Ensures a more constructive mindset by increasing focus, during the brainstorming itself, on suggestions that address hindrances Helps create more achievable solutions by modifying the typical “action item” phase of the retro to instead be a refinement phase, where previously suggested actions are analyzed and prioritized With the above goals in mind, I started by scribbling and sketching out some ideas in my notepad; after a short while I had come up with a basic draft for the structure of the retro: I bandied the idea around in my head for a day or so. The finished product looked like this: The picture above was drawn on a large whiteboard and divided into three color-coded columns (with a fourth column for action items, complete with a reminder that our final actions require a “what,” a “who,” and a “when”). This is you, huddled in the corner, with your stockpile of weaponry at the ready, bravely fighting off the ravenous horde crashing through your doorway. What’s your ammo? On green stickies, write down all those things that are fueling your team’s successes and working in your favor. This is the zombie horde—a relentless army of endless undead marching towards your destruction. Use pink stickies to identify the problems that you are facing (including potential future problems). This is your perimeter—the security measures you’ve installed to resist the horde and ensure your survival. As you’re identifying the issues you face and the current behaviors that are fueling your success, think about what actions you can take today to either address these issues or ensure continued success. The idea is to try to come up with a solution or suggestion for every problem that you can see on a pink sticky. I tried out the format on the team. I gave them about seven minutes for the brainstorming, with the usual guidelines around collaboration: encouraging people to talk to each other and to look at each other’s suggestions. As a countdown timer, I personally use the 3-2-1 dashboard widget , but there are plenty of others you can use. We then had a round of grouping and voting (each team member got three votes), with a reminder to vote on things you want to discuss , not just things you agree with (e.g., you could strongly disagree with a point on the board, and vote for it to start a discussion). Due to the nature of the board (if things go well), groups of pink stickies should have corresponding orange ones to direct the discussion towards action items. I wrote down all action items that came up, and gave the team a caveat that we’d have five minutes at the end to review the actions, prioritize them, and pick the ones that we actually wanted to address; this keeps the discussions flowing. We ended up with some conflicting action items — which was fine; the idea was to get all the potential actions down, and then at the end decide which we felt were the most valuable. During this final review of the actions, we also assigned owners and deadlines. Then we were done! Here’s what the final board looked like after our 45-minute retro was complete: Next challenge: what crazy (yet effective ) retrospective formats can you come up with?", "date": "2015-03-17"},
{"website": "Ebay-Engineering", "title": "Announcing Pulsar: Real-time Analytics at Scale", "author": ["Sharad Murthy", "Tony Ng"], "link": "https://tech.ebayinc.com/engineering/announcing-pulsar-real-time-analytics-at-scale/", "abstract": "We are happy to announce Pulsar – an open-source, real-time analytics platform and stream processing framework. Pulsar can be used to collect and process user and business events in real time, providing key insights and enabling systems to react to user activities within seconds. In addition to real-time sessionization and multi-dimensional metrics aggregation over time windows, Pulsar uses a SQL-like event processing language to offer custom stream creation through data enrichment, mutation, and filtering. Pulsar scales to a million events per second with high availability. It can be easily integrated with metrics stores like Cassandra and Druid. eBay provides a platform that enables millions of buyers and sellers to conduct commerce transactions. To help optimize eBay end users’ experience, we perform analysis of user interactions and behaviors. Over the past years, batch-oriented data platforms like Hadoop have been used successfully for user behavior analytics. More recently, we have newer use cases that demand collection and processing of vast numbers of events in near real time (within seconds), in order to derive actionable insights and generate signals for immediate action. Here are examples of such use cases: Real-time reporting and dashboards Business activity monitoring Personalization Marketing and advertising Fraud and bot detection We identified a set of systemic qualities that are important to support these large-scale, real-time analytics use cases: Scalability – Scaling to millions of events per second Latency – Sub-second event processing and delivery Availability – No cluster downtime during software upgrade, stream processing rule updates , and topology changes Flexibility – Ease in defining and changing processing logic, event routing, and pipeline topology Productivity – Support for complex event processing (CEP) and a 4GL language for data filtering, mutation, aggregation, and stateful processing Data accuracy – 99.9% data delivery Cloud deployability – Node distribution across data centers using standard cloud infrastructure Given our unique set of requirements, we decided to develop our own distributed CEP framework. Pulsar CEP provides a Java-based framework as well as tooling to build, deploy, and manage CEP applications in a cloud environment. Pulsar CEP includes the following capabilities: Declarative definition of processing logic in SQL Hot deployment of SQL without restarting applications Annotation plugin framework to extend SQL functionality Pipeline flow routing using SQL Dynamic creation of stream affinity using SQL Declarative pipeline stitching using Spring IOC, thereby enabling dynamic topology changes at runtime Clustering with elastic scaling Cloud deployment Publish-subscribe messaging with both push and pull models Additional CEP capabilities through Esper integration On top of this CEP framework, we implemented a real-time analytics data pipeline. Pulsar’s real-time analytics data pipeline consists of loosely coupled stages. Each stage is functionally separate from its neighboring stage. Events are transported asynchronously across a pipeline of these loosely coupled stages. This model provides higher reliability and scalability. Each stage can be built and operated independently from its neighboring stages, and can adopt its own deployment and release cycles. The topology can be changed without restarting the cluster. Here is some of the processing we perform in our real-time analytics pipeline: Enrichment – Decorate events with additional attributes. For example, we can add geo location information to user interaction events based on the IP address range. Filtering and mutation – Filter out irrelevant attributes and events, or transform the content of an event. Aggregation – Count the number of events, or add up metrics along a set of dimensions over a time window. Stateful processing – Group multiple events into one, or generate a new event based on a sequence of events and processing rules. An example is our sessionization stage, which tracks user session-based metrics by grouping a sequence of user interaction events into web sessions. The Pulsar pipeline can be integrated with different systems. For example, summarized events can be sent to a persistent metrics store to support ad-hoc queries. Events can also be sent to some form of visualization dashboard for real-time reporting, or to backend systems that can react to event signals. In Pulsar, our approach is to treat the event stream like a database table. We apply SQL queries and annotations on live streams to extract summary data as events are moving. The following are a few examples of how common processing can be expressed in Pulsar. Event filtering and routing Aggregate computation TopN computation Pulsar CEP processing logic is deployed on many nodes (CEP cells) across data centers. Each CEP cell is configured with an inbound channel, outbound channel, and processing logic. Events are typically partitioned based on a key such as user id. All events with the same partitioned key are routed to the same CEP cell. In each stage, events can be partitioned based on a different key, enabling aggregation across multiple dimensions. To scale to more events, we just need to add more CEP cells into the pipeline. Using Apache ZooKeeper , Pulsar CEP automatically detects the new cell and rebalances the event traffic. Similarly, if a CEP cell goes down, Pulsar CEP will reroute traffic to other nodes. Pulsar CEP supports multiple messaging models to move events between stages. For low delivery latency, we recommend the push model when events are sent from a producer to a consumer with at-most-once delivery semantics. If a consumer goes down or cannot keep up with the event traffic, it can signal the producer to temporarily push the event into a persistent queue like Kafka; subsequently, the events can be replayed. Pulsar CEP can also be configured to support the pull model with at-least-once delivery semantics. In this case, all events will be written into Kafka, and a consumer will pull from Kafka. Pulsar has been deployed in production at eBay and is processing all user behavior events. We have open-sourced the Pulsar code, we plan to continue to develop the code in the open, and we welcome everyone’s contributions. Below are some features we are working on. We would love to get your help and suggestions. Real-time reporting API and dashboard Integration with Druid or other metrics stores Persistent session store integration Support for long rolling-window aggregation Please visit http://gopulsar.io for source code, documentation, and more information.", "date": "2015-02-23"},
{"website": "Ebay-Engineering", "title": "Mobile First - A Retrospective", "author": ["Bryce Watson", "Michael Woo"], "link": "https://tech.ebayinc.com/engineering/mobile-first-a-retrospective/", "abstract": "Most engineers would agree that simply having a mobile-first mindset is not enough to build a high-quality responsive website — we also need relevant mobile-first guidelines and principles. We recently explored the challenges of scaling a mobile-oriented site to tablet and desktop. What we found was not always pretty, but the lessons learned were valuable. In this article, we will explore those lessons, in hopes that we can all improve how we build across mobile, tablet, and desktop. Building a fully scalable website requires a strong focus on code quality. Concepts such as modularity, encapsulation, and testability become extremely important as you move across domains. Whether we are scaling up to desktop or down to mobile, we need the code to stay consistent and maintainable. Every hacked, poorly planned, or rushed piece of code we might add reduces our ability to write elegant, scalable, responsive code. Perhaps creating a responsive app is not high on your team’s priority list right now. But one day it will be — and the conversion time frame might be very tight when that day comes. Ideally, all you need to do is add media query CSS and everything just works. But the only way that can happen is if the code readily adapts to responsive changes. Below are some suggestions and fixes that will make conversion to responsive easier. Some are specific to responsive design while others are general good practices. Yes, we all know about media queries. How hard can they be? Sprinkle some on any page and you have a responsive website, right? Using media queries on your pages is essential; they allow you to overwrite CSS values based on screen size. This technique might sound simple, but in a larger project it can quickly get out of hand. A few major problems can get in the way of using media queries properly: Colliding media queries: It is easy to make the mistake of writing media queries that overwrite each other if you do not stick to a common pattern. We recommend using the same boilerplate throughout all projects, and have created one here . Setting element styles from JS: This is a tempting, but inferior, approach to building responsive websites. When an element relies on JS logic to set its width, it is unable to properly use media queries. If the JS logic is setting width as an inline property, the width cannot be overwritten in CSS without using !important . In addition, you have to now maintain an ever-growing set of JS logic. Media queries not at the bottom: If your queries are not loaded last, they will not override their intended targets. Every module might have its own CSS file, and the overall ordering might not place it at the bottom, which leads us to our next point. CSS namespacing for encapsulation: If you are writing a module, its CSS selectors should be properly encapsulated via namespace. We recommend prefixing class names with the module name, such as navbar-parent . Following this pattern will prevent conflicts with other modules, and will ensure that media queries at the bottom of your module’s CSS file override their intended targets. Too many CSS selectors: CSS specificity rules require media queries to use the same specificity in order to override. It is easy to get carried away in LESS, which allows you to nest CSS multiple levels deep. While it can be useful to go one or two levels deep for encapsulation, usually this is unnecessarily complicating your code. We recommend favoring namespacing over nested specifiers as it is cleaner and easier to maintain. Using !important to override styles: Adding !important to your styles reduces maintainability. It is better to avoid relying on !important overrides and instead use CSS namespacing to prevent sharing between modules. Both responsive and adaptive web design techniques contain powerful tools, but it is important to understand the differences between the two. Responsive techniques usually include media queries, fluid grids, and CSS percentage values. Adaptive techniques, on the other hand, are focused more on JavaScript logic, and the adding or removing of features based on device detection or screen size. So, which should you use? Responsive or adaptive? The answer depends on the feature you are trying to implement. It can be tempting to jump straight into applying adaptive techniques to your feature, but in many cases it may not be required. Worse, applying adaptive techniques can quickly over-complicate your design. An example of this that we saw in many places is the use of JavaScript logic to set CSS style attributes. When styling your UI, JavaScript should be avoided whenever possible. Dynamic sizing, for example, is better done through media queries. For most UI designs, you will be deciding on layouts based on screen size , not on device type. Confusing the need for device detection with screen size can lead us to apply adaptive where responsive would be superior. Rethink any design that requires CSS attributes to change based on device detection; in almost all cases it will be better to rely on screen size alone, via media queries. So, when should we use adaptive Javascript techniques? Adaptive web design techniques are powerful, as they allow for selective loading of resources based on user agent or screen size. Logic that checks for desktop browsers, for example, can load high-resolution images instead of their mobile-optimized counterparts. Loading additional resources and features for larger screens can also be useful. Desktop browsers, for example, could show more functionality due to the increased screen size, browser capability, or bandwidth. Ideally, additional resources will be lazy-loaded for their intended platforms. Lazily loading modules helps with site speed for mobile web, while still allowing for a full set of functionality for desktop and tablet web. This technique can be applied by checking the user agent on the client or server. If done on the server, only resources supported by the user’s platform should be returned. Alternatively, client-based lazy loading can use Ajax requests to load additional resources if they are supported. This effect can be achieved using client-side JavaScript, based on browser support or user agent. Client-side detection is generally preferred, as it allows feature detection based on actual browser functionality instead of potentially complicated user agent checks. A responsive flex grid doesn’t have to be complicated. In our live demo page , we show a simple implementation that creates a horizontally scrolling section of image containers. The images are centered, allowed to expand up to 100% of their container, and will maintain their original aspect ratio. In addition, the container height values are set to 100%, allowing us to adjust the height in the parent wrapper only, and keeping our media query overrides simple and easy to read. The html and css source code use the concepts mentioned above. We plan to add more boilerplate patterns; please don’t hesitate to add your own as well. Pull requests are welcomed! We hope that the information above will come in handy when you are working on your next mobile-first web project. Below is a summary of what we mentioned above and other helpful tips. Most responsive layout can and should be done with media queries. JS manipulation of CSS (maybe with the exception of adding/removing classes) should be avoided. Setting width in JS is not as maintainable or dynamic compared to CSS. Use media query boilerplate to ensure you do not have contradicting media queries or have media queries that are always skipped. Put media queries at the bottom. Media queries override CSS and should be the final overrides, whether page level or module level. If your regular CSS rules have many selectors, your media query CSS rules will have to as well, due to CSS specificity rules. Use as few selectors as possible when defining CSS rules. Use CSS classes, not CSS IDs, to avoid CSS specificity issues. Use the fewest number of selectors possible to define your selector. Reuse classes. If an element has the same look on different parts of the page, do not create two different classes. Make a generic class and reuse it. Encapsulate your CSS selectors by using proper namespacing to prevent conflicts. e.g., class=”module-name-parent” It is very rare that you need to use !important . Before you use it, ask yourself whether you can instead add another class (parent or same level). And then ask yourself whether the rule you are trying to override has unnecessary selectors. Use LESS nesting only where needed. Nesting is good for organization, but it is also a recipe for CSS specificity issues. Check that you do not have a CSS rule that looks like this: Work with the design team and define LESS variables using good names. Then, use these LESS variables everywhere possible. If you are using a set of CSS rules repeatedly, make it a LESS mixin. Most dom structures are more complex than necessary. Add a wrapper only when needed. Do not add a wrapper when proper CSS can do the same thing. If you remove the wrapper and the layout does not change, you do not need it. Now, do a global search for this wrapper’s references (JS, CSS, rhtml, jsp, tag) and remove them. Add a placeholder to your component for lazy loading. Lazy-loaded sections will start off empty, so make sure you reserve the correct amount of space for this behavior. Otherwise, you will see the page shift as modules load in. Use media queries for the empty section so that it closely matches the filled size. If you are playing around with CSS to attempt a layout and it starts working, remember to remove the unnecessary CSS rules. Many of them are probably not needed anymore. Remove the unnecessary wrappers as well. Image source: http://upload.wikimedia.org/wikipedia/commons/e/e2/Responsive_Web_Design.png", "date": "2015-05-13"},
{"website": "Ebay-Engineering", "title": "Functional iOS Testing in Swift", "author": ["Nico Saueressig", "David Vilar Alcala"], "link": "https://tech.ebayinc.com/engineering/functional-ios-testing-in-swift/", "abstract": "One of the projects we are currently working on at Shutl involves building an iOS application. The application is essentially quite simple; it acts as a client for our API, adding animations, visuals, and notifications. Testing is a key part of our development process, so when we started developing the application, one of the first steps was to find a testing framework that suited our needs. XCode provides XCTest as a testing framework that works good for unit testing. Unfortunately, if you want to test the behavior of your app from a user perspective, XCTest’s abilities are very limited. Because we are mostly a Ruby shop, we’re familiar with using cucumber . That’s how we came across Frank , a handy framework which enables you to write functional tests for your iOS applications using cucumber. The way Frank works is that you “ frankify ” your iOS app, which then lets you use the accessibility features of iOS to emulate a user using an iOS device. You can launch the app, rotate the device, and interact with the screen in most of the ways a real user can. If you’re familiar with CSS selectors, interacting with elements on the screen should look very familiar, albeit with a slightly different syntax. Frank also provides custom selectors and predefined steps for some of the most common interactions. For instance if you want to select a label with the content “I am a label” you could use this: There are also predefined steps provided for more complex instructions like clicking a button with the content “Click me”: At first we considered testing against a live QA server but soon experienced problems with this setup. We needed predictable data for our tests, and this is difficult to achieve as the data stored in a live QA environment changes all the time. Combine this with availability issues and you’ve got yourself an unworkable solution. After some thought, the route we decided to take was to mock these services and return fixtures. The idea is to keep all the logic that directly interacts with the server inside one unique class or struct. It will provide necessary functions such as fetchUser and updateResource that can be invoked from wherever they’re needed. This allows us to easily implement alternate versions of these functions without affecting the rest of the code. In the example code below, we have two different implementations. The first one, shown here, uses our remote API to retrieve data from the server. The second implementation – our test mock – is simply returning a hard-coded value with the same structure as the ones returned by the server. Next we’ll define two different targets, one using the real client and another one using the test mock client, and we’ll use the second – mocked – target to create our frankified app. Here is a walk-through of how such an app could be implemented by using our sample app “What superhero are you?”. You provide the app with your name and gender, and it uses a highly advanced algorithm to determine which superhero you are. Set your app up with two targets. One will be using the real backend, and the other one will be using the mocked backend. Frankify your app. Write your first test. Our first feature looks like this: Feature:\n  As a user\n  I want to use the app\n  So I can determine which superhero I am\n \nScenario:\n  Put in my name and gender and have my superhero have\n  it return which superhero I am\n \nGiven I launch the app\nWhen  I enter my name\nAnd   I choose my gender\nAnd   I touch the button marked \"Which superhero am I?\"\nThen  I want to see which superhero I am And the related steps: When(/^I enter my name$/) do\n  fill_in('Name', with: 'Jon')\nend\n \nWhen(/^I choose my sex$/) do\n  touch(\"view:'UISegmentLabel' marked:'Male'\")\nend\n \nThen(/^I want to see which superhero I am$/) do\n  sleep 1\n  check_element_exists(\"view:'UILabel' marked:'Super Jon'\")\nend And the related steps: Make the tests pass! Done! Conclusions This solution works, but it’s not without its limitations. The most significant one being that you need to return some sensible data in your mocks. In our test app we work with very simple logic, and it did the trick. We return fixed responses, which means that there is no way of testing more complex interactions. These can and should be covered by unit and integration tests, which come with their own problems. It can also be hard to test certain user actions, like swiping something on the screen. The more customized your app’s interface is, the harder it will be to test it with Frank. Almost anything can be done, but the solution will most likely feel hacked. Also we have yet to find a way of testing web UIs. Frank is not a magic bullet for functional testing in Swift, but so far we’ve found it a useful addition to our codebase, and we’re liking it! Links What Superhero are you? on Github Testing with Frank (CC image by Chris Harrison ) (CC image by Esther Vargas ) This solution works, but it’s not without its limitations. The most significant one being that you need to return some sensible data in your mocks. In our test app we work with very simple logic, and it did the trick. We return fixed responses, which means that there is no way of testing more complex interactions. These can and should be covered by unit and integration tests, which come with their own problems. It can also be hard to test certain user actions, like swiping something on the screen. The more customized your app’s interface is, the harder it will be to test it with Frank. Almost anything can be done, but the solution will most likely feel hacked. Also we have yet to find a way of testing web UIs. Frank is not a magic bullet for functional testing in Swift, but so far we’ve found it a useful addition to our codebase, and we’re liking it! What Superhero are you? on Github Testing with Frank (CC image by Chris Harrison ) (CC image by Esther Vargas )", "date": "2015-04-15"},
{"website": "Ebay-Engineering", "title": "Statistical Anomaly Detection", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/statistical-anomaly-detection/", "abstract": "Complex systems can fail in many ways and I find it useful to divide failures into two classes. The first consists of failures that can be anticipated (e.g. disk failures), will happen again, and can be directly checked for. The second class is failures that are unexpected.  This post is about that second class. The tool for unexpected failures is statistics, hence the name for this post. A statistical anomaly detector hunts for things that seem off and then sends an alert. The power of statistical anomalies is that they can detect novel problems.  The downside is that they must be followed up to track down the root cause.  All this might seem abstract, but I will give a concrete example below. To avoid confusion, I find it helps to carefully define some terminology: Monitored Signals :  Results of continuous scans that look for trouble. Disruption : Behavior indicating that the site is not working as designed.  A human will need to investigate to find the root cause. Anomaly : Unusual behavior in the monitored signals, suggesting a disruption. Alert : Automatic signal that an anomaly has occurred. It is common to send an alert after detecting an anomaly. The obstacle to building useful statistical detectors  is false positives :  an alert that doesn’t  correspond to a disruption.   If the anomaly detector has too many false positives, users will turn off alerts or direct them to a spam folder or just generally ignore them.  An ignored detector is not very useful. In this post I will describe a statistical anomaly detector in use at eBay; my paper at the recent USENIX HotCloud 2015 workshop has more details.  The monitored signals in our search system are based on the search results from a set of reference queries that are repeatedly issued.  A statistical anomaly detector needs a way to encode the results of the monitoring as a set of numbers.  We do this by computing metrics on the results.  For each reference query, we compute about 50 metrics summarizing the items returned by the query.  Two examples of metrics are the number of items returned and the median price of the returned items.   There are 3000 reference queries and 50 metrics, therefore 150,000 numbers total.  Currently,  reference queries are issued every 4 hours, or 6 times a day, so there are 900,000 numbers per day.   In these days of terabyte databases, this is very small.  But the problem of sifting through those numbers to find anomalies, and do it with a low false-positive rate, is challenging. I’ll outline our approach using diagrams, and refer you to my HotCloud paper for details.  First, here is a picture of the monitored signals—that is, the metrics collected: Each (query, metric) pair is a number that can be tracked over time, resulting in a time series. That’s 150,000 times series, and it’s reasonable to expect that during every 4-hour collection window, at least one of them will appear anomalous just by chance.  So alerting on each time series is no good, because it will result in many false positives. Our approach is to aggregate, and it starts with something very simple:  examining each time series and computing the deviation between the most recent value and what you might have expected by extrapolating previous values.  I call this the surprise , where the bigger the deviation the more the surprise. Here’s a figure  illustrating that there is a surprise for each (query, metric, time) triple. The idea for our anomaly detector is this: at each collection time T we expect a few (query,metric, T ) triples to have a large surprise. We signal an alert if an unusually high number of triples have a large surprise.   To make this idea quantitative,  fix a metric,  look at the surprise for all 3000 queries, and compute the 90th percentile of surprise at time T : S 90 ( T ). This gives a new time series in T , one for each metric.  Hypothetical time series for the first two metrics are illustrated below. We’ve gone from 150,000 time series down to 50.  Aggregation like this is  a very useful technique in anomaly detection. Our final anomaly detector uses a  simple test on this aggregated time series. We define an anomaly to occur when the current value of any of the 50 series is more than 3σ from the median of that series.  Here’s an example using eBay data with the metric median sale price. There is clearly an anomaly at time T =88. For more details, see my previously cited HotCloud paper, The Importance of Features for Statistical Anomaly Detection .", "date": "2015-08-19"},
{"website": "Ebay-Engineering", "title": "Fast Approximate Logarithms, Part II: Rounding Error", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/fast-approximate-logarithms-part-ii-rounding-error/", "abstract": "In Part I , I assumed all computations were done exactly. It’s time to bring in floating-point rounding error. There’s a key fact about rounding error that might seem surprising: If , then the computation of in floating-point has no rounding error. The surprise comes for those who’ve been taught to beware of rounding error when subtracting two similar quantities. For example, the computation in floating-point of might be far from the true value of when . That’s because can have rounding error, so the subtraction might cancel most of the good bits leaving mainly the incorrect (due to rounding) bits. But in , neither nor 1 have rounding error, so the computation of is exactly equal to when . The rule of thumb is that when approximating a function with , severe rounding error can be reduced if the approximation is in terms of . For us, so , and the rule suggests polynomials written in terms of rather than .  By the key fact above, there is no rounding error in when . Let me apply that to two different forms of the quadratic polynomials used in Part I: the polynomial can be written in terms of or . If they are to be used on the interval and I want to minimize relative error, it is crucial that the polynomial be 0 when , so they become The second equation has no constant term, so they both cost the same amount to evaluate, in that they involve the same number of additions and multiplications. But one is much more accurate. You can see that empirically using an evaluation program (code below) that I will be using throughout to compare different approximations. I invoked the program as and and got the following: When the approximating polynomials are evaluated at points spaced 1/1024 apart, they have similar performance. The accuracy of both is 5.5 bits, and the one using is slightly slower. But when they are evaluated at points spaced apart, the polynomial using has poor accuracy when is slightly below 1. Specifically, the accuracy is only 1.7 bits when . To see why, note that when , is summing two numbers that have rounding error, but are of different sizes, since . But is summing two numbers of similar size, since and the sum of the first two terms is about . This is the bad case of subtracting two nearby numbers (cancellation), because they both have rounding error. I suppose it is an arguable point whether full accuracy for all is worth a time performance hit of about 2%. I will offer this argument: you can reason about your program if you know it has (in this case) 5.5 bits of accuracy on every input. You don’t want to spend a lot of time tracking down unexpectedly low accuracy in your code that came about because you used a log library function with poor precision on a small set of inputs. Here’s some more information on the output of the evalution program displayed above. The first number is accuracy in bits measured in the usual way as where is the maximum relative error. Following is the value of where the max error occured. The execution time (e.g. 2.06 nsecs for the first line) is an estimate of the time it takes to do a single approximation to , including reducing the argument to the interval . The last two numbers are self explanatory. Estimating execution time is tricky. For example on my MacBook, if the argument must be brought into the cache, it will significantly affect the timings. That’s why the evaluation program brings and into the cache before beginning the timing runs. For polynomials, using has almost the same cost and better accuracy, so there is a good argument that it is superior to using . Things are not so clear when the approximation is a rational function rather than a polynomial. For example, . Because , the numerator is actually . And because you can multiply numerator and denominator by anything (as long as it’s the same anything), it further simplifies to . This will have no floating-point cancellation, and will have good accuracy even when . But there’s a rewrite of this expression that is faster: Unfortunately, this brings back cancellation, because when there will be cancellation between and the fraction. Because there’s cancellation anyway, you might as well make a further performance improvement eliminating the need to compute , namely Both sides have a division. In addition, the left hand side has a multiplication and 2 additions. The right hand side has no multiplications and 2 additions ( is a constant and doesn’t involve a run-time multiplication, similarly for ). So there is one less multiplication, which should be faster. But at the cost of a rounding error problem when . As expected, the rational function that has one less multiplication (the line marked using x) is faster, but has poor accuracy when is near 1. There’s a simple idea for a fix. When is small, use the Taylor series, . Using is a subtraction and a multiplication, which is most likely cheaper than a division and two additions, What is the size cutoff? The error in the Taylor series is easy to compute: it is the next term in the series, , so the relative error is about . And I want to maintain an accuracy of 7.5 bits, or . So the cutoff is , where or . On my MacBook, the most efficient way to implement appears to be . In the evaluation program, most of the are greater than 1, so only the first of the inequalities is executed. Despite this, adding the check still has a high cost, but no more accuracy than using . In Part I of this series, I noted that testing whether was in the range can be done with a bit operation rather than a floating-point one. The same idea could be used here. Instead of using the Taylor series when or , use it in a slightly smaller interval The latter can be converted to bit operations on , the fraction part of x, as follows: As bit operations, this is When I tested this improvement ( in the table below). it was faster, but still slower than using , at least on my MacBook. Bottom line: having special case code when appears to significantly underperform computing in terms of . In the first post, I recommended reducing to instead of because you get one extra degree of freedom, which in turns gives greater accuracy. Rounding error gives another reason for preferring . When , reduction to will have cancellation problems. Recall the function that was optimal for the interval , . When , must be multiplied by two to move into , and then to compensate, the result is . When , , and so you get cancellation. Below are the results of running the evaluation program on . If there was no rounding error, would be accurate to 3.7 bits. As you get closer to 1 ( ) the accuracy drops. The goal of this series of posts is to show that you can create logarithm routines that are much faster than the library versions and have a minimum guaranteed accuracy for all . To do this requires paying attention to rounding error. Summarizing what I’ve said so far, my method for minimizing rounding error problems is to reduce to the interval and write the approximating expression using , for example ). More generally, the approximating expression would be a polynomial or rational function I close by giving the code for the evaluation program that was used to compare the time and accuracy of the different approximations: Powered by QuickLaTeX", "date": "2015-06-11"},
{"website": "Ebay-Engineering", "title": "Fast Approximate Logarithms, Part I:  The Basics", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/fast-approximate-logarithms-part-i-the-basics/", "abstract": "Performance profiling of some of the eBay code base showed the logarithm (log) function to be consuming more CPU than expected. The implementation of log in modern math libraries is an ingenious wonder, efficiently computing a value that is correct down to the last bit. But for many applications, you’d be perfectly happy with an approximate logarithm, accurate to (say) 10 bits, especially if it were two or three times faster than the math library version. This post is the first of a series that examines approximate logarithms and the tradeoff between accuracy and speed. By the end of the series, you will see how to design fast log functions, and will discover that on a MacBook Pro, you can get a roughly 3x speedup if you are willing to settle for 7 bits. Alternatively, if you want more accuracy you can get 11 bits with a 2x speedup. The numbers might vary for your target machine, but I will present code so that you can determine the tradeoffs for your case. You can find code for approximate logs on the web, but they rarely come with an evaluation of how they compare to the alternatives, or in what sense they might be optimal. That is the gap I’m trying to fill here. The first post in this series covers the basics, but even if you are familiar with this subject I think you will find some interesting nuggets. The second post considers rounding error, and the final post gives the code for a family of fast log functions. A very common way to to compute log (meaning ) is by using the formula to reduce the problem to computing . The reason is that for arbitrary is easily reduced to the computation of for in the interval ; details below. So for the rest of this series I will exclusively focus on computing . The red curve in the plot below shows on . For comparison, I also plot the straight line . If you’ve taken a calculus course, you know that has a Taylor series about which is . Combining with gives ( for T aylor) How well does approximate ? The plot shows that the approximation is very good when , but is lousy for near 2—so is a flop over the whole interval from 1 to 2. But there is a function that does very well over the whole interval: . I call it for better. It is shown below in red ( in blue). The plot makes it look like a very good approximation. A better way to see quality of the approximation is to plot the error . The largest errors are around and . Now that I’ve shown you an example, let me get to the first main topic: how do you evaluate different approximations? The conventional answer is minimax . Minimax is very conservative—it only cares about the worst (max) error. It judges the approximation over the entire range by its error on the worst point. As previously mentioned, in the example above the worst error occurs at , or perhaps at , since the two have very similar errors. The term minimax means you want to minimize the max error, in other words find the function with the minimum max error. The max error here is very close to 0.0050, and it is the smallest you can get with a quadratic polynomial. In other words, solves the minimax problem. Now, onto the first of the nuggets mentioned at the opening. One of the most basic facts about is that , whether it’s or or . This means there’s a big difference between ordinary and relative error when . As an example, take . The error in is quite small: . But most likely you care much more about relative error: , which is huge, about . It’s relative error that tells you how many bits are correct. If and agree to bits, then is about . Or putting it another way, if the relative error is , then the approxmation is good to about bits. The function that solved the minimax problem solved it for ordinary error. But it is a lousy choice for relative error. The reason is that its ordinary error is about near . As it follows that , and so the relative error will be roughly . But no problem. I can compute a minimax polynomial with respect to relative error; I’ll call it for relative. The following table compares the coefficients of the Taylor method , minimax for ordinary error and minimax for relative error : The coefficients of and are similar, at least compared to , but is a function that is always good to at least 5 bits, as the plot of relative error (below) shows. Here’s a justification for my claim that is good to 5 bits. The max relative error for occurs at , , and . For example, at Unfortunately, there’s a big problem we’ve overlooked. What happens outside the interval [1,2)? Floating-point numbers are represented as with . This leads to the fact mentioned above: . So you only need to compute on the interval . When you use for and reduce to this range for other , you get The results are awful for just below 1. After seeing this plot, you can easily figure out the problem. The relative error of for is about 0.02, and is almost the same as ordinary error (since the denominator is close to ). Now take an just below 1. Such an is multiplied by 2 to move it into [1,2), and the approximation to is , where the compensates for changing to . The ordinary error is still about 0.02. But is very small for , so the ordinary error of 0.02 is transformed to , which is enormous. At the very least, a candidate for small relative error must satisfy . But . This can be fixed by finding the polynomial that solves the minimax problem for all . The result is a polynomial for global. Globally (over all ), the blue curve does dramatically better, but of course it comes at a cost. Its relative error is not as good as over the interval [1, 2). That’s because it’s required to satisfy in order to have a small relative error at . The extra requirement reduces the degrees of freedom, and so does less well on [1, 2]. Finally, I come to the second nugget. The discussion so far suggests rethinking the basic strategy. Why reduce to the interval [1,2)? Any interval will do. What about using [0.75, 1.5)? It is easy to reduce to this interval (as I show below), and it imposes only a single requirement: that . This gives an extra degree of freedom that can be used to do a better job of approximating . I call the function based on reduction [0.75, 1.5) for shift, since the interval has been shifted. The result is a thing of beauty! The error of is significantly less than the error in . But you might wonder about the cost: isn’t it more expensive to reduce to [0.75, 1.5) instead of [1.0, 2.0)? The answer is that the cost is small. A floating-point number is represented as , with stored in the right-most 23 bits. To reduce to [0.75, 1.5) requires knowing when , and that is true exactly when the left-most of the 23 bits is one. In other words, it can be done with a simple bit check, not a floating-point operation. Here is more detail. To reduce to , I first need code to reduce to the interval . There are library routines for this of course. But since I’m doing this whole project for speed, I want to be sure I have an efficient reduction, so I write my own. That code combined with the further reduction to is below. Naturally, everything is written in single-precision floating-point. You can see that the extra cost of reducing to [0.75, 1.5) is a bit-wise operation to compute the value , and a test to see if is nonzero. Both are integer operations. The code does not check that , much less check for infinities or NaNs. This may be appropriate for a fast version of log. In summary: To study an approximation , don’t plot and directly, instead plot their difference. The measure of goodness for is its maximum error. The best is the one with the smallest max error (minimax). For a function like , ordinary and relative error are quite different. The proper yardstick for the quality of an approximation to is the number of correct bits, which is relative error. Computing requires reducing to an interval but you don’t need . There are advantages to picking instead. In the next post, I’ll examine rounding error, and how that affects good approximations. Powered by QuickLaTeX", "date": "2015-05-01"},
{"website": "Ebay-Engineering", "title": "Is Duplication Always a Bad Thing?", "author": ["Akash Bhalla"], "link": "https://tech.ebayinc.com/engineering/is-duplication-always-a-bad-thing/", "abstract": "No. Well, perhaps, but it can be the lesser of two evils. It depends. Disclaimer – these thoughts reflect my personal opinions and not a consensus of how the entire Shutl engineering team at eBay feels. As a software engineer, I’ve always had it drilled into my head that duplication is a bad thing, and teams I’ve been a part of have always worked diligently to reduce all forms of duplication they see. This as a general rule makes a lot of sense; I can still remember reading through Martin Fowler’s Refactoring while I was at university and the excitement of discovering design patterns. Good developers are precise and careful and understand the risks and pitfalls of duplication in a code base—see the DRY (don’t repeat yourself) principle, from The Pragmatic Programmer : “The alternative is to have the same thing expressed in two or more places. If you change one, you have to remember to change the others, or your program will be brought to its knees by a contradiction. It isn’t a question of whether you’ll remember: it’s a question of when you’ll forget.” Which makes perfect sense. It’s time well spent when you try to make your code streamlined and readable. You’ll end up with a cleaner, easier-to-maintain, and more extensible code base as a result. I briefly mention this point here just to give some context, but it’s been covered many times and I’m assuming that we’re all on board:  Duplication is a code smell, and removing it is usually a good thing. However, like most things, I believe this advice starts getting you into trouble if you take it to an extreme. Advice, just like code, does not live in a vacuum; context is king. For example, when it comes to unit tests, sometimes the cost introduced by removing duplication can be greater than the benefits gained. What I plan to show you in this post are a few examples of where I would leave duplication in place and how this duplication has, in my mind, led to cleaner and more readable code. (image source:  https://www.flickr.com/photos/pokerbrit/9455644074) I was trying to think up an appropriate term for a thought I had in my head and settled on Peacock Programmers. [Edit – I’ve discovered that an alternative name might already exist for this (and here I thought I was being original) – The Magpie Developer ] A peacock programmer is someone who loves to use every tool in the arsenal and for various reasons attempts to decorate code with every possible feature that is available to him or her. You can recognize the work of a peacock programmer through inelegant attempts at elegance. It’s wordy, flashy, complicated looking, and almost impossible to understand without deep concentrated effort and untangling. Sure it’s fun to write and a blast to experiment and learn all the different possible ways of solving a problem, but such use is as an academic tool. When you’re seeing it on a production code base, it can be frustrating. While talking through the concept of peacock programmers over lunch in the office, a colleague, Kai, raised an interesting point. The term sounds rather negative, and in truth my description in the previous section was slanted that way. However, Kai points out that It is only through the efforts of these peacock programmers and excessive use of all these features that the rest of us learn where the limits of reasonable use are. It’s a common pattern with many technologies, such as with meta-programming. The first stage is to learn the technology. The next stage is to get excited by this new knowledge and attempt to use it everywhere. It’s then, by seeing it used excessively and understanding the consequences of that, that you learn to regulate its use and ensure it’s appropriate. As Kai sees it, peacock programmers are the early adopters that the rest of us ultimately learn moderation from, and their nature is an essential part of our learning. The focus of this blog post is a real unit test from a live code base that I’ve worked on. I’ve copied the original test here below** as an example that the rest of this blog post is going to focus on. There were some more interesting examples to pick from, but they started to get a little unwieldy  for blogging purposes, so I’ve picked a simpler example. I’ll add some notes at the end on other issues I had found in the larger tests. I’ll also not be concerning myself with actual details of what is being tested; whether the testing is appropriate or well-formed is not the focus here, but rather just the overall structure of the test class. ** Disclaimer – the code and the style have been preserved exactly as the were. I’ve merely renamed classes/methods for anonymity. This test was typical for the code base, and was seen by many as a well,written one. In some ways it is; the tests are small, they’re contained, and they test one thing each. Yet I’ve always been frustrated when I see code like this. I’m a fan of simplicity and try to avoid, as much as I can, the use of superfluous language features. Maybe I’m just stupid (this is quite possible) but I look at a test like this and I really struggle to understand what’s going on. Also important to note is that I’ve picked one of the simpler samples for this post. We ran an analysis on a small set of our tests (the tests for one of our services) to figure out the worst culprits for nested describes/contexts. Our record was seven . That’s right, seven layers of nested description before you finally get to the test body. Good luck trying to figure out what’s actually being tested and finding the relevant bits of setup! Nested describes/contexts can potentially be a way of helping structure your tests, but give it a moment’s thought before you wrap that spec in a context and ask yourself “why am I doing this?” Especially when you’re creating a context with just one test inside it, why not just make the test name more descriptive?  In isolation, the idea of being able to use contexts to help structure and add meaning to your tests is a good one, and I’ve made good use of contexts. But use them sparingly, lift your head regularly from the depths of your spec, and take a look at the whole test file and ask yourself “how’s it looking?”  Once you start going more than a couple of levels deep, perhaps it’s time to rethink and ask yourself why your test is so complicated. e.g., instead of this: …how about this: Bonus: Here’s an example I found from a code base (again, with the names obfuscated; my only replacements are farm and sheep)  that I’ve worked on with 6 layers (I couldn’t find the record-breaking seven-layer spec ? ) Now imagine the message you would see when this test fails… Another feature that I think is worryingly overused is ‘let’. Now I understand that let is lazily evaluated and can improve performance, but we’re talking about unit tests here; running a suite should be taking in the order of seconds. What I’m more concerned about is the time wasted trying to decipher a test class that has ‘let’ blocks scattered all over. Again, my biggest issue with them comes from misuse and misdirection. I have seen many simple unit test classes that have been made so much more verbose and complex through the addition of unnecessary ‘let’ blocks. Their introduction led to an explosion in the scope of a test. No longer can I look at a self-contained ‘it’ block and understand the test—now I need to scroll all around, trying to piece together the nested contexts and follow the chain of ‘let’ blocks to see which data is being set up, and then see which of these many lets my test is overriding… I’m getting anxious just thinking about it (more thoughts on this, and alternative structure can be found in the When, Then, Given section below). This is probably my least-favorite feature. I can’t think of a single time in my own experience where I’ve thought that using ‘subject’ was a good idea and I am really confused as to why people are propagating its use. It just further reduces readability and adds pointless misdirection to your tests. When I first experienced the joys of RSpec after a stint working in Java, I loved the expressiveness and the flow of the test structure. But ‘subject’ feels like a step backwards. It gets even more confusing when your spec is testing something that is happening in the initialization of the class under test. Note: bonus point for combining subject with described_class. We came across a test like this by chance while working on a class, and my pairing partner saw the trailing subject and assumed it was some mistake as it looked like a pretty meaningless line accidentally added at the end of a long test. My partner deleted this line of code without giving it much thought, and then a few minutes later when we ran the test, it naturally started failing. My advice: ditch the pointless subject—new is not always better. Make your tests explicit. I’d much rather see an appropriately named variable that is instantiated within the test body where it’s used as opposed to a cryptic ‘subject’ that was defined way up near the top of the test (or somewhere in the nest of contexts I had to traverse to reach the test). I’m sure you’re starting to see a pattern here. Most of my annoyances are born from testing styles that increase the spread of focus required to understand what is happening. The idea of a unit test is that it tests a unit. I expect them to be concise and self-contained. The way I try to achieve this structure is by following the “ When, Then, Given ” pattern. I’m assuming that you’re going to be familiar with the BDD given/when/then (or the synonymous arrange/act/assert) philosophy of test structure. When I write a unit test, I start by mentally picturing these as three distinct sections of my test. At times, when trying to make this pattern explicit or to share it with a new team. I explicitly express the sections through comments: So far so good. However, the mistake people make from here is to start working through the sections, filling them in. I wouldn’t do that. Start with the When . This should be one line, and the most obvious/easiest to write. When I look at a test written in this format I can immediately recognize the trigger for your scenario. Next, move on to the Then . This is the next easiest section to fill in. What is the expected outcome? And only then should you go back and complete the Given . In order to achieve this result, what data is required? And keep everything in the test! Again, this advice is flexible; I can understand there are times when you’d want to pull things out. But I try to resist this urge as much as possible (in the rewritten version of the test at the end of this post, you’ll notice I did use one ‘let’ block). Just keep the exceptions to this practice under check, or else a few months later your test will have grown into a mess. Keep the discipline. This point brings us back to another reason I hate ‘let’ blocks. You can no longer read a spec and understand why the class under test is behaving the way it does. The data required to achieve its result is now spread all around. In addition, while you’re following all these ‘let’ blocks to see what your test is doing, you’re chasing a load of red herrings, as much of this data is entirely irrelevant to the test’s specific scenario. I couldn’t find a small enough sample to include as a case study of this point, but shared examples are just a nightmare! Imagine all the trouble of going back and forth across a long test file trying to make heads or tails of it, and then multiply that by 10. Just avoid. Back to our case study: I’ve attempted to rewrite a number of tests using the style I’m recommending, and the first time I tried, I truly did expect there to be more lines of code. I was OK with that; I was willing to accept that as a trade-off for the increased readability and comprehensibility of the tests. The original test was 162 lines of code. The rewrite…101. Technically speaking, there is more code, the lines are longer, but there are fewer of them, and they conveyed more meaning. I’ve seen the same result in almost every test I’ve rewritten, and I was surprised. So here’s the finished result. This is the above test as I would have written it. It’s not perfect and perhaps it’s just me, but I find this style so much easier to comprehend and get my head around.", "date": "2015-08-06"},
{"website": "Ebay-Engineering", "title": "Fast Approximate Logarithms, Part III: The Formulas", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/engineering/fast-approximate-logarithms-part-iii-the-formulas/", "abstract": "In this post I finally get to the punch line and construct a set of approximations to (actually ) that cover a range of speeds and accuracies. As execution time decreases, so does the accuracy. You can choose an approximation with a tradeoff that meets your needs. When making a fast but approximate function, the design parameter is the form of the approximating function. Is it a polynomial of degree 3? Or the ratio of two linear functions? The table below has a systematic list of possibilities: the top half of the table uses only multiplication, the bottom half uses division. Table 1: Approximating Functions The rewritten column rewrites the expression in a more efficient form. The expressions are used in the approximation procedure as follows: to get the approximation to , first is reduced to the interval , then is substituted into the expression. As I explained in Part I and Part II , this procedure gives good accuracy and avoids floating-point cancellation problems. For each form, the minimax values of are determined—that is, the values that minimize the maximum relative error. The bits column gives the bits of accuracy, computed as where is the maximum relative error. This value is computed using the evaluation program from my Part II post, invoked as . The “cost” column is the execution time. In olden days, floating-point operations were a lot more expensive than integer instructions, and they were executed sequentially. So cost could be estimated by counting the number of floating-point additions and multiplications. This is no longer true, so I estimate cost using the evaluation program. I put cost in quotes since the numbers apply only to my MacBook. The numbers are normalized so that the cost of log2f in the standard C library is 1. It’s hard to grasp the cost and time numbers in a table. The scatter plot below is a visualization with lines 2–10 each represented by a dot. You can see one anomaly—the two blue dots that are (roughly) vertically stacked one above the other. They have similar cost but very different accuracy. One is the blue dot from line 9 with cost .43 and 7.5 bits of accuracy. The other is from line 3 which has about the same cost (0.42) but 8.5 bits of accuracy. So clearly, line 9 is a lousy choice. I’m not sure I would have guessed that without doing these calculations. which means that the denominators must be roughly equal. Multiplying each by , this becomes . The only way this can be true is if and are very large, so that it’s as if the term doesn’t exist and the approximation reduces to the quotient of two linear functions. And that is what happens. The optimal coefficients are on the order of , which makes (now writing in terms of ) . Plugging in the optimal values , , gives , , which are very similar to the coefficients in line 7 shown in Table 2 later in this post. In other words, the optimal rational function in line 9 is almost identical to the one in line 7. Which explains why the bit accuracy is the same. In the next plot I remove line 9, and add lines showing the trend. The lines show that formulas using division outperform the multiplication-only formulas, and that the gain gets greater as the formulas become more complex (more costly to execute). You might wonder: if one division is good, are two divisions even better? Division adds new power because a formula using division can’t be rewritten using just multiplication and addition. But a formula with two divisions can be rewritten to have only a single division, for example Two divisions add no new functionality, but could be more efficient. In the example above, a division is traded for two multiplications. In fact, using two divisions gives an alternative way to write line 10 of Table 1. On my Mac, that’s a bad tradeoff:  the execution time of the form with 2 divisions increases by 40%. Up till now I’ve been completely silent about how I computed the minimax coefficients of the functions. Or to put it another way, how I computed the values of , , , etc. in Table 1. This computation used to be done using the Remez algorithm, but now there is a simpler way that reduces to solving a convex optimization problem. That in turn can then be solved using (for example) CVX, a Matlab-based modeling system. Here’s how it works for line 8. I want to find the minimax approximation to . As discussed in the first post of this series, it’s the relative error that should be minimized. This means solving This is equivalent to finding the smallest for which there are , , and satisfying For the last equation, pick . Of course this is not exactly equivalent to being true for all but it is an excellent approximation. The notation may be a little confusing, because are constants, and , and are the variables. Now all I need is a package that will report if has a solution in , , . Because then binary search can be used to find the minimal . Start with that you know has no solution and that is large enough to guarantee a solution. Then ask if the above has a solution for . If it does, replace ; otherwise, . Continue until has the desired precision. The set of satisfying the equation above is convex (this is easy to check), and so you can use a package like CVX for Matlab to quickly tell you if it has a solution. Below is code for computing the coefficients of line 8 in Table 1. This matlab/CVX code is modified from http://see.stanford.edu/materials/lsocoee364a/hw6sol.pdf . It is a peculiarity of CVX that it can report for a value of , but then report for a smaller value of . So when seeing I presume there is a solution, then decrease the upper bound and also record the corresponding values of , , and . I do this for each step of the binary search. I decide which to use by making an independent calculation of the minimax error for each set . You might think that using a finer grid (that is, increasing so that there are more ) would give a better answer, but it is another peculiarity of CVX that this is not always the case. So in the independent calculation, I evaluate the minimax error on a very fine grid that is independent of the grid size given to CVX. This gives a better estimate of the error, and also lets me compare the answers I get using different values of . Here is the CVX code: Here are the results of running the above code for the expressions in the first table. I don’t bother giving all the digits for line 9, since it is outperformed by line 7. Table 2: Coefficients for Table 1 So, what’s the bottom line? If you don’t have specific speed or accuracy requirements, I recommend choosing either line 3 or line 7. Run both through the evaluation program to get the cost for your machine and choose the one with the lowest cost. On the other hand, if you have specific accuracy/speed tradeoffs, recompute the cost column of Table 1 for your machine, and pick the appropriate line. The bits column is machine independent as long as the machine uses IEEE arithmetic. Finally, I’ll close by giving the C code for line 8 (almost a repeat of code from the first posting). This is bare code with no sanity checking on the input parameter . I’ve marked the lines that need to be modified if you want to use it for a different approximating expression. In response to comments, here is some sample code to compute logarithms via table lookup. A single precision fraction has only 23 bits, so if you are willing to have a table of 2 23 floats (2 25 bytes) you can write a logarithm that is very accurate and very fast. The one thing to watch out for is floating-point cancellation, so you need to split the table into two parts (see the log_biglookup() code block below). The log_lookup() sample uses a smaller table.  It uses linear interpolation, because ordinary table lookup results in a step function. When x is near 0, log(1+ x ) ≈ x is linear, and any step-function approximation will have a very large relative error. But linear interpolation has a small relative error. In yet another variation, log_lookup_2() uses a second lookup table to speed up the linear interpolation. Powered by QuickLaTeX", "date": "2015-06-25"},
{"website": "Ebay-Engineering", "title": "Collocations: The Secret Web of Language", "author": ["Jose Sanchez"], "link": "https://tech.ebayinc.com/research/collocations-the-secret-web-of-language/", "abstract": "Imagine this. You are a beginning English learner. You enjoy the methodical approach, so you tackle the language systematically, memorizing lists of irregular verbs, spelling norms, and syntactic rules. No conversational practice, no watching movies. You want to get the theory right first. One day, you think you have mastered it. You are a walking grammar book. You after 6 months of English studies Then you realize you have been so engrossed in your studies that you skipped lunch, so you ask a passer-by: Excuse me, sir. I am heavily hungry . Could you point me to the nearest swift-food restaurant? Which he greets with a baffled stare. What went wrong? You studied the standard rules of English, but there is a part of the language (of any language) that will never fit in that tidy set of axioms — collocations . Collocations — a vast n-gram web that connects all words in a language. According to the Oxford English Dictionary , collocations are pairs or groups of words that are habitually juxtaposed, such as strong coffee or heavy drinker . As such, they are the final touch foreign learners (or say, machine translation systems) need to “speak properly.” You can communicate without knowing them, but you will sound pretty weird to the native ear. In a wider sense, collocations are a sort of lightweight idioms , but they differ from idioms in a couple of ways: Their meaning is transparent — you can guess it the first time you see them (which you can’t with proper, metaphorical idioms, such as kick the bucket or a piece of cake ). There are vastly more collocations than idioms. (The largest explanatory collocation dictionary in existence covers only 1% of all possible vocabulary.) Most importantly, collocations don’t follow clear rules. There is no apparent reason why we should say a burning thirst and not a blazing thirst , except that most people say the former and not the latter. In a way, these whimsical word patterns are like an unexplored realm at the edges of grammar — a lush rainforest with all sorts of curious and apparently random species. At the edge of the forest, the human language learner and the MT system both face the same problem — how to chart it? Playing Linnaeus to the human language “biosphere” is no trivial task, but fortunately there is help — massive computational power applied to vast sets of texts (linguistic corpora) is producing resources for us all: Humans: Collocation (aka Combinatory) Dictionaries, some of them online: The Online Oxford Collocation Dictionary (also with a convenient app ) The Diccionario de Colocaciones del Español Zanichelli´s Dizionario delle collocazioni The French-Canadian Dictionnaire des Cooccurrences The Online Oxford Collocation Dictionary (also with a convenient app ) The Diccionario de Colocaciones del Español Zanichelli´s Dizionario delle collocazioni The French-Canadian Dictionnaire des Cooccurrences Machine Translation Systems: Statistical Language Models , that is, tables assigning a likelihood score to sets of words based on their frequency. MT Systems usually produce several “candidate” translations for a given source. Each of them is checked against the Language Model table, and the one with the highest score (the most frequent expression in the language) is used. The work with collocations is far from over. For MT, the challenge is finding enough corpora. (Except for a few — such as English, French, and Spanish — most languages don’t have enough online texts to create accurate models.) For human learners, there is the additional problem of analyzing and describing the vast amount of data in terms useful to the language student. The good news is that here, as in other areas, human linguists and MT systems can leverage each other’s efforts. Every new language model provides helpful data that can be used by the next generation of dictionaries, and every dictionary throws new light on the relationship patterns between words that MT can incorporate. Meanwhile, language students will do well heading to the pub every once in a while for some conversational practice. Want to learn more? Go academic — Check out Mike Dillinger’s and Brigitte Orliac’s paper on Collocation extraction for MT . Go deeper — Learn about collocations’ extended family: the phrasemes . Go pro — Find your own collocations: Juan Rowda’s article on MT tools (the “Concordance Tool” section) tells you how. And if you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2016-04-25"},
{"website": "Ebay-Engineering", "title": "The Biggest Dictionary in the World", "author": ["Jose Sanchez"], "link": "https://tech.ebayinc.com/research/the-biggest-dictionary-in-the-world/", "abstract": "By now, we are all familiar with the use of machine translation to produce versions of a text in different languages. Did you know, though, that scientists are also working on producing a massive multilingual dictionary with minimal human intervention? And when I say massive, I mean over 9,000 language varieties – in other words, all the languages in the world . The project, named Panlex , started in 2004, and there is nothing comparable to it. Google Translate , for instance, covers a meager 103 languages. So how does Panlex work? The math is complicated, but the logic isn’t. Example: Let’s say you want to translate the Swahili word ‘nyumba’ (house) into Kyrgyz (a Central Asian language with about 3 million speakers). You are unlikely to find a Swahili–Kyrgyz dictionary; if you look up ‘nyumba’ in PanLex, you’ll find that even among its >1 billion direct (attested) translations, there isn’t any from this Swahili word into Kyrgyz. So you ask PanLex for indirect translations via other languages. PanLex reveals translations of ‘nyumba’ that, in turn, have four different Kyrgyz equivalents. Of these, three (‘башкы уяча’, ‘үй барак’, and ‘байт’) have only one or two links to ‘nyumba’. But a fourth, ‘үй’, is linked to it by 45 different intermediate language translations. You look them over and conclude that ‘үй’ is the most credible answer. One of the beauties of this system is that it works as a semantic web, with no central hub. Most multilingual projects (including MT) have to rely on a pivot language (often English) in order to harvest translations between two minority ones (say, Basque and Quechua ). With the Panlex model, this is not necessary; translations are validated without having to go through the English “funnel.” I like this example of technology working for the little (language) guy. And there is more. Panlex’s sponsor, The Long Now Foundation , has some other interesting stuff in the works, like the 10,000 Year Clock or the Rosetta Project , all centered on making us take the really long-term view on the world. If you are local to the San Francisco Bay Area, you are in luck — you can explore their events at the Blog of the Long Now . And if you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2016-05-03"},
{"website": "Ebay-Engineering", "title": "The Path to JavaScript Next", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/the-path-to-javascript-next/", "abstract": "Like all other JavaScript enthusiasts around the globe, engineers at eBay were eagerly watching the space of ECMAScript 6. We have been playing around with the new syntax and trying it in a few internal projects, but nothing concrete happened at an organizational level. Once the spec was standardized in June 2015, however, there was a sudden excitement around it, and it was at both ends of the spectrum. Some engineers wanted to just dive in and use all ES6 features in production applications, whereas other engineers were very skeptical about the new shiny stuff and would rather spend time improving the product or fixing other low-hanging fruits. Obviously we needed a balance, and this post summarizes how we started adopting ES6 into eBay’s ecosystem. Our approach should resonate with engineers in big organizations with huge codebases that span across multiple domains. Before getting started, we needed to convince ourselves of the need for ES6. Yes, ES6 is cool, our industry peers are using it, and engineers can learn new stuff (good for their careers), but how effective is it as a technology, for our product as well as the overall organization? Also, engineers are comfortable with ES5 and things are going well. So why ES6? Firstly, ES6 is official. It is no longer a group of proposals that browsers have eagerly implemented. The TC39 committee has approved the spec and the next version of JavaScript is finally a reality. As with any other programming language, we eventually need to move to the newer improved versions, so why not start now? ES6 represents the biggest change to the language since its inception. Although there have been five earlier editions to the language, all of them were incremental revisions, none at this scale. Also, going forward there will be ongoing new additions to the language, some of which built on top of ES6 concepts. So the sooner we start adopting ES6, the more likely we can leverage the new powerful features. One of the things that ES6 clearly succeeded in is making JavaScript very expressive. A good programming language should provide the syntax to express something from our mind into code as easily and quickly as possible. JavaScript is great in many aspects, but even with ES5, expressing a simple logic (such as default parameters ) or a common pattern (such as Object.assign ) required many lines of code or a custom abstraction or an external library. ES6 has now made it easy and straightforward, thus increasing productivity. Finally, ES6 fixes some of the confusing aspects of the language. Even experienced JavaScript developers get confused by things like variable hoisting, the this keyword in callback functions, and prototypical inheritance. With ES6 offering much better ways of programming, these JavaScript intricacies will be gone. The end result is more elegant and bug-free code and easier training of new developers into JavaScript. Now that we had some answers to the question “Why pursue ES6?” the next step was to deep dive into ES6 features and figure out which of them are most useful to our world. Which should we start with? ES6 has introduced a large set of features to the current JavaScript world. Should we go ahead and embrace all of them in our application code or be selective in what we use? Here is where we needed to strike a balance and be cognizant of various aspects before adopting a feature: performance, debugging, maturity, day-to-day use cases, and roadmap (ES7 and beyond), for example. This analysis also boosted the confidence of engineers who were skeptical about the whole ES6 momentum. At a high level, ES6 features can be grouped under two buckets: syntactic sugars and new features. Syntactic sugars are those which are technically possible now but require many lines of code or a custom abstraction or a library to achieve (such as classes and default parameters). New features, as the name suggests, are something newly introduced in ES6. The inspiration for the new feature could be from either a popular library or another language, or it could be a completely new feature (such as the Promise object, generators, and the Proxy object). With this background, we did some deep dives, followed by discussions with a few folks, and finally shortlisted the below set of ES6 features to get started on. Block Scoping Extended Parameter Handling: Default Parameters, Rest Parameters, and the Spread Operator Arrow Functions Template Literals Destructuring Promises Below are some of the reasons why only the above features were shortlisted: Common use case: The above list covers most of our day-to-day coding use cases. We went through a major portion of our critical code and observed that these were the missing patterns and features for which a library was used or sometimes handwritten. With the code in front of us, we were looking at the features that can be immediately applied, most of which made it to the final list. ES6 offers some powerful features like Proxies, Reflection, and Symbols. But as the saying goes, “With great power comes great responsibility”. These power features need to be used wisely to get the most benefit out of them. Also, use of these features in application code is very rare, and they are more suited for meta programming or building libraries. Performance: The other important criterion for a feature to be shortlisted was performance, both native and transpiled (ES6 to ES5) performance. The Six Speed performance comparison was a good place to start with. In addition, we did a little bit of our own benchmarking. With all results in hand, the above features were selected. Most of the shortlisted features had identical performance when compared to the ES5 equivalent or just native ES6. Features like Map and Set are useful, but our tests indicated that they still need more maturity to be ready for prime time. Also, we did not feel the desperate need to use them right away. Productivity and debugging: Before shortlisting the features, we tried using a lot of them in internal projects just to get a feel of the whole process. The features that we shortlisted were the ones that boosted our productivity tremendously after a short learning curve. Some of these features were so appealing that we immediately started missing them when writing in ES5. Another aspect to it is debugging. The shortlisted features were easy to debug even in the transpiled version, as most of them are simple wrappers that don’t affect the core logic. Growing list: Lastly, this list is not final. It is an initial vetted set of ES6 features to get started and feel comfortable with (in terms of not bringing additional overhead to the site). As and when the need for other features arises, we will quickly do our checks and expand the list. To that point, the new ES6 module syntax is at the top of our next review. Other than just establishing an official syntax for JavaScript modules, the new module syntax also bring in tree-shaking capabilities, which is incredible. At eBay we are immersed in the CommonJS world of writing isomorphic JavaScript. Unfortunately, that means adopting a new module system will take time. Classes are another ES6 feature that we may start using soon, although there is not much excitement around it. This exercise also helped us decide what ES6 features to avoid now. One of these is the new ES6 generators. Although they bring in great functionality, they are still not ready for prime time. In addition to performance problems, we feel that the upcoming Async/ Await feature offers a better alternative to generators, providing a much cleaner syntax and readability. So for now we are a strict NO to generators. Now that we have a clear picture on ES6, the next thing was to put it in motion. The first obvious thing was to decide on a transpiler , and the unanimous choice was Babel . In the beginning, we will transpile all ES6 unconditionally to ES5. We know it is not ideal , as we are sending ES5 code even to browsers that support ES6. This is just a short-term solution until we figure out a smarter (browser-based or feature-based) transpiling mechanism, which is already in the works. The below steps highlight the things we have put in place to get started with ES6. Our module bundler and asset pipeline tool, Lasso , was updated to support ES6 transpilation through Babel. For now, we will only transpile files with the .es6 extension. This was done as a preemptive action to avoid running the transpiler on the vast existing JavaScript code and introduce any production regression. We wanted to keep the production environment stable when working on ES6, and this was one way to achieve it. This also helps test the stability and speed of the transpilation process in various environments (dev, QA, and production) with a limited set of files. This restriction may be removed in the future as ES6 adoption grows. In the Node.js world, we do not transpile the code, as version 4.x supports almost all shortlisted features natively. We created an ESLint config similar to eslint-config-airbnb , which will enforce ES6 coding guidelines and code styles for the shortlisted features. It is an integrated part of developer IDEs and CI jobs, and violations will be reported for each build. Using non-recommended features (for example, generators) will also be reported as errors. This is a critical step to guide engineers in the right direction when starting with ES6, as they get trained with the best practices from day one. Our recommendation to developers newly starting with ES6 is to start writing unit tests with ES6. Unit testing seems to be a good place to try out and get comfortable with the various features and syntax of ES6. It can be used as a playground to ramp up in. Since unit tests are not shipped to production, developers can be more proactive and confident to try things that otherwise require caution. Once comfortable, you can get going with new application files created in ES6. We did a couple of internal Brown Bags to spread ES6 awareness and ramp up developers on new features. This will also be added as a part of our new hire training. It will be an ongoing activity, with the training content updated with our latest recommendations. As mentioned earlier, smart transpiling is something we want to set up in our build and release process. We do not have an exact answer yet, but it is at the top of our queue. We will onboard more ES6 features (especially the new module syntax) as and when our criteria are met. We will keep this exercise going to bring in newer JavaScript features as and when they reach stage 4 . This summarizes the path we took to bring ES6 into eBay. Our main goal was to organically adopt ES6, instead of it being a forcing function. Teams have slowly started using the new features and the feedback is very positive. Lastly, two online resources “ Understanding ECMAScript 6 ” and “ Exploring ES6 ” have been incredibly helpful for our research and analysis. Huge thanks to Nicholas Zakas and Axel Rauschmayer for making it available. – Senthil", "date": "2016-03-15"},
{"website": "Ebay-Engineering", "title": "Announcing Marko v3: From HTML to HTML-JS", "author": ["Patrick Steele-Idem"], "link": "https://tech.ebayinc.com/engineering/announcing-marko-v3-from-html-to-html-js/", "abstract": "Marko is one of the fastest, lightest, and most powerful HTML templating engines for Node.js and the browser, and we are very pleased to see a healthy and growing community. Marko has been downloaded over 100k times in the first few months of 2016, and the project has a very active Gitter chat room . We are excited to announce some huge improvements to the Marko templating engine as part of the v3 release. Marko v3 introduces a new HTML-JS syntax and a new parser that makes Marko more intuitive. Marko’s clean, HTML-based syntax has been a strength, but over time it became clear that using a strict HTML parser was actually putting unnecessary constraints on the Marko language that negatively impacted code readability and usability. The new HTML-JS syntax that ships with Marko v3 breaks away from the limitations associated with the standard HTML syntax while still maintaining the look and feel of HTML. Read more on the MarkoJS blog.", "date": "2016-03-23"},
{"website": "Ebay-Engineering", "title": "Application Resiliency Using Netflix Hystrix", "author": ["Senthilkumar Gopal"], "link": "https://tech.ebayinc.com/engineering/application-resiliency-using-netflix-hystrix/", "abstract": "Resilience is the ability of the network to provide and maintain an acceptable level of service in the face of various faults and challenges to normal operation. -Wikipedia Ever since the term services and recently microservices came into usage, application developers have been converting monolithic APIs into simple and single-function microservices. However, such conversions come with the cost of ensuring consistent response times and resiliency when certain dependencies become unavailable. For example, a monolithic web application that performs a retry for every call is potentially resilient to some extent, as it can recover when certain dependencies (such as databases or other services) are unavailable. This resilience comes without any additional network or code complexity . For a service that orchestrates numerous dependencies, each invocation is costly, and a failure can lead to diminished user experience as well as to higher stress on the underlying system that is attempting to recover from the failure. Consider a typical use case:  An e-commerce site that is overloaded with requests on Black Friday, and the vendor providing the payment operations goes offline for a few seconds due to heavy traffic. The users begin to see long wait times for their checkouts due to the high concurrency of requests. These conditions also keep all of the application servers clogged with the threads that are waiting to receive a response from the vendor. After a long wait time, the eventual result is a failure. Such events lead to abandoned carts or users trying to refresh or retry their checkouts, increasing the load on the application servers—which already have long-waiting threads, leading to network congestion. A circuit breaker is a simple structure that constantly remains vigilant, monitoring for faults. In the above-mentioned scenario, the circuit breaker identifies long waiting times among the calls to the vendor and fails-fast, returning an error response to the user instead of making the threads wait. Thus, the circuit breaker prevents the users from having a very sub-optimal response time. The basic idea behind the circuit breaker is very simple. You wrap a protected function call in a circuit breaker object, which monitors for failures. Once the failures reach a certain threshold, the circuit breaker trips, and all further calls to the circuit breaker return with an error, without the protected call being made at all. Usually you’ll also want some kind of monitor alert if the circuit breaker trips. -Martin Fowler Recovery time is crucial for the underlying resource, and having a circuit breaker that fails-fast without overloading the system ensures that the vendor can recover quickly. A circuit breaker is an always-live system keeping watch over dependency invocations. In case of a high failure rate, the circuit breaker stops the calls from going through for a small amount of time, rather than responding with a standard error. In earlier times, we used a simple setup called AUTO_MARK_DOWN , which prevented such long wait problems in dependencies by short-circuiting the invocations until they were brought back up via MARK_UP. A bot periodically checks for AUTO_MARK_DOWN on various machines for each of its dependencies and performs MARK_UP . However, the bot and MARK_UP infrastructure is not embedded within the system, but rather located externally. Due to the absence of live and constant feedback about the request volume and failure rates, the MARK_UP of a failing system dependency would occur without verifying its availability. Relying on this setup also leads to false positives, as the bot system is outside the client and cannot evaluate the continuity of failures. Another major flaw with the setup is the absence of a comprehensive and real-time monitoring structure for all of the dependencies of any application. This old system is slow and erratic, does not have ongoing telemetry, and blindly marks all systems back up on the assumption that the application will AUTO_MARK_DOWN a dependency in the event of further failures. The result is unpredictable behavior and incorrect evaluation. A circuit breaker takes care of tripping the dependencies at the appropriate time. However, a more sophisticated system needs to continue the vigilance to determine if the dependency is available, and if so to close the circuit again to let dependent calls go through. This behavior can be achieved in two ways: Allow all calls to go through during a regular time interval and check for errors. Allow one single call to go through at a more frequent rate to gauge the availability. AUTO_MARK_DOWN was a variant of Type 1, where the circuit is closed without any proof of recovery, relying on errors to identify an issue. Type 2 is a more sophisticated mechanism as it does not allow multiple calls to go through because the calls might take a long time to execute and still fail. Rather, allowing  only a single call ensures more frequent execution, enabling faster closure of the circuit and revival of the system. A harmonious system is one where we have an ideal circuit breaker, real-time monitoring, and a fast recovery variable setup, making the application truly resilient. Circuit Breaker + Real-time Monitoring + Recovery = Resiliency – Anonymous Using the example of the e-commerce site from above, with a resilient system in place, the circuit breaker keeps an ongoing evaluation of the faults from the payments processor and identifies long wait times or errors from the vendor. On such occurrences, it breaks the circuit, failing fast. As a result, users are notified of the problem and the vendor has enough time to recover. In the meantime, the circuit breaker also keeps sending one request at regular intervals to evaluate if the vendor system is back again. If so, the circuit breaker closes the circuit immediately, allowing the rest of the calls to go through successfully, thereby effectively removing the problem of network congestion and long wait times. Hystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure and enable resilience in complex distributed systems where failure is inevitable. -Netflix From its inception in 2012, Hystrix has become the go-to solution for many systems attempting to improve their capabilities to sustain failures. It has a fairly mature API and a highly tunable configuration system, enabling application developers to provide optimal utilization of their underlying service dependencies. The following state diagram and narrative depicts how a resilient system functions during various states of a circuit breaker’s lifecycle. When a system is functioning smoothly, the resiliency is measured by the state of its success counters, while any failures are tracked using the failure gauges. This design ensures that when the threshold for failures is reached, the circuit breaker opens the circuit to prevent further calls to the dependent resource. At this juncture, every call to the dependency is short-circuited with a HystrixRuntimeException and FailureType of SHORTCIRCUIT, giving clear indication of its cause. Once the sleepInterval passes, the Hystrix circuit breaker moves into a half-open state. In this state, Hystrix takes care of sending the first request to check system availability, letting other requests fail-fast until the response is obtained. If the call is successful, the circuit breaker is reset to Closed; in case of failure, the system goes back to the Open state, and the cycle continues. Hystrix Github has comprehensive documentation of how to use the library. It is as simple as creating a class for invoking the Hystrix library for service consumption. Reference: https://github.com/Netflix/Hystrix/wiki/Getting-Started Internally, this class utilizes the RxJava library to perform asynchronous invocation of the service dependencies. This design helps the application manage its resources intelligently by using the application threads to its maximum potential. For application developers who perform parallel processing and manage their dependencies using lazy invocations, Hystrix also exposes Future<?> and Observable<?> . Multiple applications within eBay have started using Hystrix either as a standalone library or  with our platform wrappers. Our platform wrappers exposes the Hystrix configurations in the form of JMX beans for centralized management.  Our wrappers also inject custom Hystrix plugin implementations to capture the real-time metrics being published and to feed them to the site monitoring systems for critical applications. The Hystrix dashboard is integrated as part of the core server-monitoring systems, enabling teams to view how their application dependencies are performing during various times of the day. The execution hook provided by Hystrix is a critical component of this integration, as it helps monitor/alert various failures in real time—especially on errors and fallback failures, thereby helping investigate and resolve issues more quickly with little to no user impact. eBay hosts a slew of service APIs for both internal and external consumption. All of these services are authenticated via tokens, with the Secure Token service acting as the issuer and validator of these tokens. The Guards in all of the services are now upgraded with the Hystrix-based circuit breaker, which enables the Secure Token service to be highly available. In times of heavy traffic from one of the services, the circuit breaker for that service trips and opens the circuit, failing calls only to that specific service while allowing the other services to function normally. Secure Token Service protected using Hystrix The circuit breaker is the default one available through the Hystrix library. The functioning of the circuit breaker can be summarized as follows: Every incoming call is verified against the current state of the circuit breaker. A Closed state of the Circuit allows the requests to be sent through. An Open state fails all requests. A Half-Open state (which occurs when the sleep time is completed), allows one request to go through, and on success or failure moves the circuit to the Closed or Open state as appropriate. Hystrix is not just a circuit breaker, but also a complete library with extensive monitoring capabilities, which can be easily plugged into existing systems. We have started exploring the usage of the library’s Request Collapsing and Request Caching abilities for future use cases. There are a few other Java-based implementations available, such as Akka and Spring circuit breakers; but Hystrix has proven to be a sound and mature library for maintaining a resilient environment for our critical applications, providing high availability during any time period. http://wiki.ittc.ku.edu/resilinets_wiki/index.php/Definitions#Resilience Martin Fowler Hystrix Github http://techblog.netflix.com/2012/11/hystrix.html https://github.com/Netflix/Hystrix/wiki http://techblog.netflix.com/2011/12/making-netflix-api-more-resilient.html", "date": "2015-09-08"},
{"website": "Ebay-Engineering", "title": "Running Arbitrary DAG-based Workflows in the Cloud", "author": ["Sachin Tilloo"], "link": "https://tech.ebayinc.com/engineering/running-arbitrary-dag-based-workflows-in-the-cloud/", "abstract": "We were working on a problem that involved processing workflows in the cloud. The problem was how we design a system that is scalable and fault tolerant, allowing dynamic allocation of cloud nodes to every step in the workflow. To explain with an example, imagine a workflow like the following: Diagram A We need to execute A based on a trigger, followed by B; then we need to execute C, D, and E in parallel; and when they are complete, we need to execute F, which will trigger the end of the flow. There is a strict dependency hierarchy in the above graph that needs to be followed as we visit each node. The problem was that we needed to process a series of different independent workflow graphs like the above, but depending on the trigger. So we were faced with two potential issues: How do we execute different workflows in a generic, coherent manner? How do we scale our solution so that we have a system where we can run thousands or more arbitrary workflows concurrently in the cloud? We started with a brute-force idea:  Why not just hard-code all of the workflows (initially we had just a handful)? We could have, for instance, six nodes running each of the tasks corresponding to the diagram above, and the tasks could communicate state information via a shared database. This primitive approach worked well initially. All of the tasks could run concurrently on different nodes and would start doing actual work only when the dependency was met as per the database state. However, we started facing problems when the number of workflow variations increased. We also had an issue hard-coding the dependency above since, for instance, task B depended on A and it was harder to insert a new task between the two.  Moreover, the larger we got the trickier it became to maintain, enhance, and optimize the system. We started hashing out ideas on an effective solution that is quick and easy and that can work with all of the eBay configurations. We looked at each problem and took a deep dive on it. How do we execute different workflows in a generic, coherent manner? The best approach we came up with for this first problem was to use metadata services. Each of the workflows was defined as part of the metadata based on the trigger point type. So for instance, for the above diagram we defined metadata that explicitly stated these dependencies.  There are multiple ways this approach can be accomplished. One way is to use the GraphML library. We can then define the above graph using GraphML XML to define all of the dependencies. If the use case is a lot simpler, another way might be to create a table of tasks, names, and sequence numbers showing the ordering to be followed. The actual representation of the graph-based workflow can be done in multiple ways, and then we need services to assemble the metadata and isolate the implementation details from all of the clients. This first problem was the easier one to deal with. How do we scale our solution so that we have a system where we can run thousands or more arbitrary workflows concurrently in the cloud? To deal with this second problem, we wanted to start by getting rid of the static bounding of tasks and nodes. The aim was to make the bounding dynamic based on load on the platform. So we wanted to come up with two things: A way of assigning and running tasks on a node A way of load-balancing the above assignment The second item immediately prompted us towards a load balancer. It’s a fairly common approach now, and we needed just that:  a load balancer that will know the state of the pool and allocate tasks to all nodes on the cloud. Therefore, we needed a way to describe the state of each node. We came up with a simple format consisting of node name, tasks to execute, each task’s current run status, etc. The load balancer then has to keep track of this data, which represents what’s happening within the pool. Based on this data, the load balancer can decide which nodes have fewer tasks or are overloaded, and can allocate accordingly. Now onto the first item:  once we have this data we can have a daemon service/task running on all of the nodes in the pool which will then just look for this data and spawn the given task that is assigned to it. The last thing we wanted was to form a queue of all incoming trigger types, which would result in a series of workflows, and then to dynamically come up with the workflow graph by reading this queue. We were putting all of these dependencies in place and came up with a really simple, elegant, yet powerful solution that achieved what we wanted to do. The platform met all of our constraints and was scalable and, from an implementation perspective, easy to start with. Our project actually involved more business logic that corresponded to the specific workflows, but we also came up with this generic platform for putting it all together. Our final design looked something like this: Diagram B (For simplicity, other design aspects like monitoring, disaster recovery, and node provisioning are omitted from the above diagram.) Let’s go through some details about each of the above components. Input Queue Manager — The role of an Input Queue Manager is to maintain a queue of what’s to be processed by taking the input events and sorting them by time of arrival.  Note that these events can be input files, crontab-like scheduled tasks, or any set of external services/dependencies that need a workflow to be processed downstream.  The role of the Input Queue Manager is also to avoid duplicates and discard invalid requests going into the pipeline. Workflow Manager — This is one of the core components of the platform. Given an input queue type, the Workflow Manager looks for the workflow metadata services to get an instance of workflow tasks to be executed. The metadata services have pre-configured data backed by all possible input types that result in the workflow. Again referring to Diagram A above, this sample workflow can be configured as an input to a given file, a service call, etc.  The output is a queue of workflow task instances that need to be executed as per the graph ordering. Task Scheduler — The primary purpose of this component is to schedule tasks on any node in the pool. Figuring out a good node to schedule requires the help of a load balancer, which will return a healthy node back to the Task Scheduler. It then uses the Task Assignment Service to allocate a task to the selected node. Load Balancer — This component calls the Task Allocation Service, which constructs the complete graph of the state of the pool at any given instant. The Load Balancer then uses this graph to come up with the best possible node that can be assigned to process a given task. Load balancing can be implemented in multiple ways, from round robin to random to load-based. The Load Balancer can also be extended to have knowledge of all of the tasks in the system, and can use machine learning to extract average times, based on past performances, as training data. Again, depending on the use case, load balancing can be altered as the platform matures. Node Executor — This is a relatively simpler component. The primary function of this is to find out all the assigned tasks and then just spawn them as separate processes on that node. All the nodes in the pool need to have these executors as daemon processes which look out for any new tasks that need to be spawned.  Each task that runs can do a heartbeat ping to these executors so that they can be made smarter in cases where things get stuck or node restarts. The above model can easily be extended to provide better fault tolerance by putting master/slave dependencies on components. Also, cases like node restarts, nodes going down when tasks are running, etc. can be accommodated by adding a monitor to the pool to keep track of what is going on in the overall system, and then using the Load Balancer to perform the necessary steps for automatic failover/recovery. Note that using the above system we were not only able to process DAG workflows in the cloud, but also were able to process simple crontab-scheduled tasks dynamically and not bounding it to any particular node in the cloud. We are in the process of open-sourcing some of our solution. We’ll announce the GitHub location once we’re ready.", "date": "2016-04-05"},
{"website": "Ebay-Engineering", "title": "A Surprising Pitfall of Human Judgement and How to Correct It", "author": ["David Goldberg"], "link": "https://tech.ebayinc.com/research/a-surprising-pitfall-of-human-judgement-and-how-to-correct-it/", "abstract": "Algorithms based on machine learning, deep learning, and AI are in the news these days. Evaluating the quality of these algorithms is usually done using human judgment. For example, if an algorithm claims to detect whether an image contains a pet, the claim can be checked by selecting a sample of images, using human judges to detect if there is a pet, and then comparing this to the results to the algorithm. This post discusses a pitfall in using human judgment that has been mostly overlooked until now. In real life, human judges are imperfect. This is especially true if the judges are crowdsourced. This is not a new observation. Many proposals to process raw judgment scores and improve their accuracy have been made. They almost always involve having multiple judges score each item and combining the scores in some fashion. The simplest (and probably most common) method of combination is majority vote: for example, if the judges are rating yes/no (for example, is there a pet), you could report the rating given by the majority of the judges. But even after such processing, some errors will remain. Judge errors are often correlated, and so multiple judgments can correct fewer errors than you might expect. For example, most of the judges might be unclear on the meaning of “pet”, incorrectly assume that a chicken cannot be a pet, and wrongly label a photo of a suburban home showing a child playing with chickens in the backyard. Nonetheless, any judgment errors remaining after processing are usually ignored, and the processed judgments are treated as if they were perfect. Ignoring judge errors is a pitfall. Here is a simulation illustrating this. I’ll use the pet example again. Suppose there are 1000 images sent to the judges and that 70% of the images contain a pet. Suppose that when there is no pet, the judges (or the final processed judgment) correctly detect this 95% of the time. This is actually higher than typical for realistic industrial applications. And suppose that some of the pet images are subtle, and so when there is a pet, judges are correct only 90% of the time. I now present a simulation to show what happens. Here’s how one round of the simulation works. I assume that there are millions of possible images and that 70% of them contain a pet. I draw a random sample of 1000. I assume that when the image has a pet, the judgment process has a 90% chance reporting “pet”, and when there is no pet, the judgment process reports “no pet” 95% of the time. I get 1000 judgments (one for each image), and I record the fraction of those judgments that were “pet”. That’s one round of the simulation. I do the simulation many times and plot the results as a histogram. I actually did 100,000 simulations, so the histogram has 100,000 values. The results are below in red. Since this is a simulation, I know the true fraction of images with pets: it’s 70%. The estimated fraction from the judges in each simulation is almost always lower than the true fraction. Averaged over all simulations, the estimated fraction is 64%, noticeably lower than the true value of 70%. This illustrates the pitfall: by ignoring judge error, you get the wrong answer. You might wonder if this has any practical implications. How often do you need a precise estimate for the fraction of pets in a collection of images? But what if you’re using the judgments to determine the accuracy of an algorithm that detects pets? And suppose the algorithm has an accuracy rate of 70%, but the judges say it is 64%. In the machine learning/AI community, the difference between an algorithm that is 70% accurate vs 64% is a big deal. But maybe things aren’t as bad as they seem. When statistically savvy experimenters report the results of human judgment, they return error bars . The error bars (I give details below) in this case are So even error bars don’t help: the actual accuracy rate of 0.7 is not included inside the error bars. The purpose of this post is to show that you can compensate for judge error. In the image above, the blue bars are the simulation of the compensating algorithm I will present. The blue bars do a much better job of estimating the fraction of pets than the naive algorithm. This post is just a summary, but full details are in Drawing Sound Conclusions from Noisy Judgments from the WWW17 conference. The histogram shows that the traditional naive algorithm (red) has a strong bias. But it also seems to have a smaller variance, so you might wonder if it has a smaller mean square error (MSE) than the blue algorithm. It does not. The naive algorithm has MSE 0.0037, the improved algorithm 0.0007. The smaller variance does not compensate for the large bias. Finally, I can explain why judge error is so important. For a typical problem requiring ML/AI, many different algorithms are tried. Then human judgment can be used to detect if one is better than the other. Current practice is to use error bars as above, which do not take into account errors in the human judgment process. But this can lead the algorithm developer astray and suggest that a new algorithm is better (or worse) when in fact the difference is just noise. I’m going to assume there are both an algorithm that takes an input and outputs a label (not necessarily a binary label) and also a judge that decides if the output is correct. So the judge is performing a binary task: determining if the algorithm is correct or not. From these judgments, you can compute the fraction of times (an estimate of the probability ) that the algorithm is correct. I will show that if you can estimate the error in the judgment process, then you can compensate for it and get a better estimate of the accuracy of the algorithm. This applies if you use raw judge scores or if you use a judgment process (for example, majority vote) to improve judge accuracy. In the latter case, you need to estimate the accuracy of the judgment process. A simple example of this setup is an information retrieval algorithm. The algorithm is given a query and returns a document. A judge decides if the document is relevant or not. A subset of those judgments is reevaluated by gold judge experts, giving an estimate of the judges’ accuracy. Of course, if you are rich enough to be able to afford gold judges to perform all your labeling, then you don’t need to use the method presented here. A slightly more subtle example is the pet detection algorithm mentioned above. Here it is likely that there would be a labeled set of images (a test set) used to evaluate the algorithm. You want to know how often the algorithm agrees with the labels, and you need to correct for errors in the judgment about agreement. To estimate the error rate, pick a subset of images and have them rejudged by experts (gold judges). The judges were detecting if an image had a pet or not. However, what I am really interested in is the error rate in judging whether the algorithm gave the correct answer or not. But that is easily computed from the gold judgments of the images. In the introduction, I mentioned that there are formulas that can correct for judge error. To explain the formula, recall that there is a task that requires labeling items into two classes, which I’ll call positive and negative. In the motivating example, positive means the algorithm gives the correct output for the item, negative that it is incorrect. I have a set of items, and judges perform the task on each item, getting an estimate of how many items are in the positive class. I’d like to use information on the accuracy of the judges to adjust this estimate. The symbols used in the formula are: The first formula is This formula is not difficult to derive. See the aforementioned WWW paper for details. Here are some checks that the formula is plausible. If the judges are prefect then , and the formula reduces to . In other words, the judges’ opinion of is correct. Next, suppose the judges are useless and guess randomly. Then , and the formula makes no sense because the denominator is infinity. So that’s also consistent. Notice the formula is asymmetric: the numerator has but not . To see why this makes sense, first consider the case when the judges are perfect on negative items so that . The judges’ only error is to take correct answers and claim they are negative. So an estimate of by the judges is always too pessimistic. On the other hand, if , then the judges are optimistic, because they sometimes judge incorrect items as correct. I will now show that the formula achieves these requirements. In particular, if then I expect and if then . To verify this, note that if the formula becomes . And if then the last inequality because and . Up until now the formula is theoretical, because the precise values of , and are not known. I introduce some symbols for the estimates. The practical formula is In the introduction, I motivated the concern with judgment error using the problem of determining the accuracy of an ML algorithm and, in particular, comparing two algorithms to determine which is better. If you had perfect judges and an extremely large set of labeled data, you could measure the accuracy of each algorithm on the large labeled set, and the one with the highest accuracy would clearly be best. But the labeled set is often limited in size. That leads to some uncertainty: if you had picked a different labeled set you might get a difference answer. That is the purpose of error bars: they quantify the uncertainty. Traditionally, the only uncertainty taken into account is due to the finite sample size. In this case, the traditional method gives 95% error bars of where The judge gave a positive label times out of , is the estimated mean and the estimate of the variance of . I showed via simulation that if there are judgment errors, these intervals are too small. The corrected formula is where The formula shows that when taking judge error into account, the variance increases, that is, . The first term of the equation for is already larger than , since is it divided by a number less than one. The next two terms add additional error, due to the uncertainty in estimating the judge errors and . The theme of this post is correcting for judge errors, but I have only discussed the case when the judge gives each item a binary rating, as a way to estimate what fraction of items are in each of the two classes. For example, the judge reports if an algorithm has given the correct answer, and you want to know how often the algorithm is correct. Or the judge examines a query and a document, and you want to know how often the document is relevant to the query. But the methods presented so far extend to other situations. The WWW paper referred to earlier gives a number of examples in the domain of document retrieval. It explains how to correct for judge error in the case when a judge gives documents a relevance rating rather than just a binary relevant or not. And it shows how to extend these ideas beyond the metric “fraction in each class” to the metric DCG , which is a common metric for document retrieval. Analyses of the type presented here always have assumptions. The assumption of this work is that there is a task (for example, “is the algorithm correct on this item?”) with a correct answer and that there are gold judges who are expert enough and have enough time to reliably obtain that correct answer. Industrial uses of human judgment often use detailed guidelines explaining how the judgment should be done, in which case these assumptions are reasonable. But what about the general case? What if different audiences have different ideas about the task? For example, there may differences from country to country about what constitutes a pet. The analysis presented here is still relevant. You only need to compute the error rate of the judges relative to the appropriate audience. I treat the judgment process as a black box, where only the overall accuracy of the judgments is known. This is sometimes very realistic, for example, when the judgment process is outsourced. But sometimes details are known about the judgment process would lead you to conclude that the judgments of some items are more reliable than others. For example, you might know which judges judged which items, and so have reliable information on the relative accuracy of the judges. The approach I use here can be extended to apply in these cases. I close this post with the R code that was used to generate the simulation given earlier. Powered by QuickLaTeX .", "date": "2017-05-04"},
{"website": "Ebay-Engineering", "title": "Types of Content at eBay: Titles", "author": ["Tatyana Badeka"], "link": "https://tech.ebayinc.com/engineering/types-of-content-at-ebay-titles/", "abstract": "All eBay-generated content is currently translated by our talented localization team, whereas eBay’s user-generated (UG) content is handled by our Machine Translation (MT) engine. It is common knowledge that UG text can get pretty noisy due to typos, non-dictionary terms, etc. At eBay, however, MT deals with more than that. We work with multiple types of UG content — search queries, item titles, and item descriptions — and each presents its own challenges. In the previous post we talked about search queries . This post discusses item titles. Translating item titles (IT) provides our buyers from Russia, Brazil, Spanish Latin America, France, Italy, Germany, and Spain with an option to view eBay search results in their own language. This allows customers to look through pages of results and make an informed decision on which listings to open, because an image alone does not contain enough information. Being able to read and understand item titles is essential to a positive customer experience, which is why we invest a lot of effort into improving the MT engine for titles. This type of content is very specific and presents a number of challenges for MT. A title is a summarized item description composed of keywords. The eBay Help article on writing effective titles encourages sellers to omit punctuation and avoid trying to create a grammatically correct sentence. Following these and other tips is supposed to help sellers create a clear picture of an item and a good first impression, so it is important that the MT translation meets the same expectations. However, the lack of syntax and punctuation presents a problem for an MT engine that is normally trained on sentences. If it tries to translate a sequence of nouns, adjectives, and numbers as a sentence, meaning errors are unavoidable. It may start looking for a subject and a predicate and in general for a sentence structure, thus translating adjectives as verbs, moving words around, and so on. As an example, let’s take a title for a can of paint: “20g Glow in the Dark Acrylic Luminous Paint Bright Pigment Graffiti Party DIY”. What might go wrong here? “Glow” may get translated as an imperative form of the verb, and “dark acrylic” — as a noun phrase with “acrylic” being a noun. (as in “Stay in the shaded area!”) – and that is just part of the title. Similar transformation may happen with polysemous words or those that belong to different parts of speech: “can”, “paint”, “party”, etc. The result of such translation may be a completely different item. This is closely related to the previous issue. Segmenting a title and correctly identifying semantic units is of utmost importance for machine translation. For example, “Gucci fake snake leather purse”: in case of an incorrect segmentation, we may get a translation of a “Gucci fake” instead of the intended “fake snake leather”. Such translations are the most dangerous because they sound correct and believable yet present misleading information, which in the end may leave both a buyer and a seller unhappy with the experience. To address these major issues, the science team created an engine just for item titles; it is trained on separate data sets. In addition, they have been working on a named entity recognition (NER) algorithm that identifies semantic units in a title before it goes in the MT engine for translation. Sellers tend to use multiple synonyms in a title assuming this will increase the chances of matching search queries and coming up high in search results (which is a common misconception ). For MT this means several things: The machine needs to learn to translate them independently of each other. This is similar to the first issue described above, because the engine may try to create agreement where there should be none. Example, Baby Toddler Kids Child Mini Cartoon Animal Backpack Schoolbag Shoulder Bag We see four synonyms for the age reference and three synonyms for the item itself. The age reference terms are not all adjectives nor can all of them be translated as adjectives. Even a human translator would have to get creative and produce something like “for a baby/toddler, kids’, child’s” – because we could not simply leave all four of them as nouns; it would sound too abrupt and possibly confusing. The task is much more challenging for a machine. Not only should it avoid creating noun phrases ( Kids child may turn into a kid’s child ), it also needs to rephrase or insert prepositions where necessary (baby toddler child -> for baby, toddler, child; kids –> kids’). The best ways to approach this would vary depending on the target language. In our example, there are three synonyms for a head noun: Backpack – Schoolbag – Shoulder Bag . What if they are of different gender in the target language? Which one should the adjectives agree with? A human translator would probably pick the first one, but MT may not think the same way. Here is a bigger challenge: the head noun does not immediately follow the adjectives describing it. In our example there are two other nouns between the attributes “Kids Child” and the head noun “Backpack”. The machine is supposed to figure out that “kids” describes “backpack”, not “cartoon” or “animal”. As you can imagine, however, the most logical decision for a machine would be to connect “kids” with “cartoon”. Agreement plays a very important role in translating item titles, because it provides a customer with a description of features and qualities of the item. If you connect an attribute with the wrong noun, it will modify an incorrect object and produce an overall misleading translation. In our example, with the incorrect agreement, a user will read: “backpack with a kids’ cartoon animal”, which is in essence a different item than a “kids’ backpack with a cartoon animal”. One may argue that an image would be a clear indication that the item is a kids’ backpack. Unfortunately, a picture is not always a reliable source of information. In our case, there are similar backpacks for adults, which is why an accurate translation will make a difference. Sellers use multiple acronyms to save space and fit as much information in a title as possible. For MT this presents several challenges. Rare, unknown acronyms or acronyms that sellers made up on the spot. Gathering more training data and compiling additional lists of expanded out-of-vocabulary (OOV) acronyms is helping address that. Polysemic acronyms that have different translations in different categories. The most challenging acronyms are the ones that have more than one meaning in the same category. For example, “RN” appears in Clothing, Shoes and Accessories as “registered nurse”, “Rapa Nui”, “Rusty Neal”, and as part of model names for Nike, Hugo Boss, A&F and other brands. This is common content for certain categories. Singling out a movie or song title out of the rest of the string may be difficult because there is often no contextual information pointing to the fact that it is a movie or a song. It is not much of a problem in the DVD or Music category, but quite often you will find reference to a movie title or a music band name in other categories such as Collectables or Clothing. It is also common for sellers to quote a writing on the item they are selling. Ideally, we would want to have the writing to be left as is so that the customer would know exactly what the item depicts. As you can imagine, however, literally anything can be written on a t-shirt or a poster, which is why it is very difficult for a machine to differentiate a writing from the actual item description. In such cases a user would have to rely on the quality and size of an image, which may not be the best on the search results page. In this example, “New York Vermont Quebec” is part of the poster design, but it is barely visible. In the text of the item title, however, it may be interpreted as locations of the poster, places it originally came from, etc. Identifying this as verbatim writing, thus keeping it in English, would be a very difficult task for an MT engine, but it would clearly benefit an eBay customer. With so many aspects to keep in mind, training the engine to translate eBay item titles is certainly a challenge. Our teams of scientists and linguists are actively and successfully working on ways to improve the quality of the training data and the MT output.", "date": "2016-10-24"},
{"website": "Ebay-Engineering", "title": "Machine Translation: The Basics of Quality Estimation", "author": ["Juan Rowda"], "link": "https://tech.ebayinc.com/engineering/machine-translation-the-basics-of-quality-estimation/", "abstract": "Quality Estimation is a method used to automatically provide a quality indication for machine translation output without depending on human reference translations. In more simple terms, it’s a way to find out how good or bad are the translations produced by an MT system, without human intervention. A good point to make before we go into more detail on QE is the difference between evaluation and estimation . You can evaluate the quality of MT output in two main ways: human evaluation (a person will check the translation and provide feedback) and a utomatic evaluation (there are different methods that can provide a score on the translation quality without human intervention). Traditionally, automatically evaluating the quality of any given MT output has required a reference translation created by a human translator. The differences and similarities between the MT output and the reference translation can then be turned into a score to determine the quality of said output. This is the approach followed by certain methods, such as BLEU and NIST . The main differentiator of quality estimation is that it does not require a human reference translation. QE is a prediction of the quality based on certain features. These features can be, for example, the number of noun or prepositional phrases in the source and target (and their difference), the number of named entities (names of places, people, companies, etc.), and many more. With these features, using techniques like machine learning, a QE model can be created to obtain a score that represents the estimation of the translation quality. At eBay, we use MT to translate search queries, item titles and item descriptions; an earlier post discussed our work with search queries . To train our MT systems, we work with vendors that help us post-edit content. Due to the challenging nature of our content (user-generated, diversity of categories, millions of listings, etc.), a method to estimate the level of effort required for post-editing definitely adds value. QE can help you obtain important information on this effort in an automated manner. For example, one can estimate how many segments have a very low-quality translation and could be just discarded instead of post-edited. So, what can you do with the help of QE? First and foremost, you can estimate the quality of translations at the segment and file level. Segment-level scores can help you target post-editing, focusing only on content that makes sense to post-edit. You can also estimate post-editing effort/time – it would be rather safe to assume that segments with a low-quality score take more time to post-edit. It is also possible to compare MT systems based on QE scores and see which one performs better. This last application example is especially helpful if you are trying to decide which engine you should use, or whether a new version of an engine is working better than its predecessor.", "date": "2016-03-28"},
{"website": "Ebay-Engineering", "title": "Secure Communication in Hadoop without Hurting Performance", "author": ["Benoy Antony"], "link": "https://tech.ebayinc.com/engineering/secure-communication-in-hadoop-without-hurting-performance/", "abstract": "Apache Hadoop is used for processing big data at many enterprises. A Hadoop cluster is formed by assembling a large number of commodity machines, and it enables the distributed processing of data. Enterprises store lots of important data on the cluster. Different users and teams process this data to obtain summary information, generate insights, and gain other very useful information. Figure 1. Typical Hadoop cluster with clients communicating from all sides Depending on the size of the enterprise and the domain, there can be lots of data stored on the cluster and a lot of clients working on the data. Depending on the type of client, its network location, and the data in transit, the security requirements of the communication channel between the client and the servers can vary. Some of the interactions need to be authenticated. Some of the interactions need to be authenticated and protected from eavesdropping. How to enable different qualities of protection for different cluster interactions forms the core of this blog post. At eBay, we have multiple large clusters that hold hundreds of petabytes of data and are growing by many terabytes daily. We have different types of data. We have thousands of clients interacting with the cluster. Our clusters are kerberized , which means clients have to authenticate via Kerberos. Some clients are within the same network zone and interact directly with the cluster. In most cases, there is no requirement to encrypt the communication for these clients. In some cases, it is required to encrypt the communication because of the sensitive nature of the data. Some clients are outside the firewall, and it is required to encrypt all communication between these external clients and Hadoop servers. Thus we have the requirement for different qualities of protection. Figure 2. Cluster with clients with different quality of protection requirements (Lines in red indicate communication channel requiring confidentiality) Also note that there is communication between the Hadoop servers within cluster. They also can fall into category of clients within the same network zone or internal clients. All Hadoop servers support both RPC and HTTP protocols. In addition, Datanodes support the Data Transfer Protocol. Figure 3. Hadoop Resource Manager with multiple protocols on different ports Hadoop servers mainly interact via the RPC protocol. As an example, file system operations like listing and renaming happen over the RPC protocol. All Hadoop server expose their status, JMX metrics, etc. via HTTP. Some Hadoop servers support additional functionality to offer a better user experience over the HTTP. For example, the Resource Manager provides a web interface to browse applications over the HTTP. Datanodes store and serve the blocks via the Data Transfer Protocol. Figure 4. A Hadoop Datanode with multiple protocols on different ports The quality of protection for the RPC protocol is specified via the configuration property hadoop.rpc.protection . The default value is authentication on a kerberized cluster. Note that hadoop.rpc.protection is effective only in a cluster where hadoop.rpc.authentication is set to Kerberos . Hadoop supports Kerberos authentication and quality protection via the SASL (Simple Authentication and Security Layer) Java library. The Java SASL library supports the three levels of Quality of protection shown in Table 2. If privacy is chosen for hadoop.rpc.protection , data transmitted between client and server will be encrypted. The algorithm used for encryption will be 3DES. Figure 5: Resource Manager supporting privacy on its RPC port The quality of protection for HTTP can be controlled by the policies dfs.http.policy and yarn.http.policy . The policy values can be one of the following: HTTP_ONLY . The interface is served only on HTTP HTTPS_ONLY . The interface is served only on HTTPS HTTP_AND_HTTPS . The interface is served both on HTTP and HTTPS Figure 6. Resource Manager supporting privacy on its RPC and HTTP ports The quality of protection for the Data Transfer Protocol can be specified in a similar way to that for the RPC protocol. The configuration property is dfs.data.transfer.protection . Like RPC, the value can be one of authentication , integrity , or privacy . Specifying this property makes SASL effective on the Data Transfer Protocol. Figure 7. Datanode supporting privacy on all its ports Specifying authentication as the value of dfs.data.transfer.protection forces the client to require a block access token while reading and storing blocks. Setting dfs.data.transfer.protection to privacy results in encrypting the data transfer. The algorithm used for encryption will be 3DES. It is possible to agree upon a different algorithm by setting dfs.encrypt.data.transfer.cipher.suites on both client and server sides. The only value supported is AES/CTR/NoPadding . Using AES results in better performance and security. It is possible to further speed up encryption and decryption by using Advanced Encryption Standard New Instructions (AES-NI) via the libcrypto.so native library on Datanodes and clients. Enabling privacy comes at the cost of performance. Encryption and decryption require extra performance. Setting hadoop.rpc.protection to privacy encrypts all communication from clients to Namenode, from clients to Resource Manager, from datanodes to Namenodes, from Node Managers to Resource managers, and so on. Setting dfs.data.transfer.protection to privacy encrypts all data transfer between clients and Datanodes. The clients could be any HDFS client like a map-task reading data, reduce-task writing data or a client JVM reading/writing data. Setting dfs.http.policy and yarn.http.policy to HTTPS_ONLY causes all HTTP traffic to be encrypted. This includes the web UI for Namenodes and Resource Managers, Web HDFS interactions, and others. While this guarantees the privacy of interaction, it slows down the cluster considerably. The cumulative effective will be a fivefold decrease in the cluster throughput. We had set quality of protection for both RPC and data transfer protocols to privacy in our main cluster. We experienced severe delays in processing that resulted in days of backlogs in data processing. The Namenode throughput was low, the data read/writes were very slow, and this increased the completion times of individual applications. Since applications were occupying containers for five times longer than before, there was severe resource contention that ultimately caused the application backlogs. The immediate solution was to change the quality of protection back to authentication for both protocols, but we still required privacy for some of our interactions, so we explored the possibility of choosing a quality of protection dynamically during connection establishment. As noted, the RPC protocol and Data Transfer Protocol internally use SASL to incorporate security in the protocol. On each side (client/server), SASL can support an ordered list of QOPs. During client-server handshake, the first common QOP is selected as the QOP of that interaction. Table 2 lists the valid QOP values. Figure 8. Sequence of client and server agreeing on a QOP based on configured QOPs In our case, we wanted most client-server interactions to require only authentication and very few client-server interactions to use privacy. To achieve this, we implemented an interface SASLPropertiesResolver . The method getSaslProperties on SASLPropertiesResolver will be invoked during handshake to determine the potential list of QOP to be used for the handshake. The default implementation of the SASLPropertiesResolver simply returned the value specified in hadoop.rpc.protection and dfs.data.transfer.protection for RPC and data transfer protocols. More details on SASLPropertiesResolver can be found by reviewing the work on the related JIRA HADOOP-10221 . In our implementation of SASLPropertiesResolver , we use a whitelist of IP addresses on the server side. If the client’s IP address is in the whitelist, then the list of QOP specified in hadoop.rpc.protection/dfs.data.transfer.protection is used. If the client’s IP address in not in whitelist, then we use the list of QOPs specified in a custom configuration, namely hadoop.rpc.protection.non-whitelist . To avoid very long whitelists, we use CIDRs to represent IP address ranges. In our clusters, we set hadoop.rpc.protection and dfs.data.transfer.protection on both client and servers to be authentication,privacy . The configuration hadoop.rpc.protection.non-whitelist was set to privacy . When whitelisted clients connect to servers, they will agree upon authentication as the QOP for the connection, since both client and servers support a list of QOP values, authentication,privacy . The order of QOP values in the list is important. If both client and servers support a list of QOP values as privacy,authentication , then client and servers agree upon privacy as the QOP for their connections. Figure 9. internal clients, external clients, cluster Only clients inside the firewall are in the whitelist. When these clients connect to servers, the QOP used will be authentication as both clients and servers use authentication,privacy for RPC and data transfer protocols. The external clients are not in the whitelist. When these clients connect to servers, QOP used will be privacy as servers offer only privacy based on the value of hadoop.rpc.protection.non-whitelist . The clients in this case could be using authentication,privacy or privacy in their configuration. If the client only specifies authentication , the connection will fail as there will be no common QOP supported between client and servers. Figure 10. Sequence of Hadoop client and server choosing QOP using SASL PropertiesResolver The diagram above depicts the sequence of an external client and Hadoop server agreeing on a QOP. Both client and server support two potential QOP, authentication,privacy . When the client initiates the connection, the server consults the SASLPropertiesResolver with the client IP address to determine the QOP. The whitelist-based SASLPropertiesResolver checks the whitelist, finds that the client’s IP address is not whitelisted and hence offers only privacy as the QOP. The server then offers only privacy as the only QOP choice to the client during SASL negotiation. Thus the QOP of the subsequent connection will be privacy . This necessitates the need for setting up a secret key based on the cipher suites available at the client and server. In some cases, internal clients need to transmit sensitive data and prefer to encrypt all communication with the cluster. In this case, the internal clients can force encryption by setting both hadoop.rpc.protection and dfs.data.transfer.protection to privacy . Even though the servers support authentication,privacy , connections will use the common QOP, privacy . Supporting multiple QOPs on Hadoop protocols enables selective encryption. This will protect any data transfer. The data could be real data or signal data like delegation tokens. Another approach will be to use a reverse proxy like Apache Knox. Here the external clients will have connectivity only to Apache Knox servers and Apache Knox will allow only HTTPS traffic. The cluster will support only one QOP, which is Authentication . Figure 11. Protecting data transfer using Reverse Proxy (Apache Knox) As shown in the diagram, the external clients interact with the cluster via knox servers. The transmission between the client and knox will be encrypted. knox , being an internal client, forwards the transmission to the cluster in plain text. As we discussed, it is possible to achieve a different quality of protection for the client-cluster communication in two ways. With a reverse proxy to receive encrypted traffic By enabling the cluster to support multiple QOPs simultaneously There are pros and cons with both approaches, as shown in Table 4. Depending on the organization’s requirements, the proper approach should be chosen. A process can either be a client or server or both. In some environments, it is desirable to use different logic for choosing QOP depending on whether the process is a client or server for the specific interaction. Currently, there is provision to specify only one SaslPropertiesResolver via configuration. If a client needs a different SaslPropertiesResolver , it needs to use a different configuration. If the same process needs a different SaslPropertiesResolver while acting as client and server, there is no way to do that. It would be a good enhancement to be able to specify different SaslPropertiesResolver for client and server. Currently the whitelist of IP addresses is maintained on local files. This introduces the problem of updating thousands of machines and keeping them all in sync. Storing the whitelist information on a Zookeeper node may be a better alternative. Currently, the whitelist is cached by each server during initialization and then reloaded periodically. A better approach may be to reload the whitelist based on an update event.", "date": "2016-11-08"},
{"website": "Ebay-Engineering", "title": "Elasticsearch Cluster Lifecycle at eBay", "author": ["Sudeep Kumar"], "link": "https://tech.ebayinc.com/engineering/elasticsearch-cluster-lifecycle-at-ebay/", "abstract": "eBay’s Pronto, our implementation of the “Elasticsearch as service” (ES-AAS) platform, provides fully managed Elasticsearch clusters for various search use cases. Our ES-AAS platform is hosted in a private internal cloud environment based on OpenStack . The platform currently manages around 35+ clusters and supports multiple data center deployments. This blog provides guidelines on all the different pieces for creating a cluster lifecycle to allow streamlined management of Elasticsearch clusters. All Elasticsearch clusters deployed within the eBay infrastructure follow our defined Elasticsearch lifecycle depicted in the figure below. This lifecycle stage begins when a new use case is being onboarded onto our ES-AAS platform. Customers’ requirements are captured onto an onboarding template that contains information such as document size, retention policy, and read/write throughput requirement. Based on the inputs provided by the customer, infrastructure sizing is performed. The sizing uses historic learnings from our benchmarking exercises. On-boarding information has helped us in cluster planning and defining SLA for customer commitments. We collect the following information from customers before any use case is onboarded: Use case details : Consists of queries relating to use case description and significance. Sizing Information : Captures the number of documents, their average document size, and year-on-year growth estimation. Data read/write information : Consists of expected indexing/search rate, mode of ingestion (batch mode or individual documents), data freshness, average number of users, and specific search queries containing any aggregation, pagination, or sorting operations. Data source/retention : Original data source information (such as Oracle, MySQL, etc.) is captured on an onboarding template. If the indices are time-based, then an index purge strategy is logged. Typically, we do not use Elasticsearch as the source of data for critical applications. Before undertaking any benchmarking exercise, it’s really important to understand the underlying infrastructure that hosts your VMs. This is especially true in a cloud-based environment where such information is usually abstracted from end users. Be aware of different potential noisy-neighbors issues, especially on a multi-tenant-based infrastructure. Like most folks, we have also performed extensive benchmarking exercise on existing hardware infrastructure and image flavors. Data stored in Elasticsearch clusters are specific to customer use cases. It is near to impossible to perform benchmarking runs on all data schemas used by different customers. Therefore, we made assumptions before embarking on any benchmarking exercise, and the following assumptions were key. Clients will use a REST path for any data access on our provisioned Elasticsearch clusters. (No transport client) To start with, we kept a mapping of 1GB RAM to 32GB disk space ratio. (This was later refined as we learnt from benchmarking) Indexing numbers were carefully profiled for different numbers of replicas (1, 2, and 3 replicas). Search benchmarking was done always on GetById queries (as search queries are custom and profiling different custom search queries was not viable). We used fixed-size 1KB, 2KB, 5KB, and 10 KB documents Working from these assumptions, we derived at a maximum shard size for performance (around 22GB), right payload size for _bulk requests (~5MB), etc. We used our own custom JMeter scripts to perform benchmarking. Recently Elasticsearch has developed and open-sourced the Rally benchmarking tool , which can be used as well. Additionally, based on our benchmarking learnings, we created a capacity-estimation calculator tool that can take in customer requirement inputs and calculate the infrastructure requirement for a use case. We avoided a lot of conversation with our customers on infrastructure cost by sharing this tool directly with end users. Our ES clusters are deployed by leveraging an intelligent warm-cache layer. The warm-cache layer consists of ready-to-use VM nodes that are prepared over a period of time based on some predefined rules. This ensures that VMs are distributed across different underlying hardware uniformly. This layer has allowed us to quickly spawn large clusters within seconds. Additionally, our remediation engine leverages this layer to flex up nodes on existing clusters without errors or any manual intervention. More details on our cache pool are available in another eBay tech blog at Ready-to-use Virtual-machine Pool Store via warm-cache Cluster deployment is fully automated via a Puppet / Foreman infrastructure. We will not talk in detail about how Elasticsearch Puppet module was leveraged for provisioning Elasticsearch clusters. This is well documented at Elasticsearch puppet module . Along with every release of Elasticsearch, a corresponding version of the Puppet module is generally made publically available. We have made minor modifications to these Puppet scripts to suit eBay-specific needs. Different configuration settings for Elasticsearch are customized based on our benchmarking learnings. As a general guideline, we do not set the JVM heap memory size to more than 28 GB (because doing so leads to long garbage collection cycles), and we always disable in-memory swapping for the Elasticsearch JVM process. Independent clusters are deployed across data centers, and load balancing VIPs (Virtual IP addresses) are provisioned for data access. Typically, with each cluster provisioned we give out two VIPs, one for data read operations and another one for write operations. Read VIPs are always created over client nodes (or coordinating nodes), while write VIPs are configured over data nodes. We have observed improved throughput from our clusters with such a configuration. Deployment diagram We use a lot of open source on our platform such as OpenStack , MongoDB , href=\"https://airflow.incubator.apache.org/\">Airflow, Grafana , InfluxDB (open version), openTSDB , etc. Our internal services, such as cluster provisioning, cluster management, and customer management services, allow REST API-driven management for deployment and configuration. They also help in tracking clusters as assets against different customers. Our cluster provisioning service relies heavily on OpenStack. For example, we use NOVA for managing compute resources (nodes), Neutron APIs for load balancer provisioning, and Keystone for internal authentication and authorization of our APIs. We do not use federated or cross-region deployments for an Elasticsearch cluster. Network latency limits us from having such a deployment strategy. Instead, we host independent clusters for use cases across multiple regions. Clients will have to perform dual writes when clusters are deployed in multiple regions. We also do not use Tribe nodes. We create cluster topology during customer onboarding. This helps to track resources and cost associated with cluster infrastructure. The metadata stored as part of a cluster topology maintains region deployment information, SLA agreements, cluster owner information, etc. We use eBay’s internal configuration management system (CMS) to track cluster information in form of a directed graph. There are external tools that hook onto this topology. Such external integrations allow easy monitoring of our clusters from centralized eBay-specific systems. Cluster topology example Security is provided on our clusters via a custom security plug-in that provides a mechanism to both authenticate and authorize the use of Elasticsearch clusters. Our security plug-in intercepts messages and then performs context-based authorization and authentication using an internal authentication framework. Explicit whitelisting based on client IP is supported. This is useful for configuring Kibana or other external UI dashboards. Admin (Dev-ops) are configured to have complete access to Elasticsearch cluster. We encourage using HTTPS (based on TLS 1.2) for securing communication between client and Elasticsearch clusters. The following is a sample simple security rule that can configure be configured on our platform of provisioned clusters. In the above sample rule, the enabled field controls if the security feature is enabled or not. whitelisted_ip_list is an array attribute for providing all whitelisted Client IPs. Any Open/Close index operations or delete index operations can be performed only by admin users. Cluster monitoring is done by custom monitoring plug-in that pushes 70+ metrics from each Elasticsearch node to a back-end TSDB-based data store. The plug-in works on a push-based design. External dashboards using Grafana consume the data on TSDB store. Custom templates are created on a Grafana dashboard, which allows easy centralized monitoring of our own clusters. We leverage an internal alert system that can be used to configure threshold-based alerts on data stored on OpenTSDB. Currently, we have 500+ active alerts configured on our clusters with varying severity. Alerts are classified as ‘Errors’ or ‘Warnings’. Error alerts, when raised, are immediately attended to either by DevOps or by our internal auto-remediation engine, based on the alert rule configured. Alerts are created during cluster provisioning based on various thresholds. For Example, if a cluster status turns RED, an ‘Error’ alert is raised or if CPU utilization of node exceeds 80% a ‘Warning’ alert is raised. Our ES-AAS platform can perform an auto-remediation action on receiving any cluster anomaly event. Such actions are enabled via our custom Lights-Out-Management (LOM) module. Any auto-remediation module can significantly reduce manual intervention for DevOps. Our LOM module uses a rule-based engine which listens to all alerts raised on our cluster. The reactor instance maintains a context of the alerts raised and, based on cluster topology state (AUTO ON/OFF), takes remediation actions. For example, if a cluster loses a node and if this node does not return  to its cluster within the next 15 minutes, the remediation engine replaces that node via our internal cluster management services. Optionally, alerts can be sent to the team instead of taking a cluster remediation action. The actions of the LOM module are tracked as stateful jobs that are persisted on a back-end MongoDB store. Due to the stateful nature of these jobs, they can be retried or rolled back as required. Audit logs are also maintained to capture the history or timeline of all remediation actions that were initiated by our custom LOM module. Along with the standard Elasticsearch distribution, we also ship our custom logging library. This library pushes all Elasticsearch application logs onto a back-end Hadoop store via an internal system called Sherlock. All centralized application logs can be viewed at both cluster and node levels. Once Elasticsearch log data is available on Hadoop, we run daily PIG jobs on our log store to generate reports for error log or slow log counts. We generally have our logging settings as INFO, and whenever we need to triage issues, we use transient a logging setting of DEBUG, which collects detailed logs onto our back-end Hadoop store. We follow a cluster decommissioning process for major version upgrades of Elasticsearch. For major upgrades for Elasticsearch clusters, we spawn a new cluster with our latest offering of the Elasticsearch version. We replay all documents from old or existing version of Elasticsearch clusters to the newly created cluster. Client (user applications) starts using both cluster endpoints for all future ingestion until data catches up on the new cluster. Once data parity is achieved, we decommission the old cluster. In addition to freeing up infrastructure resources, we also clean up the associated cluster topology. Elasticsearch also provides a migration plug-in that can be used to check if direct, in-place upgrades can be done on major Elasticsearch versions. Minor Elasticsearch upgrades are done on an as-needed basis and are usually done in-place.", "date": "2017-04-12"},
{"website": "Ebay-Engineering", "title": "ColorizerJS — My First Hack Project at eBay", "author": ["Weston Mossman"], "link": "https://tech.ebayinc.com/engineering/colorizerjs-my-first-hack-project-at-ebay/", "abstract": "First blog on the eBay Tech Blog, whooooh! Hello! This blog will mostly be about my project, ColorizerJS, but since this is my first post on the eBay Tech Blog, I thought I’d introduce myself. For those unaware of some of the terminology and nomenclature, I will try to provide some brief short-bracket explanations[“[]”]. If you are submerged in web software or already cool enough to know the terminology, just pretend like there are no brackets. I started working at eBay in May after graduating from the University of California, Santa Cruz, with a bachelor’s degree in Computer Science Game Design (now called CGPM). CSGD at UCSC was a great program for front-facing developers, as games are as much about the user (or player) as they are the algorithm. Plus, at least at UCSC, I got a whole lot more experience with managing assets and interfacing between technologies in my Game Design classes than in my vanilla CS classes. The interview process at eBay was great, not too niche-specific and not too broad, with great questions on both niche and broad ends of the interview spectrum. I was hired as a full-time IC [individual contributor] on the Search Front-End [abbreviated to FE, meaning closest to the client] engineering team. This means that whenever you enter a search query, the stuff you get back — JavaScript [functionality, the bulk of it], CSS [look and feel], HTML [structure] — is what my team handles. I was lucky enough to be on a great team at a great time. When Hack Week 2016 rolled in mid-June, I was too new to have many responsibilities keeping me from joining the Hack Week, yet experienced enough to know our codebase a bit and have some ideas for my potential project. After some thought, I determined something that would be fun to work on, enhance the user experience, and bring in revenue to eBay through increased buyer activity. The initial problem arose from eBay sellers often having non-standard sized images, either shorter or thinner than a square. With the current redesign of Search Results Page (SRP), the method to combat this problem was to overlay all images onto a grey bounding square. This technically fixed the problem of non-standard image dimensions, but it did not lend itself to good aesthetics. This is where ColorizerJS comes in. Current background implementation in production — not so hot I was inspired to do ColorizerJS by my coworker (and one of my many mentors) Yoni Medoff, who made a script that drew loaded images to an HTML Canvas, averaged the RGB [red, green, and blue channels. You definitely know this. These brackets are so condescending.] values of each image in the search results for a query, then created an RGB color out of those average channel numbers, and set the containing square (normally just white or grey) behind the image to be the average color. This was intended to provide a more visually appealing way of standardizing the size and shape of each image in search results than just slapping them on top of a grey square. Yoni’s implementation looked significantly better than just a grey square, but it could definitely be improved upon. There were four problems with his approach: It did not look good in many cases. Sometimes the generated average does match any color in an image. A strong foreground color can throw that average out of whack, and most of all, the average may not resemble the “background” at all. It depended on HTML <img/> elements [images on the (meta)physical webpage], which limited use to the DOM, so that isomorphic [using the same code in a browser and on the server] server-side use was impossible and parallelized web-workers [multiple CPU threads running JavaScript] wouldn’t work. It converted the <img/> to an HTML Canvas element to extract the pixel data, a slow process that required drawing to large Canvas objects before being of any use. This was not time- or memory-efficient. It was very tightly coupled to our codebase, meaning that it could not be used in other parts of the site or on other websites without huge changes. My code running average algorithm — it’s alright I thought, “Oh snap, here’s my chance to win $10k and become VP of engineering or whatever the Hack Week prize is.” Easy! All I needed to do was come up with a way to load the image into the browser cache to reduce double-requests [when a browser makes two requests to the server for the same image, BAD] and then read that cached image as binary [1000100101], analyze the binary file to extract its RGB pixel data, organize that in a logical way to analyze, and then analyze it and determine the most likely background color of the image to be appended to the background bounding square of the image, all while keeping the analysis code modular so as to be used on the client, web-worker, or server, and independent of our codebase and modular so that I could open-source it as an NPM module to be used by anyone on any page. Mayyybe not so easy. So now I have a considerable job — meet all these criteria in a week — but where do I start? I decided starting with a client-side [user’s browser] image decoding [decoders analyze an image and give back a string of numbers indicating the channel values of each pixel] NPM module. eBay supports JPEG and WebP (Google’s new super-great image compression format) formats, and I figured since JPEG was older, I’d have more luck with it, so off I went looking for a client-side decoder. There weren’t any client-side JPEG decoders. Only Node [server-side]. Nice. After a few PRs [Pull Request] to jpeg-js (to support native UInt8Arrays instead of just Node-specific Buffer arrays) and just like that I had a client-side JPEG decoder. Nice. Next I had to figure out how to request the image file as binary, and found a great article on JQuery Ajax custom transport overriding. This allowed me to send the data to jpeg-js as a bitstream in a JavaScript typed-array ( UInt8Array in this case). jpeg-js sadly only supported 4-channel (including alpha-transparency) output, so in my color analysis code I handled both 3-channel and 4-channel output flags as 4-channel. This increased data overhead by about 1/3 more bytes per image [since each channel of a pixel is one byte] — inconvenient but not a dealbreaker. With my now merged pull request to support client side analysis and array type, jpeg-js analyzed my binary input (after some array conversion) and gave me back an object with a height, width, and array of bytes [number from 0 – 255], each four array indices corresponding to a pixel in the image binary. I found a great WebP analysis library called libwebp (I got the code by viewing the page source) and got it working. Now it’s time to do some background analysis! I started with the simple average-of-the-edge-pixels algorithm and appended the resulting color to the bounding box behind each image result, which expectedly yielded sub-par results, but at least the analysis was working. I then I decided to up the size of the pixels analyzed to around 10 pixels per side. If the image was tall I would check the outer 8 pixels on the left and right side, if it was short I would check the top and bottom. I made a function that determined two modes for each color channel, determined the weight of each mode against each other and against the average, shaving off outliers and conversely, fully using the mode if it was significant enough in the image. This yielded great results, especially for images with fairly flat background colors or heavily dominating colors. Lookin’ pretty good But some images, especially those with sporadic colors, colors that are very different from side to side, edges with very different colors than the rest of the image, borders, and other complex images, sometimes did not cooperate with this implementation. Some generated colors would clearly not occur at all in the picture or just not fit as a background. WHERE IS THAT MINT COMING FROM? — Mr Nice Guy could have fixed this if he were still here. I did not want this algorithm to be just KINDA similar to the background for an image, I wanted it to be almost EXACT. I came up with a few modifications, more modes, more pixels analyzed, some analysis of sectors and their potential significance to determine their weight, and doing analysis of 2 pixels as well as large sections of 20–30 pixels from each edge and determining their weights. Also I fine-tuned the cutoffs for modes and averages to be more likely to exclude an average and include a mode. Some modifications to weights for each sector was required before I came up with a fairly finished product. Niiiiiiice. It actually looks like a background. Particularly impressed with the fighter jet analysis, good job Colorizer I presented the Colorizer at the end of the week of Hack Week 2016 and got some positive responses from the judges. Fun project, and some hard work for a little more than a week. I hope to make all magic numbers into parameters that can be defined by the user to widen the range of use cases that ColorizerJS can apply to. I also want to make some more functions in the Colorizer that can make it work with path image files to be used on the server, satisfying my original requirement of the module being isomorphic. If it ends up looking good, I might also come up with separate background colors for each side (top and bottom or left and right) if they are different enough. This would especially work well with unprofessionally shot images in sellers’ homes that might have vastly different upper and lower backgrounds in their images. Soon I will get the code up and tested on Search Results Page, working with Web workers, and published as an NPM module with some examples of usage so that you and all of your friends can do some sweet image analysis. All that is needed is an image URL passed to ColorizerJS, the image is cached and analyzed, and out comes the background color! Thanks for reading. I will update soon with more info as the project progresses. When I open-source ColorizerJS, I will post a link to the Git repo. Big thanks to Yoni Medoff for the idea and initial implementation along with encouragement along the way, Eugene Ware for jpeg-js and the help getting it implemented, and Senthil Padmanabhan for the inspiration to write this blog.", "date": "2016-10-27"},
{"website": "Ebay-Engineering", "title": "Multiple Authentication Mechanisms for Hadoop Web Interfaces", "author": ["Benoy Antony"], "link": "https://tech.ebayinc.com/engineering/multiple-authentication-mechanisms-for-hadoop-web-interfaces/", "abstract": "Apache Hadoop is a base component for Big Data processing and analysis. Hadoop servers, in general, allow interaction via two protocols: a TCP-based RPC (Remote Procedure Call) protocol and the HTTP protocol. The RPC protocol currently allows only one primary authentication mechanism: Kerberos . The HTTP interface allows enterprises to plug in different authentication mechanisms. In this post, we are focusing on enhancing Hadoop with a simple framework that allows us to plug in multiple authentication mechanisms for Hadoop web interfaces. Note that the Hadoop HTTP Authentication module (deployed as the hadoop-auth-version.jar file) is reused by different Hadoop servers like NameNode, ResourceManager, NodeManager, and DataNode as well as other Hadoop-based components like Hbase and Oozie. We can follow the steps below to plug in custom authentication mechanism. Implement interface AuthenticationHandler , which is under the org.apache.hadoop.security.authentication.server package. Specify the implementation class in the configuration. Make sure that the implementation class is available in the classpath of the Hadoop server. The implementation of the AuthenticationHandler will be loaded by the AuthenticationFilter , which is a servlet Filter loaded during startup of the Hadoop server’s web server. The definition of AuthenticationHandler interface is as follows: The init method accepts a Properties object. This contains the properties read from the Hadoop configuration. Any config property that is prefixed by hadoop.http.authentication. Type will be added to the Properties object. The authenticate method does the job of performing the actual authentication. For successful authentication, an AuthenticationToken is returned. The AuthenticationToken implements java.user.Principal and contains the following set of properties: Username Principal Authentication type Expiry time There are a few implementations of AuthenticationHandler interface that are part of the Hadoop distribution. KerberosAuthenticationHandler — Performs Spnego Authentication. PseudoAuthenticationHandler — Performs simple authentication. It authenticates the user based on the identity passed via the user.name URL query parameter. AltKerberosAuthenticationHandler — Extends KerberosAuthenticationHandler . Allows you to provide an alternate authentication mechanism by extending AltKerberosAuthenticationHandler. The developer has to implement the alternateAuthenticate method in which to add the custom authentication logic. At eBay, we like to provide multiple authentication mechanisms in addition to the Kerberos and anonymous authentication. The operators prefer to turn off any authentication mechanism by modifying the configuration rather than rolling out new code. For this reason, we implemented a CompositeAuthenticationHandler . The CompositeAuthenticationHandler accepts a list of authentication mechanisms via the property hadoop.http.authentication.composite.handlers . This property contains a list of classes that are implementations for AuthenticationHandler corresponding to different authentication mechanisms. The properties for each individual authentication mechanism can be passed via configuration properties prefixed with hadoop.http.authentication. Type . The following table lists the different properties supported by CompositeAuthenticationHandler . The source code for CompositeAuthenticationHandler is attached to the JIRA page HADOOP-10307 .", "date": "2016-10-31"},
{"website": "Ebay-Engineering", "title": "Building a UI Component in 2017 and Beyond", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/building-a-ui-component-in-2017-and-beyond/", "abstract": "As web developers, we have seen the guidelines for building UI components evolve over the years. Starting from jQuery UI to the current Custom Elements , various patterns have emerged. To top it off, there are numerous libraries and frameworks, each advocating their own style on how a component should be built. So in today’s world, what would be the best approach in terms of thinking about a UI component interface? That is the essence of this blog. Huge thanks to folks mentioned in the bottom Acknowledgments section. Also, this post leverages a lot of learnings from the articles and projects listed in the Reference section at the end. Before we get started, let’s set the context on what this post covers. By UI components, we mean the core, standalone UI patterns that apply to any web page in general. Examples would be Button, Dropdown Menu, Carousel, Dialog, Tab, etc. Organizations in general maintain a pattern library for these core components. We are NOT talking about application-specific components here, like a Photo Viewer used in social apps or an Item Card used in eCommerce apps. They are designed to solve application-specific (social, eCommerce, etc.) use cases. Application components are usually built using core UI components and are tied with a JavaScript framework ( Vue.js , Marko , React , etc.) to create a fully featured web page. By UI components, we mean the core, standalone UI patterns that apply to any web page in general. Examples would be Button, Dropdown Menu, Carousel, Dialog, Tab, etc. Organizations in general maintain a pattern library for these core components. We are NOT talking about application-specific components here, like a Photo Viewer used in social apps or an Item Card used in eCommerce apps. They are designed to solve application-specific (social, eCommerce, etc.) use cases. Application components are usually built using core UI components and are tied with a JavaScript framework ( Vue.js , Marko , React , etc.) to create a fully featured web page. We will only be talking about the interface (that is, API) of the component and how to communicate. We will not go over the implementation details in depth, just a quick overview. We will only be talking about the interface (that is, API) of the component and how to communicate. We will not go over the implementation details in depth, just a quick overview. Our fundamental principle behind building a UI component API is to make it agnostic in regard to any library or framework. This means the API should be able to work in any context and be framework-interoperable. Any popular framework can just leverage the interface out of the box and augment it with their own implementation. With this principle in mind, what would be the best way to get started? The answer is pretty simple — make the component API look like how any other HTML element would look like. Just like how a <div> , <img> , or a <video> tag would work. This is the idea behind having a declarative HTML-based interface/API. So what does a component look like? Let’s take carousel as an example. This is what the component API will look like. What does this mean? We are declaratively telling the component that the rendered markup should do the following. Start at the second item in the carousel. Display the left and right arrow key controls. Use \"previous\" and \"next\" as the aria-label attribute values for the left and right arrow keys. Also, autoplay the carousel after it is loaded. For a consumer, this is the only information they need to know to include this component. It is exactly like how you include a <button> or <canvas> HTML element. Component names are always lowercase. They should also be hyphenated to distinguish them from native HTML elements. A good suggestion would be to prefix them with a namespace, usually your organization or project name, for example ebay- , core- , git- , etc. Attributes form the base on how you will pass the initial state (data and configuration) to a component. Let’s talk about them. An attribute is a name-value pair where the value is always a string. Now the question may arise that anything can be serialized as a string and the component can de-serialize the string to the associated data type (JSON for example). While that is true, the guideline is to NOT do that. A component can only interpret the value as a String (which is default) or a Number (similar to tabindex ) or a JavaScript event handler (similar to DOM on-event handlers). Again, at the end of the day, this is exactly how an HTML element works. An attribute is a name-value pair where the value is always a string. Now the question may arise that anything can be serialized as a string and the component can de-serialize the string to the associated data type (JSON for example). While that is true, the guideline is to NOT do that. A component can only interpret the value as a String (which is default) or a Number (similar to tabindex ) or a JavaScript event handler (similar to DOM on-event handlers). Again, at the end of the day, this is exactly how an HTML element works. Attributes can also be boolean. As per the HTML5 spec , “The presence of a boolean attribute on an element represents the true value, and the absence of the attribute represents the false value.” This means that as a component developer, when you need a boolean attribute, just check for the presence of it on the element and ignore the value. Having a value for it has no significance; both creator and consumer of a component should follow the same. For example, <button disabled=\"false\"> will still disable the button, even if it is set to false, just because the boolean attribute disabled is present. Attributes can also be boolean. As per the HTML5 spec , “The presence of a boolean attribute on an element represents the true value, and the absence of the attribute represents the false value.” This means that as a component developer, when you need a boolean attribute, just check for the presence of it on the element and ignore the value. Having a value for it has no significance; both creator and consumer of a component should follow the same. For example, <button disabled=\"false\"> will still disable the button, even if it is set to false, just because the boolean attribute disabled is present. All attribute names should be lowercase. Camel case or Pascal case is NOT allowed. For certain multiword attributes, hyphenated names like accept-charset , data-* , etc. can be used, but that should be a rare occurrence. Even for multiwords, try your best to keep them as one lowercase name, for example, crossorigin , contenteditable , etc. Check out the HTML attribute reference for tips on how the native elements are doing it. All attribute names should be lowercase. Camel case or Pascal case is NOT allowed. For certain multiword attributes, hyphenated names like accept-charset , data-* , etc. can be used, but that should be a rare occurrence. Even for multiwords, try your best to keep them as one lowercase name, for example, crossorigin , contenteditable , etc. Check out the HTML attribute reference for tips on how the native elements are doing it. We can correlate the above attribute rules with our <carousel> example. aria-label-next and aria-label-previous as string attributes. We hyphenate them as they are multiwords, very similar to the HTML aria-label attribute. index attribute will be deserialized as a number, to indicate the position of the item to be displayed. controls and autoplay will be considered as boolean attributes. A common pattern that used to exist (or still exists) is to pass configuration and data as JSON strings. For our carousel, it would be something like the following example. This is not recommended. Here the component developer reads the data attribute data-config , does a JSON parse, and then initializes the component with the provided configuration. They also build the items of the carousel using data-items . This may not be intuitive, and it works against a natural HTML-based approach. Instead consider a declarative API as proposed above, which is easy to understand and aligns with the HTML spec. Finally, in the case of a carousel, give the component consumers the flexibility to build the carousel items however they want. This decouples a core component from the context in which it is going to be used, which is usually application-specific. There will be scenarios where you really need to pass an array of items to a core component, for example, a dropdown menu. How to do this declaratively? Let’s see how HTML does it. Whenever any input is a list, HTML uses the <option> element to represent an item in that list. As a reference, check out how the <select> and <datalist> elements leverage the <option> element to list out an array of items. Our component API can use the same technique. So in the case of a dropdown menu, the component API would look like the following. It is not necessary that we should always use the <option> element here. We could create our own element, something like <dropdown-option> , which is a child of the <dropdown-menu> component, and customize it however we want. For example, if you have an array of objects, you can represent each object ({\"userName\": \"jdoe\", \"score\": 99, \"displayName\": \"John Doe\"}) declaratively in the markup as <dropdown-option value=\"jdoe\" score=\"99\">John Doe</dropdown-option> . Hopefully you do not need a complex object for a core component. You may also argue that there is a scenario where I need to pass a JSON config for it to work or else usability becomes painful. Although this is a rare scenario for core components, a use case I can think about will be a core analytics component. This component may not have a UI, but it does all tracking related stuff, where you need to pass in a complex JSON object. What do we do? The AMP Project has a good solution for this. The component would look like the following. Here again we piggyback the interface based on how we would do it in simple HTML. We use a <script> element inside the component and set the type to application/json , which is exactly what we want. This brings back the declarative approach and makes it look natural. Till now we talked only about the initial component API. This enables consumers to include a component in a page and set the initial state. Once the component is rendered in the browser, how do you interact with it? This is where the communication mechanism comes into play. And for this, the golden rule comes from the reactive principles of Data in via attributes and properties, data out via events This means that attributes and properties can be used to send data to a component and events send the data out. If you take a closer look, this is exactly how any normal HTML element ( input , button , etc.) behaves. We already discussed attributes in detail. To summarize, attributes set the initial state of a component, whereas properties update or reflect the state of a component . Let’s dive into properties a bit more. At any point in time, properties are your source of truth. After setting the initial state, some attributes do not get updated as the component changes over time. For example, typing in a new phrase in an input text box and then calling element.getAttribute('value') will produce the previous (stale) value. But doing element.value will always produce the current typed-in phrase. Certain attributes, like disabled , do get reflected when the corresponding property is changed. There has always been some confusion around this topic, partly due to legacy reasons. It would be ideal for attributes and properties to be in sync, as the usability benefits are undeniable. If you are using Custom Elements, implementing properties is quite straightforward. For a carousel, we could do this. Here the index property gets all its associated characteristics. If you are doing carouselElement.index=4 , it will update the internal state and then perform the corresponding DOM operations to move the carousel to the fourth item. Additionally, even if you directly update the attribute carouselElement.setAttribute('index', 4) , the component will still update the index property, the internal state and perform the exact DOM operations to move the carousel to the fourth item. However, until Custom Elements gain massive browser adoption and have a good server-side rendering story, we need to come up with other mechanisms to implement properties. And one way would be to use the Object.defineProperty() API. Here we are augmenting the carousel element DOM node with the index property. When you do carouselElement.index=4 , it gives us the same functionality as the Custom Element implementation. But directly updating an attribute with carouselElement.setAttribute('index', 4) will do nothing. This is the tradeoff in this approach. (Technically we could still use a MutationObserver to achieve the missing functionality, but that would be an overkill.) Hopefully as a team, if you can standardize that state updates should only happen through properties, then it should be less of a concern. With respect to naming conventions, since properties are accessed programmatically, they should always be camel-cased. All exposed attributes (an exception would be ARIA attributes) should have a corresponding camel-cased property, very similar to native DOM elements. When the state of a component has changed, either programmatically or due to user interaction, it has to communicate the change to the outside world. And the best way to do it is by dispatching events, very similar to click or touchstart events dispatched by a native HTML element. The good news is that the DOM comes with a built-in custom eventing mechanism through the CustomEvent constructor. So in the case of a carousel, we can tell the outside world that the carousel transition has been completed by dispatching a transitionend event as shown below. By doing this, we get all the benefits of DOM events like bubbling, capture etc. and also the event APIs like event.stopPropagation() , event.preventDefault() , etc. Another added advantage is that it makes the component framework-agnostic, as most frameworks already have built-in mechanisms for listening to DOM events. Check out Rob Dodson’s post on how this works with major frameworks. Regarding a naming convention for events, I would go with the same guidelines that we listed above for attribute names. Again, when in doubt, look at how the native DOM does it. Let me briefly touch upon the implementation details, as they give the full picture. We have been only talking about the component API and communication patterns till now. But the critical missing part is that we still need JavaScript to provide the desired functionality and encapsulation. Some components can be purely markup- and CSS-based, but in reality, most of them will require some amount of JavaScript. How do we implement this JavaScript? Well, there a couple of ways. Use vanilla JavaScript. Here the developer builds their own JavaScript logic for each component. But you will soon see a common pattern across components, and the need for abstraction arises. This abstraction library will pretty much be similar to those numerous frameworks out in the wild. So why reinvent the wheel? We can just choose one of them. Use vanilla JavaScript. Here the developer builds their own JavaScript logic for each component. But you will soon see a common pattern across components, and the need for abstraction arises. This abstraction library will pretty much be similar to those numerous frameworks out in the wild. So why reinvent the wheel? We can just choose one of them. Usually in organizations, web pages are built with a particular library or framework (Angular, Ember, Preact, etc.). You can piggyback on that library to implement the functionality and provide encapsulation. The drawback here is that your core components are also tied with the page framework. So in case you decide to move to a different framework or do a major version upgrade, the core components should also change with it. This can cause a lot of inconvenience. Usually in organizations, web pages are built with a particular library or framework (Angular, Ember, Preact, etc.). You can piggyback on that library to implement the functionality and provide encapsulation. The drawback here is that your core components are also tied with the page framework. So in case you decide to move to a different framework or do a major version upgrade, the core components should also change with it. This can cause a lot of inconvenience. You can use Custom Elements. That would be ideal, as it comes default in the browsers, and the browser makers recommend it. But you need a polyfill to make it work across all of them. You can try a Progressive Enhancement technique as described here , but you would lose functionality in non-supportive browsers. Moreover, until we have a solid and performant server-side rendering mechanism, Custom Elements would lack mass adoption. You can use Custom Elements. That would be ideal, as it comes default in the browsers, and the browser makers recommend it. But you need a polyfill to make it work across all of them. You can try a Progressive Enhancement technique as described here , but you would lose functionality in non-supportive browsers. Moreover, until we have a solid and performant server-side rendering mechanism, Custom Elements would lack mass adoption. And yes, all options are open-ended. It all boils down to choices, and software engineering is all about the right tradeoffs. My recommendation would be to go with either Option 2 or 3, based on your use cases. Though the title mentions the year “2017”, this is more about building an interface that works not only today but also in the future. We are making the component API-agnostic of the underlying implementation. This enables developers to use a library or framework of their choice, and it gives them the flexibility to switch in the future (based on what is popular at that point in time). The key takeaway is that the component API and the principles behind it always stay the same. I believe Custom Elements will become the default implementation mechanism for core UI components as soon as they gain mainstream browser adoption . The ideal state is when a UI component can be used in any page, without the need of a library or polyfill and it can work with the page owner’s framework of choice. We need to design our component APIs with that ideal state in mind and this is a step towards it. Finally, worth repeating, when in doubt, check how HTML does it, and you will probably have an answer . Many thanks to Rob Dodson and Lea Verou for their technical reviews and valuable suggestions. Also huge thanks to my colleagues Ian McBurnie , Arun Selvaraj , Tony Topper , and Andrew Wooldridge for their valuable feedback. Custom Elements That Work Anywhere by Rob Dodson HTML APIs by Lea Verou The AMP Project Marko JS", "date": "2017-05-03"},
{"website": "Ebay-Engineering", "title": "A Creative Visualization of OLAP Cuboids", "author": ["Qiaoneng Qian", "Julian Pan"], "link": "https://tech.ebayinc.com/engineering/a-creative-visualization-of-olap-cuboids/", "abstract": "eBay is one of the world’s largest and most vibrant marketplaces with 1.1 billion live listings every day, 169 million active buyers, and trillions of rows of datasets ranging from terabytes to petabytes. Analyzing such volumes of data required eBay’s Analytics Data Infrastructure (ADI) team to create a fast data analytics platform for this big data using Apache Kylin , which is an open-source Distributed Analytics Engine designed to provide a SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets. Apache Kylin creatively applies data warehouse OLAP technology to the Hadoop domain, which makes a query return within milliseconds to seconds against datasets in the PBs. The magic of Apache Kylin is that it pre-calculates the metrics against defined dimensions. So when a query is launched, it doesn’t need to scan PBs worth of source data, but instead it scans the pre-calculated metrics, which is much smaller than the source data to accelerate the query speed. Currently there are hundreds of cubes running on the Kylin production environment within eBay, serving dozens of Business domains in Inventory Healthy Analysis, User Behavior Analysis, eBay APIs Usage Analysis and eBay Business Partner Channel Performance Analysis, etc. This post showcases the creative visualization of OLAP cuboids that has been implemented in the Cube Planner feature built on Apache Kylin by eBay’s ADI team to solve the challenge of showing huge OLAP cuboids in a fixed space. To better understand the challenge as well as the value of the visualization of sunburst charts that have been introduced into OLAP cuboids, some basic concepts need to be covered. An OLAP cube is a term that typically refers to multi-dimensional array of data [ 1 ] [ 2 ] . OLAP is an acronym for online analytical processing [ 3 ] , which is a computer-based technique of analyzing data to look for insights. The term cube here refers to a multi-dimensional dataset, which is also sometimes called a hypercube if the number of dimensions is greater than 3. A cuboid is one combination of dimensions. For example, if a cube has 4 dimensions — time, item, location, and supplier — it has 16 cuboids, as shown here. A basic cuboid has the most detailed data, except for the source data itself; it is composed of all dimensions, like (time, item, location, supplier). It can be rolled up to all the other cuboids. For example, a user can roll up the basic cuboid (time, item, location, supplier) along dimension “supplier” to cuboid (time, item, location). And in this case, the basic cuboid is the Parent Cuboid, and a 3-D cuboid (time, item, location) is a Child Cuboid. The OLAP cuboids visualization has the following characteristics: All the cuboids have a root parent — the basic cuboid. The relationship “rollup to” between two cuboids is directed, from parent to child. The relationship “rollup to” is m:n mapping. One parent cuboid can be rolled up to multiple child cuboids, while one child cuboid can be rolled up from multiple parent cuboids. So the visualization of cuboids is typically a directed graph . But, in the real OLAP world, not all the relationships are kept. The m:n mappings are simplified to 1:n mappings. Every child cuboid would have just one parent. Usually we keep the relationship with the parent, who has lowest row count, and eliminate others, because the lowest row count of parent cuboid means the lowest cost of aggregation to the child cuboid. Hence, the visualization of cuboids is simplified to a tree, where the basic cuboid is the root, as shown below. Even with the simplified relationships between cuboids, there can still be some challenges to cuboids layout with a tree: The tree must be collapsed to fit in a fixed space. It is impossible to have an overall view of all cuboids. Multiple clicks are needed to the target node from root layer by layer. It’s hard to focus on the whole view of all the child cuboids from a given cuboid. Cube Planner makes OLAP cubes more resource-efficient. It intelligently builds a partial cube to minimize the cost of building a cube and at the same time maximizes the benefit of serving end user queries. Besides, it learns patterns from queries at runtime and dynamically recommends cuboids accordingly. In Cube Planner, we want to show query usage down to the cuboid level, which enables the cube owner to gain more insight into their cubes. We want to have a color-coded heat map with all cuboids in one view to give the cube owner an overall feeling of its cube design. Furthermore, when the user hovers over each cuboid, details of individual cuboid — cuboid ID, query count, row count, rollup rate, etc. — would be displayed. We will also recommend a new cube design with recommended cuboids based on the query statistics, thus we will need to put the old cuboids and new cuboids together in one page to show the benefit of cube optimization. We are not able to use any tree or directed graph component to meet all our requirements above. Luckily, our GUI engineer discovered a means to produce sunburst charts, which greatly meet our expectations. Our sunburst charts are created with Angular-nvD3 , an AngularJS directive for NVD3 re-usable charting library (based on D3). Users can easily customize their charts via a JSON API. Go to the Angular-nvD3 quick start page if you want to know more about how to include these fancy charts in your GUI. Basically, at the back end we create a REST API to return the cuboid tree with the necessary information, and at the front end a JavaScript controller parses the REST response to a relative JSON format and then renders the sunburst chart. Below are samples of code from these two layers. Below are some screenshots from the eBay Kylin production environment. With a sunburst chart, cube owners can easily understand their overall cube design with a color-coded cuboid usage heat map. The greater number of dark blue elements in the sunburst chart, the more resource efficient this cube is. The greater number of light blue elements, the more room there is for efficiency. Putting two sunburst charts of both current and recommended cuboids together, the changes become obvious. The cuboids that are recommended to be removed are marked with gray, and greens are recommended to be added. A popup window with more details of the cuboid will be shown when mouse hover over the cuboid element in a sunburst chart. The value of Cube Planner is now apparent. Interaction with a sunburst chart is fast and convenient. The user is able to focus on any cuboid and its children with just one click, and the view changes automatically, like from the left chart to the right chart below. If you want to specify the parent of a leaf, click on the center circle (the part marked yellow). In short, leveraging sunburst charts as OLAP cuboid visualizations introduces a creative way to discover cube insights down to the cuboid level. With these insights, the cube owner is able to have a resource-efficient cube, thus make Apache Kylin more competitive as an OLAP engine on Hadoop. ^ Wikipedia contributors, “OLAP cube,” Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=OLAP_cube&oldid=758240631 (accessed April 11, 2017). ^ Gray, Jim; Bosworth, Adam; Layman, Andrew; Pirahesh, Hamid (1996). “ Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals “. Proceedings of the International Conference on Data Engineering (ICDE). pp. 152–159. ^ “ Just What Are Cubes Anyway? (A Painless Introduction to OLAP Technology) “. Msdn.microsoft.com. Retrieved 2012-07-25. Some graphics copyright Apache Kylin", "date": "2017-05-09"},
{"website": "Ebay-Engineering", "title": "Google Sign-In Integration for the eBay Android App", "author": ["Senthilkumar Gopal"], "link": "https://tech.ebayinc.com/engineering/google-sign-in-integration-for-the-ebay-android-app/", "abstract": "Every day we hear news of data breaches and passwords being stolen or compromised. At eBay, as part of the Identity platform, we work diligently to remove such risk factors. As a part of that diligence, we introduced single-use code-based login last year. Being a mobile first company, we take pride in providing capabilities for seamlessly utilizing the underlying platform, but ensuring high standards of security at the same time. Using Google Sign-In for eBay Android application is a logical evolution of utilizing Google’s verified identity to ensure that the user is provided a secure, but a simpler, login method in their eBay mobile app. The integration steps to integrate the “Sign-in with Google” functionality are pretty simple and well laid out by Google documentation . However, eBay users already have existing credentials they use to login to eBay.com, which raises an interesting problem of managing their identity profiles. The login components should seamlessly enable the users to login to the eBay app with their login credentials and also perform Google-based sign-in as well. We created a user experience that orchestrates an eBay user login flow and allows the user to sign up for Google Sign-In. This login flow helps users establish their identity and also ensures that their future login attempts are simple and just a click away. User Enrollment flow for Google Sign-in Once the user ascertains their identity via their password (after risk evaluations), they will be provided the opportunity to use their Google account for future logins. The prescriptive architecture of the Identity system that orchestrates this flow and persists the identity for future resolution is shown in the following diagram. Enrollment system flow for Google Sign-In Once the user has successfully enrolled, they can use the Google Identity to login for their future attempts. The user experiences a more streamlined and secure mechanism of logging into the eBay app, as shown in the following screenshots. The user is signed in successfully to the app with the enrolled eBay credentials. A representation of the application flow and the system that orchestrates this flow is illustrated below: System flow for Google Sign-In usage for authentication To help match the identity profiles (eBay and Google users), the id_token generated as a result of the user signing into the app is sent to the eBay Identity servers. The id_token is validated and the linkage created between the eBay and Google Identifiers, enabling simpler future logins. The steps for validating are clearly documented . The id_token follows JWT structure and needs to be verified for the following characteristics: The ID token is signed by Google, helping verify its authenticity. The JWT token claim “aud” (audience) identifies the id_token as issued for eBay Mobile application. The claim “exp” (expiration time) identifies the expiration of the id_token . The claim “iss” (issuer) is recognized as Google via the values “accounts.google.com” or “https://accounts.google.com.” Google also provides an open source library that performs these checks. The GooglePublicKeysManager was customized to get the Google public certificates in an eager fashion and is cached for faster authentication. Similar to all the other detection and remediation actions performed as part of risk analysis, login attempts and Identity resolution for users utilizing Google Sign-in are analyzed diligently. In case of risk detection, the Identity resolution system is notified and appropriate actions are performed to ensure safety of the user’s account. The users will also be provided the capability to disable Google Sign-In from Google App permissions and from eBay settings as well. In eBay Identity team, we strive every day to improve a user’s interaction on eBay without compromising the security aspects and, at the same time, provide seamless and non-intrusive authentication mechanisms using state of the art technologies, ranging such as fingerprint recognition to well-established industry standards such as Google Sign-In. These capabilities provide the user with a myriad of authentication and Identity mechanisms, each capable of helping the user shop on eBay with seamless integration with their own device-specific capabilities. The next steps will be expanding the role of Google Sign-In for other devices and soon on the web, too.", "date": "2017-10-24"},
{"website": "Ebay-Engineering", "title": "The Design of A/B Tests in an Online Marketplace", "author": ["Jason (Xiao)  Wang"], "link": "https://tech.ebayinc.com/research/the-design-of-a-b-tests-in-an-online-marketplace/", "abstract": "A/B testing is at the heart of data-driven decision making at eBay when launching product features to our site. However, the tests must be designed to carefully manage the interaction between the test and control groups. Typically, when developing a test, a part of the live traffic is randomly selected to receive a new feature (test) while another part is given the product status quo (control). The randomization is based on a cookie ID or user ID (provided when the customer logs in). It is assumed that customers’ behavior is independent of each other (not affected by other people’s treatment), so that we can extrapolate from the observed difference test vs. control regarding the effect when launching to all customers. The independence assumption however doesn’t always hold, given the complex dynamics inside our marketplace. In one scenario, the experiment is a new ranking algorithm for auction items that introduces factors like predicted item quality, in addition to the default of time-ending-soonest. Imagine that the algorithm does indeed do a better job and that people in the test engage more with the auction. However, a surprisingly large amount of credit will be claimed by the control group. The reason is that, by the time an auction is close to ending, it is easier to get prominent impressions in control and to convert. The observed lift of auction conversions therefore doesn’t provide a good estimation to the actual impact if we launch the algorithm to site. Another scenario where we face the challenge of test-control interaction is with customer-based randomization. The experiment is about offering price guidance to sellers so that they don’t list their items at an unrealistically high price, which reduces the chance that the item will sell. The hypothesis, in this case, is that sellers will take the advice, and eBay overall will sell enough additional items so that the total GMV ( gross merchandise value ) will at least be the same. A natural design is to randomize by seller ID, i.e. some sellers are placed in the test receiving price guidance while others are in the control receiving no guidance. The experiment can be run for weeks, then the amount sold by the test group is compared to the amount sold by the control group. There are at least two possibilities of interaction with this way of testing: Buyers will shift their purchases from control to test to purchase at a lower price. The experiment will show test selling more than control, but there is no net sales gain for eBay ( cannibalization ). Sellers in the control group monitor price changes of sellers in the test group, if they are competitors. The effect of price guidance will spill over from test to control. Martin Saveski, et al. 1 discussed the violation of the independency assumption in social networks and proposed cluster-based randomization. We have a different context and strategy to mitigate test-control interaction. Buyers often shift purchases from seller to seller, but they are less likely to substitute the intention of purchasing one type of merchandise with another type, e.g. replacing a cellphone with shoes. At the same time, cellphone sellers may be competing with each other, but they won’t care how shoe sellers manage their inventories. So instead of randomizing on the seller, the idea is to randomize by the type of merchandise so as to control both cannibalization and spillover. eBay has various ways to define merchandise types, and we chose the leaf category of listed items for randomization. Examples include Cases Covers & Skins , Men's Shoes:Athletic , and Gold:Coins . If a definition has too fine a granularity, it won't help much to control the interactions (e.g. people easily switch from buying an iPhone 8 to buying an iPhone 8 Plus). On the contrary, too coarse a definition, e.g. Electronics , Fashion, and Collectibles , may diminish the sample size so severely that the experiment becomes too insensitive and even useless. The leaf category provides a reasonable balance. There are total about 10,000 leaf categories on the eBay site. Some of them have very sparse sales, for example Collectibles:Rocks Fossils & Minerals:Fossils:Reproductions . Even if we do an even split of 50% vs 50%, the test and control groups will each only have a few thousand effective samples. Moreover, these samples are often incomparable with regards to GMV or BI ( bought item ). Taking the category DVDs & Movies:DVDs & Blu-ray Discs as an example, it sells nearly twice the number of items as the next most sold category. If an experiment is to measure BI and the Discs category is assigned to test by chance, then there is a valid concern of selection bias. Mathematically let $i$ be the index of a category and $x_i$ be the metric of interest during experiment, the *treatment effect* is measured as $$mean(\\{x_i \\thinspace : i \\in test\\})-mean(\\{x_i \\thinspace : i \\in control\\})$$ The concern is that it will largely reflect the inherent variation among selected categories rather than the actual price guidance effect. Difference in differences is a technique commonly used for a remedy. Let $x.pre_i$ be the value of the metric for a period before the experiment, $x_i-x.pre_i$ is the change over time for category $i$. If price guidance works, we expect on average bigger change in test, so the treatment effect can be measured instead as $$mean(\\{x_i-x.pre_i \\thinspace : i \\in test\\})-mean(\\{x_i-x.pre_i \\thinspace : i \\in control\\})$$ While the idea of using pre-experiment data for variance reduction is intuitive, a better way than difference in differences is to use post-stratification . Thinking of pre-experiment characteristics as covariates, Alex Deng and others 2 studied both building regression models as well as constructing strata and established their connection. In our implementation, the data is fit into a linear regression model $$x_i = a + b*I_i + c*x.pre_i$$ where $I_i$ is the indicator variable which equals $1$ if category $i$ falls in test and $0$ in control. Coefficient $b$ is then the treatment effect: when category switches from control to test, the expectation of $x_i$ increases by $b$. Recall that we put forward category-based randomization, but are worried that the small number of categories and their large variation will make the estimate of treatment effect noisy. We plot the daily BI of test categories vs. control categories below. Notice that the green line is consistently higher than the red, and it is exactly because categories like DVDs & Movies:DVDs & Blu-ray Discs are randomized into the test group. The experiment was started on Feb. 1, but due to a logging glitch, data is missing during Feb. 1-14. The missing data is not relevant to our study. The following graph shows the pre-experiment period Jan. 9-31 and the experiment period Feb. 15-March 22. We are interested in not just the testing result, but also comparing the different analysis methods. For that purpose, we do A/A simulation where categories are randomly divided into test and control with no regard to actual treatment and all three equations are computed. The simulation was run multiple times. Since there is no treatment effect, we expect and do see the mean of simulated lift ( treatment effect divided by control mean) is close to 0. Its standard deviation, however, provides a measure of the noise level. As clearly shown in the above table, it is vastly better to leverage pre-experiment data than directly comparing test and control, yielding over a 10X reduction in the standard deviation. Post-stratification provides a further 12% reduction over difference in differences . In summary, we discussed the issue of test-control interaction when conducting A/B tests in an online marketplace and proposed category-based randomization rather than the traditional user-based strategy. To cope with the selection bias, we advocate for leveraging pre-experiment data as covariates in a regression model. Together it gives us a better design to detect the treatment effect with trust and sensitivity. 1 Martin Saveski, Jean Pouget-Abadie, Guillaume Saint-Jacques, Weitao Duan, Souvik Ghosh, Ya Xu and Edoardo M. Airoldi, 2017, Detecting Network Effects: Randomizing Over Randomized Experiments, in Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , p1027-1035 2 Alex Deng, Ya Xu, Ron Kohavi and Toby Walker, 2013, Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data, in Proceedings of the sixth ACM international conference on Web search and data mining , p123-132 Leadership for the development of this method provided by Pauline Burke and Dave Bhoite.", "date": "2018-05-07"},
{"website": "Ebay-Engineering", "title": "The Future of Marko", "author": ["Patrick Steele-Idem"], "link": "https://tech.ebayinc.com/engineering/the-future-of-marko/", "abstract": "At eBay, we’ve completely transformed how we build web applications, starting with the transition from a Java-based stack to a Node.js-based stack. Node.js has enabled teams to move faster, and it offers an abundant ecosystem of tools and libraries that are essential to modern web application development. At eBay, we’ve completely transformed how we build web applications, starting with the transition from a Java-based stack to a Node.js-based stack. Node.js has enabled teams to move faster, and it offers an abundant ecosystem of tools and libraries that are essential to modern web application development. We built Marko , a library for building UI components with minimal boilerplate, five years ago and it has evolved based on feedback from our vibrant and growing community. Marko is completely open source and to ensure that it remains a healthy open source project, we are thrilled to announce that eBay will be contributing Marko to the JS Foundation . Marko will continue to be a key component of eBay’s web application development stack. It takes care of automatically and efficiently updating the DOM in response to data changes. On the server, Marko takes advantage of the asynchronous and streaming primitives provided by Node.js to greatly accelerate the performance of eBay’s pages, ensuring shoppers are getting the fastest experience available when browsing on eBay. At eBay, we were founded with the core belief that we should use technology to empower and connect people globally. In the technology world, we’re a core contributor to and believer in open source technology. Not only does a company culture of open source help us empower our developers, but it also enables our technologists to collaborate across the organization and with peers across the industry. eBay is a member of the Linux Foundation (including the Cloud Native Computing Foundation and the Open API Initiative ) and will continue to actively participate in the open source software community. So what does it mean for Marko to join the JS Foundation? First off, with nearly 20,000 UI components within eBay, we are committed to evolving Marko and expanding the surrounding ecosystem. The Marko core team members that are employed by eBay will continue to maintain and lead the project. As part of the JS Foundation, Marko will reside alongside other notable projects such as webpack and Mocha. By moving Marko to the JS Foundation, we feel that we will be able to more closely align with other projects in the JavaScript ecosystem. In addition, we want to make it clear that Marko has and always will be open to outside contributions and outside maintainers. While we have seen great growth in the Marko community, we believe there is still a lot of potential yet to be unlocked. Through neutral governance and close ties with other prominent projects, we believe the JS Foundation will allow the Marko community to grow and flourish. Marko has a long history within eBay that dates back to 2012, when we started exploring using Node.js as our web application development stack. This was at a time when JavaScript HTML templating was starting to take off. At eBay, server-side rendering was very important, and we wanted support for UI components that provided encapsulation of rendering logic, client-side behavior and styling, and progressive and asynchronous HTML rendering (features that we had on our previous Java-based stack). Dust.js was used by a few teams because it offered streaming and asynchronous rendering, but it lacked support for UI components. Dust.js also provided very few hooks to optimize templates at compile-time, and it promoted what we considered the bad practice of global helpers. eBay open sourced a JavaScript toolkit named RaptorJS that included a very early version of Marko called Raptor Templates. RaptorJS is now defunct, but many of the modules that were part of RaptorJS now live on as independent projects (including Marko). Marko has evolved a lot over the years. While Marko has always had very strong performance on the server and support for basic UI components, many other features came later and were inspired by other UI libraries/frameworks. For example, after React was announced and gained popularity due to virtual DOM (VDOM) rendering and diffing, we also introduced VDOM rendering and DOM diffing/patching into Marko to avoid manual DOM manipulation. However, unlike with React, the Marko VDOM was and will continue to be an implementation detail that could change at any time. Support for single file UI components was inspired by a similar feature found in Vue and Riot.js. Marko has always aimed to stay competitive with other UI libraries by innovating and closely following industry trends while also focusing on keeping the runtime fast and small. Marko is now heavily used within eBay, and it is also starting to be used by outside companies, startups, government agencies and educational institutions. The Marko ecosystem has continued to grow and is now supported in many different IDEs and editors and on services like GitHub. The core Marko team has continued to grow, and it consists of a mix of eBay employees and outside developers. Delivering JS, CSS, images, and other front-end assets to the browser is a fundamental requirement of building any web application. As such, we believe Marko should offer first-level support for an “asset pipeline” to simplify the build chain that most developers are used to. At eBay, we do not have a separate build step. Instead, at runtime we generate all of the JavaScript and CSS bundles required to make the page function. In addition, our tools automatically inject the required <script> tags into the page body and the required <link> tags into the page head. Furthermore, front-end assets such as images and fonts automatically get uploaded to the eBay Resource Server that backs our Content Distribution Network (CDN). We want to introduce this ease of use to all users of Marko. Progressive Web Apps offer a compelling user experience that is reliable, fast, and engaging. We want to help more developers build PWAs and will be rolling out more sample PWAs built on Marko to help guide developers. Integrations with editors and IDEs is a challenge for any new language or framework. We have implemented advanced support for the Atom editor , including autocomplete of both core and user-defined tags, hyperclick to jump to tag definitions, and more. But for other editors, we only provide basic syntax highlighting. Microsoft’s Language Server Protocol gives us the opportunity to write this advanced functionality in a way that can be shared across a growing number of editors. Compiler checks have been used to improve the developer experience: things like misspelled tag names or using deprecated features.  And while these checks are pretty comprehensive, there are certain checks that can only be done at runtime. In the past, we have kept runtime code size small and fast by limiting error messages and runtime error checking. We recently updated Marko to support both a development mode and a production mode, and now we want to take the logical next step to add additional code to our runtime that will provide much friendlier error messages and catch problems earlier. Starting a new web application can be daunting, but having an arsenal of UI components to choose from can be a huge time saver. While it can be challenging to create a UI component that works well for every application, we believe it is extremely helpful for developers to showcase their UI components, even if they are to be forked and adapted for slightly different use cases. With nearly 20k components at eBay, we want to make it easy for our own developers to find the right component for the job, and we’d like to extend this marketplace to the open source community to make it easy to find quality components for use in your app. We are excited about the future of Marko and look forward to building it with the support of the JS Foundation. If you are interested in learning more about Marko, you can get additional information on the Marko website . Join the conversation and contribute on GitHub . – the Marko team at eBay Patrick Steele-Idem Patrick Steele-Idem is a Principal Engineer on the eBay Platform team and is co-leading eBay’s open source program. He is actively engaged in many open source projects (including Marko , Lasso , and morphdom ). Patrick is the original author of Marko and is now leading the Marko core team and the Lasso core team. Michael Rawlings Michael Rawlings is a Senior Software Engineer on the eBay Platform team where he works closely with product teams to improve the way front-end applications are built. He enjoys building tools that improve the developer experience and make it simpler to build scalable and performant apps. Austin Kelleher Austin is a Software Engineer on the eBay Platform team. He graduated from Penn State University in 2016 with a degree in Computer Science. Previous to joining eBay, Austin contributed to Marko and Lasso in his free time.", "date": "2017-10-26"},
{"website": "Ebay-Engineering", "title": "eBay’s New Feature Lets You Organize Your Watch List by Category", "author": ["Gilad Ayalon"], "link": "https://tech.ebayinc.com/product/ebays-new-feature-lets-you-organize-your-watch-list-by-category/", "abstract": "Our new filtering option makes it easier to sift through the Watch List and find what you are looking for. When you are shopping on eBay and you aren’t quite ready to buy—you can add that item to your Watch List and keep track of it while you continue to search. If you’re like me, after a while of shopping and searching, you might end up with more than 30 items from across eBay in your Watch List. I might spend a week or two looking for the best deals on new sneakers. Do I want the latest Stan Smiths ? Tech steel or mint green? I might even take a break and start searching for things for my newborn—and suddenly my Watch List is half Adidas and half Aden + Anais . Maybe I want to see just my shoes or maybe just the baby blankets. Today, we’ve added category filters to the eBay Watch List to help do just that. The new filter feature lets you sift through your Watch List by selecting a specific category of items to view. Watching items on eBay has never been easier with this filtering option that helps you compare and find what you are looking for. When you go to your Watch List, you’ll see different categories for the different types of items you have saved, whether it’s Men’s Shoes or Nursery Bedding! Once you select the category you are interested in filter to, you’ll be able to see all of those specific items in your Watch List. Now, all you have to do is decide which to buy. So, add as much as you want to your Watch List, because this filtering feature will make it easier to sort through. It’s just one of the ways we are expanding the Watch List to make your shopping experience uniquely you on eBay. We’re continuing to improve the filtering system to offer a more flexible and personalized experience. The Watch List category filters are currently live on desktop and on iOS in the U.S., U.K., Germany, Australia, Canada, France, Italy and Spain. They’ll be coming to Android later this year. BIO Gilad Ayalon is a Senior Product Manager at eBay where he focuses on personalization and engagement. He loves the privilege of catering to a diverse, vibrant and global set of engaged customers, and enjoys the unique challenges the scale of eBay’s marketplace offers.", "date": "2017-10-30"},
{"website": "Ebay-Engineering", "title": "Enhancing the User Experience of the Hadoop Ecosystem", "author": ["Benoy Antony"], "link": "https://tech.ebayinc.com/engineering/enhancing-the-user-experience-of-the-hadoop-ecosystem/", "abstract": "At eBay, we have multiple large, multi-tenant clusters. Each of these clusters stores hundreds of petabytes of data. These clusters offer tens of thousands of cores to run computations on the data. We have thousands of internal users who use Hadoop in their roles, including data analysts, data scientists, engineers, and product managers. These users use multiple technologies like MapReduce , Hive , and Spark to process data. There are thousands of applications that push and pull data from Hadoop and run computations. Figure 1: Hadoop clusters, auxiliary components, users, applications, and services The users normally interact with the cluster via the command line by SSHing to specialized gateway machines that reside in the same network zone as the cluster. To transfer job files and scripts, the users need to SCP over multiple hops. Figure 2: Old way of accessing a Hadoop cluster The need to traverse multiple hops as well as the command-line-only usage was a major hindrance to the productivity of our data users. On the other side, our website applications and services need to access data and perform compute. These applications and services reside in a different network zone and hence need to set up network rules to access various services like HDFS , YARN , and Oozie . Since our clusters are secured with Kerberos , the applications need to be able to use Kerberos to authenticate to the Hadoop services. This was causing an extra burden for our application developers. In this post, I will share the work in progress to facilitate access to our Hadoop clusters for data and compute resources by users and applications. We need better ways to achieve the following goals: Our engineers and other users need to use multiple clusters and related components. Data Analysts and other users need to run interactive queries and create shareable reports. Developers need to be able to develop applications and services without spending time on connectivity problems or Kerberos authentication. We can afford no compromise on security. To improve user experience and productivity, we added three open-source components: Hue — to perform operations on Hadoop and related components. Apache Zeppelin — to develop interactive notebooks with queries, programs, and reports. Apache Knox — to serve as a single point for applications to access HDFS, Oozie, and other Hadoop services. Figure 3: Enhanced user experience with Hue, Zeppelin, and Knox We will describe each product, the main use cases, a list of our customizations, and the architecture. Hue is a user interface to the Hadoop ecosystem. It provides user interfaces to several components including HDFS, Oozie, Resource Manager, Hive, and HBase . It is a 100% open-source product, actively supported by Cloudera, and stored at the Hue GitHub site . Apache Airflow allows users to specify workflows in Python. Since we did not want a Python learning curve for our users, we chose Hue instead of Airflow. But we may find Airflow compelling enough to deploy it in future so that it can be used by people who prefer Airflow. Hue allows a user to work with multiple components of the Hadoop ecosystem. A few common use cases are listed below: To browse, manage, upload, and download HDFS files and directories To specify workflows comprising MapReduce, Hive, Pig , Spark, Java, and shell actions Schedule workflows and track SLAs To manage Hive metadata, run Hive queries, and share the queries with other users To manage HBase metadata and interact with HBase To view YARN applications and terminate applications if needed Two-factor authentication — To ensure that the same security level is maintained as that of command-line access, we needed to integrate our custom SAML -based two-factor authentication in Hue. Hue supports plugging in new authentication mechanisms, using which we were able to plug in our two-factor authentication. Ability to impersonate other users — At eBay, users sometimes operate on behalf of a team account. We added capability in Hue so that users can impersonate as another account as long as they are authorized to do so. The authorization is controlled by LDAP group memberships. The users can switch back between multiple accounts. Working with multiple clusters — Since we have multiple clusters, we wanted to provide single Hue instance serving multiple Hadoop clusters and components. This enhancement required changes in HDFS File Browser, Job Browser, Hive Metastore Managers , Hive query editors, and work flow submissions. Figure 4: Hue architecture at eBay A lot of our users, especially data scientists, want to run interactive queries on the data stored on Hadoop clusters. They run one query, check its results, and, based on the results, form the next query. Big data frameworks like Spark, Presto , Kylin , and to some extent, HiveServer2 provide this kind of interactive query support. Apache Zeppelin ( GitHub repo ) is a user interface that integrates well with products like Spark, Presto, Kylin, among others. In addition, Zeppelin provides an interface where users can develop data notebooks. The notebooks can express data processing logic in SQL or Scala or Python or R. Zeppelin also supports data visualization in notebooks in the form of tables and charts. Zeppelin is an Apache project and is 100% open source. Zeppelin allows a user to develop visually appealing interactive notebooks using multiple components of the Hadoop ecosystem. A few common use cases are listed below: Run a quick Select statement on a Hive table using Presto. Develop a report based on a dataset by reading files from HDFS and persisting them in memory as Spark data frames. Create an interactive dashboard that allows users to search through a specific set of log files with custom format and schema. Inspect the schema of a Hive table. Two-factor authentication — To maintain security parity with that command-line access, we plugged in our custom two-factor authentication mechanism in Zeppelin. Zeppelin uses Shiro for security, and Shiro allows one to plug in a custom authentication with some difficulty. Support for multiple clusters — We have multiple clusters and multiple instances of components like Hive. To support multiple instances in one Zeppelin server, we created different interpreters for different clusters or server instances. Capability to override interpreter settings at the user level — Some of the interpreter settings, such as job queues and memory values, among others, need to be customized by users for their specific use cases. To support this, we added a feature in Zeppelin so that users can override certain Interpreter settings by setting properties. This is described in detail in this Apache JIRA ticket ZEPPELIN-1625 Figure 5: Zeppelin Architecture at eBay Apache Knox ( GitHub repo ) is an HTTP reverse proxy, and it provides a single endpoint for applications to invoke Hadoop operations. It supports multiple clusters and multiple components like webHDFS , Oozie , WebHCat , etc. It can also support multiple authentication mechanisms so that we can hook up custom authentication along with Kerberos authentication It is an Apache top-level project and is 100% open source. Knox allows an application to talk to multiple Hadoop clusters and related components through a single entry point using any application-friendly non-Kerberos authentication mechanism. A few common use cases are listed below: To authenticate using an application token and put/get files to/from HDFS on a specific cluster To authenticate using an application token and trigger an Oozie job To run a Hive script using WebHCat Authentication using application tokens — The applications and services in the eBay backend use a custom token-based authentication mechanism. To take advantage of the existing application credentials, we enhanced Knox to support our application token-based authentication mechanism in addition to Kerberos. Knox utilizes the Hadoop Authentication framework, which is flexible enough to plug in new authentication mechanisms. The steps to plug in an authentication mechanism on Hadoop’s HTTP interface is described in Multiple Authentication Mechanisms for Hadoop Web Interfaces Figure 6: Knox Architecture at eBay In this blog post, we describe the approach taken to improve user experience and developer productivity in using our multiple Hadoop clusters and related components. We illustrate the use of three open-source products to make Hadoop users’ life a lot simpler. The products are Hue, Zeppelin, and Knox. We evaluated these products, customized them for eBay’s purpose, and made them available for our users to carry out their projects efficiently.", "date": "2017-05-12"},
{"website": "Ebay-Engineering", "title": "Use Your iPhone X to Shop on eBay with Face ID  ", "author": ["Dave Comer"], "link": "https://tech.ebayinc.com/product/use-your-iphone-x-to-shop-on-ebay-with-face-id/", "abstract": "eBay enables Face ID integration for shopping as easy and secure as ever. From more powerful devices with faster processors, larger screens with richer displays, and seamless biometric authentication options, to the increasingly personalized and streamlined experiences built on top of them, each innovation sets a new, higher expectation for the next, and, as consumers, we expect them to simply work. Apple’s launch of the iPhone X and its much-anticipated Face ID authentication technology was no different. Face ID makes it easier to log in to your phone or your apps. No more having to remember a password or even scan a finger—now all you need is your face. And shopping on the eBay app is just as easy because we’ve enabled Face ID for the latest iPhone X. At eBay, supporting Face ID in our iOS app was key to delivering a great experience and aligned with our promise to make access to eBay feel effortless, while still being secure. Apple made integration with Face ID easy for applications which already supported Touch ID— we only needed to modify our iOS application, and no changes were needed on the server-side. To build the feature in time for the iPhone X release, it just required us to complete development and testing using platform simulation tools, as the devices were not yet available. Additionally, we had a very small window of opportunity to make this happen. Our latest app release was within a few days of the iPhone X release date, which meant we needed to deliver in a compressed timeframe to ensure we were release-ready. Our team of engineers and testers were up to the challenge, motivated to enable a truly delightful experience for our users. At eBay, we are prepared for future innovations in multi-factor authentication, as we have built an extensible architecture based on the Universal Authentication Framework (or “UAF”), as defined by an industry consortium called the FIDO Alliance , of which eBay is a member. This approach allows us to more readily add future authenticators we choose to support. The authentication industry moves fast—and supporting the latest iOS and Android device capabilities is a high priority for us. Using Face ID is truly a magical experience. The depth mapping and infrared imaging technology behind it is brilliant, and it was designed to work with hats, scarves and glasses (even some sunglasses!). And it can be used inside, outside, or even in the dark. From a security and privacy perspective, Apple wants its users to know that a facial ‘image’ is captured as a mathematical representation—not an actual picture—that is encrypted and stored in a secure space on the device. So, the data never leaves the device. If you haven’t tried it, see if you can find a friend or colleague with an iPhone X and check it out. This feature is just another example of eBay leveraging new device and mobile platform technologies to improve our buying and selling experiences, making access to eBay effortless and secure. Logging into eBay with Face ID is now available to our global community of users on the iPhone X, with version 5.16 (or greater) of the eBay iOS app. BIO Dave Comer is Head of Identity Product and Engineering at eBay, where he oversees company efforts related to user onboarding, account access and management.", "date": "2017-11-20"},
{"website": "Ebay-Engineering", "title": "How eBay Service Has You Rollin’ on New Wheels", "author": ["Kenny Crookston"], "link": "https://tech.ebayinc.com/product/how-ebay-service-has-you-rollin-on-new-wheels/", "abstract": "Get the details on our new tire installation service from the product manager who helped create it. How easily can you fit four tires in the back of your car? Can they all fit in your trunk or will two of them need to ride in the back seat? With eBay’s new tire installation service, you can shop eBay’s amazing selection of tires and never have to answer these questions. We’re taking our Parts & Accessories game to the next level, making it even easier to actually get your new tires on your car with our tire installation service. As you might have heard , we rolled this service out in Germany today and it will be coming to the U.S. later this summer. I’m excited about this because it simplifies the journey for our customers and it saves them time. We aren’t just getting you the item but helping you take the next step. You can get the best deal on tires on eBay and then get those tires installed by a local company who knows what they are doing. My team built this new product with eBay’s Add-on services, using Elasticsearch’s geospatial search—a tool that lets us quickly search through our database and find available tire installation locations near you. So, when you buy your tires on eBay, you just have to add on the installation and we’ll use that search tool to pull up a list of pre-screened automotive shops near you, including their locations and price details, then based on that information, you can choose where you want them installed. We will use details about the size of the tires and how many you are buying to match you with the right installation service— all thanks to the work of our Structured Data Team . Then we will actually send the tires directly to the installer because of the work of our Checkout Team—the team responsible for building and enhancing your checkout experience—that allows us to update your shipping location on the fly. Once you pick your tires and choose the auto shop, the auto shop will contact you to set up an appointment and all you have to do is show up with your car—your tires will be there waiting for you. This is just one of the many new and cool changes we are rolling out this year on eBayMotors to make your shopping experience even better. Tires→Trunk: Problem solved. BIO Kenny Crookston is a Senior Product Manager at eBay where he focuses on shopping experiences. He loves the unique challenges offered by eBay’s marketplace, working on new experiences for eBay’s very passionate customers and for a tech leadership team that pushes him and his peers to make our experience better. Kenny is also an avid photographer and formerly worked as a photojournalist.", "date": "2017-05-10"},
{"website": "Ebay-Engineering", "title": "Stepping Towards a Password-Free World", "author": ["Ashok Balasubramanian"], "link": "https://tech.ebayinc.com/engineering/stepping-towards-a-password-less-world/", "abstract": "eBay Identity has taken an ambitious goal of killing passwords for eBay users once for all. However, killing passwords is not a trivial task, as users have been using passwords as a primary authentication mechanism to access e-commerce and financial websites over the past 20+ years. Even with the advent of multi-factor authentication (MFA) mechanisms like one-time passwords (OTP), voice, touch, and face, passwords still prevail as the primary factor for authentication. The adoption rate for these MFA mechanisms is still not substantial, which helps us to understand that users are not comfortable moving away from passwords. We’re aiming to make a password-less experience one that users feel comfortable using, while keeping their trust. eBay Identity’s core strategy is “Seamless when possible, friction when needed.” As part of this journey toward providing seamless access to eBay users without passwords, we launched Google Sign-In for Android last year. We even open sourced “ Universal Authentication Framework (UAF) ,” a component mainly focused on password-free authentication working with FIDO Alliance . We also enabled Touch ID and Face ID on devices that supported them as soon as they were launched. The user on-boarding experience becomes a key factor in realizing this ambitious vision of removing passwords. Because the legacy on-boarding service was running on a monolithic SOA service that was built in 2005, we were not able to make rapid changes to the on-boarding experience. We were crawling when we were supposed to fly. To change the way a user on-boarded with eBay, we had to be on a platform that supported quick Live to Site. So, we took up the task of creating a new user on-boarding service, using latest coding standards, that can facilitate seamless on-boarding of customers to eBay. We recently launched the new on-boarding service live to the customers. The on-boarding service is a state-of-the-art RESTful service that focuses on exposing a portfolio of service operations aimed at managing the complete user life cycle within eBay. We hear about password breaches every day, which reiterates the requirement of sufficient risk checks during account creation. At eBay Identity, we embed security and risk detection at every phase of user state management. The on-boarding service integrates with various components and microservices, which helps provide seamless access to the user and add friction when needed. Let’s take a look the various components involved in making the service seamless and secure: Device Profiling Device Profiling RateLimiter RateLimiter SMART SMART Buyer Risk Detection and Remedy Buyer Risk Detection and Remedy Offline Risk evaluation Offline Risk evaluation Session creation Session creation The service is a combination of a domain service and an orchestration service. The service takes care of all aspects of security and risk by integration with a bunch of microservices. Given microservices integration, parallelizing tasks becomes a key to have a robust service-level agreement (SLA) on the service. The Raptor Orchestration framework comes to the rescue to support both parallelizing and managing dependencies across multiple services. Device profiling We run device profiling as part of on-boarding, and feed in the device signature details to Risk systems. These device signature details are used during subsequent sign in and transaction flows to validate the user authority. Device information is one of the key signals to the risk system in detecting and preventing ATO (Account Take Over) for the user. Ratelimiter Like all other commerce OS services, the onboarding service is also protected by OAuth . To prevent distributed denial of service (DDOS), the on-boarding service is well-protected by multiple layers of rate limiters using our new rate limiting system, Shield. This guarantees throttling traffic based on IP and applications invoking the on-boarding service. Apart from these security measures, we also have client-based whitelisting for the service and whitelisting even at the individual operation level. This means that only specific clients can request specific operations. SMART Project SMART (Stopping Mass Registration At near Real Time) is aimed at stopping DDOS and other BOT attacks while on-boarding users. Every account that is created in eBay goes through a SMART risk assessment system, which runs a whole lot of velocity checks against user attributes and device signature attributes. If the SMART system flags an account during on-boarding, then the account creation might be blocked or a challenge issued. This challenge is usually a captcha, but this can be enhanced in the future. Verification Every user trying to onboard with eBay is also subjected to a risk check. As a part of this process, information such as the IP of the machine, useragent of the browser, etc., is used in assessing the risk level of the account as HIGH or LOW. Based on this risk level, all HIGH risk users will be either blocked or challenged with email, phone, and or credit card verification to confirm their identity. The user will be on-boarded only when this verification step is complete. Session Creation After account creation, the session for the new user is also provided for by the service, as it returns either cookies or a token. The cookies returned by the service can be directly used by the clients to create a web session for the newly created account. The token, encoded with user credentials, can be used by mobile clients to create a session in native applications. Given that the new state-of-the-art on-boarding system gives us the ability to run, now it's time to spread our wings and fly. Social sign up is the next big step in the on-boarding experience charter. Social sign up focuses on delegating authentication to social websites. Another big stride in account management would be the ability to handle sub-accounts. A clear separation between an account and a login provides many opportunities to explore.", "date": "2018-05-15"},
{"website": "Ebay-Engineering", "title": "eBay’s Latest Feature Aims to Inspire with a Store Just for You", "author": ["Evan Thomas"], "link": "https://tech.ebayinc.com/product/ebays-latest-feature-aims-to-inspire-with-a-store-just-for-you/", "abstract": "Hear from the product manager who created our newest personalization experience. I’ve found the perfect store for me. It’s filled with Marvel superheroes, NBA superstars, 80s bands, 90s cartoons, smart home electronics, dragons, luxury watches, retro kicks, Portuguese coffee, and Scandinavian furniture. No other store like this exists in the world. This is my store—the “Store of Evan”—and it’s open 24 hours a day. I am literally the only customer, and it’s not on any city block, it’s on every city block in existence, because it’s in my front pocket. This is the new experience on eBay’s mobile homepage: Interests . eBay’s Interests takes personalization to the next level—nowhere else can you visit a store filled with basketball gear, Herman Miller furniture and even indie board games. But on eBay, you can select the Portland Trail Blazers, Mid-Century Modern and Tabletop Gaming as Interests, and we’ll surface relevant listings on your customized homepage. With more than 1 billion listings in our global marketplace, we are the only company who can actually support the hundreds of constructs of Interests simultaneously. What makes the work that we’re doing with Interests unique is how we’re personalizing based on the things you love in life, not just what you’ve shopped for recently. Interests allows shortcuts into eBay worlds of Heavy Metal, Hiking, Scrapbooking or Nintendo—allowing you to cut through a billion listings and surface items just for you. We’ve constructed an ecosystem of things, more than a category, and centered on the lifestyle of activities, passions and styles. In this new eBay, yoga isn’t just about fitness mats and leggings—it’s about woven blankets, meditation and aromatherapy—things that wouldn’t conventionally fall into the category of “Sports & Fitness.” When my team and I started out, we embodied a philosophy of creating human-centric structures based on simple questions that frame each individual’s shopping experience and will ultimately lead you to find your version of perfect. What do you like to do after work? What do you obsess about? What is your style? Who do you cheer for at concerts or sports games? This is a radically different way of thinking about our taxonomy when compared to products or categories—but it is actually reflective of how real people shop. You don’t want to go into a mall in which every store is “Sweater Store,” “Shoe Store” and “Chair Store.” Instead, you walk into a boho chic clothing store because you’re dressing for Coachella, or you walk into an outdoor store because you love camping. Our aim with Interests is to match the experience of those stores that tap into narrowly focused passions. Creating these human-centric structures out of eBay’s diverse marketplace is quite difficult. We had to consider that each Interest has unique brands, styles, characters and materials that make it distinct, and that only its specific customers know about. Real high-end audiophiles don’t care about Skullcandy. They care about Klipsch speakers and Sennheiser headphones . Knowing this, we built the foundation of Interests with a team focused on curating and researching these attributes. We had to start at the foundation: bring in the people who know, or can research, what those attributes are. Once we identified each Interest based on demand (on and off eBay), and each of its most important attributes, we use or create that metadata in eBay’s Structured Data platform, so we can stitch together thousands of pages and items into our new Interest Taxonomy. eBay already has pre-structured pages with a single brand and single category (like: Wolverine Boots ), but we had to build a capability to attach hundreds of these individual pages’ metadata together, then generate a new SEO rich page at that intersection. In many cases, we had to do that two or three levels deep to adequately create the shopping pathway we aspired to achieve. Doing that for every Interest identified was just the first step. Then, we made the pages diverse and displayed the best of our selection within each concept. Hiking included Wolverine Boots, but it also included Coleman tents , Yeti canteens , Arc'teryx daypacks and more. We spent a long time trying to problem solve how to distill eBay’s massive selection down to singular pages. To keep the pages diverse and fresh, we added a presentation layer on top of our new SEO rich pages that would strategically cycle through inventory to optimize per individual, per Interest, per week. We can control the frequency of how often the pages update, and the inventory selected within each Interest on the fly. If we leave them alone, the weighting of inventory, the SEO traffic and the confidence we have in the inventory can drive the ship all on their own. We sprinkled in some graphic design and imagery, and we now have a page that can have hundreds of millions of permutations and inventory cycling, to keep the hikers of eBay coming back for more. Creating pages is one thing, but to get you to organically find the pages that belong to an Interest during your shopping journey, is a whole different challenge. We want our taxonomy to be the simpler way to dive into eBay for our customers—several hundred that are incredibly distinct, rather than thousands of categories with some so similar in nature that customers don’t know where to begin browsing. That’s where we turned to our data science algorithms. Based on your prior shopping funnel behavior, the types of items you look at, and purchases, we could effectively develop a confidence score that maps you, as a shopper, to Browse pages. Take it one level of data extraction further, and if we see that you are mapped to numerous Browse pages within an Interest construct, then we can implicitly associate you with that Interest at another confidence score. Deeper still, we started looking at the attributes within an Interest that you look at most and recommending other attributes within the same Interest that you’ve never looked at before. We want to do more than just help you find what you’re looking for, but also help you find what you didn’t know you were looking for. This is just the beginning. The Store of You is the eBay of tomorrow. eBay Interests is now live on the eBay app.", "date": "2018-05-17"},
{"website": "Ebay-Engineering", "title": "Finding That Perfect Gift on eBay Is as Easy as Asking a Friend ", "author": ["Jay Vasudevan"], "link": "https://tech.ebayinc.com/product/finding-that-perfect-gift-on-ebay-is-as-easy-as-asking-a-friend/", "abstract": "Inspired by the service in a traditional retail store, eBay is enabling a truly personalized shopping experience using Google Assistant. Searching for that perfect holiday gift or the best deal this holiday season? Finding that special gift, let alone one at a great price during the holidays is hard. But this holiday season, finding a great deal can be as easy as asking a friend — you can do it right from your couch, without lifting a finger, literally. Just tell your Google Home, “Ask eBay...” And in addition to snagging that perfect holiday deal, you can also find out what things lying around your house are worth, so you can turn them into cash and get something new right in time for that holiday shopping spree. We are reimagining the shopping experience, using artificial intelligence to make it easier and more personalized. Starting today, we are bringing the best of eBay to your home with Google Assistant, making shopping as simple as having a conversation. Using AI, we are able to create a truly personalized shopping experience inspired by the service you’d expect in a traditional retail setting. By combining eBay’s breadth of inventory and unique selection, the eBay Shopping Assistant can create a shopping experience for virtually everyone. This is the inspiration behind some of the exciting work we are doing at the intersection of artificial intelligence, cloud computing and a new commerce service that’s available wherever shoppers are. Talk to eBay on Google Home We’ve collaborated with Google to be among the first to take advantage of the multi-surface switching capability, which lets you carry a voice conversation on Google Home over to your phone, seamlessly. So when you tell Google Home that you want to buy an item on eBay, you can easily switch to your phone, take a peek at the deal recommendation to make sure everything is correct and buy with confidence. The ability to switch from a voice platform like Google Home to your phone offers a convenient personal shopping experience that you can start in your kitchen, living room or your car and continue on your phone, wherever you are, whenever you need it. We’ve also made updates to the What’s It Worth? experience on Google Assistant, so you can quickly find the value of your things to sell on eBay. AI’s Potential to Transform Shopping At eBay, we’ve developed some of the world’s most transformative and disruptive marketplace capabilities and commerce experiences. For over two decades, we’ve worked to make buying and selling effortless. As a result of these foundational ecommerce pillars, shoppers expect an experience nothing short of seamless. As we look to the future of shopping, powerful voice interfaces like Google Assistant will most certainly drive these experiences. Natural language is rich with context and meaning. When paired with the science of understanding language, deep learning and predictive modeling, we can begin to harness the power of human intent, enabling us to create a radically better and more personalized shopping experience for everyone. As you develop a relationship with the eBay Shopping Assistant by giving more information about your preferences, you will be rewarded with a more tailored experience over time. We are in the early stages of this new commerce frontier, but we are excited about the prospects of bringing the eBay shopping and selling experience closer to our customers’ everyday lives. AI has the potential to dramatically reshape online shopping with breakthrough merchandising and personalized shopping experiences. Paired with the power and reach of cloud computing, commerce can take place where and when you want it. And, when combined with eBay’s inventory and knowledge base, we see something really exciting and transformative: online shopping will be as easy as talking to a friend. Google Assistant is available on your Google Home, Home Mini devices, as well as via Google Assistant apps on Android and iOS . You can talk to eBay on Google Assistant, including Google Home, in the U.S. and Australia, just by saying “ Hey Google, ask eBay .” BIO Jay Vasudevan is a Lead Product Manager at eBay, where he and his team focus on AI-augmented shopping assistants to offer a truly ubiquitous personal shopping experience.", "date": "2017-11-28"},
{"website": "Ebay-Engineering", "title": "Identity Management Strategy as a Relying Party for OAuth 2.0", "author": ["Senthilkumar Gopal"], "link": "https://tech.ebayinc.com/engineering/identity-management-strategy-for-oauth-2-0/", "abstract": "Users encounter new products every day, and their real-world entity manifests itself in multiple virtual identities across the internet. Creating an account or authenticating one’s virtual identity is a necessary and ubiquitous precursor to performing any meaningful action on a website, such as purchasing an item, posting a message, etc. This article reflects upon a well-defined orchestration on how to integrate identity providers while acting as a relying party, the strategies followed to resolve collision in identities, and its overall architectural flow. Due to the fragmented nature of identity systems and duplication of similar sets of principles across the web, users are forced to generate virtual identities for every different website, leading to churn and business loss. One of the key standards that was established to address this problem was OAuth 2.0 , which provides a specific set of secure and easily configurable steps to seamlessly and securely transfer one’s virtual identity and use the established identity to authenticate. Any identity provider that supports OAuth 2.0 usually provides extensive samples and documentation on how to integrate with their Authorization Code Profile , which is the most commonly used grant type for securely integrating the provider’s identity. The Relying Party follows the typical OAuth 2.0 authorization code grant protocol to obtain a user’s identity information, such as first name, last name and email address. Typical OAuth 2.0 Flow Because of well laid-out documentation, integration with the OAuth system of an identity provider is straightforward, from registering an OAuth app to implementing the authorization code grant and code to access token exchange. However, the complexity arises when the User Identity determined by the identity provider already exists within the relying parties’ account system, such as when a user who is not authenticated yet on the relying party clicks on “Sign in with Identity Provider,” authenticates with the IDP, and shares their primary identifier as xyz@email.com . If this primary identifier already belongs to an existing account, it formulates an interesting problem to solve. To address user scenarios such as above, the following account identity resolution strategy ensures that a user’s account in the relying party is provisioned and secured via established authentication mechanisms. These identity providers can be added as factors for authentication in this multi-factor authentication journey. The following flowchart provides a step-by-step flow on the functioning of the account resolution process and how the user is seamlessly either authenticated or provisioned with the help of an identity provider as its authentication factor. Unlike other applications that utilize the identity provider as their User Identity system, the relying party can instead integrate the identity provider as an “Authentication Method,” where their function as identity provider is translated into one of the factors that the user’s account can use to access their account. This enables the relying party to enforce contextual authentication and higher degrees of security controls when required. At any point of its lifecycle, the user account would be provisioned and have capabilities enabled via multiple identity providers. Authentication methods associated with the account can then be managed by the user and contextually authenticated by the relying party itself without any dependencies on the Identity provider if required. A representative application architecture of such an integration with an identity provider is depicted below. The various systems interact with each other to identify the account under the relying party user account system or allow the user to create an account with the identity provider as one of its authenticators. One of the key goals while integrating with other identity providers is to protect a user’s privacy and ensure that no information is being shared either directly or implicitly with the identity provider other than the essential OAuth 2.0 specification parameters. To achieve high levels of privacy, the key integration points with the identity provider should be proxied or obfuscated to prevent any implicit inferences from being identified. For example, when a user is being redirected to the identity provider, the session information or its identifiers should be obfuscated, even within the state parameter, either using encryption or randomly generated proxy references. When the identity provider redirects back to the relying party, the callback URL acts as a proxy which in turn redirects to the original page and prevents any information from being shared with the identity provider implicitly. The relying party, in order to ensure privacy, should request only basic information (such as first_name, last_name and/or email) and minimally required permissions from the identity provider during the OAuth 2.0 integration. This information should be used primarily for identity resolution or user account creation based upon the resolution results. The relying party should not request any other information and nor request access to act on behalf of the user if the intent is to use the identity provider as an Authenticator. The relying party should have enhanced anomaly detection and risk analytics that factors in identity provider-based authentications as a new plane of investigation. Similar to any regular login attempts, success or failure attempts to use identity provider authentications should be diligently investigated for risk and appropriate actions should be taken for mitigation in case of any red flags. In such scenarios, identity resolution systems should be immediately notified to ensure that the suspected identity provider linkages are deactivated or completely disabled based on the analysis results to protect the user’s account from malicious activities. The above detailed steps and strategy can be used to perform identity resolution with other identity provider schemes such as Open ID Connect as well. The integration points to transfer the identity information from the identity provider to the relying party might vary depending on the grant profile being utilized. However, using the strategy of the provider as an authenticator, resolution strategy flow, and other patterns for privacy, contextual authentication is easily adaptable for other schemes of identity providers. This is just one of the steps toward a more open, multi-factor contextual authentication that allows users to continue using their relying party in a more secure and trusted manner. With the improved usage of commonly used identity providers for authentication, relying parties are one step closer to eradicating passwords from the world of secure authentication. This can be further extended to help users secure their account with specific identity providers as an authentication and authorization factor at various checkpoints and to enable other relevant identity providers based on their geographical significance to achieve multi-factor authentication. https://oauth.net/2/ https://oauth.net/2/grant-types/authorization-code/ https://openid.net/connect/", "date": "2018-10-10"},
{"website": "Ebay-Engineering", "title": "Functional Thinking", "author": ["Tony Da Silva Bernardino"], "link": "https://tech.ebayinc.com/engineering/functional-thinking/", "abstract": "Functional Programming is more than a set of principles. It's a paradigm; a different way of thinking. It focuses on the “what” is being done, as opposed to the “how” it’s being done as in Object Oriented and Imperative paradigms.", "date": "2018-05-09"},
{"website": "Ebay-Engineering", "title": "Going the Distance — Edit Distance 3", "author": ["Silvio Picinini"], "link": "https://tech.ebayinc.com/research/going-the-distance-edit-distance-3/", "abstract": "How do you normalize Edit Distance? Some simple ideas to get useful numbers about the changes in your text. One thing is absolutely true: the absolute number that is Edit Distance (see our previous post) is not very useful in most MT cases. A change of 5 words in a sentence of 20 words is an Edit Distance of 5 (and is 25%). A change of 2 words in a sentence with 4 is an Edit Distance of 2 (and is 50%). So, 5 words looks like much more than 2 words, but 25% is less than 50%. That is why we need to use percentages or relative numbers, because the change effort needs to be placed in context, related to the length of the text. I used word counts below because they are easier to visualize. Let’s talk about naming: TAUS is calling this Edit Density, which is a fine name. I used to call it “Percent Edit Distance” or “Percentage of Change”. It is easy to slip into saying an “Edit Distance of 30%”, but strictly the “pure” Edit Distance is in words or characters (or operations of change applied to words or characters) and it is not a percentage. TAUS presents it as number of edits per 100 characters, which is a percentage. Now that we see that we need a percentage, comes the question: the Edit Distance should be divided by… what? Should be a percentage of what? You may hear this question as “how should we normalize Edit Distance?”. There are various statistical definitions for normalization, but what we want to do with Edit Distance is simply called scaling: we want to bring the values into the same range, so that we can compare them. It is a simple form of normalization. There are three possibilities: Divide by the initial (MT) count In MT, this means that you are calculating based on the number of words that the posteditor started working with. Divide by the final (PE) count In MT, this is the postedited word count, the number of words that the posteditor ended with. Based on the maximum between both initial and final The numbers are between 0 and 1 One could argue that using the MT count lets you know the costs before the work is done, while the others require the work to be finished to get the number. I think that this is traditionally why the industry uses source word counts, because you know the count before you work on it. Then you can give your price to the client, and the translators knows how much they will make at the beginning. But it doesn’t necessarily have to be that way. Translators many times struggle with projects that took much more effort than they initially estimated. I can’t think of a reason to use the PE count. If you have one, please share. The best metric is to use the maximum between both MT and PE. Placing the results between 0 and 1 mean that your percentage is between 0 and 100%, which is very convenient to create charts. Here is how this works. Let’s take our previous example of “Roses are sometimes red > Violets are blue and you are sweet”. There were 7 changes in this transformation, the Edit Distance is 7 words. You can see the % based on MT getting a high score of 175%. But something different could happen: The % based on the PE could go high, or the MT could go really high to 400%. Meanwhile, the % on Max is always well behaved, between 0 and 100%. So, let’s use Max. What do you think? You can find the first two articles in this series at Going the Distance — Edit Distance 1 and Going the Distance — Edit Distance 2 . If you enjoyed this article, please check other posts from the eBay MT Language Specialists series .", "date": "2019-08-28"},
{"website": "Ebay-Engineering", "title": "Adapting Continuous Integration and Delivery to Hardware Quality", "author": ["Ashvini Mangalvedhekar"], "link": "https://tech.ebayinc.com/engineering/adapting-continuous-integration-and-delivery-to-hardware-quality/", "abstract": "A hyperscale infrastructure demands a high level of automation to hardware testing to increase productivity and rigor. The idea was to automate the traditional methods of qualifying servers and server components by applying CI/CD (Continuous Integration and Continuous Deployment) principles of software development to the hardware development lifecycle. What got us started Systems hardware engineering has always had a different and a lower degree of automation than software development. In some ways, physical tests and infrastructure makes it a harder problem. Traditionally, hardware engineers have been qualifying servers primarily with manual testing. This is not only a time-consuming process, but it also limits the rigor of testing. And we all know manual testing cannot be scalable. Wait, we do similar things in software Qualifying hardware involves a lot of similar activities to qualifying software, such as configuration, writing and maintaining test cases/test scripts, triggering various test scenarios, collecting metrics, analyzing the results, keeping track of the logs, notifications, etc. etc. Orchestrating these tasks in the right sequence to implement an automated regression testing pipeline would simplify the tedious hardware validation processes. We came up with a novel approach using CI/CD concepts. We developed four key modules in-house and for the core, we used familiar DevOps (Development Operations) tools like Jenkins, GIT, and Puppet, and deployed our custom modules around a Jenkins pipeline. The Whiteboard The design is a hardware regression system that enables the hardware team to productively conduct reliability and performance testing of a continuously changing physical infrastructure at a click of a button. It interfaces seamlessly with platform software via standard GIT repositories and familiar Grafana dashboards. Something old, something new The tools we use include the following: Reuse Source Control System: GIT Orchestrator: Jenkins Server Configuration System: Puppet Analyze Time Series Data: InfluxDB/Grafana Create Lab reservation System: In-house developed tool to keep track of our assets in the lab Composer: In-house developed utility to compose the required testing sequence Test Executor: In-house developed framework to execute tests on the desired systems under test Results Dashboard: In-house developed UI to display test results, links to logs and graphs Just like software code needs to be deployed into production environment, we treat our severs as hardware to be deployed into our data center. Any code changes made to a software application goes through a series of testing, starting with unit testing, integration testing and regression testing. Similarly any changes made to our servers and server components like drives (SSD, HDD), DIMMS, NIC, BMC, BIOS, even firmware/driver upgrade, go through similar testing activities. Features This regression testing facility has the ability to trigger tests: On a schedule By manually clicking the start button By automatically triggering tests by detecting changes made to the source control system. It has three different types of testing categories: Unit Testing: To qualify individual server components including firmware/driver upgrades Integration Testing: To qualify the server as a whole (Sysbench testing) Regression testing: Full stack application benchmarking of various eBay applications like Cassini (eBay Search Engine), Cloud (front-end applications), NoSQL, Hadoop, Zoom (Object Store), etc. that run in our data centers Regression testing: Full stack application benchmarking of various eBay applications like Cassini (eBay Search Engine), Cloud (front-end applications), NoSQL, Hadoop, Zoom (Object Store), etc. that run in our data centers Proof of the Pudding…. We have successfully deployed this Regression Facility to qualify and release two generations of eBay servers (Intel Broadwell and Intel Skylake) and continue to on-board a variety of hardware/SKU combinations. Our customers are primarily our hardware engineers who work with various external vendors to get evaluation hardware into our labs. Then we partner with our internal eBay application teams that consumes these servers to develop performance application benchmark which qualifies these server using our automation framework. … is in the Eating! This automated regression system can execute tests at a click of a button, in parallel, on a variety of SKUs and different applications at the same time. The results are presented on a dashboard along with links to logs, metrics, and charts, making it easy for our hardware engineers to analyze the data. This helps them make informed and quick decisions regarding the hardware reliability and performance. People time slashed dramatically! At least an order of improvement in many cases. Manual errors are avoided, except when someone writes wrong test case, of course. 24/7 efficiency of regressions - automatically triggered Hardware engineers have now more time for new hard engineering challenges The final product: eBay built, tested and deployed server in our data center Metrics Real-time Cassini application (eBay Search Engine) testing metrics: Queries Per Second (qps) and Latency captured during a Cassini benchmark regression testing run in our lab Real-time Cassini application (eBay Search Engine) testing metrics: Queries Per Second (qps) and Latency captured during a Cassini benchmark regression testing run in our lab Real-time Cloud application testing using SpecJBB benchmark: Critical jOPS & Max jOPS captured during regression testing run in our lab Real-time Cloud application testing using SpecJBB benchmark: Critical jOPS & Max jOPS captured during regression testing run in our lab Real-time System metrics: CPU and memory utilization captured during regression testing run in our lab Real-time System metrics: CPU and memory utilization captured during regression testing run in our lab Acknowledgements Implementing the regression system was indeed a collaborative effort. Thanks to my team, Mike Bernat, Jay Subramani, and Vedang Joshi for their contributions. And special thanks to Manoj Wadekar and Jay Shenoy for guiding us through this process.", "date": "2018-09-18"},
{"website": "Ebay-Engineering", "title": "Drag and Drop Search: A New Way to Search on eBay", "author": ["Ravi Pitapurapu"], "link": "https://tech.ebayinc.com/product/drag-and-drop-search-a-new-way-to-search-on-ebay/", "abstract": "eBay has over a billion listings across a wide variety of categories that millions of buyers search every day. It is a significant challenge to connect the right buyer to the right listing and in the process to present the user with similar items that are suited to the buyers inputs. Each of those listings has multiple images in various forms and styles. The challenge is how to use computer vision to sift through the billions of listings on eBay and show the buyer visually similar items. The word “similar” could have different implications when we consider that the shopping context varies widely from user to user. To classify our buyers, consider these two scenarios: Let’s say a buyer knows exactly what she wants and searches for it on eBay. From the extensive list of search results, she manages to find something that suits her requirements. Now before she wants to buy it or search elsewhere for a better deal, she may not have selected the best possible item that is available on eBay. Similar listings in this scenario could be a set of listings with all the buying options and price points. Knowing this can enable the user to make a more informed decision. Now, let’s say we have another buyer who has some idea of what he wants. He came to eBay and is exploring all the variety of listings that serve his needs. He browses, looking for that special treasure. We all know eBay has a wide variety of similar items that can serve a specific need, but showing them in relevance could be the challenge. \"Similar\" here could mean different listings from different manufacturers, different quality, or different aspects. eBay has solved some of these key issues by introducing multiple Image Search features. However, there are some limitations, such as the quality of the picture, access to the image, and the clarity of the item we want in the image. How can we address all these points while keeping the user at the center? We present to you our new visual shopping Drag and Drop Image Search experience on the native iOS and Android eBay apps. The idea is to make use of the rich content that our sellers put together and trigger an image-based search while keeping all the aspects in the search context, thereby delivering results that clearly match the users' interests. eBay will be the first ever ecommerce platform that engages the user with this image-based search that adds a fun factor to the search process. How do you use Drag and Drop? Just open the eBay app, search for an item that piques your interest. On the search results page, tap and hold on a listing image. The rest of the flow is intuitive. You see a drop zone where you can drag the listing image and drop it to trigger a visually similar search that is more contextual than ever. (You can also use Drag and Drop on the app's Home page.) The simplicity in the process and the richness in relevant results shows the engineering excellence and the fruits of structured data and emerging computer vision technologies we harbor. The engineering teams at eBay are dedicated to delivering the most intuitive experiences that help delight users on our platform. Touch is the fundamental input means on our mobile platforms. And with touch, we have a host of gestures that could be that much more effective when used in accordance with the right context. The native apps are powered by a host of services, including Experience standard services and the Domain services. Both of these services are governed by certain common guidelines which we call Commerce Operating Standards. Depending on the service layer, we have different frontend components that power the overall experience. We needed a solution that works all over the place. We made use of the Drag interaction APIs from Apple iOS 11.0 that has no dependency on the service layer. However, plumbing the required image data and relevant information called for generic Drag and Drop handlers that could be incorporated on both service standards with little code changes to the View layers. With Drag and Drop APIs, views with content can be dragged from within and outside the host app into specific areas of the host app. But incorporating this into the eBay iOS app came with its own challenges. The Search Results Page is especially tricky, as we have both service versions catering to our user base. All the experiences that we built were following our in-house built Model View Content Controller architecture. The idea behind this architecture is to transform the data models generated from JSON responses to view models. These view models, when passed to a content controller, are translated to views. When adding the Drag and Drop API on the image view inside the listing view, a wrapper for all listing-related subviews is not straightforward. On top of this, the ListingSummaryView is inside the ComponentUI target. All the drag delegate handlers are implemented on the eBay app target, which led to further challenges. To overcome these challenges, we masked the drag handler object as NSObjectProtocol and plumbed it into the ListingSummaryCellModel. This, in turn, is applied by the cell controller to the imageView. For optimization, we reuse this view as the user scrolls down. We add the UIDragInteraction with the delegateHandler each time the view is prepared for use, and remove the same when the view is prepared for reuse. iOS is a big platform for ecommerce that could deliver delightful front end experiences. Visual Shopping is an existing feature on the platform, but bringing it out of the search ecosystem and stitching it with every listing possible is a challenge that we had to overcome. Initial designs were to introduce a button icon to one of the corners over each listing image that could potentially launch a visual search with that image. However the challenge was to capture the users intent to go to a View Item page, but accidentally tapped the icon or vice versa. To solve this, we came up with a drag gesture-based trigger that clearly separates a tap from tap and hold. The next challenge was to present a valid drop zone. We needed some place to drop the dragged image that the user could intuitively feel would initiate a search. At the same time, we should make that drop zone the obvious place to drop the image. We came up with a design to mask all the available area and present a big search box-like drop zone on top of the screen, making it unmistakably the only place you could drop the image. We will continue to improve Visual Shopping, adding more options and expanding the feature to more platforms. eBay continues to spearhead ecommerce with latest intuitive technological advancements and this moves us one step ahead in that direction. Enhancing user experiences and catering to users' needs in more than one way has always been at the core of what we are and what we do. There were many people involved in the design, build and testing of Drag and Drop search without which this could not have been possible. Recognizing them: Jimmy Lui from the Verticals Android Team, the Verticals Product and Design teams, and the Computer Vision team.", "date": "2018-09-27"},
{"website": "Ebay-Engineering", "title": "Unicorn—Rheos Remediation Center", "author": ["Lubin Liu"], "link": "https://tech.ebayinc.com/engineering/unicorn-rheos-remediation-center/", "abstract": "Rheos is eBay's near-line data platform, and it owns thousands of stateful machines in the cloud. The Rheos team has been building and enhancing the automation system over the past two years. However, it’s time to unify the past work and build a modern, automatic remediation system, Unicorn. Unicorn includes a centralized place to manage operation jobs, the ability to handle alerts more efficiently, and the ability to store alert and remediation history for further analysis. Managing thousands of stateful machines in the cloud is challenging. Operation tasks come from two categories: Hardware failures in the cloud happen everyday Stateful application-specific issues when scale grows Previously, Rheos had tools to remediate clusters. Many tools are scripts and run in separate places. This kind of dispersive tool-based remediation has several limitations: Hard to develop and maintain Conflicts between tools Learning curve for new support candidates Not truly automated, costing human efforts Rheos already delivers a bunch of metrics and alerts. If we can collect these inputs in a centralized place and associate them with other external information, a rule-based remediation service could heal the clusters automatically. Handling the alert with automation flow will: Reduce human efforts Reduce the service-level agreement (SLA) response time to handle alerts Automation can’t cover all the issues at the very beginning and may also introduce new issues if the algorithm is not good enough. Storing the history in a centralized place could help the team improve the system. On the other hand, by analyzing the alert history, the team can try to find new automation scope. Building such a centralized remediation center is not easy, for at least two reasons. Building a generic model to cover known experiences that is easy to extend—Building a specific tool to solve a specific problem is a very straightforward solution. On the contrary, identifying a common pattern and model to apply to all the existing experiences is much harder. To avoid building another group of tools in the future, the pattern proposed also needs to be extensible. Building a generic model to cover known experiences that is easy to extend—Building a specific tool to solve a specific problem is a very straightforward solution. On the contrary, identifying a common pattern and model to apply to all the existing experiences is much harder. To avoid building another group of tools in the future, the pattern proposed also needs to be extensible. Avoiding over-automation with good efficiency—Automation sometimes can be dangerous: Avoiding over-automation with good efficiency—Automation sometimes can be dangerous: Doing something not allowed: the actions done by an automation system must be limited and under control. Doing anything that is unexpected may make things worse. Doing something too quickly: the concurrence of an automation system must be limited, especially for a stateful cluster. It needs time to sync the state back after an operation and must be easy to roll back when bugs are detected in the automation system. Doing something not allowed: the actions done by an automation system must be limited and under control. Doing anything that is unexpected may make things worse. Doing something too quickly: the concurrence of an automation system must be limited, especially for a stateful cluster. It needs time to sync the state back after an operation and must be easy to roll back when bugs are detected in the automation system. Serializing all the tasks may avoid part of the over-automation issues. But it’s better to define a model to guarantee efficiency. In a big complicated system, handling tasks in a more efficient way offers a better SLA. The illustration above shows the overview design of Unicorn. Unicorn defines a key resource called “Event” to abstract all the issues it needs to solve. Event has three sources: Alerts sent out from upstream: this is the most common source Manually dropped by an admin: a way to manually operate in an urgent case Periodic remediation job: some routine check tasks The event controller reads the events periodically and triggers workflow to handle events. This module needs to consider how to avoid over-automation and guarantee efficiency. The workflow engine handles the workflow life cycle and sends back the event status to the event controller. Each workflow may invoke some external dependency services, such as the configuration management system and underlying provisioning, deployment, monitoring, and remediation (PDMR) systems to do the remediation logics. The reporter module sends out a summary/detail report to subscribed targets. The state of each event defines the file cycle of it. Emit: Event is first dropped in the state store Processing: Event is picked by the event controller and handling by the workflow engine Locked: Another event in the same group is processing and marked the remaining as locked Finished: Event has been processed successfully Skipped: Event has been processed, but for some reason, it has reached the final state. Usually this means that no further action needs to be taken Failed: Event has been processed, but failed. Need to take a look ASAP Ignored: Event is not considered in the current scope Unicorn introduces the concept of “GroupID” to achieve isolation between events. The semantic of GroupId is as follows: The events in the same group must be handled sequentially The events in different group could be handled concurrently Even this field is totally open to self-definition; the common case is to use the physical cluster ID. From the past operation experience, we found that it is safer to avoid handling multiple nodes in one stateful cluster. Based on the GroupId field, the event controller works as follows to pick events in each round: List all the group Iterate all the groups, check whether there is already one event in this group in “Processing” state If yes, skip the current group. If no, pick one event and kick off processing Wait for the next round The next question to answer is how to determine which group/event should be picked first. That’s the reason Unicorn introduces the concept of “Priority.” The semantic of priority is as follows: The group with the higher priority event will always be scanned first The event with the higher priority in one group will always be picked first The priority is related to event type, i.e., the same type of issue has the same priority. The priority of each event type is not defined in Unicorn. When a new event type is introduced, the user must define this field. To avoid over-automation, Unicorn introduces several concepts to control the rate: Max processors: The max concurrent events in the“Processing” state. When there are equal or larger than the max processors events in the processing state, the scan process will stop, even if there are still groups that need to be scanned. “Vip priority threshold” could break this rule. Rate control window: The max event count could be handled within some sliding window for this specific event type. VIP priority threshold: The group whose highest priority event exceeds this threshold could break the “Max Processors” limitation, i.e., even if the concurrent processing events already reach the max processors, the high priority event could also be processed. Note that this rule never breaks the “GroupId” limitation. The detail workflow defers between event types. However, most of the workflow in Unicorn follows the above common pattern: Issue exists stage: check whether this issue still exists. This stage could help to avoid do some action duplicated. Query metadata stage: query metadata based on input. In most of the cases, the input only contains the index information, like cluster id, node id, etc. For detail information, it needs to be queried in the workflow. Do action stage: this stage is event-type specific and when it needs to retry, it should return to “Issue exists stage.” Verify action stage: the implementation of this stage may also be event-type specific, but it is a good pattern to verify what has been done in this flow. Finish stage: update the status and sync state to the event. NAP is the RESTful framework contributed by the NuData team. Unicorn highly relies on the features provided by NAP. Unicorn workflows leverage the NAP workflow engine. Basically, Unicorn implements separate tasks and build workflows through configuration files. Each task may fail with many reasons, which will create multiple branches in a workflow diagram. All the tasks in Unicorn have an output variable called “flowStatus.” “flowStatus” describes the result of the current task, and Unicorn leverages the “SwitchBy” semantic in the NAP workflow engine to determine what should be the next task. For the detailed information of the current task, Unicorn outputs to the “log” field of the event. Besides basic workflow, Unicorn has a lot of scheduled workflows. The scheduler in NAP is much more powerful than the ScheduledExecutorService in Java and even supports a crontab style. This feature is very useful for Unicorn to control daily and weekly jobs. Unicorn RESTful services all leverage the NAP micro-service RESTful framework. The services in Unicorn could be divided into two categories. Resource CRUD There are two main resources in Unicorn: event and alert. Event is the key resource in Unicorn, and Unicorn manages the whole life cycle of it, including create, get, update, and delete. The alert interface is opened for the alerting system to inject through a webhook, and the main logic in AlertHandler transfers alerts to a standard event. Note that in current implementation, the GroupId of the event, which is transformed from an alert, is also injected in the AlertHandler. The GroupId that Unicorn chooses is the clusterId. As an automation system, Unicorn also exposes an API to query the clusters that were recently handled. This is useful for a support person as well as for generating reports. Workflow resources also fall into this type, but the NAP framework already covered this part. Admin APIs This group of APIs help admin to debug issues. NAP has two amazing interfaces: Thread dump: returns the thread dump info of the current service. Useful for deadlock analysis and thread hang issue debugging. Logs: streams the log file content. Useful for detailed debugging. The Unicorn portal is totally a leveraged NAP config-driven UI framework. The portal of Unicorn is a standard operation that focuses on resource query and status update. To show the overall status of the system, the main page of Unicorn shows some statistical information. For example: The total event type supported currently The count of clusters handled recently How many events are handled successfully recently How many events are handled failed recently Kafka depends on the stability of the disk. An HDD with high media sector error will impact the Kafka application and cause the following two main issues: Traffic drop dramatically High end-to-end latency When Unicorn receives a media sector error alert, it follows the flow shown on image above. Two steps should be highlighted: Alert firing: check with the monitoring system to see whether this issue still exists. Ready for replace: removing the old node and bringing back a new node with the same broker ID is the best solution to solve this issue. However, when the in-sync replica (ISR) is not in a healthy state, it is dangerous to replace the node directly. In this case, Unicorn will only restart the Kafka process to temporary slow down the impact. This flow is a standard issue caused by the underlying hardware failure. Each Kafka broker holds some number of topic-partitions. When the traffic in one cluster increases dramatically, Rheos will increase the virtual machine count in that cluster. However, Kafka will not move the existing topic-partition to the new brokers automatically. Guaranteeing the balance of traffic keeps the cluster stable and gets better throughput. The PartitionReassignEvent is dropped periodically. Unicorn remediate the traffic imbalance issue with a greedy algorithm. The main process of this event workflow is as follows: Pick one broker with the most partitions Pick one broker with the least partitions Generate a partition reassign plan and kick off the process with partition reassign tool provided by Kafka Wait and check until the reassign finishes This flow is solves a standard issue caused by application-specific design. It’s important to know what operations have been done by an automation system. Rheos periodically sends out an operation report. In Unicorn, the reporter is implemented with the scheduled workflow provided by NAP. Most of the reports it uses today are related to event statistical information. To serve a different target receiver, Unicorn makes the reporter pluggable with the following configurations: Handler: the class that implements the reporter logic SchedulerPattern: the pattern to send out the report, in a crontab style ReportWindowHour: the time window of this report EventTypeSubscribed: the event type list that the report will include Receiver: the receiver list that will receive this report Type: the protocol of this report to send out; currently, only email is supported A developer and a manager may need different granularities and different dimension reports. Making this module pluggable provides such kind of capability. Unicorn has run on production for several months. This section introduces the statistical information based on production data. Currently, 10 event types are supported on production. A brief description and the percentage of each event type in the total event count are as follows: The following observations are based on the table above: Virtual machine down and network shake contribute the most alerts Application-specific flows take an important part in Unicorn Disk failure causes a lot of issues in Rheos The distribution of the three final event status (Finished, Failed, Skipped) for each event type is as follows: Several observations based on the above table: Most of the events are marked as “Skipped.” When the problem is solved once, the other related events waiting in the queue will not need to take any action. Handling false alerts is an important advantage of Unicorn. Disk-related issues could be remediated efficiently. Replace flow has the highest failed rate. As the final step of many flows, replace flow suffers a lot of underlying system failures. A simple count of handled events after deploying to production is shown in below image: Several observations from this diagram: Each week, Unicorn will scan thousands of events. After enabling new features, the handled events count increased, obviously. When some issues were solved, the events that need to be handled decreased. Two concepts to show the efficiency of Unicorn are as below: Time to response (Blue color on the chart): processing_timestamp - creation_timestamp, the time between Unicorn received the event and start processing timestamp Time to resolve (Red color on the chart): last_modified_timestamp - processing_timestamp, the time that Unicorn to mark an event as “Finished” or “Failed.” Several observations from this chart: Event with rate control (NodeDown and BehaviorLagHighFor5Min) and low priority (LeaderReelection and PartitionReassign) have a bigger response time Urgent events, including disk related and manually dropped events, have a small response time: under 5 minutes Most of the resolve time is around 10 minutes BehaviorMMLagHighFor5Min needs some sampling time; we may need to enhance the algorithm Automation is the key to saving effort and providing a better SLA response. Unicorn saves the support efforts, shortens the SLA to remediate urgent issues, and guarantees the stability of stateful clusters in Rheos. From a technical perspective, the contributions of Unicorn are as follows: provides an event-driven solution to modeling the past Rheos support experiences and easy to extend for new issues; defines the concept of GroupId and Priority to isolate conflicting operations and make sure there is acceptable efficiency; and some best practices based on NAP may provide an example to other tools to manage clusters running on C3. Unicorn needs to be continuously enhanced in Rheos daily work. New issues and new flows continue to be found from going through the alert list and from daily support work. It is also very important to add intelligence to Unicorn in the future to handle brand new issues automatically.", "date": "2018-10-02"},
{"website": "Ebay-Engineering", "title": "SRE Case Study: URL Distribution Issue Caused by an Application", "author": ["Charles Li"], "link": "https://tech.ebayinc.com/engineering/sre-case-study-url-distribution-issue-caused-by-application/", "abstract": "One of the frequently asked questions from new site reliability engineers is: Where to begin when troubleshooting a problem in a cloud environment? I always tell them: You should begin with understanding the problem. Let me demonstrate the reasons and methods with a real troubleshooting case. There are many applications behind www.ebay.com. Each application serves a unique subset of the URLs, such as /a/* and /b/* in following example. The HTTP requests are distributed to the applications by layer 7 policies on the load balancer: Policy A: if request = http://www.ebay.com/a/* , then send traffic to application-a Policy B: if request = http://www.ebay.com/b/* , then send traffic to application-b Policy C: if request = http://www.ebay.com/c/* , then send traffic to application-c Default Policy: if request = http://www.ebay.com/any-other-url , then send traffic to application-x (the default application) A client establishes a TCP connection with the virtual server on the load balancer and sends multiple HTTP transactions sequentially by reusing this TCP connection. The flow is illustrated as below: Everything was fine until one day a developer reported a problem: \"Hey, I'm the owner of application-b. I deployed new code earlier today and have been monitoring the logs since then. The new code is serving the /b/* URLs without any problem. However, I noticed that my application is randomly getting other URLs such as /a/*, /c/*, and /index.html, which shouldn't be sent to my application at all. It appears the layer 7 policies are not configured properly on the load balancer. \"Furthermore, if my application is getting other applications' URLs, I'd assume some of my /b/* URLs could be mis-distributed to the other applications as well? If so, it might be impacting multiple data flows.\" The Site Reliability Engineering (SRE) team took the following steps to triaging this issue: Step 1: Verify whether the alarm is true. The team checked the log of application-b and did see it randomly getting the other applications' URLs. Step 2: Scope the client side of the problem. Are the mis-routed URLs coming from the load balancer or from other clients connecting directly to application-b, bypassing the load balancer? The team checked the log of application-b for the source IP address of the misrouted URLs and found that all the source IP addresses belong to the load balancer. This confirmed that the misrouted URLs were indeed coming from the load balancer. The scope was narrowed down to the connections from the load balancer. The other sources were out of concern. Step 3: Scope the server side of the problem. Is application-b the only one getting wrong URLs? Or the other applications are also getting wrong URLs? The team checked the log of the other applications behind www.ebay.com and confirmed that only application-b was getting wrong URLs, so the scope was further narrowed down to the connections between the load balancer and application-b. Step 4: Scope the timing of the problem. The team checked the log of application-b to see when it started getting the wrong URLs. It turned out it started getting wrong URLs after the new code was deployed. With this systematic approach, the scope was narrowed down to the connection between the load balancer and application-b. The problem statement became: After deployment of new code, application-b started to receive wrong URLs from the load balancers. \"Wait,\" you may ask. \"The flow is from client to load balancer to application. How could a downstream application attract wrong URLs from the upstream load balancer? It's like a flood caused a hurricane, which is ridiculous, isn't it?\" Yes, normally a downstream application couldn't impact the decision on the upstream load balancer, just like it's impossible for a flood to cause a hurricane. The result of the previous investigation appeared to contradict common sense. What should the team do in this case? Well, remember during troubleshooting, the rule of thumb is this: a) If you couldn't find where the problem is, dig wider. b) If you found something that can barely be explained, dig deeper. In this case, the team should dig deeper by collecting the first hand data. They took a tcpdump for the HTTP transactions between the load balancer and application-b. Checking the raw data in Wireshark , a clear pattern was observed: Initially, application-b was getting the right URLs on a TCP connection. For certain HTTP requests, application-b sent HTTP 302 redirect back to the client, which was by design. Whenever application-b sent back an HTTP 302 redirect, it began to receive wrong URLs. It appears that the HTTP 302 redirects triggered the problem. But how? Taking a closer look at the raw data in the tcpdump, it turned out that the HTTP 302 redirects were not ended properly. Bingo, the root cause of the problem was found. Now let's take a look at the deciphered version of the story. According to RFC 7230 , an HTTP response header must end with an empty line, which is a line contains nothing but CRLF (0D0A in Hex ASCII Code). In the HTTP 302 redirect header generated by application-b however, the CRLF was missing. Why does the missing CRLF make URLs such as /a/* or /c/* be routed to application-b? Well, in order for the layer 7 policies to work properly, the load balancer must be able to keep tracking and identifying the HTTP headers in the requests and responses. The missing CRLF makes the load balancer think the previous HTTP response wasn't over, so the load balancer was confused and lost track of the HTTP headers in the subsequent requests and responses. It considers all the subsequent requests as part of the previous HTTP transaction, and routes them to application-b without applying any layer 7 policies, as illustrated below: What is the next step? \"Fix the code to end HTTP 302 with CRLF,\" someone might say. In reality however, the top priority is to mitigate the impact first. What the team did was to rollback application-b to the previous known-good version, as this could be done much faster than fixing the code. In parallel, the owner of application-b fixed the code, went through the QA process, and redeployed the code to production. To wrap up the story, let's look at the the takeaways. In summary: 1. Troubleshooting begins with verifying, confirming, and reproducing the issue. 2. A systematic approach is the key to quickly converging on the scope of the problem. 3. If you couldn't find where the problem is, dig wider. If you found something strange that can't be easily explained, dig deeper. 4. First-hand raw data is essential to determining the root cause. 5. Protocol, protocol, protocol. It must be implemented and tested thoroughly. An incomplete implementation could result in unexpected problems. 6. In a production environment, the top priority of troubleshooting is to mitigate the impact instead of fix the problem. Well begun is half done.", "date": "2019-03-14"},
{"website": "Ebay-Engineering", "title": "ShipShip—The Automated Kanban Board", "author": ["Mayur Dave"], "link": "https://tech.ebayinc.com/engineering/shipship-the-automated-kanban-board/", "abstract": "At eBay, each development team works with multiple distributed teams. To keep everyone on the same page with the different projects that they are working on, we built a tool called ShipShip to help keep information flowing.", "date": "2019-03-27"},
{"website": "Ebay-Engineering", "title": "eBay Makes Visual Shopping More Intuitive While You Browse", "author": ["Jonathan Chard"], "link": "https://tech.ebayinc.com/product/ebay-makes-visual-shopping-more-intuitive-while-you-browse/", "abstract": "Our newest experience helps you find more products you just can't describe. You’re browsing on eBay and you see a pair of shoes that catches your eye. You like them, but you want to explore your options. Now it’s easy to discover more of what you like with a tap. Our latest feature lets you use words and pictures in a new way to shop, discover and explore on eBay. If you see something that piques your interest while shopping through eBay’s catalog, just tap on the three dots on the top right corner of the listing and start exploring similar items. If this sounds familiar, it’s a feature that couples our advances in computer vision with relevant filters. It’s leveraging existing technologies and putting them together in a way that creates a seamless, intuitive shopping experience. Here’s how to use the feature. Open the eBay app, search or tap browse to visit one of our categories or interests, then tap on the three dots next to an item you would like to explore. This will pull up a screen that lets you explore a number of options related to the item, including the popular “looks like this” feature. See a pair of shoes that inspires you, but they aren’t exactly what you’re looking for? Now you can use this feature to explore the most popular characteristics in that category. Or if you want a look-a-like item, tap “looks like this” to help find similar items. This started as an idea when creating a filter for locally sold items. As my coworkers and I were shopping for local items on eBay, we wanted a way to quickly filter for items in a specific category. Then we thought, “What if we expanded it to searching and browsing?” We took that concept to eBay’s 2018 Hack Week , where we were able to build on the idea. We added more options and integrated with our image recognition capabilities built by the eBay Computer Vision team for “looks like this”. When we designed this feature, we thought hard about which options to put on the menu. We didn’t want to take every aspect of every item, as this would be overwhelming. So we made it concise with just a few options that felt natural. For example, in sneakers, options could be brand, style, color or features. Fewer options make it simple and intuitive for our buyers. By combining pictures and words, this new experience empowers our customers to explore eBay and discover items that they may not have known how to describe. Try it now live on eBay iOS and Android apps in the U.S., U.K., Germany and Australia.", "date": "2019-03-27"},
{"website": "Ebay-Engineering", "title": "eBay's Hyperscale Platforms", "author": ["Lam Dong"], "link": "https://tech.ebayinc.com/engineering/odm/", "abstract": "In the era of artificial intelligence, machine learning, and cloud technology, data is growing exponentially. eBay data continues to grow, serving more than 182 million buyers and $13.4 billion of transactions completed on mobile devices. Understanding how to manage data is a key to success. System hardware platforms must be designed for the data. Our approach is that hardware platforms must be developed for eBay’s specific data applications. It is not optimal to simply choose the most high-end premium and powerful hardware for data-driven applications. Starting in 2017, as part of eBay’s replatforming initiative, eBay customized hardware to develop the most reliable cost effective platforms for eBay’s applications. What that means is to serve up to 10,000 queries per seconds at 10 milliseconds latency, the team had characterized the workload, apply new CPU technology to develop a system of choice for eBay search application. The rack-mounted system design fits into 1U standard 19” racks with density 2 server nodes per 1U. Other features include: System cooling and power supplies that are shared between server nodes. It is designed with N+1 fans and dual redundant AC feeds. System power at full workload utilization is within a 1800W power consumption limit. System cooling and power supplies that are shared between server nodes. It is designed with N+1 fans and dual redundant AC feeds. System power at full workload utilization is within a 1800W power consumption limit. Dual-socket CPU server nodes with Intel (or equivalent) Skylake 6138 processors for optimum cost/performance. Dual-socket CPU server nodes with Intel (or equivalent) Skylake 6138 processors for optimum cost/performance. An optimized, efficient storage subsystem for distributing a bulk dataset with 2T SATA HDDs in software RAID 10 mode. An optimized, efficient storage subsystem for distributing a bulk dataset with 2T SATA HDDs in software RAID 10 mode. An embedded 10G SFP+ network adapter per server node with out-of-band BMC management. The system has robust BMC resilient firmware features and a hard reset path to BMC in addition to the standard one. This design resolved common BMC failure/hang issues in a DC environment. An embedded 10G SFP+ network adapter per server node with out-of-band BMC management. The system has robust BMC resilient firmware features and a hard reset path to BMC in addition to the standard one. This design resolved common BMC failure/hang issues in a DC environment. This system is designed based on a Skylake 20C, 2GHz CPU at 128GB in 1U form factor, and it is capable of holding the eBay search index database. Figure 1. System Block Diagram of the distributed search performance server Because the demand for storage capacity increases as the image database grows, the new storage server is designed to meet the required latency at the lowest cost and power, and it optimizes CPU, memory, and storage ratios for system design with an acceptable failure domain. eBOD (​Ethernet attached Bunch Of Disks​) is an eBay initiative to adopt a new cost-effective storage infrastructure approach of using a custom design to support the independent scaling of compute and storage, an independent refresh of compute and storage, and to provide an acceptable blast radius for storage nodes. The new motherboard is designed based on System on Chip (SoC), with embedded network and storage controllers in 1U form factor fit into standard 19” rack. The system is equipped with dual AC power feed and N+1 fans for redundancy. Platform throughput is targeted at 3200 MB/s for system power of less than 300W. The system design can be scaled to higher capacity drives, which can drive system storage costs lower for the same performance and power. Figure 2. System Block Diagram of low-cost, high-density image storage servers The new ODM (Original Design Manufacturer) approach that entails developing customized eBay platforms for specific eBay applications, enables eBay to control business, technical outcomes and life cycles of its hardware infrastructure. eBay has extended its ODM platforms to cover high-performance servers and storage servers, as well as artificial intelligence/machine learning and distributed data analytic servers.", "date": "2019-09-04"},
{"website": "Ebay-Engineering", "title": "eBay OAuth Client Library in Python and Best Practices", "author": ["Catherine Wong"], "link": "https://tech.ebayinc.com/engineering/ebay-oauth-client-library-in-python-and-best-practices/", "abstract": "To make it easier to integrate with eBay RESTful APIs, eBay provides client SDK libraries in C#, Java, and Python. Learn how to quickly set up the OAuth SDK in Python and learn about eBay OAuth best practices. These client libraries reduce the amount of code application developers have to write to get OAuth access tokens. In this article, I focus on detailing the features of the Python client SDK library. I show how to set up and use this library, as well as offer best practices on how to use OAuth tokens. For detailed information on eBay OAuth tokens, please refer to our standard documentation . First, to help you effectively use the SDK, let’s have a quick review of eBay OAuth tokens.  If you are already familiar with eBay OAuth token types and scopes, please skip ahead to the Getting the SDK Up & Running section later in this document . As indicated by our developer documentation , eBay provides OAuth tokens in two grant flows: client credentials flow and authorization flow. Application Access Token Application Access Token Represents an application Represents an application Mint access tokens with the Client Credentials Grant flow Mint access tokens with the Client Credentials Grant flow User Access Token User Access Token Represents an application and an eBay user Represents an application and an eBay user Mint access tokens with the Authorization Code Grant flow Mint access tokens with the Authorization Code Grant flow The expiration time is determined when token is minted. In the API Reference documentation, each API method page indicates both the grant type of the token required to make a call to the method and the scope that you must use when you create an access token to call the method. Below is a section from the getItemFeed API reference. This section indicates that in order to make a getItemFeed API call, you must use an access token obtained via the client credentials grant flow and include the \"https://api.ebay.com/oauth/api_scope/buy.item.feed\" scope when generating the OAuth token: Not all scopes are available for an application. For example, to make Feed API calls, your application must be must be configured with the \"https://api.ebay.com/oauth/api_scope/buy.item.feed\" scope. To find out the scopes that are available to your application, log into developer.ebay.com, navigate to your “Application Access Keys” page, and click the “OAuth Scopes” link. You should see a list of available scopes, like this: This SDK is written in Python 2, and you must use this environment to use the SDK. Because this is an open source project, please feel free to upgrade the SDK to Python 3. 1. Clone the repo and run the command below to install all dependent components: $ pip install -r requirements.txt 2. Configure the file ebay-config-sample.yaml (under /test/config) with your application information, as outlined here: 3. If you are running the test script GetUserAccessToken.py that comes with this SDK: Download the latest Chromedriver for your OS here . Place Chromedriver in the /test directory. Download the latest Chromedriver for your OS here . Place Chromedriver in the /test directory. Configure “ebay-config-sample-user.yaml” with your application information and “test-config-sample.yaml” (under /test/config) with your eBay user ID and password. Configure “ebay-config-sample-user.yaml” with your application information and “test-config-sample.yaml” (under /test/config) with your eBay user ID and password. 1. Import oauth2api, credentialutil, and environments: import os, sys sys.path.insert(0, os.path.join(os.path.split(__file__)[0], '..')) from oauthclient.oauth2api import oauth2api from oauthclient.credentialutil import credentialutil from oauthclient.model.model import environment 2. Define the path of your application configuration file (i.e. ebay-config-sample.yaml), then load the application information. For example: config_path = os.path.join(os.path.split(__file__)[0], 'config' ,'ebay-config-sample.yaml') credentialutil.load(config_path) 3. Declare the scopes need for your OAuth access token using a list (refer to “OAuth Token Scope” section above on how to define proper scope), and be sure you don’t use an incompatible scope (refer to “Common Issues” section below): app_scopes = [\"https://api.ebay.com/oauth/api_scope\", \"https://api.ebay.com/oauth/api_scope/buy.item.feed\"] 4. Create an instance of oauth2api: oauth2api_inst = oauth2api() 5. Call any method within oauth2api using the instance: app_token = oauth2api_inst.get_application_token(environment.SANDBOX, invalid_app_scopes) signin_url = oauth2api_inst.generate_user_authorization_url(environment.SANDBOX, app_scopes) user_token = oauth2api_inst.exchange_code_for_access_token(environment.SANDBOX, code) user_token = oauth2api_inst.get_access_token(environment.SANDBOX, user_token.refresh_token, app_scopes) Getting Application Access Token — Client Credentials Grant flow Getting User Access Token—Authorization Code Grant Initial Flow Getting User Access Token—When User OAuth Access Token Expired This SDK accepts configuration files in both JSON and YAML formats. Please refer to the sample template in the /test/config folder. This SDK accepts configuration files in both JSON and YAML formats. Please refer to the sample template in the /test/config folder. To avoid caching user information, this SDK generates authorization URLs that forces the user to log in each time. This is accomplished by using the prompt=login parameter. To avoid caching user information, this SDK generates authorization URLs that forces the user to log in each time. This is accomplished by using the prompt=login parameter. Debug OAuth-related issues within your application with the eBay_Oauth_log.txt log file. Debug OAuth-related issues within your application with the eBay_Oauth_log.txt log file. Please read the Best Practices Info . Also, pay attention to the following common token or scope concerns: Do not discard your OAuth token after a one-time use. API calls to retrieve an OAuth token are rate-limited per application. To avoid hitting your call limit, cache and re-use tokens until expiration time. Do not discard your OAuth token after a one-time use. API calls to retrieve an OAuth token are rate-limited per application. To avoid hitting your call limit, cache and re-use tokens until expiration time. Do not refresh an OAuth User access token with different or new scopes; you must use the exact same scopes that you used to get the original User access token in the Authorization Code Grant flow. Do not refresh an OAuth User access token with different or new scopes; you must use the exact same scopes that you used to get the original User access token in the Authorization Code Grant flow. When making a get_access_token call to refresh a User access token, make sure the list of scopes indicated in the call request is the same as or a subset of the scopes given in the original Auth Code Grant request. Scopes can be user-specific or application-specific. Do not input an incompatible scope when retrieving an OAuth token. For example, if you call get_application_token with a user specific scope, the call will respond with an error. Scopes can be user-specific or application-specific. Do not input an incompatible scope when retrieving an OAuth token. For example, if you call get_application_token with a user specific scope, the call will respond with an error.", "date": "2019-09-11"},
{"website": "Ebay-Engineering", "title": "Relation Embedding with Dihedral Group in Knowledge Graph", "author": ["Canran Xu", "Ruijiang Li"], "link": "https://tech.ebayinc.com/research/relation-embedding-with-dihedral-group-in-knowledge-graph/", "abstract": "eBay researchers recently published a paper about a method for KG relation embedding using dihedral group. Experimental results on benchmark KGs show that the model outperforms existing bilinear form models and even deep learning methods. Large-scale knowledge graphs (KG) play a critical role in the downstream tasks such as semantic search, dialog management, and question answering. In most cases, despite its large scale, a KG is not complete due to the difficulty to enumerate all facts in the real world. The capability of predicting the missing links based on existing dataset has been one of the most important research topics for years. A common representation of a KG is a set of triples (head, relation, tail), and the problem of link prediction can be viewed as predicting new triples from the existing set. A popular approach is KG embeddings, which maps both entities and relations in the KG to a vector space, such that the scoring function of entities and relations for ground truth distinguishes from false facts. The standard task for link prediction is to answer queries (h,r,?) or (?r,t). In this context, recent works on KG embedding focusing on bilinear form methods are known to perform reasonably well. The success of this pack of models resides in the fact they are able to model relation (skew-) symmetries. Furthermore, when serving for downstream tasks such as learning first-order logic rule and reasoning over the KG, the learned relation representation is expected to discover relation composition by itself. One key property of relation composition is that in many cases, it can be non-commutative. For example, exchanging the order between parent_of and spouse_of will result in completely different relation (parent_of as opposed to parent_in_law_of). We argue that, in order to learn relation composition within the link prediction task, this non-commutative property should be explicitly modeled. Applied researchers from eBay recently published a long paper in Association for Computational Linguistics (ACL) 2019 to tackle this problem. In this paper, the researchers proposed DihEdral to model the relation in KG with the representation of dihedral group. The elements in a dihedral group are constructed by rotation and reflection operations over a 2D symmetric polygon. As the matrix representations of dihedral group can be symmetric or skew-symmetric, and the multiplication of the group elements can be Abelian or non-Abelian, it is a good candidate to model the relations with all the corresponding properties desired. Details of the paper can be found in here. A dihedral group is a finite group that supports symmetric operations of a regular polygon in two dimensional space. Here, the symmetric operations refer to the operator preserving the polygon. For a K-side (K∈Z+) polygon, the corresponding dihedral group is denoted as DK that consists of 2K elements, within which there are K rotation operators and K reflection operators. A rotation operator Ok rotates the polygon anti-clockwise around the center by a degree of (2πm/K), and a reflection operator Fk mirrors the rotation Ok vertically.  The element in the dihedral group DK can be represented as 2D orthogonal matrices: OK(m)=[cos⁡(2πmK)−sin⁡(2πmK)sin⁡(2πmK)cos⁡(2πmK)]FK(m)=[cos⁡(2πmK)sin⁡(2πmK)sin⁡(2πmK)−cos⁡(2πmK)], where m∈{0,1,⋯,K}. The elements of D4 is visualized in the following figure, with each subplot representing the result after applying the corresponding operator to the square of ‘ACL’ on the upper left corner. We propose to model the relations by the group elements in DK. We assume an even number of latent dimensions 2L. More specifically, the relation matrix takes a block diagonal form R=diag[R(1),R(2),···,R(L)], where R(l)∈DK for l∈{1,2,⋯,L}. As a result, the score for a triple (h,r,t) in bilinear form can be written as a sum of these L components h⊤Rt=∑l=1Lh(l)⊤R(l)t(l). This model is able to learn the following properties for a KG: Relation symmetry and skew symmetry. A relation R is symmetric if (t,R,h) is true when (h,R,t) is true. A relation R is skew-symmetric if (t,R,h) is false when (h,R,t) is true. Relation inversion. R1 is the inverse of R2 if (t,R2,h) is true when (h,R1,t) is true. Relation composition. If R3 is the composition of R1 and R2, then when both (h,R1,t1)  and (t1,R2,t2) are true then (h,R3,t3) is also true. That means, R3=R1R2. Moreover, dihedral group is able to learn a non-Abelian relation composition, which means it can distinguish the order of the composition for R1 and R2. To train the relation embedding with dihedral group, we proposed two optimization methods. The first one utilizes the commonly used Gumbel-softmax trick to train discrete values. This model is called Dk-Gumbel. The second one parametrizes the group elements by binary variables, which are optimized by a straight-through estimator, and the model is named as Dk-STE. We performed experimental studies on 5 public KG datasets: WN18, WN18RR, FB15K, FB17K-237 and YAGO3-10. Here's the result: We observe that the proposed model performs similarly to or better than the state-of-the-art models. We proposed a method for KG relation embedding using dihedral group. By leveraging the desired properties of dihedral group, relation (skew-) symmetry, inversion, and (non-) Abelian compositions are all supported. Our experimental results on benchmark KGs showed that the model outperforms existing bilinear form models and even deep learning methods. To see more details, please read our paper.", "date": "2020-05-14"},
{"website": "Ebay-Engineering", "title": "Low Latency and High Throughput in CAL Ingress", "author": ["Cristal YU"], "link": "https://tech.ebayinc.com/engineering/low-latency-and-high-throughput-cal-ingress/", "abstract": "CAL Ingress, the main component of eBay's Centralized Application Logging (CAL) system, is responsible for collecting, processing, and dispatching application server local logs from thousands of CAL clients in the cloud. This article introduces the whole system architecture design and performance optimization to meet the requirements of low latency and high throughput for huge amounts of traffic. The main purpose of CAL is to facilitate a centralized collection of application servers' local logs and provide reporting on the collected data. These reports provide great insight into important areas of the eBay site—invaluable for making decisions on all aspects of business at eBay. As a main component of CAL, CAL Ingress is responsible for log collecting, processing, and dispatching. It accepts logs from thousands of CAL Clients, and dispatches those logs to several destinations after processing. Three logs are collected and processed by CAL Ingress: Raw logs VID metrics Heartbeat metrics Figure 1 shows the data flow. CAL Ingress receives data from the CAL client via the TCP connection and dispatches different types of data to different destinations. The raw log is sent to Filer directly. VID metrics, which are retrieved from the logs, are aggregated and sent to OLAP 2.0 via KAFKA. Heartbeat metrics are sent to Metric-Store Frontier. Figure 1. Data flow As a centralized logging system, data traffic levels are huge. CAL Ingress processes about 2.5PB logs a day for eBay. A CAL client sends logs to CAL by NIO (Non-blocking I/O)  long-lived TCP connections with a high QoS (Quality of Service) requirement. Once a bunch of data cannot be sent together, the connection between CAL and client is considered unhealthy. Cached data both in the client and server will be discarded. That is called \"TCP Disconnect.\" Once TCP Disconnect occurs, there is data loss. In order to alleviate TCP Disconnect and data loss, CAL Ingress should retrieve and acknowledge the data quickly. Low latency is required. Currently the network buffer in most CAL client machines is 128k. The average data volume is 200k/s for one connection, so the latency of CAL Ingress is less than 0.64s. For some pools with large volumes, the latency should be even less. CAL Ingress services millions of connections at the same time. Latency time is calculated via the following formula: Currently, CAL processes average 105TB/hour logs, so the average volume is 29.87GB/s. The peak volume can reach 58GB/s. CAL Ingress uses Netty to implement servers that accept client requests and uses a long-lived TCP connection to transfer data. To comply with the low latency and high throughput requirements, we separate reading and handling in different EventLoopGroups. See Figure 2. Figure 2. CAL Ingress architecture CAL Ingress services millions of connections (about 1 million in our production environment) at the same time. Ingress alternately reads data from one connection to another. So Latency Time is calculated via the following formula: T period is the read interval. N connection is the number of connections, and N thread is the number of total threads to handle these connections. So for each connection, the time interval to read data from it is N connection /N thread * T period. T gc is the GC pause time. To decrease the latency, the read interval and GC pause time should be decreased. Non-stop reading and GC optimization is the relative approach. First, we enable TCP_NODELAY and TCP_QUICKACK to make sure the socket reading has not stopped at the network level. Second, we break the normal data pipeline, read->handle->read; data-read and data-handle are separated. As a result, reading does not stop. As shown in Figure 2, data-read and data-handle belong to separate EventLoopGroups. There is one dedicated EventLoopGroup to read data from the socket. In this way, data-reading is not affected by the heavy data handling processing. Several dedicated threads respond to retrieve/read data from network sockets all the time. After it reads the data into direct memory, it continuously reads the next bunch of data. Now the reading speed can catch up to the sending speed of CAL clients. Less data is blocked in the client socket buffer, and TCP disconnect seldom occurs. In previous generation of log-collecting component, the number of TCP disconnections is high; the min is 2500 and the max is 21000. But in CAL Ingress, the number has decreased quite a bit; the min is 4, and the max is just 65. Figure 3. TCP disconnect for pool r1reco (before) Figure 4. TCP disconnect for pool r1reco (after) There is a term \"stop-the-world” related to GC. Stop-the-world means that JVM will stop the application from running to execute a GC. Stop-the-world will occur no matter which GC algorithm you choose. When stop-the-world occur, CAL Ingress will stop receiving data, but the CAL client will continue sending data. If the network buffer is full with data, the CAL client cannot send the next bunch of data all at once, and TCP disconnection will occur. We chose G1GC as our GC collector. In order to let CAL Ingress survive stop-the-world, we must tune GC in two ways: Reduce the GC frequency Reduce the GC frequency Decrease the GC pause time Decrease the GC pause time In a normal way, all the data is disposed of in memory. That means the data will be consumed from direct memory into heap memory at first. CAL Ingress adopted an off-heap mechanism. Instead of reading data from a network buffer and creating the internal objects in heap memory, the received data is copied to pre-allocated off-heap memory directly and operates the memory without copying them to the heap as an object. Because all the data is stored off-heap, less heap memory is used, and the GC frequency is reduced. GC contains three major time-cost processes: Object scan in GC young area Object scan in GC young area Object copy Copy the survival object from one survivor area, if it's full, to another An object is promoted from the young area to the old area Object copy Copy the survival object from one survivor area, if it's full, to another An object is promoted from the young area to the old area Termination attempts To reduce #1 and #2, one effectual way is to reduce the number of objects in heap memory and the number of survival objects. We use Java MAT to dump the memory heap to investigate. (See Figure 5 for an example of one heap memory dump analyzation.) According to the memory dump, we found that most of the objects are short-lived objects, so it is better to release them when they are still in the young generation area. We enlarged the young generation size by setting the JVM parameter to enlarge it: -XX:G1NewSizePercent=50. Reduce the objects According to the memory dump, most objects (62.82%) in a heap is SlicedByteBuf. And most of those objects are created in the VID handler isVIDRelated function. So we refine the function. Instead of creating lots of slicedByteBuf when parsing, we copy the content to the heap, and compare it in heap. In that way, no SlicedByteBuf is needed. The objects in the heap are reduced quite a bit. Figure 5. Java MAT - Memory Optimize For #3, our solution is to reduce the number of threads used. Too many threads will cause too much contention in the CPU. If there are other processes or system activity that needs CPU resources, chances are that some of the GC workers are scheduled late because of that contention. After investigating, we decided to use a thread-pool for a scheduled executor, instead of creating thread at each executor evoke. All the data that is read from the socket is maintained in direct memory and is not released until processing is complete. If data processing speed does not catch up with the reading speed, the direct memory will be used up, and out of memory errors (OOM) will occur. Traffic is the data traffic; T process is the time to process data. Figure 6. Data process pipeline As shown in Figure 6, the data flows by several handlers: decompress, parser, VID handler, heartbeat handler and filer handler. In order to shorten the processing time, we try to optimize each handler. As a result, three areas are improved: Refine time-costing function Refine time-costing function Batch compress Batch compress Lazy-write and controllable memory usage Lazy-write and controllable memory usage We calculated the time-cost for some frequently used functions and found out that some functions are time-cost. We refined those function to reduce the time. For example, the Search/indexof function of ByteBuf. To search in a ByteBuf, it will read one byte from direct memory once and compare one byte by one. Reading from direct memory is time-consuming, so this function costs a lot. We re-wrote the search function. Instead of reading one byte and comparing it one by one, we read a bunch of bytes once to compare. The number of read calls decreased, and execution time was reduced 30%. See Figure 7. Figure 7. Function execution time In the real production environment, the data size of one socket reading is variable. In most cases, the size of data is small. In the previous design, we parsed the data and compressed it. Compression is time expensive, and as the time to compress data increases, the time cost increases. So we combined those small data packets and compressed them until the size reached a predefined size. Currently, we set the data size threshold to 160K, and the compression count decreases to 1/10. The total compression time decreases, too. Filer is a slow device, so it takes a long time to write data to filer. To alleviate the effect of filer's slowness, we decided to use \"lazy-write.\" Instead of writing the data to slow filers immediately, the data is written to ByteBuf in direct memory as compressed data at first, and then flushed into filers in other threads. To control the direct memory usage, we selected the Ring-Buffer approach. All the to-write-to-filer data is stored in a Ring Queue, which has the predefined memory size. When too much memory is used because of filer slowness, the Ring Queue is full, and any incoming data will be dropped. Besides the above optimization of CAL Ingress Java processes, we also do some JVM and system tuning and optimization. As a summary, 1. We use the Cloudflare zlib library to reduce compression time. 2. We enable RSS and CPU Numa pinning to improve the CPU efficiency. 3. We do an off-cpu check to reduce cpu context switching. CAL Ingress now has a low latency and high throughput, and it can support huge amounts of data traffic in our production environment. When one CAL Ingress services 1000 connection, the overall throughput can reach 220MB/s without TCP disconnection. Figure 8 gives the results of benchmark LnP (Load and Performance) testing. It shows the total throughput without TCP disconnection and the GC pause time when CAL Ingress serves multiple connections. Figure 8. Throughput and average GC pause time CAL Ingress has improved quite a bit in throughput, latency, rate of disconnect, and rate of data loss, as shown in Figure 9. Figure 9. Improvements", "date": "2019-09-18"},
{"website": "Ebay-Engineering", "title": "Automate Problems Away with Token Jobs", "author": ["Mayur Dave"], "link": "https://tech.ebayinc.com/engineering/automate-problems-away-with-token-jobs/", "abstract": "Got a problem? Spend some time writing a script to do it for you. Got a bigger problem? Write a service that fixes it for you. Want to solve problems with software engineering? Use Token Jobs. A developer in the wild can be confronted by many problems. Some of these problems will be slow or repetitive tasks, some will be the lack of adoption of tools which could help and some will be convincing others that the first two are problems. How does the developer manage to fend off these problems? One way the humble developer might fix this problem is by massaging any pain points when they see fit and working away on making everything pristine. While this seems like an appropriate solution, it only works in the short term. In the long term, it’s not cost effective and when the business suffers, so does the developer. Another solution is to use Gold Cards — tickets that the developer can present to the team that allows them to work on personal development for half a day per week, for a total of one day per two-week sprint. However we didn't want a time limit to pressure our tinkering. It also wouldn't work because we are a team that continuously delivers rather than using sprints. Here at Shutl, we created something called The Token. The Token is a physical statue with wrenches, as shown above. While it is a physical object, The Token is more of an idea. Whomever has The Token can work on an issue that annoys them and that they feel fixing would improve their work. One benefit from The Token is that there is no backlog to circumvent, no product owners or business people to convince. The reason this works is because of trust. There is a high level of trust among the developers and, in turn, we feel trusted and supported. Whenever we have a pain point, we know that we can simply take The Token and fix it the way we do best, by programming. We chose this solution because we believe it is empowering. Shutl is made up entirely of motivated individuals. The Token gives them support to do their jobs with full autonomy. Allowing our engineers to develop in such a way has been empowering for us, making us feel like players rather than pawns and ensuring we have a healthy team . Another benefit we gain is having space to fail. It is important to have space to fail in order to innovate. If you have an idea that could help, but doesn’t work, that’s OK. By having the space to fail, you can experiment freely and explore, which allows us to be innovative. Most new technologies and concepts started off as a token task. I think it’s safe to say The Token has been a success. Since its introduction, we’ve had fewer problems, and the team has been generally happier. In the beginning, the token was used very often, but as time goes on we’ve used the token less. At the moment there are no plans to change or improve on The Token. The Token is just a tool for improving our code quality, which it has done and continues to do. This isn’t a silver bullet, however; it’s important to consider any downfalls. One possible concern is that by giving the engineers free rein, they may not work on what the business needs. However by having one token and, at most, one ongoing job at any given time, we can be sure that there will still be deliverable change. It is clear that a token and the ongoing work on tokens can slow down the pace of delivery in the short term. However having a nicer and healthier system leads to happier developers and can speed up the pace of delivery in the long term. Although not new, we know most extreme programming practices lead to long-term solutions of sustainable development practices. Two of the main tools that were developed in house started off as Token jobs. ShipShip : We found a problem with coordinating stand ups and deployments with the USA. We took The Token, built a very simple version of the one seen here. We solved the stand up issue and made it easier to deploy. ShipShip has been improved since then, but the difficult first step was a token job. FailBot : We had an issue with flaky tests and being unsure if a failure was real or not. Once again, The Token gave us a start with FailBot, which was considerably improved on by later Token jobs. All in all, the token has been serving us well. It has certainly helped us find solutions, from keeping track of stories to ensuring we are not blocked by flaky tests . Things work best when engineers do what they do best and automate the problems away. Ultimately, The Token is simply an efficient means to an end.", "date": "2019-12-30"},
{"website": "Ebay-Engineering", "title": "Automation via the Accessibility Ruleset Runner", "author": ["Scott Izu", "Ian McBurnie", "Sean Gates", "Valliappan Thenappan"], "link": "https://tech.ebayinc.com/engineering/automation-via-the-accessibility-ruleset-runner/", "abstract": "Automated testing helps organizations build better software, faster. Automation can also be used to standardize user experiences or maintain code compliance. With respect to accessibility, automation can be used to bring awareness by helping people with little to no accessibility knowledge easily find, file and fix bugs. At eBay, we take accessibility very seriously. As mentioned, in our Accessibility Statement , we are committed to building a community enabled by people and supported by technology that’s open to the broadest audience possible. We’re committed to ensuring digital accessibility for people with disabilities . With those goals in mind, we are continually improving the user experience for everyone, and applying the relevant accessibility standards. Imagine you are a developer and you have requirements to make a web page fast, secure, available and accessible. These requirements are above and beyond the functional requirements. For two months, you balance your time: attending meetings, planning, coding, reviewing, integrating and testing. Priorities and deadlines keep your schedule tight, but finally, you release the product. After release, an independent accessibility audit is done and the auditors have found several accessibility issues, despite all of the testing that was done. Sample audit report courtesy of Level Access . As you investigate how these issues slipped through, you discover that several issues are related to inaccessible images. These images were flagged because they were missing alt text and you quickly realize that this type of issue could have been caught as part of automated testing. To be continued... Automated testing allows issues to be found upstream in early development stages like pre or post commit stages. These bugs can be fixed early on with minimal cost. On the other hand, the cost of fixing bugs exponentially grows as the product moves from development to QA to production. The eBay Accessibility Ruleset Runner automates a number of WCAG 2.0 AA recommendations, saving time on manual testing. This includes checking images for alt tags, validating heading structure, and much more. Because the Accessibility Ruleset Runner is released as open source, we encourage the community to contribute more rules in order to cover more WCAG 2.0 AA recommendations. We provide five examples of how to use the Accessibility Ruleset Runner : Chrome Developer Console NodeJS with Selenium/Mocha/Chai Python with Selenium Chrome Extension Java with Selenium/TestNG To help better illustrate how the tool works, let’s walk through our NodeJS with Selenium/Mocha/Chai example. Front end developers that use Node.js may find this particularly useful. We assume the following are installed: ChromeDriver Node.js From the terminal, change directories to go into the appropriate examples folder: Then, perform an npm install to install the required dependencies for the Selenium/Mocha/Chai framework: selenium-webdriver mocha chai The ruleset runner examples require zero configuration. This “one click” setup allows new users to quickly run the examples, to get an idea of what the ruleset runner does.  In other words, the ruleset runners are preconfigured to test a default web page and the example can be run, using a single command: After going through the example, developers may want to make modifications to test another website or include the Accessibility Ruleset Runner in their project . Let’s get back to our story, where you are the developer… You research tools online and find the Accessibility Ruleset Runner . You quickly run through the NodeJS with Selenium/Mocha/Chai example and make the following modifications to test your web page using localhost. You use npm run to invoke the custom ruleset runner and the results are shown on the console in a JSON array. You quickly discover that 3 images are inaccessible because they are missing alt tags. However, you find that it is difficult to read JSON from the console and decide that you will create an HTML report from the results JSON array. You would like the HTML report to include additional information which will help others to quickly fix the issue. You want the HTML report to include information about the rule that failed, identification information for any elements that were flagged (screenshot, attributes, locator) and clear error messages. Fortunately, you search the Topic Guide and find a sample HTML Report , which you use to investigate the 3 images that were flagged earlier. Now that you see the power of using automated testing to find accessibility issues, your next steps are to introduce the Accessibility Ruleset Runner into your development pipeline. In addition, you plan on enhancing the HTML report to include links that will allow people to file bugs directly into your bug tracking system (e.g. Jira , Bugzilla ). After that, you will expand upon the Chrome Extension example, which allows quick testing of web pages without having to modify code. You are well on your way… We are a proud contributor to open source accessibility tools and documentation. The Accessibility Ruleset Runner demonstrates how to automate accessibility testing, which can help catch bugs upstream with minimal cost. Other open source contributions shown in our Accessibility Statement include: eBay MIND Patterns - design patterns for accessible web components OATMEAL - a collection of manual accessibility testing methods In the Topic Guide , we include a link to some general principles for creating a ruleset. We created these general principles after reviewing publically available accessibility tools.  These principles were used to build our Custom Ruleset . Later, we started using the Axe Ruleset from Deque Systems due to their alignment with these principles. Rulesets should place an emphasis on 0 false positives. By having 0 false positives, there is no room for interpretation and teams can be required to have 100% pass rate prior to launching a new feature. Rulesets should place an emphasis on 0 false positives. By having 0 false positives, there is no room for interpretation and teams can be required to have 100% pass rate prior to launching a new feature. Rulesets should be written in vanilla javascript and published as a single javascript file. This makes the rulesets highly portable so they can run in a variety of environments. Rulesets should be written in vanilla javascript and published as a single javascript file. This makes the rulesets highly portable so they can run in a variety of environments. Rulesets should return a well formed JSON. JSON is also highly portable. Results can be stored in a database for tracking, aggregated/displayed in dashboards and even converted directly into user friendly HTML Reports. Rulesets should return a well formed JSON. JSON is also highly portable. Results can be stored in a database for tracking, aggregated/displayed in dashboards and even converted directly into user friendly HTML Reports. Rulesets should be vetted against a library of html code snippets. There should be examples of good/bad code that pass/fail various rules, as expected. Covering a large number of code variations tends to make the ruleset more robust. See also Testing Methodology . Rulesets should be vetted against a library of html code snippets. There should be examples of good/bad code that pass/fail various rules, as expected. Covering a large number of code variations tends to make the ruleset more robust. See also Testing Methodology . Contributions in terms of patches, features, or comments are always welcome. Refer to the Contributing Guidelines for help. Submit Github issues for any feature enhancements, bugs, or documentation problems as well as questions and comments.", "date": "2020-01-07"},
{"website": "Ebay-Engineering", "title": "Safe ACL Change through Model-based Analysis", "author": ["Antonio Ceseracciu"], "link": "https://tech.ebayinc.com/engineering/safe-acl-change-through-model-based-analysis/", "abstract": "Model-based analysis makes it possible to rigorously test and validate changes prior to deployment. This approach enabled the eBay Network Engineering staff to undertake a project to refactor a large business critical Access Control List, resulting in an 80% reduction in size, without any adverse business impact. The eBay Site Network hosts ebay.com, a core part of eBay’s business. As is best practice for a large Internet site, the first line of defense for the eBay Site Network is an Access Control List (ACL) at the border of the network. This ACL is programmed on the edge network routers which connect the eBay Site to the Internet. It protects the Site by restricting traffic flows that are allowed to enter the eBay Site Network.  The default action for the border ACL is to block network flows. Therefore, allowed network flows are referred to as “ACL exceptions,” and the majority of ACL changes consist of either adding or removing ACL exceptions. The border ACL changes match the pace of eBay Site infrastructure changes. At any time, thousands of ACL entries are programmed in the border ACL, which are necessary for the correct operation of the eBay Site. Over time, a large amount of no-longer-necessary ACL entries tend to build up in the border ACL, and the organization of the ACL itself tends to become fragmented. This is a form of technical debt. The main underlying causes are: 1) unused ACL entries that do not immediately affect Site operation; 2) the inherent risk in removing ACL exceptions, as doing so can result in blocking necessary network flows and reduce Site Availability and 3) the inherent risk in re-factoring the structure of the border ACL to consolidate it, as seemingly simple operations such as changing the order of ACL entries can cause a change in behavior. The eBay border ACL is maintained with the help of an open source tool called Capirca. Among other things, this tool provides separation between policy and definitions. If leveraged properly, this separation makes the policy smaller and easier to work with. The definitions can be generated automatically from the authoritative source of truth for network assets. When we started this project, the Capirca source files themselves carried much redundancy and fragmentation. This compounded the issues with no-longer-necessary ACL entries, and created severe technical debt. One metric that we use to track the complexity of the border ACL is the number of ACL entries. At eBay, this metric is in multiple thousands. The same metric is used to track the amount of change over time. One discipline developed primarily to manage complexity is Software Engineering. We observed that a network ACL and a computer program have similar properties: both require maintaining a large set of ordered instructions; and both are very sensitive to small variations, such as a mistyped character. Therefore, it made sense to adopt Software Engineering tools to manage the complexity of the border ACL. One such tool is a popular Revision Control System called Git. Figure 1 shows the trend of the border ACL  Lines Of Code (LOC) metric over the last few years. Derived metrics capture the rate of change. This can be expressed in the frequency of atomic changes, and the amount of lines changed. In software management, the two metrics correspond to frequency of commits, and size of commits respectively. Figure 1. LOC metrics of the two components of the border ACL over time: the main policy (blue) and source of truth for network IP space ranges (orange). The large drop in complexity metrics on the right hand side reflects the work described in this article. We have long-standing, internal processes to control changes and regularly audit the border ACL to manage the inherent risks described above. Due to the size and complexity of the ACL, such processes had become effort-intensive and less effective. This made it difficult to meet the SLA for ACL exception requests, and also made every ACL modification a more dangerous change. Refactoring the border ACL became crucial to address this technical debt. A well-structured ACL is necessary for streamlined auditing and fast changes. The major challenge with refactoring the border ACL is to ensure that the refactored ACL is functionally equivalent to the currently deployed ACL, before applying the change to the live Site environment. Each change set could be hundreds of lines, and at this scale, manual verification (eyeballing) is ineffective and highly risky. Further, this type of change is difficult to test in an emulation-based Staging environment. We do not have an effective way to enumerate and inject the full set of network flows that the eBay Site requires in order to operate. That would require a Staging environment as complex as the Site itself. In a typical software environment, this kind of challenge is addressed using a comprehensive test-suite that analyzes the software’s behavior prior to deployment. We had to find a similar way to validate the candidate ACL. We identified model-based analysis as the solution to this challenge. Model-based analysis builds a model of network behavior based on its configuration and then analyzes it to check behaviors in a range of scenarios. It can perform both testing (i.e., analyze individual inputs) and verification (i.e., prove correctness for all possible inputs). This blog explains different forms of network validation in detail. We selected Batfish , an open source tool that provides model-based analysis capabilities. Using Batfish, we developed an iterative procedure to: Model the full ACL behavior before and after changes. Model the full ACL behavior before and after changes. Calculate the space of ACL actions (allow, block) for all possible network flows. Calculate the space of ACL actions (allow, block) for all possible network flows. Use an algorithm to attempt to prove that the space of ACL actions is identical before and after the change. Use an algorithm to attempt to prove that the space of ACL actions is identical before and after the change. If the algorithm is unable to prove the assertion above, it would produce a list of counter examples, that is, network flows for which the ACL action is different before and after the change. If the algorithm is unable to prove the assertion above, it would produce a list of counter examples, that is, network flows for which the ACL action is different before and after the change. Use the counter examples to correct issues with the new ACL. Use the counter examples to correct issues with the new ACL. Add a unit test to the ACL to prevent any regression of the issues. Add a unit test to the ACL to prevent any regression of the issues. Commit the results of this iteration to the git feature branch. Commit the results of this iteration to the git feature branch. We describe an instance to illustrate the process described above. In one of the consolidation steps, the original ACL has the following terms: Permit TCP traffic from any source to host A. Permit TCP traffic from any source to host A. Deny TCP traffic to subnet S, which contains A. Deny TCP traffic to subnet S, which contains A. Permit TCP traffic from any source to host B. Permit TCP traffic from any source to host B. A candidate ACL consolidates terms a and b, since they express the same policy. The resulting ACL is: Deny TCP traffic to subnet S, which contains A. Deny TCP traffic to subnet S, which contains A. Permit TCP traffic from any source to host A or B. Permit TCP traffic from any source to host A or B. When the procedure is applied to compare the original ACL (before change) with the candidate ACL (after change), step 4 reports a change in behavior. The example flow is: TCP, any source, to destination host A; and the behavior change is “permit” to “deny”. Based on this information, the developer prepares a new candidate ACL, which looks as follows: Permit TCP traffic from any source to host A or B. Permit TCP traffic from any source to host A or B. Deny TCP traffic to subnet S, which contains A. Deny TCP traffic to subnet S, which contains A. As the procedure is executed on the new ACL, step 4 reports that the behavior is now identical. Therefore, it is safe to commit this specific change set to the repository. The issue with this simple ACL would have been easy to spot - much less so, when hundreds of terms change order in a set of many thousands. By iterating the procedure described above, a small team was able to re-factor the border ACL with an acceptable amount of risk to Site Availability. Consistent with the assessed risk, throughout the project, no impact to any business flow was observed. Conversely, the beneficial effects of this refactoring include: Lowering the risk for future border ACL changes Lowering the risk for future border ACL changes Lowering the turnaround time for border ACL changes from months to under a week Lowering the turnaround time for border ACL changes from months to under a week Enabling effective auditing of the border ACL, due to its new modular structure Enabling effective auditing of the border ACL, due to its new modular structure Freeing up many hours of time for the team to spend on Operations and Engineering. Freeing up many hours of time for the team to spend on Operations and Engineering. Reducing the size of the border policy by 80%. Reducing the size of the border policy by 80%. Enabling further consolidation by de-risking the process of removing no-longer-needed ACL exceptions. Enabling further consolidation by de-risking the process of removing no-longer-needed ACL exceptions. We are now leveraging model-based analysis, and Batfish specifically, in other areas of Network Engineering, such as testing a new Network Product before deploying; staging Firewall changes; and automatically validating network changes. The case study presented in this article is just one of the ways in which eBay is advancing the state of networking using modern tools and practices. Get in touch if you’d like to learn more!", "date": "2020-01-15"},
{"website": "Ebay-Engineering", "title": "Building a Product Catalog: What we Learned from our University Machine Learning Competition", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/product/building-a-product-catalog-what-we-learned-from-our-university-machine-learning-competition/", "abstract": "We challenged more than 100 college students at seven universities to structure listing data using AI and machine learning. At eBay, we use state-of-the-art machine learning (ML), statistical modeling and inference, knowledge graphs, and other advanced technologies to solve business problems associated with massive amounts of data, much of which enters our system unstructured, incomplete, and sometimes, incorrect. To help surface fresh ideas on how we can solve this problem, we partnered with university students at institutions across the country to host an ML competition to spur more research in the ecommerce domain using our very own dataset — 1 million selected public data from unlabeled listings. What we didn’t expect was the number of learnings that surfaced from the submissions. Here are some of the key takeaways that stood out to us. 1. Students are interested in solving these problems. When we started outreach to universities, we were initially skeptical of the dataset we selected, and whether an ecommerce domain challenge would attract students. Academic curiosity and competitions of this nature typically skew toward the areas of vision and language. On the other hand, commerce doesn't get the required attention, so we were thrilled by the excitement and response. Our original plan was to onboard two university teams, and we surpassed our goal with more than 100 participants from seven universities, spread across 37 teams. Thanks to word of mouth and the uniqueness of the dataset, we realized that there is a genuine interest in this topic by students and researchers alike. 2. A scalable platform and streamlined evaluation criteria are key to a successful ML competition. We assessed various platforms to host the competition and EvalAI proved to be an ideal choice. EvalAI is under an open-source license and allowed architectural flexibility, enabling us to scale efficiently. The challenge is most naturally addressed with an unsupervised learning method. However, in order to evaluate the submissions, we needed to obtain a Golden Set of correctly clustered listings, which proved to be difficult. Even when we sent the same pair of listings to several human reviewers and asked the apparently simple question on whether these two listings were the same, we often received conflicting results, which in turn required several rounds of review. We chose the overall Rand Index as the evaluation criteria, which is an objective measure that evaluates overall accuracy. While the Rand Index served its purpose in evaluating the submissions, we would opt to use a different metric in future challenges that gives a higher weight to pairs of listings that should be identified as being identical. 3. eBay’s unstructured data poses an ongoing challenge. eBay is a platform that allows sellers to enter listing data in an unstructured way. As a result, listings sometimes lack certain information, contain redundant information, etc. While the results obtained by the winning team are promising, the problem is far from being completely solved, and this competition has only affirmed its difficulty. However, the winning method provides a solid baseline, which eBay product teams will continue to build and iterate on. Following a thorough evaluation of the models, methodologies, code, and more, we are excited to announce the winners of eBay's 2019 ML Challenge. The student winner, in a solo member team, was Yang Zhao from Stanford University . In addition, eBay awarded an internship to Rabiraj Bandopadhyay who was part of a runner-up team from the State University of New York-Buffalo . Yang and Rabiraj will join our virtual summer internship program and have the opportunity to work with the eBay product team that is using ML and AI to solve the unique challenge that pertains only to eBay — making sense of more than 1.5 billion listings. Yang Zhao is a Ph.D. student at Stanford University, majoring in Civil Engineering. He is a member of the computational geomechanics group, developing numerical models for anisotropic rocks. Other than academics, Yang is interested in basketball and finance, especially in figuring out how the macroscopic economy works. He has a dual bachelor’s degree in Economy from Tsinghua University and passed the first two levels of CFA exams. Rabiraj Bandopadhyay is a Master's student at SUNY – Buffalo, majoring in Computer Science and Engineering. His key areas of interest are theoretical machine learning, matrix decompositions for neural networks, and linear algebra techniques in unsupervised and supervised learning. His other hobbies include listening to classic rock music and reading nonfiction. Our interns help us reimagine eBay’s marketplace for millions of customers around the world. While today’s climate poses many unknowns for students entering the workforce, we are more committed than ever to providing our interns the best possible learning experience. In response to the COVID-19 pandemic, eBay’s 12-week summer internship will be held virtually to promote the health and safety of our interns. Combining real work experience and programming, the internship will give interns the unique opportunity to see into various business verticals, meet our executives and network with like-minded peers. Throughout the internship, students will be challenged to propose solutions to complex problems that have a positive impact on customers and sellers alike. Congratulations to our winners and huge thanks to all the participants for their enthusiasm and support.", "date": "2020-05-18"},
{"website": "Ebay-Engineering", "title": "eBay Open Source — 2019 in the Rear View Mirror; 2020 Ahead", "author": ["Brian D. Haslam"], "link": "https://tech.ebayinc.com/engineering/ebay-open-source-2019-in-the-rear-view-mirror-2020-ahead/", "abstract": "As we move further into 2020, it’s worth reviewing a selection of key eBay Open Source projects released in 2019. The Christmas lights are long since down, the turkey was picked clean an eternity ago, and (if it was ever out), the mistletoe is safely back in the attic. Notwithstanding the completion of 2019, and as we move towards an even more massive open source effort in 2020, it’s still not too late to review a selection of last year’s eBay Open Source work. This article highlights a selection of some of the new and popular 2019 eBay open source projects , as well as some other notable works that should bring significant value to the open source community. The projects described here are grouped by technical field. At eBay, huge amounts of listing data must be stored, retrieved, catalogued, and managed. It is not surprising that eBay is a leader in database and replication technology. Accordingly, in 2019, there were some notable releases in this field. In terms of GitHub stars, the most popular project of 2019 was Akutan (named in honor of a “strato-volcano” in Alaska — Mount Akutan ). This is a very significant project written in C++, and involves many components. From a separate blog article , Akutan’s architecture is shown below: Importantly, Akutan has an architecture that scales out horizontally. It was run internally on a 20-server deployment, supported tens of thousands of changes per second, and loaded with over 2.5 billion facts. As the authors described, “we haven’t yet pushed Akutan to its limits.” See the detailed eBay Tech Blog article here . If you are interested in contributing to Akutan, and contributions are always welcome for eBay open source projects, more details are found here . Later in 2019, another significant key-value store was released under an Apache license , also written in C++. It is an “embedded key-value storage library, based on a combined index of LSM-tree and copy-on-write (append-only) B+tree.” Jungle works with Akutan and serves as a replicated high-performance log store that can also be used with the NuRAFT protocol.  To get further insight into fitting the components together, it is also worth reading the accompanying eBay Tech Blog article “ NuRaft: a Lightweight C++ Raft Core .” For details about using or contributing to the project, please see the detailed README file in the root of the repo (that accompanies each and every newly released eBay open source project). As already described, Jungle was actually part of a set of technologies intended to work together. Another key part of that architecture was a consensus-based protocol implementation of “RAFT.”  Also under an Apache license , the NuRaft release was written about here . The underlying Raft protocol itself was partially developed at Stanford University, but the eBay team considered a number of enhancements including logical snapshot support, a pre-vote mechanism to avoid disruption of a leader, custom quorum sizes, asynchronous replication, and many other features documented here . eBay uses machine learning to optimize listings in all kinds of ways, and from time to time, our developers release this code and technology to the open source community also. One important set of advances in 2019 was some very significant work to improve and optimize training time for machine learning algorithms: In their own words, the architects described the reasoning behind AutoOpt: Manual adjustment of hyperparameters is very costly and time-consuming, and even if done correctly, it lacks theoretical justification which inevitably leads to “rule of thumb” settings ... we propose a generic approach that utilizes the statistics of an unbiased gradient estimator to automatically and simultaneously adjust two paramount hyperparameters: the learning rate and momentum. ... The results match the performance of the best settings obtained through an exhaustive search and therefore, removes the need for a tedious manual tuning. See Selcuk Kopru and Tomer Lancewicki, Automatic and Simultaneous Adjustment of Learning Rate and Momentum for Stochastic Gradient Descent, 2019 . To make these gains in machine learning time possible, there’s a lot of math, but third-party developers need not dust off their college math books or wade through the equations (unless that was a New Year’s resolution for 2020). The code is available for use under the permissive Apache license here and can be used to improve machine learning training time. eBay continuously seeks to improve, strengthen, and contribute to code that helps with accessibility of software. In 2018, our eBay researchers made a notable release of HeadGazeLib , software that allows control of a cursor without fingers, and instead allows control of a cursor by a user’s gaze. This year, a number of developers led the effort to release a framework to help with accessibility testing of web sites. Accessibility-Ruleset-Runner (“[t]his project demonstrates how accessibility testing is done upstream during the development process. The project includes two rulesets, which is what we use internally (Custom Ruleset, aXe Ruleset). Developers can reuse our custom ruleset, exchange rulesets or add their own.”). This project is explained even further in the \" Automation via the Accessibility Ruleset Runner .\" The Developer division of eBay has a super history of releasing SDKs as open source, and that was strengthened further in 2019, which saw five new SDK releases, including: Ebay-oauth-csharp-client Ebay-oauth-python-client Ebay-oauth-nodejs-client Ebay-oauth-android-client FeedSDK-Python The astute reader may notice a predominant theme in the list: OAuth authentication that can be used to call eBay APIs. eBay continues to fully support the legacy APIs that use the SOAP protocol , but eBay also encourages a move towards the newer REST APIs . The OAuth SDKs above, in a programming language of choice, will ease the transition to call the newer REST APIs - a transition that can take place in minutes. It is worth pointing out that detailed eBay Tech Blog articles in 2019 also documented use of the CSharp OAuth library and Python OAuth library. See eBay OAuth Client Library and eBay OAuth Client Library in Python and Best Practice . The principles of use for the OAuth SDKs remain the same in the other languages. The one SDK outlier in the 2019 release set above was the FeedSDK in Python. It complements a prior FeedSDK written for Java clients. The intent and purpose of the feed libraries is to help with APIs that involve, as the name implies, feed files that can reach gigabytes in size. To quote from the detailed Python FeedSDK README , the SDK “abstracts the complexity involved in calculating the request header 'range' based on the response header 'content-range' and downloads and appends all the chunks until the whole feed file is downloaded.” eBay has a long history of releasing new open source code and contributing back to the open source community. Each open source project on the GitHub eBay site is intended to come with great documentation, well-written code, a clearly defined open source license, and each project represents a helpful addition to a technical field. In 2020, we fully expect to continue to expand the open source offerings that meet those criteria. If not too late for well wishes, may you have an excellent and prosperous 2020, and if you have a mind, please pop by any eBay repo and feel free to add a GitHub star, use a library, or better still, make an open source contribution to a project. (Hint: Even beginner developers are very welcome in the community, and can easily learn how to make a “pull request” by finding and making a first typo fix contribution to a README!)", "date": "2020-01-29"},
{"website": "Ebay-Engineering", "title": "Dark Mode Now Live on eBay Native Apps", "author": ["Bradford Shellhammer"], "link": "https://tech.ebayinc.com/product/dark-mode-now-live-on-ebay-native-apps/", "abstract": "eBay becomes one of the first ecommerce companies to launch Dark Mode on iOS and Dark Theme on Android. Today, we are excited to launch Dark Mode on our eBay native devices. Dark Mode is designed to ease the shopping experience, honor device settings, create more accessibility and provide best-in-class service to our customers, and it is one of the newest feature updates our customers are seeing on mobile. eBay is now one of the first ecommerce companies to launch Dark Mode on iOS and Dark Theme on Android. Dark Mode is our customers’ most requested app feature so far in 2020. Today’s digital users know only too well that bright screens can cause eye strain. When eye strain is prolonged throughout the entire day and into the night, our sleep patterns, focus and overall health can be negatively impacted. By implementing a color scheme with light-colored text, icons and graphical user interface elements on a dark background, Dark Mode allows for a comfortable viewing experience that’s easy on the eyes. Meeting all contrast guidelines and standards, Dark Mode also helps customers with light sensitivities and visual impairments navigate the app. As an added benefit, Dark Mode saves battery life due to low power usage. We've updated the native apps to support dark appearance options for both Android and iOS platforms. The applications are presented using either light or dark color palettes depending on user preferences. To access Dark Mode on iOS, a user selects “Display & Brightness” in the iOS Settings app. From there, the user is provided the option to toggle between “Light” and “Dark.” The user also has the option to set an “Appearance Schedule” that dictates when Dark Mode is activated. To access Dark Theme on Android 10 and above, a user enables Dark Theme across their device under Android display settings. In addition, all Android users can enable the theme in the eBay app settings under the “Theme” menu. The user can toggle Dark Theme on and off within the app or the device settings. Through the teams’ mobile explorations, the feature has expanded and evolved to address user feedback, making it more human-centric and touch forward. Teams are now thinking more contextually about handheld devices and considering factors like target sizes and ergonomic positions. This has informed some changes to user interface elements and layout. Bringing Dark Mode to our apps was possible due to eBay’s Component Architecture technology working closely with eBay’s modern Design System technology. Component Architecture is a framework that includes a set of reusable UI components, which define the building blocks of the app at the UI level. This allows design, engineering and product to move quickly, while ensuring our users have a stable and consistent experience. Using these common components in combination with our Design System’s centralized design library allows us to make design theme changes at the platform level. This was critical for supporting Dark Mode relatively quickly. Instead of needing to make a “dark” style change to every button in our apps, for example, we were able to make a single style change to update all buttons. As we roll out Dark Mode, we will review user feedback through multiple channels: customer satisfaction surveys; contacts to customer support; App Store and Google Play reviews; and on social media channels, such as Twitter, Reddit, Facebook, etc.. In addition, we are monitoring app usage metrics, which will give us greater insight on our customers’ experience in using the new theme and how they’re engaging with the feature. Are users sticking with dark mode or switching back and forth? Will we see more nighttime shopping sessions? Longer shopping sessions? Once we learn more about how users are engaging with Dark Mode, we can start defining new features and optimizations based on customer behavior. Elsewhere on our app, users are also seeing updates to the iconography and fonts, along with the navigation bar background’s colors changing to complement users’ chosen theme. All updates to the app including Dark Mode will be available on all iOS devices. The release also includes an equivalent Dark Theme for Android users. The feature is currently rolling out to users in the U.S., U.K., Germany and Australia, and will expand to all other regions in July. We continue to modernize and personalize the customer experience. Everything we do is in service of our customers, and we’ll continue to innovate on their behalf to move our marketplace forward. Experience the new Dark Mode in your mobile settings today.", "date": "2020-06-08"},
{"website": "Ebay-Engineering", "title": "Speed By A Thousand Cuts", "author": ["Senthil Padmanabhan"], "link": "https://tech.ebayinc.com/engineering/speed-by-a-thousand-cuts/", "abstract": "In 2019, eBay prioritized a company-wide initiative, aptly called “Speed,” focused on improving the performance of critical eBay flows across all platforms — iOS, Android, and Web. This article explains the journey and outcomes. Death by a thousand cuts is a popular figure of speech that refers to a failure that occurs as a result of many small problems. It has a negative connotation to it and is referenced on many occasions when things go wrong, and there is no one primary reason to blame. We have a similar story at eBay, but this time on a positive note. In 2019, we started working on an initiative called “Speed” to improve the performance of end-user experiences across major consumer channels — iOS, Android, and Web. Fast forward today, we have made significant improvements to our speed numbers, both globally and across all platforms, but there was no one major contributing factor. It was a culmination of many small enhancements (or “cuts” as we call it) that moved the needle. So, let’s look at what happened. Speed has always been a critical component in eBay’s product lifecycle. In the past, we periodically worked on several speed-related projects that kept improving the performances of our experiences. But in 2018, the focus was more towards product-related features, and speed took a backseat. As a result, the speed numbers remained constant and in a few cases, even degraded a little. Towards the fall of 2018, we realized that this was not the best thing for our customers. There was unanimous agreement across the board, including senior leadership, that we should re-focus our efforts on speed. The idea was to not only make up for the lost performance opportunities but also to set a new bar for ourselves. Fortunately, it was also the time of the year where 2019 roadmap planning happens. The next step was to leverage the momentum into the 2019 roadmap. We proposed a company-wide initiative called “Speed,” which focused on improving the performance of critical eBay pages across all platforms. We set clear goals and scoped the effort. The proposal was accepted, and Speed became one of the important initiatives of 2019. Forming the right team is crucial for any effort to be successful, and it is especially true for a cross-functional endeavor. We identified key engineers and product owners across various domains who were passionate about this topic and formed the core team. They were very determined to make the experiences faster. With the team in place, we began the journey. For an effort like Speed, an essential prerequisite is to have the right metrics and goals in place. This was a vital step before execution to guide the initiative in the correct direction. Let’s talk about metrics first. Though there are numerous user-centric performance metrics to be observed, for this initiative, we focused only on the few below — the main reason being that our past analytics have shown that improving these metrics will increase customer satisfaction and conversion. Web TTFB (Time To First Byte) — the time that it takes for a user’s browser to receive the first byte of page content TATF (Time to Above The Fold) — a custom metric that is very similar to First Meaningful Paint . The point at which this metric is calculated depends on the page. For example, in our search results page, TATF for desktop is fired after the sixth item image is loaded. This is a good indicator that all of the Above The Fold content is displayed to the user. We have similar heuristics for other pages and screen sizes E2E (end-to-end) — the traditional page on load event, which indicates when the page is completely loaded Native (iOS/Android) VVC (Virtual Visual Complete) — the TATF equivalent for native apps Though we have our custom terminologies for these metrics, some of them are indeed industry standards. Now let’s look into goals. The tricky part about setting a speed/performance goal is the difficulty in answering the question, “ How fast is fast? ” The answer to this question depends on who you ask. People always want to get a faster experience than what they have now. But at some point, Prof. Donald Knuth’s quote kicks in, and we may end up with diminishing returns . Considering these facts, the way we set our goals is by again piggybacking the industry-standard approach — Performance Budgets . We established a speed budget in terms of milliseconds against the metrics mentioned above for three critical pages — homepage, search results, and item page. Separate budgets were set for synthetic and RUM environments for each of the platforms (iOS, Android, desktop, and mobile web). The goal of the initiative was to meet the budget across both web and native. At the start, as expected, the metrics for all three pages were above the budget at varying degrees. We derived the budget based on two factors: Historical data. We looked at the metrics in the past and took the numbers where we were consistently best performing. We did a competitive study using the Chrome User Experience Report and tweaked the above numbers appropriately (considering eBay’s infrastructure). Instead of coming up with imaginary numbers that were impossible to achieve, deriving the speed budget based on these two factors kept us close to reality. Teams felt confident that they could meet the budget with sound engineering and science. For a moment, let’s jump ahead and look at the progress. We started the initiative in November of 2018 and are close to wrapping up the efforts for 2019. We have also met the speed budget in most experiences. It’s time for some stats. The following table highlights the percentage of improvements in Above The Fold rendering time since November 2018, across the critical pages and platforms. For instance, the home screen in the eBay iOS app is natively rendered 12% faster when compared to the same time last year. Figure 1. Percentage of improvements in Above The Fold rendering time since November 2018. Another source of web performance metrics is the public dataset available through Chrome User Experience Report. The following image shows the progress that eBay’s US web property (ebay.com) and Australia web property (ebay.com.au) have made since November 2018. The metrics below represent the DOM Content Loaded (DCL) event. Other metrics in the report have similar stats. In other words, only 48% of our users in the US had DCL fired within 1 second in Nov 2018. Whereas in Oct 2019, that number is 56%. Figure 2. Chrome User Experience Report for DOM Content Loaded (DCL). ebay.com (left) and ebay.com.au (right). To check on how we fare against other competitor benchmarks, we used Akamai’s real user monitoring (RUM) tool , which again is built on top of the Chrome User Experience Report. Below is a comparison of the full page load time with other eCommerce players. Figure 3. Page Load times: eBay vs. industry benchmarks. As you can see, we are 2.4 seconds faster than our slowest competitor and at least as fast as our fastest competitor now in terms of page load times. We are pretty happy with the progress that has been made. It was only possible because we, as an organization, believed that performance is the key to good customer experience and dedicated enough resources to make it happen. We saw how the speed initiative started and where we are today. So what happened in-between? This is where the “cuts” come into play. The improvements we made were possible due to the reduction or cuts (in size and time) of various entities that take part in a user’s journey. As we go over the list below, we will have a better understanding of what these cuts mean. Also, to make it readable, we are only providing an overview of each item on the list. There will be follow-up blog posts on some of these items. The list is not exhaustive, either. We selected the topics that would resonate with the community at large, rather than being eBay specific. Reduce payload across all textual resources — This is basically trimming all the unused and unnecessary bytes of CSS, JavaScript, HTML, and JSON response (for native apps) served to users. With every new feature, we keep increasing the payload of our responses, without cleaning up unused stuff. This adds up over time and becomes a performance bottleneck. Teams usually procrastinate on this cleanup activity, but you will be surprised by the savings. The cut here is the wasted bytes in the response payload. Native app parsing improvements — Native apps (iOS and Android) talk to backend services whose response format is typically JSON. These JSON payloads can be large. Instead of parsing the whole JSON to render something on the screen, we introduced an efficient parsing algorithm that optimizes for content that needs to be displayed immediately. Users can now see the content quicker. In addition, for the Android app, we start initializing the search view controllers as soon as the user starts typing in the search box. Previously this happened only after they press the search button. Now users can get to their search results faster (iOS already had this optimization). The cut here is the time spent by devices to display relevant content. Critical path optimization for services — Not every pixel on the screen is equally important. The content above the fold is obviously more critical than something below the fold. Native and web apps are aware of this, but what about services? Our service architecture has a layer called Experience Services , which the frontends (native apps and web servers) talk to. This layer is specifically designed to be view- or device-based, rather than entity-based like item, user, or order. We then introduced the concept of the critical path for Experience Services. The idea is that when a request comes to these services, they work on getting the data for above the fold content immediately, by calling other upstream services in parallel. Once data is ready, it is instantly flushed. The below the fold data is sent in a later chunk or lazy-loaded. The outcome: users get to see above the fold content quicker. The cut here is the time spent by services to display relevant content. Image optimizations — Images are the largest asset on the internet, and even more for eCommerce. Even the smallest optimization will go a long way. We did two optimizations for images. First, we standardized on the WebP image format for search results across all platforms. The search results page is the most image-heavy page at eBay, and we were already using WebP, but not in a consistent pattern. Through this initiative, we made WebP the image format across iOS, Android, and supported browsers. Second, though our listing images are heavily optimized (size and format), the same rigor did not apply for curated images (for example, the top module on the homepage ). eBay has a lot of hand-curated images, which are uploaded through various tools. Previously the optimizations were up to the uploader, but now we enforced the rules within the tools, so all images uploaded will be optimized appropriately. The cut here is the wasted image bytes sent to users. Native apps startup time improvements — This applies to cold start time optimizations for native apps, in particular, Android. When an app is cold started, a lot of initialization happens both at the OS level and application level. Reducing the initialization time at the application level helps users see the home screen quicker. We did some profiling and noticed that not all initializations are required to display content and that some can be done lazily. More importantly, we observed that there was a blocking analytic third-party call that delayed the rendering on the screen. Removing the blocking call and making it async further helped for example cold start times. The cut here is the unnecessary startup time for native apps. Predictive prefetch of static assets — A user session on eBay is just not one page. It is a flow. For example, the flow can be homepage to search to item. So why don’t pages in the flow help each other? That is the idea of predictive prefetch, where one page prefetches the static assets required for the next likely page. So when a user navigates to the predicted page, the assets are already in the browser cache. This is done for CSS and JavaScript assets, where the URLs can be retrieved ahead of time. One thing to note here is that it helps only on first-time navigations, as for subsequent ones, the static assets will already be in the cache. The cut here is the network time for CSS and JavaScript static assets on the first navigation. Item prefetch — When a user searches eBay, it is highly likely that they will navigate to an item in the top 10 of the search results. Our analytics data support this statement. So we went ahead and prefetched the items from search and kept it ready when the user navigates. The prefetching happens at two levels. One on the server-side, where item service caches the top 10 items in search results. When the user goes to one of those items, we save server processing time. Server-side caching is leveraged by native apps and is rolled out globally. The other happens at browser level cache, which is available in Australia. Item prefetch was an advanced optimization due to the dynamic nature of items. There are also many nuances to it — page impressions, capacity, auction items, etc. You can learn more about it in my talk , or watch for a detailed blog post. The cut here can either be server processing time or network time, depending on where the item is cached. Search images eager download — In the search results page, when a query is issued at a high level, two things happen. One is the recall/ranking step, where the most relevant items matching the query are returned. The second step is augmenting the recalled items with additional user-context related information such as shipping. Previously the search results were rendered only after both the steps were done. It is still the same now, but after the first step, we immediately send the first 10 item images to the browser in a chunk along with the header, so the downloads can start before the rest of the markup arrives. As a result, the images will now appear quicker. This change is rolled out globally for the web platform. The cut here is the download start time for search results images. Autosuggest edge caching — When users type in letters in the search box, suggestions pop-up. These suggestions do not change for letter combinations for at least a day. They are ideal candidates to be cached and served from a CDN (for a max of 24 hours), instead of requests coming all the way to a data center. International markets will especially benefit from CDN caching. There was a catch, though. We had some elements of personalization in the suggestions pop-up, which goes against caching. Fortunately, it was not an issue in the native apps, as the user interface for personalization and suggestions can be separated. For the web, in international markets, latency was more important than the small element of personalization. With that out of the way, we now have autosuggestions served from a CDN cache globally for native apps and non-US markets for the web. The cut here is the network latency and server processing time for autosuggestions. Homepage unrecognized users edge caching — For the web platform, the homepage content for unrecognized users is the same for a particular region. These are users who are either first-time to eBay or start with a fresh session, hence no personalization. Though the homepage creatives keep changing frequently, there is still room for caching. So we decided to cache the unrecognized user content (HTML) on our edge network ( PoPs ) for a short period. First-time users can now get homepage content served from a server near them, instead of from a data center. We are still experimenting with this in international markets, where it will have a bigger impact. The cut here is again both network latency and server processing time for unrecognized users. As the title suggests, there are no specific callouts to the above list that were more significant than others. All the cuts collectively contributed towards moving the needle, and it happened over a period of time. The releases were phased in throughout the year, with each release shaving off tens of milliseconds, ultimately reaching the point where we are now. Also, note that the optimization techniques vary from things that are very basic to a few that are advanced. But it is the basic that is often overlooked, and the whole opportunity in front of us goes unnoticed. We were very keen on addressing the basics first. Please watch this space for detailed articles on some of the topics above. Meanwhile, please check out the case study \" Shopping for Speed on eBay.com \" by Google's Addy Osmani on web.dev, highlighting our journey. Performance improvements are a never-ending journey. The key is to strike the right balance. As noted above, we made significant progress on speed in 2019. But that is not the end of the story. Going forward, we have put a few things in place that will always keep us on the edge when it comes to performance. We formed a committee of Speed Champions. This includes the core speed team and performance experts from critical domains across the web, iOS, and Android. The Speed Champions own the budget for their areas and are responsible for monitoring and keeping them within range. They are also involved before starting a major feature development, so performance is considered right from the beginning, instead of it being an afterthought. Before every code release, our systems will check the speed metrics against the budget (which is the current baseline, as the budget has been met). This will happen in a synthetic environment as a part of the release pipeline. If the metrics are not within the acceptable range of the budget, the release to production is blocked until the degradation is fixed. This process ensures that we do not introduce any new performance regressions, and the budget is always met. The speed budget is not something that is set in stone. Things change over a period of time. To acknowledge this fact, Speed Champions will meet on a quarterly cadence to review the budget and update it as needed. The updates are based on a couple of factors — competitive benchmarks, upcoming critical product features, and the state of global internet connectivity. If an update is due, we will give a heads-up to associated teams to plan for ideas and methods to meet the new budget. Finally, we are also adding a couple of new metrics into our monitoring systems. The idea is to go beyond just page loading metrics to also include metrics that deal with interactivity and responsiveness. It will include things like First input delay (FID) , Time to Interactive (TTI) . 2019 has indeed been a meaningful year for us, as we got a chance to deliver something of value to our customers, in this case, a faster experience. The impact here is very real and certainly a key differentiator in the eCommerce landscape. Looking into 2020, we have a couple of speed-related projects lined up, which may further help to improve performance. Above all, as an organization, our mindset towards performance has significantly changed. Speed has now become a foundational element in our product release cycle, following the footsteps of security, availability, and accessibility. The speed initiative was truly a cross-functional effort. People from various parts of the organization joined the initiative, and I was fortunate enough to lead the team. Calling out the incredible team members, starting with Roya Foroud — Program Management; Kandakumar Doraisamy — Performance Engineering; Kalieswaran Rayar, Prakasam Kannan, Anirudh Kamalapuram, Anoop Koloth, Fnu Sreekanth, and Saravana Chilla — Speed Tooling & Infrastructure; Cari Moore, Thomas Graft, Darin Glatt, Matthew Gearhart, Viswa Vaddi, Billy Sword, Honghao Wang, Rustin Holmes, Justin Daly, and Vijay Chandrasegaran — Homepage; Jesse Block, Peter Wong, Kevin Cearns, Ashwin Ranade, Deepu Joseph, Harish Narayanappa, Travis West, Ramesh Mandapati, Prafull Jande, Priya Dhawan, Manojkumar Kannadasan, Praveen Settipalli, Raffi Tutundjian, and Yoni Medoff — Search; Tuhin Verma, Earnest McCoy, Ramesh Periyathambi, Vineet Bindal, Darrekk Hocking, Triny Francis Xavier, Jeganathan Vasudevan, Kayal Alagupackiam, Sheetal Vartak, Jonathan Calvin, Vidya Lingineni, Abdullah Rababah. and Raghuram Nimishakavi — Item; Shyamala Sriramulu, Pramod Mamidipudi, Shalini Pachineela, Abhishek Gupta, and Pham Tiffany Nguyen — Tracking & Experimentation; Jatin Gupta, Roy Tai, and Gopi Chitluri — Analytics; Sultan Abdul Kader, Ulrich Hangler, Viraj Pateliya, Nikhil Bhatnagar, and Dasa Djuric — Content management.", "date": "2020-01-23"},
{"website": "Ebay-Engineering", "title": "eBay Integrates Video into the eBay Motors App", "author": ["Geetanjali Gupta", "Gilbert Bouzeid"], "link": "https://tech.ebayinc.com/product/ebay-integrates-video-into-the-ebay-motors-app/", "abstract": "The eBay Video Platform will allow sellers to add in videos of the cars they are selling on the eBay Motors App. You only get one shot at a first impression – being able to show off your listing in an engaging video can improve buyer confidence. Today, eBay announced a new video platform called REEL, which will integrate videos into the eBay Motors App . The feature allows sellers to upload one or more videos — in addition to adding photos of the car they are listing — which improves the shopping experience. Through video, sellers will have an easier time capturing the condition and usage of the car. Building a video platform that can scale across eBay’s 1.5 billion listings will provide the following: ● High throughput with low latency for both upload and download services. ● Resilient video storage at a very low storage cost. This will require transcoding the original video into smaller but perceptually good quality videos. ● Durable and reliable storage. Any loss of data or missing chunks for a big video file can spoil the experience. ● Appropriate Service Level Agreement (SLAs) for ecommerce specific use cases of short-duration (~1 minute) videos. ● Smooth playback across multiple devices and unreliable networks. ● Efficient monitoring and visualization for a system’s state and performance. ● Visualization for business metrics. ● Simplicity and maintainability. Building on a standard stack with existing tools for deployments and configurations makes it easier for development teams and operations to monitor and evolve. REEL’s architecture is comprised of the following in order of user/system interaction: ● Video Upload pipeline ​ for ingesting large video files into the system using REEL’s resumable APIs. ● Video Processing pipeline ​ for producing smaller sized versions of the source video with similar perceived quality. ● Video Delivery pipeline ​ to enable fast streaming on players running on heterogeneous devices across the world. To start the process of uploading a video, the seller signs into the eBay Motors App and starts to create an item listing for sale. Then, they upload the video stored on their local device to the app. The app will internally use REEL APIs to upload the videos to our platform. Once the whole video is received on the server end, it is broken into smaller chunks and stored into our backend storage. This triggers an async processing pipeline, where the uploaded video is further transcoded into a smaller resolution but high perceptual quality. Transcoded bytes are statically stored and retrieved during the video’s playback using ​ DASH protocol. REEL relies on multiple levels of caching for faster delivery. HTTP servers are public-facing services that are responsible for receiving ​video uploads through resumable APIs​. If the video fails to upload due to network fault, resumable protocol lets sellers resume an incomplete upload at a future time, a feature that can be useful for large files. This can save sellers extra bandwidth and time to re-upload larger files from scratch again. Service application is an orchestrator between storage and metadata services, and serves as a video “upload complete” event producer for the async transcoding of video. We currently use Kafka​ as our message bus, which carries Video IDs for videos once they are persisted in the backend. Upload APIs require ​OAuth​tokens for ​access ​and are not public yet. They are only available for consumption by eBay applications. The REEL API currently accepts seller-provided metadata, like tags and a short description, along with a link to previously uploaded video thumbnail images. Current upload SLAs are aligned with search indexing. Uploaded bytes are stored in Binary Large Object (​BLOB) Storage​ which was built in-house to run on low cost commodity hardware and provide random reads at very low latencies, with an extremely high durability (~10 -​ 24​ probability for losing an object). It is a generic object store and is used for use cases other than images and videos as well. The key drivers for choosing it were​resilience ​and high performance. We chunk the uploaded video and store the chunked bytes as individual objects in storage with corresponding ​metadata​ in a database ​store​. Metadata includes a video’s original dimensions, size, bitrate, durations and sorts, user-provided details and peripheral things like storage object IDs. This enables metadata to be independently retrieved by eBay’s internal applications or be archived for analytics purposes. Currently, we use a homegrown, ​fault tolerant, geo-distributed datastore​. All operations on this datastore are fronted by an Entity service layer. Before reaching backend servers, upload requests are proxied through eBay’s ​Point of Presence​(PoPs). We leverage PoPs for eBay’s own ​UFES ​ (Unified Front End Services) deployments as software edge proxies between a user and our origin. This proxy connects clients to eBay’s network, bringing a customer’s request on eBay’s internet backbone with increased bandwidth and primed connections, resulting in lower latencies. It also helps with SSL terminations and other TCP tunings to achieve maximum performance. Video files are big, and they can require a lot of space for storage and bandwidth to download or stream. Files need to be compressed while maintaining a high quality and keeping the size as small as possible. Transcoding is the process to compress the original source file. We can achieve different levels of compression by altering FPS, video dimension, aspect ratio and audio/video bitrates. ​Videos are transcoded​ ​with appropriate codec to compress the original uploaded file for best perceived quality. ​Reducing the size of individual video files​significantly lowers costs of distribution​. ​The transcoding compute engine​ consumes a previous referred event (containing video ID) from Kafka, pushed by the upload pipeline once the upload is complete and starts the transcoding​ ​pipeline. Transcoding is designed around DASH protocol. The Adaptive Bitrate Streams requires a video to be divided into chunks, ideally between 1-10 seconds, and the client device can pick the chunk which best fits the device environment. This is important if the network degrades, for instance. Enabling ABR requires an encoder, which encodes a single video source into multiple bitrate adaptations as required per output resolution. Our individual target chunks are three seconds long. Each chunk is stored as an individual object in our backend storage and can be referenced as a separate entity in the metadata store. Video transcoding is a highly computationally intensive task and so we use SKUs with many CPU cores to perform transcoding. REEL has a workflow to orchestrate video processing work, storage and metadata persistence — this way, it can parallelize the expensive workload of video transcoding. This workflow is managed in a ​Kubernetes cluster​. With the myriad devices and camera settings, we standardized on our acceptable inputs and outputs for our initial use cases. The table below provides a summary: Typically, a player on a device would be playing a video associated with an item. A video is divided and stored as logical chunks as we saw in the video processing pipeline. When the backend HTTP server receives a request for a video, it fetches all the metadata required for serving the requested portion of the video. Since storage IDs are part of metadata, the server can fetch the required video files from storage and statically serve them. For delivery we make use of ​ Adaptive Bitrate Streaming ​ (ABR). ABR is designed to deliver video and audio streams efficiently over large distributed HTTP networks. REEL delivers playbacks using ​ Dynamic Adaptive Streaming over HTTP ​(DASH) streams and supports MPEG-DASH​ and ​ HTTP Live Streaming ​ with ​fMP4​ segments. Below is a brief overview on how ABR and DASH work. The client streaming videos has a pretty good idea around resources — such as bandwidth, CPU and screen size — available on the device and what the device is capable of. Clients supporting DASH can use this information to control what best resolution or bitrate it can currently support for a smooth playback with minimal stalls or re-buffering. It can switch between various bitrates on a per segment basis. As the above diagram explains, based on changing network bandwidth conditions on the device’s end, it can request a bigger or smaller resolution segment for the next fetch. All the critical information needed to switch between different encodings is stored in an ​ Media Presentation Description​ (MPD) file or ​HLS manifest files ​. To move videos closer to the user, REEL leverages two layers of caching. It tremendously helps with customer experience by bringing down the latencies from a few seconds to a few milliseconds by reducing repeated requests to origin. Manifest files (MPD/HLS) and the video segments requested are cached on their way back to the user. Third-party ​ Content Delivery Networks (CDNs)​ serve as the first layer closest to the user. At the CDN layer, we have enabled Tiered Distribution, which sends the cache misses from edge servers to smaller sets of larger-capacity, second-tier servers. This greatly increases cache hit percentage on CDN. eBay has several ​PoPs across the globe for edge computing. REEL leverages some of these PoPs to serve as a second layer of caching and the PoPs host Apache Traffic Server ​ (ATS). We provide a large amount of dedicated storage to ATS to reduce chances of cache eviction before object TTLs. This helps with the long tail cache misses from the CDNs. We leverage Unified Front End Services (UFES) deployments as software edge proxies between CDN and our origin caches as well. As you can see, there are lots of moving pieces. A robust logging and monitoring system is required for the visibility across the systems. We use eBay’s standard observability platform called Unified Monitoring Platform (UMP), which enables ​logging, monitoring and graphing capabilities​. Since we are chunking/segmenting a video at several places and there are multiple components involved, distributed tracing ​​ is necessary for tracing a request end-to-end. UMP provides ​ Prometheus ​ support for​ metrics and alerting​. Currently, we push metrics on latencies, counts and failures on each component along with system and custom metrics per component. This helps achieve the visibility required to understand the current utilization of the system and catch the issues timely. A sample dashboard for some of our video upload metrics is below: In this blog, we went deeper into the architecture of the video platform by detailing how all of the components in the platform are loosely coupled and perform a specific business capability. With our low latency storage, big compute engines, adaptive bitrate streaming and caching, we can lower our SLAs and provide smooth playbacks. As more diverse use cases are onboarded to REEL, our scale will grow even further and will require a peripheral ecosystem to support growing needs. We want to move more toward making our video processing more efficient and extending our infrastructure to work with eBay’s Artificial Intelligence and Machine Learning platform.", "date": "2020-05-20"},
{"website": "Ebay-Engineering", "title": "Streamlining Language Technology from Idea to Deployment", "author": ["Daniel Stein", "Gregor Leusch", "Praseeda Sathaye", "Selcuk Kopru"], "link": "https://tech.ebayinc.com/engineering/streamlining-language-technology-from-idea-to-deployment/", "abstract": "In recent eBay Tech Blog articles, we presented the Unified AI platform called Krylov and our pythonic tool to interact with the platform, PyKrylov. In this article, we introduce our Natural Language Processing framework built on top of the AI platform. The birth of Natural Language Processing (NLP) is often accredited to Alan Turing’s famous test: can a human distinguish a human conversation from a computer conversation? Seven decades later, the NLP field continues to see vast progress. Attempts to analyze, structure and generate human languages have been tackled by rule-based, statistical and, most recently, neural-net based approaches. eBay is unique with its unmatched inventory in many languages and categories, as well as user-generated input and queries. For a global ecommerce company, NLP continues to be at the core of our business such as: Language representation models that improve search relevance and ranking Machine translation that connects buyers and sellers, even if they speak different languages Named entity recognition (NER) that extracts product aspects and brand names in a query Spelling correction that guides our customers to their intended items Within both eBay and NLP research communities, these tasks have been approached in several different programming languages and toolkits. Of note, Python-based toolkits having found most traction in the last couple of years. To streamline and facilitate NLP usage at eBay, we built an internal framework called PyNLP. It grafts into the AI platform Krylov (see Figure 1). Figure 1: Simple layered representation of PyNLP and the “Krylov” AI platform components they interact with. Several existing pain points led to these efforts, which we group into Exploration, Customization and Deployment. Out-of-the-box NLP approaches almost never work when applied to ecommerce data. When looking at inventory items such as, Hello Kitty Cheer Leader Plush Doll TY 6” !!! PINK & WHITE, it seems clear that a standard parts-of-speech analyzer optimized on belles-lettres will not detect the brand “Hello Kitty.” Instead, it may assume a greeting interjection and struggle to detect that 6” is the product size. PyNLP combines models trained on e-commerce aware data and implementations that make best use of eBay’s infrastructure and rich data to handle above input. Another exploration hurdle is discovering and utilizing NLP models trained by other teams across eBay. PyNLP aims to serve as the single source of truth for NLP models. Onboarding new models and accessing existing models are streamlined via a pythonic interface which are designed to work smoothly on top of the existing eBay AI infrastructure. Standardized interfaces allow for easy and intuitive access for exploration as well as for batch processing. This ensures that our users only need to think through more details when they actively want to switch the implementations themselves. The sheer number of publicly available NLP solutions is already overwhelming. Each toolkit typically comes with its own software environment requirements and often, also includes sparse documentation. While this is true for many software products out there, it’s especially true in machine learning. This is because open-source software often stems from a research perspective and the code can accrue debt. If a user wants to train on a downstream task, it is easy to pass the training data through all available models and directly compare the performance to find the best matching solution. For the models itself, PyNLP shows how to customize many of them via QuickStart training and fine-tuning recipes on Jupyter notebooks. The requirements for each model are encapsulated and maintained in docker build files. New language modelling paradigms, such as neural embeddings (e.g., BERT [1] ), can take several days to retrain with suitable data on a significantly double-digit GPU rack so they should be handled with prudence. It is crucial that these efforts, already during development state, can be shared across all teams rapidly. PyNLP makes onboarding easy for researchers to explore and for model developers to share by providing templates and libraries which help connect to PyNLP’s core. With a vast collection of best practices and helper tools, we eliminate roadblocks from a PyNLP microservice prototype towards a production-ready service in a Kubernetes environment. Below, we explore how we tackle these challenges by laying out three use cases: exploration of existing solutions, customizing specific models, and how to converge prototypes into deployed services. This use case assumes that a tech-savvy applied researcher wants to explore existing solutions for a specific task, -here- a Named Entity Recognition (NER) service. The four lines above serve a powerful neural net trained on eBay-specific data. Under the hood, registry connects to Krylov Core (cf. Figure 1) to access the model management system. A global registry endpoint can tell an eBay researcher which models we already onboarded and which docker image has the matching implementation details to serve the model data. load_model will download the image from the internal docker container registry, mount the model data into this docker and start a microservice. NER is a stub that connects to this microservice via REST, providing a pythonic way of model interaction. If the model is already provided as a central microservice, PyNLP can also directly connect to it without having to host the microservice itself. However, we need to ensure that older model versions are still runnable and reproducible in case a downstream application needs this output to run properly. All interfaces and result types are predefined by PyNLP. All NER microservices serve the same interface and with a simple Python for loop, you can access process specific test data through them. Since they are started via docker, the underlying implementation might use Python, Java, C++ or other arbitrary programming languages. The models paradigms might also range from simple rule-based solutions to modern neural paradigm. Comparing NLP task performance across updated model data, different implementations or even paradigms is made very easy through this process. For our example, the deep neural net model can finally tell us that Hello Kitty is a brand, Plush is a material, Doll is a type, 6” is a measurement and that PINK and WHITE are colors. Assume that through exploration, a specific model caught the eye of an interested researcher. For most of the applied researcher tasks, a first assessment of the baseline performance is the beginning in a model development lifecycle. The next use case of PyNLP is to provide easy access to the model implementation itself and allow for customization of the algorithm or fine-tuning of the model. For typical recipes such as fine-tuning a BERT embedding, you can start the images in Jupyter notebook [2] mode and work your way through provided notebooks. Since these notebooks can be started in the AI platform Krylov, researchers also have direct access to the Hadoop clusters and other data access points. This allows them to request heavier hardware support if needed. Figure 2: Sphinx-generated documentation page for PyNLP. The Jupyter notebooks depicted are executable. A central design aspect of PyNLP is its living documentation. Our code does not speak for itself – it speaks with you. Through docker bases, interface templates and framework libraries for our microservices, we provide code slots for the concrete model implementation parts. This means that even when you are looking at a first prototype implementation that was just copied and pasted, it most likely already comes with a battery of onboarding functionality. This includes support for inline documentation, Sphinx [3] integration, a SwaggerUI [4] with examples to showcase their APIs and support for Prometheus [5] metrics. Figure 3: SwaggerUI interface for an in-house BERT model. It interactively connects with the microservice so that you can run quick examples to test the API. Assuming that a strong model has been identified, customized, and optimized on a specific task, ensuring business impact in a commercial environment is the next step in a lifecycle. Of course, security consideration and service-level agreement need to be tackled individually. We can now run microservices in cloud-native environment as a containerized and dynamically orchestrated platform, as well as monitor logs and metrics. For the deployment into Tess, our cloud infrastructure based on Kubernetes [6] , PyNLP facilitates as much as possible. Sherlock.io is our event processing system that logs both the standard out/err of the microservices as well as the aforementioned Prometheus metrics. To simplify the above, we use Helm Chart [7] , a template-based approach, to deploy PyNLP microservices into Tess and manage its lifecycle. Using this, users will be able to deploy the service in just one line of command (illustrated below), thereby shielding away the complexity of Kubernetes resource creation. Figure 4: Grafana board reporting on metrics such as average response time. Individual metrics can be emitted by the microservice implementation, if needed. PyNLP is built to significantly accelerate the NLP model development life cycle by reducing any obstacles in terms of software-specific requirements, data exchange, and interaction with the eBay infrastructure. We foster comparability and reproducibility through standardized interfaces and make use of living documentation to share (language-specific) considerations and best practices. While heaving NLP microservices into production can never be an automated process for security considerations, we facilitate the process as much as possible. [1] https://github.com/google-research/bert [2] https://jupyter.org/ [3] https://www.sphinx-doc.org/en/master/ [4] https://swagger.io/tools/swagger-ui/ [5] https://prometheus.io/ [6] https://tech.ebayinc.com/engineering/scalability-tuning-on-tess-io-cluster/ [7] https://github.com/helm/charts", "date": "2020-05-26"}
]